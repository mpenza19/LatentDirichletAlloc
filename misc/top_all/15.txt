{u'doi': u'10.1109/INFOCOM.2018.8486026', u'author': u'C. Chen and W. Wang and B. Li', u'title': u'Performance-Aware Fair Scheduling: Exploiting Demand Elasticity of Data Analytics Jobs', 'ENTRYTYPE': u'inproceedings', u'abstract': u"Efficient resource management is of paramount importance in today's production clusters. In this paper, we identify the demand elasticity of data-parallel jobs. Demand elasticity allows jobs to run with a significantly less amount of resources than they ideally need, at the expense of only a modest performance penalty. Our EC2 experiment using popular Spark benchmark suites confirms that running a job using 50% of demanded slots is sufficient to achieve at least 75% of the ideal performance. We show that such an elasticity is an intrinsic property of data-parallel jobs and can be exploited to speed up average job completion. In this regard, we propose Performance-Aware Fair (PAF) scheduler to identify the demand elasticity and use it to improve the average job performance, while still attaining near-optimal isolation guarantee close to fair sharing. PAF starts with a fair allocation and iteratively adjusts it by transferring resources from one job to another, improving the performance of resource-taker without penalizing resource-giver by a noticeable amount. We implemented PAF in Spark and evaluated its effectiveness through both EC2 experiments and large-scale simulations. Evaluation results show that compared with fair allocation, PAF improves the average job performance by 13%, while penalizing resource-givers by no more than 1%.", u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'504-512', u'year': u'2018', u'keywords': u'cloud computing;cluster computing;data analysis;parallel processing;resource allocation;scheduling;Performance-Aware Fair scheduling;demand elasticity;data analytics jobs;data-parallel jobs;Performance-Aware Fair scheduler;PAF;fair allocation;resource management;Spark benchmark suites;Task analysis;Elasticity;Resource management;Sparks;Data analysis;Runtime;Parallel processing', 'ID': u'8486026', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8486026', u'author': u'C. Chen and W. Wang and B. Li', u'title': u'Performance-Aware Fair Scheduling: Exploiting Demand Elasticity of Data Analytics Jobs', 'ENTRYTYPE': u'inproceedings', u'abstract': u"Efficient resource management is of paramount importance in today's production clusters. In this paper, we identify the demand elasticity of data-parallel jobs. Demand elasticity allows jobs to run with a significantly less amount of resources than they ideally need, at the expense of only a modest performance penalty. Our EC2 experiment using popular Spark benchmark suites confirms that running a job using 50% of demanded slots is sufficient to achieve at least 75% of the ideal performance. We show that such an elasticity is an intrinsic property of data-parallel jobs and can be exploited to speed up average job completion. In this regard, we propose Performance-Aware Fair (PAF) scheduler to identify the demand elasticity and use it to improve the average job performance, while still attaining near-optimal isolation guarantee close to fair sharing. PAF starts with a fair allocation and iteratively adjusts it by transferring resources from one job to another, improving the performance of resource-taker without penalizing resource-giver by a noticeable amount. We implemented PAF in Spark and evaluated its effectiveness through both EC2 experiments and large-scale simulations. Evaluation results show that compared with fair allocation, PAF improves the average job performance by 13%, while penalizing resource-givers by no more than 1%.", u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'504-512', u'year': u'2018', u'keywords': u'cloud computing;cluster computing;data analysis;parallel processing;resource allocation;scheduling;Performance-Aware Fair scheduling;demand elasticity;data analytics jobs;data-parallel jobs;Performance-Aware Fair scheduler;PAF;fair allocation;resource management;Spark benchmark suites;Task analysis;Elasticity;Resource management;Sparks;Data analysis;Runtime;Parallel processing', 'ID': u'8486026', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2017.8057153', u'author': u'J. Fan and C. Guan and Y. Zhao and C. Qiao', u'title': u'Availability-aware mapping of service function chains', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Network Function Virtualization (NFV) is a promising technique to greatly improve the effectiveness and flexibility of network services through a process named Service Function Chain (SFC) mapping, with which different network services are deployed over virtualized and shared platforms in data centers. However, such an evolution towards software-defined network functions introduces new challenges to network services which require high availability. One effective way of protecting the network services is to use sufficient redundancy. By doing so, however, the efficiency of physical resources may be greatly decreased. To address such an issue, this paper defines an optimal availability-aware SFC mapping problem and presents a novel online algorithm that can minimize the physical resources consumption while guaranteeing the required high availability within a polynomial time. Simulation results show that our proposed algorithm can significantly improve SFC mapping request acceptance ratio and reduce resource consumption.', u'issn': '', u'number': '', u'month': u'May', u'volume': '', u'pages': u'1-9', u'year': u'2017', u'keywords': u'computer centres;computer network management;virtualisation;availability-aware SFC mapping problem;data centers;resource consumption;Service Function Chain mapping;Network Function Virtualization;service function chains;availability-aware mapping;SFC mapping request acceptance ratio;software-defined network functions;virtualized shared platforms;Redundancy;Delays;Computational modeling;Bandwidth;Conferences;Approximation algorithms', 'ID': u'8057153', u'booktitle': u'IEEE INFOCOM 2017 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2017.8057153', u'author': u'J. Fan and C. Guan and Y. Zhao and C. Qiao', u'title': u'Availability-aware mapping of service function chains', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Network Function Virtualization (NFV) is a promising technique to greatly improve the effectiveness and flexibility of network services through a process named Service Function Chain (SFC) mapping, with which different network services are deployed over virtualized and shared platforms in data centers. However, such an evolution towards software-defined network functions introduces new challenges to network services which require high availability. One effective way of protecting the network services is to use sufficient redundancy. By doing so, however, the efficiency of physical resources may be greatly decreased. To address such an issue, this paper defines an optimal availability-aware SFC mapping problem and presents a novel online algorithm that can minimize the physical resources consumption while guaranteeing the required high availability within a polynomial time. Simulation results show that our proposed algorithm can significantly improve SFC mapping request acceptance ratio and reduce resource consumption.', u'issn': '', u'number': '', u'month': u'May', u'volume': '', u'pages': u'1-9', u'year': u'2017', u'keywords': u'computer centres;computer network management;virtualisation;availability-aware SFC mapping problem;data centers;resource consumption;Service Function Chain mapping;Network Function Virtualization;service function chains;availability-aware mapping;SFC mapping request acceptance ratio;software-defined network functions;virtualized shared platforms;Redundancy;Delays;Computational modeling;Bandwidth;Conferences;Approximation algorithms', 'ID': u'8057153', u'booktitle': u'IEEE INFOCOM 2017 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2017.8056972', u'author': u'H. Xu and G. de Veciana and W. C. Lau', u'title': u'Addressing job processing variability through redundant execution and opportunistic checkpointing: A competitive analysis', 'ENTRYTYPE': u'inproceedings', u'abstract': u'The completion times of jobs in a computing cluster may be influenced by a variety of factors including job size and machine processing variability. In this paper, we explore online resource allocation policies which combine size-dependent scheduling with redundant execution and opportunistic checkpointing to minimize the overall job flowtime. We introduce a simplified model for the job service capacity of a computing cluster while leveraging redundant execution/checkpointing. In this setting, we propose two resource allocation algorithms, SRPT+R and LAPS+R(\u03b2) subject to checkpointing overhead not exceeding the number of jobs which are processed. We provide new theoretical performance bounds for these algorithms: SRPT+R is shown to be O(1/\u03f5) competitive under (1 + \u03f5)-speed resource augmentation, while LAPS+R(\u03b2) is shown to be O(1/\u03b2\u03f5) competitive under (2+ 2\u03b2 + 2\u03f5)-speed resource augmentation.', u'issn': '', u'number': '', u'month': u'May', u'volume': '', u'pages': u'1-9', u'year': u'2017', u'keywords': u'checkpointing;job shop scheduling;resource allocation;scheduling;job processing variability;opportunistic checkpointing;computing cluster;job size;machine processing variability;online resource allocation policies;combine size-dependent scheduling;job flowtime;job service capacity;resource allocation algorithms;checkpointing overhead;resource augmentation;redundant execution;SRPT+R algorithm;LAPS+R(\u03b2) algorithm;Redundancy;Algorithm design and analysis;Multitasking;Checkpointing;Servers;Scheduling algorithms;Job Scheduling;Redundancy;Optimization;Competitive Analysis;Dual-Fitting', 'ID': u'8056972', u'booktitle': u'IEEE INFOCOM 2017 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8486246', u'author': u'C. Zeng and F. Liu and S. Chen and W. Jiang and M. Li', u'title': u'Demystifying the Performance Interference of Co-Located Virtual Network Functions', 'ENTRYTYPE': u'inproceedings', u'abstract': u"Network function virtualization (NFV) decouples network functions from the dedicated hardware and enables them running on commodity servers, facilitating widespread deployment of virtualized network functions (VNFs). Network operators tend to deploy VNFs in virtual machines (VMs) due to VM's ease of duplication and migration, which enables flexible VNF placement and scheduling. Efforts have been paid to provide efficient VNF placement approaches, aiming at minimizing the resource cost of VNF deployment and reducing the latency of service chain. However, existing placement approaches may result in hardware resource competition of co-located VNFs, leading to performance degradation. In this paper, we present a measurement study on the performance interference among different types of co-located VNFs and analyze how VNFs' competitive hardware resources and the characteristics of packet affect the performance interference. We disclose that the performance interference between co-located VNFs is ubiquitous, which causes the performance degradation, in terms of VNFs' throughput, ranging from 12.36% to 50.3%, and the competition of network I/O bandwidth plays a key role in the performance interference. Based on our measurement results, we give some advices on how to design more efficient VNF placement approaches.", u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'765-773', u'year': u'2018', u'keywords': u'computer networks;interference;virtual machines;virtualisation;performance interference demystifying;colocated VNF placement approaches;colocated virtual network functions;VM;network I-O bandwidth;hardware resource competition;scheduling;virtual machines;Interference;Servers;Bandwidth;Degradation;Hardware;Resource management;Throughput', 'ID': u'8486246', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8486246', u'author': u'C. Zeng and F. Liu and S. Chen and W. Jiang and M. Li', u'title': u'Demystifying the Performance Interference of Co-Located Virtual Network Functions', 'ENTRYTYPE': u'inproceedings', u'abstract': u"Network function virtualization (NFV) decouples network functions from the dedicated hardware and enables them running on commodity servers, facilitating widespread deployment of virtualized network functions (VNFs). Network operators tend to deploy VNFs in virtual machines (VMs) due to VM's ease of duplication and migration, which enables flexible VNF placement and scheduling. Efforts have been paid to provide efficient VNF placement approaches, aiming at minimizing the resource cost of VNF deployment and reducing the latency of service chain. However, existing placement approaches may result in hardware resource competition of co-located VNFs, leading to performance degradation. In this paper, we present a measurement study on the performance interference among different types of co-located VNFs and analyze how VNFs' competitive hardware resources and the characteristics of packet affect the performance interference. We disclose that the performance interference between co-located VNFs is ubiquitous, which causes the performance degradation, in terms of VNFs' throughput, ranging from 12.36% to 50.3%, and the competition of network I/O bandwidth plays a key role in the performance interference. Based on our measurement results, we give some advices on how to design more efficient VNF placement approaches.", u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'765-773', u'year': u'2018', u'keywords': u'computer networks;interference;virtual machines;virtualisation;performance interference demystifying;colocated VNF placement approaches;colocated virtual network functions;VM;network I-O bandwidth;hardware resource competition;scheduling;virtual machines;Interference;Servers;Bandwidth;Degradation;Hardware;Resource management;Throughput', 'ID': u'8486246', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2017.8057135', u'author': u'H. Pan and G. Xie and Z. Li and P. He and L. Mathy', u'title': u'FlowConvertor: Enabling portability of SDN applications', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Software-Defined Networking (SDN) provides network administrators opportunities to control network devices more simply and easily than in traditional networking. However, heterogeneity in switch hardware, especially in forwarding pipeline architecture, renders the task of network application developers and network administrators tedious, by hampering portability across switch models. In this paper, we propose FlowConvertor, an algorithm capable of converting rules from any forwarding pipeline to any other different forwarding pipeline, as long as both pipelines offer compatible operations. More precisely, FlowConvertor is an online algorithm that operates on flow updates issued to the origin pipeline and computes the corresponding updates for the target pipeline in real time. Performance evaluation shows that the latency introduced by FlowConvertor on the path between the SDN controller and the target switch is of the order of 1ms in most cases, and is thus acceptable for practical deployment.', u'issn': '', u'number': '', u'month': u'May', u'volume': '', u'pages': u'1-9', u'year': u'2017', u'keywords': u'computer network management;pipelines;software defined networking;forwarding pipeline architecture;switch hardware;traditional networking;network devices;network administrators opportunities;Software-Defined Networking;SDN applications;target switch;SDN controller;target pipeline;origin pipeline;flow updates;online algorithm;FlowConvertor;switch models;network application developers;time 1.0 ms;Switches;Pipelines;Metadata;Pipeline processing;Hardware;Engines;Ports (Computers)', 'ID': u'8057135', u'booktitle': u'IEEE INFOCOM 2017 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2017.8057135', u'author': u'H. Pan and G. Xie and Z. Li and P. He and L. Mathy', u'title': u'FlowConvertor: Enabling portability of SDN applications', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Software-Defined Networking (SDN) provides network administrators opportunities to control network devices more simply and easily than in traditional networking. However, heterogeneity in switch hardware, especially in forwarding pipeline architecture, renders the task of network application developers and network administrators tedious, by hampering portability across switch models. In this paper, we propose FlowConvertor, an algorithm capable of converting rules from any forwarding pipeline to any other different forwarding pipeline, as long as both pipelines offer compatible operations. More precisely, FlowConvertor is an online algorithm that operates on flow updates issued to the origin pipeline and computes the corresponding updates for the target pipeline in real time. Performance evaluation shows that the latency introduced by FlowConvertor on the path between the SDN controller and the target switch is of the order of 1ms in most cases, and is thus acceptable for practical deployment.', u'issn': '', u'number': '', u'month': u'May', u'volume': '', u'pages': u'1-9', u'year': u'2017', u'keywords': u'computer network management;pipelines;software defined networking;forwarding pipeline architecture;switch hardware;traditional networking;network devices;network administrators opportunities;Software-Defined Networking;SDN applications;target switch;SDN controller;target pipeline;origin pipeline;flow updates;online algorithm;FlowConvertor;switch models;network application developers;time 1.0 ms;Switches;Pipelines;Metadata;Pipeline processing;Hardware;Engines;Ports (Computers)', 'ID': u'8057135', u'booktitle': u'IEEE INFOCOM 2017 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8486400', u'author': u'Y. Xu and W. Chen and S. Wang and X. Zhou and C. Jiang', u'title': u'Improving Utilization and Parallelism of Hadoop Cluster by Elastic Containers', 'ENTRYTYPE': u'inproceedings', u'abstract': u"Modern datacenter schedulers apply a static policy to partition resources among different tasks. The amount of allocated resource won't get changed during a task's lifetime. However, we found that resource usage during a task's runtime demonstrates high dynamics and it only reaches full usage at few moments. Therefore, the static allocation policy doesn't exploit the dynamic nature of resource usage, leading to low system resource utilization. To address this hard problem, a recently proposed task-consolidation approach packs as many tasks as possible on the same node based on real-time resource demands. However, this approach may cause resource over-allocation and harm application performance. In this paper, we propose and develop ECS, an elastic container based scheduler that leverages resource usage variation within the task lifetime to exploit the potential utilization and parallelism. The key idea is to proactively select and shift tasks backward so that the inherent paralleled tasks can be identified without over-allocation. We formulate the scheduling scheme as an online optimization problem and solves it using a resource leveling algorithm. We have implemented ECS in Apache Yarn and performed evaluations with various MapReduce benchmarks in a cluster. Experimental results show that ECS can efficiently utilize resource and achieves up to 29% reduction on average job completion time while increasing CPU utilization by 25%, compared to stock Yarn.", u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'180-188', u'year': u'2018', u'keywords': u'cloud computing;computer centres;data handling;optimisation;parallel processing;resource allocation;scheduling;elastic container;static policy;partition resources;static allocation policy;real-time resource demands;harm application performance;ECS;resource leveling algorithm;CPU utilization;resource allocation;resource utilization;hadoop cluster parallelism;datacenter schedulers;datacenter schedulers;task-consolidation;Task analysis;Containers;Dynamic scheduling;Resource management;Benchmark testing;Parallel processing;Runtime', 'ID': u'8486400', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
