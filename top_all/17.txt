{u'doi': u'10.1109/INFOCOM.2017.8057101', u'author': u'H. Chen and F. Li and Y. Wang', u'title': u'EchoTrack: Acoustic device-free hand tracking on smart phones', 'ENTRYTYPE': u'inproceedings', u'abstract': u"This paper explores the limits of acoustic ranging on smart phone in the scenario of device-free hand tracking. Tracking the hand is challenging since it requires continuously locating the moving hand in the air with fine resolution. Existing work on hand tracking relies on special hardware or requires users hold the mobile device. This paper presents EchoTrack, which continuously locates the hand by leveraging mobile audio hardware advances without special infrastructure supported. EchoTrack measures the distance from the hand to the speaker array embedded in smart phone via the chirp's Time of Flight (TOF). The speaker array and hand yield a unique triangle. The hand can be located with this triangular geometry. The trajectory accuracy can be improved with the method of Doppler shift compensation and trajectory correction (i.e., roughness penalty smoothing method). We implement a prototype on smart phone and the evaluation shows that EchoTrack can achieve tracking accuracy within about three centimeters of 76% and two centimeters of 48%.", u'issn': '', u'number': '', u'month': u'May', u'volume': '', u'pages': u'1-9', u'year': u'2017', u'keywords': u'Doppler shift;gesture recognition;human computer interaction;smart phones;tracking accuracy;EchoTrack;Acoustic device-free hand tracking;smart phone;acoustic ranging;moving hand;mobile device;speaker array;mobile audio hardware;Doppler shift compensation;Chirp;Microphones;Smart phones;Distance measurement;Trajectory;Hardware;Doppler shift', 'ID': u'8057101', u'booktitle': u'IEEE INFOCOM 2017 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2017.8057101', u'author': u'H. Chen and F. Li and Y. Wang', u'title': u'EchoTrack: Acoustic device-free hand tracking on smart phones', 'ENTRYTYPE': u'inproceedings', u'abstract': u"This paper explores the limits of acoustic ranging on smart phone in the scenario of device-free hand tracking. Tracking the hand is challenging since it requires continuously locating the moving hand in the air with fine resolution. Existing work on hand tracking relies on special hardware or requires users hold the mobile device. This paper presents EchoTrack, which continuously locates the hand by leveraging mobile audio hardware advances without special infrastructure supported. EchoTrack measures the distance from the hand to the speaker array embedded in smart phone via the chirp's Time of Flight (TOF). The speaker array and hand yield a unique triangle. The hand can be located with this triangular geometry. The trajectory accuracy can be improved with the method of Doppler shift compensation and trajectory correction (i.e., roughness penalty smoothing method). We implement a prototype on smart phone and the evaluation shows that EchoTrack can achieve tracking accuracy within about three centimeters of 76% and two centimeters of 48%.", u'issn': '', u'number': '', u'month': u'May', u'volume': '', u'pages': u'1-9', u'year': u'2017', u'keywords': u'Doppler shift;gesture recognition;human computer interaction;smart phones;tracking accuracy;EchoTrack;Acoustic device-free hand tracking;smart phone;acoustic ranging;moving hand;mobile device;speaker array;mobile audio hardware;Doppler shift compensation;Chirp;Microphones;Smart phones;Distance measurement;Trajectory;Hardware;Doppler shift', 'ID': u'8057101', u'booktitle': u'IEEE INFOCOM 2017 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2015.7218440', u'author': u'Q. Zhai and S. Ding and X. Li and F. Yang and J. Teng and J. Zhu and D. Xuan and Y. F. Zheng and W. Zhao', u'title': u'VM-tracking: Visual-motion sensing integration for real-time human tracking', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Human tracking in video has many practical applications such as visual guided navigation, assisted living, etc. In such applications, it is necessary to accurately track multiple humans across multiple cameras, subject to real-time constraints. Despite recent advances in visual tracking research, the tracking systems purely relying on visual information fail to meet the accuracy and real-time requirements at the same time. In this paper, we present a novel accurate and real-time human tracking system called VM-Tracking. The system aggregates the information of motion (M) sensor on human, and integrates it with visual (V) data based on physical locations. The system has two key features, i.e. location-based VM fusion and appearance-free tracking, which significantly distinguish itself from other existing human tracking systems. We have implemented the VM-Tracking system and conducted comprehensive experiments on challenging scenarios.', u'issn': u'0743-166X', u'number': '', u'month': u'April', u'volume': '', u'pages': u'711-719', u'year': u'2015', u'keywords': u'image sensors;object tracking;real-time systems;video signal processing;VM-tracking;visual motion sensing integration;real-time human tracking;visual guided navigation;assisted living;track multiple humans across multiple cameras;real-time constraints;visual tracking research;visual information;real-time requirements;real-time human tracking system;multiple cameras;Visualization;Tracking;Real-time systems;Cameras;Accuracy;Trajectory;Acceleration', 'ID': u'7218440', u'booktitle': u'2015 IEEE Conference on Computer Communications (INFOCOM)'}
{u'doi': u'10.1109/INFOCOM.2018.8485933', u'author': u'N. Xiao and P. Yang and Y. Yan and H. Zhou and X. Li', u'title': u'Motion-Fi: Recognizing and Counting Repetitive Motions with Passive Wireless Backscattering', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Recently several ground-breaking RF-based motion-recognition systems were proposed to detect and/or recognize macro/micro human movements. These systems often suffer from various interferences caused by multiple-users moving simultaneously, resulting in extremely low recognition accuracy. To tackle this challenge, we propose a novel system, called Motion-Fi, which marries battery-free wireless backscattering and device-free sensing. Motion-Fi is an accurate, interference tolerable motion-recognition system, which counts repetitive motions without using scenario-dependent templates or profiles and enables multi-users performing certain motions simultaneously because of the relatively short transmission range of backscattered signals. Although the repetitive motions are fairly well detectable through the backscattering signals in theory, in reality they get blended into various other system noises during the motion. Moreover, irregular motion patterns among users will lead to expensive computation cost for motion recognition. We build a backscattering wireless platform to validate our design in various scenarios for over 6 months when different persons, distances and orientations are incorporated. In our experiments, the periodicity in motions could be recognized without any learning or training process, and the accuracy of counting such motions can be achieved within 5% count error. With little efforts in learning the patterns, our method could achieve 93.1% motion-recognition accuracy for a variety of motions. Moreover, by leveraging the periodicity of motions, the recognition accuracy could be further improved to nearly 100% with only 3 repetitions. Our experiments also show that the motions of multiple persons separated by around 2 meters cause little accuracy reduction in the counting process.', u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'2024-2032', u'year': u'2018', u'keywords': u'backscatter;feature extraction;image motion analysis;learning (artificial intelligence);radiofrequency interference;backscattering signals;irregular motion patterns;motion recognition;backscattering wireless platform;ground-breaking RF-based motion-recognition systems;extremely low recognition accuracy;battery-free wireless backscattering;interference tolerable motion-recognition system;Backscatter;Wireless communication;Wireless sensor networks;Antennas;Wireless fidelity;Impedance;Interference', 'ID': u'8485933', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8485933', u'author': u'N. Xiao and P. Yang and Y. Yan and H. Zhou and X. Li', u'title': u'Motion-Fi: Recognizing and Counting Repetitive Motions with Passive Wireless Backscattering', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Recently several ground-breaking RF-based motion-recognition systems were proposed to detect and/or recognize macro/micro human movements. These systems often suffer from various interferences caused by multiple-users moving simultaneously, resulting in extremely low recognition accuracy. To tackle this challenge, we propose a novel system, called Motion-Fi, which marries battery-free wireless backscattering and device-free sensing. Motion-Fi is an accurate, interference tolerable motion-recognition system, which counts repetitive motions without using scenario-dependent templates or profiles and enables multi-users performing certain motions simultaneously because of the relatively short transmission range of backscattered signals. Although the repetitive motions are fairly well detectable through the backscattering signals in theory, in reality they get blended into various other system noises during the motion. Moreover, irregular motion patterns among users will lead to expensive computation cost for motion recognition. We build a backscattering wireless platform to validate our design in various scenarios for over 6 months when different persons, distances and orientations are incorporated. In our experiments, the periodicity in motions could be recognized without any learning or training process, and the accuracy of counting such motions can be achieved within 5% count error. With little efforts in learning the patterns, our method could achieve 93.1% motion-recognition accuracy for a variety of motions. Moreover, by leveraging the periodicity of motions, the recognition accuracy could be further improved to nearly 100% with only 3 repetitions. Our experiments also show that the motions of multiple persons separated by around 2 meters cause little accuracy reduction in the counting process.', u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'2024-2032', u'year': u'2018', u'keywords': u'backscatter;feature extraction;image motion analysis;learning (artificial intelligence);radiofrequency interference;backscattering signals;irregular motion patterns;motion recognition;backscattering wireless platform;ground-breaking RF-based motion-recognition systems;extremely low recognition accuracy;battery-free wireless backscattering;interference tolerable motion-recognition system;Backscatter;Wireless communication;Wireless sensor networks;Antennas;Wireless fidelity;Impedance;Interference', 'ID': u'8485933', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2017.8057185', u'author': u'X. Chen and C. Ma and M. Allegue and X. Liu', u'title': u'Taming the inconsistency of Wi-Fi fingerprints for device-free passive indoor localization', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Device-free Passive (DfP) indoor localization releases the users from the burden of wearing sensors or carrying smartphones. Instead of locating devices, DfP technology directly locates human bodies. This promising technology upgrades and even redefines many services, such as intruder alarm, fire rescue, fall detection, baby monitoring, etc. Using Wi-Fi based fingerprints, DfP approaches can achieve a nearly perfect accuracy with a resolution less than one meter. However, Wi-Fi localization profiles may easily drift with a minor environment change, resulting in an inconsistency between fingerprints and new profiles. This inconsistency issue could lead to large errors, and may quickly ruin the whole system. To address this issue, we propose a approach named AutoFi to automatically calibrate the localization profiles in an unsupervised manner. AutoFi embraces a new technique that online estimates and cancels profile contaminants introduced by environment changes. It applies an autoencoder to preserve critical features of fingerprints, and reproduces them later in new localization profiles. Experiment results demonstrate that AutoFi indeed rescues the Wi-Fi fingerprints from variations in the surrounding. The localization accuracy is improved from 18.8% (before auto-calibration) to 84.9% (after auto-calibration).', u'issn': '', u'number': '', u'month': u'May', u'volume': '', u'pages': u'1-9', u'year': u'2017', u'keywords': u'fingerprint identification;indoor radio;telecommunication security;wireless LAN;profile contaminants;AutoFi;inconsistency issue;environment change;Wi-Fi localization profiles;Wi-Fi based fingerprints;fire rescue;intruder alarm;DfP technology;device-free passive indoor localization;Wi-Fi fingerprints;localization accuracy;baby monitoring;direct human bodies localization;Wireless fidelity;Antenna arrays;Databases;Wireless communication;Signal resolution;OFDM;Array signal processing', 'ID': u'8057185', u'booktitle': u'IEEE INFOCOM 2017 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2017.8057185', u'author': u'X. Chen and C. Ma and M. Allegue and X. Liu', u'title': u'Taming the inconsistency of Wi-Fi fingerprints for device-free passive indoor localization', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Device-free Passive (DfP) indoor localization releases the users from the burden of wearing sensors or carrying smartphones. Instead of locating devices, DfP technology directly locates human bodies. This promising technology upgrades and even redefines many services, such as intruder alarm, fire rescue, fall detection, baby monitoring, etc. Using Wi-Fi based fingerprints, DfP approaches can achieve a nearly perfect accuracy with a resolution less than one meter. However, Wi-Fi localization profiles may easily drift with a minor environment change, resulting in an inconsistency between fingerprints and new profiles. This inconsistency issue could lead to large errors, and may quickly ruin the whole system. To address this issue, we propose a approach named AutoFi to automatically calibrate the localization profiles in an unsupervised manner. AutoFi embraces a new technique that online estimates and cancels profile contaminants introduced by environment changes. It applies an autoencoder to preserve critical features of fingerprints, and reproduces them later in new localization profiles. Experiment results demonstrate that AutoFi indeed rescues the Wi-Fi fingerprints from variations in the surrounding. The localization accuracy is improved from 18.8% (before auto-calibration) to 84.9% (after auto-calibration).', u'issn': '', u'number': '', u'month': u'May', u'volume': '', u'pages': u'1-9', u'year': u'2017', u'keywords': u'fingerprint identification;indoor radio;telecommunication security;wireless LAN;profile contaminants;AutoFi;inconsistency issue;environment change;Wi-Fi localization profiles;Wi-Fi based fingerprints;fire rescue;intruder alarm;DfP technology;device-free passive indoor localization;Wi-Fi fingerprints;localization accuracy;baby monitoring;direct human bodies localization;Wireless fidelity;Antenna arrays;Databases;Wireless communication;Signal resolution;OFDM;Array signal processing', 'ID': u'8057185', u'booktitle': u'IEEE INFOCOM 2017 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2016.7524398', u'author': u'Z. Jiang and J. Han and C. Qian and W. Xi and K. Zhao and H. Ding and S. Tang and J. Zhao and P. Yang', u'title': u'VADS: Visual attention detection with a smartphone', 'ENTRYTYPE': u'inproceedings', u'abstract': u"Identifying the object that attracts human visual attention is an essential function for automatic services in smart environments. However, existing solutions can compute the gaze direction without providing the distance to the target. In addition, most of them rely on special devices or infrastructure support. This paper explores the possibility of using a smartphone to detect the visual attention of a user. By applying the proposed VADS system, acquiring the location of the intended object only requires one simple action: gazing at the intended object and holding up the smartphone so that the object as well as user's face can be simultaneously captured by the front and rear cameras. We extend the current advances of computer vision to develop efficient algorithms to obtain the distance between the camera and user, the user's gaze direction, and the object's direction from camera. The object's location can then be computed by solving a trigonometric problem. VADS has been prototyped on commercial off-the-shelf (COTS) devices. Extensive evaluation results show that VADS achieves low error (about 1.5\xb0 in angle and 0.15m in distance for objects within 12m) as well as short latency. We believe that VADS enables a large variety of applications in smart environments.", u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'1-9', u'year': u'2016', u'keywords': u'computer vision;face recognition;gaze tracking;image capture;object detection;smart phones;VADS;human visual attention detection;automatic services;smart environments;gaze direction;smartphone;user face capture;front cameras;rear cameras;computer vision;user gaze direction;object direction;object location;trigonometric problem;Face;Visualization;Cameras;Computational modeling;Solid modeling;Three-dimensional displays;Estimation', 'ID': u'7524398', u'booktitle': u'IEEE INFOCOM 2016 - The 35th Annual IEEE International Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2014.6847973', u'author': u'Y. Zheng and M. Li', u'title': u'Read bulk data from computational RFIDs', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Without the need of local energy supply, computational RFID (CRFID) sensors are emerging as important platforms enabling a variety of sensing and computing applications. Nevertheless, the data throughput of CRFIDs is very low. This paper aims at efficiently transferring bulk data from CRFIDs to commodity RFID readers. We first investigate the problem of low data throughput of CRFIDs. We then propose several simple yet effective techniques to allow CRFIDs to meet stringent timing requirement of commodity RFID readers and achieve efficient data transfer. We implement a prototype system based on the WISP CRFIDs and commercial off-the-self RFID reader. The experiment results show that our approach provides better compatibility with EPCglobal C1G2 compliant RFID devices and works perfectly with the commodity RFID readers.', u'issn': u'0743-166X', u'number': '', u'month': u'April', u'volume': '', u'pages': u'495-503', u'year': u'2014', u'keywords': u'radiofrequency identification;sensors;read bulk data;computational RFID sensors;CRFID sensors;CRFID data throughput;commodity RFID readers;data transfer efficiency;WISP CRFID;commercial off-the-self RFID reader;EPCglobal C1G2 compliant RFID devices;Radiofrequency identification;Data transfer;Standards;Timing;Microcontrollers;Computers', 'ID': u'6847973', u'booktitle': u'IEEE INFOCOM 2014 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8486006', u'author': u'T. Zhao and J. Liu and Y. Wang and H. Liu and Y. Chen', u'title': u'PPG-based Finger-level Gesture Recognition Leveraging Wearables', 'ENTRYTYPE': u'inproceedings', u'abstract': u"This paper subverts the traditional understanding of Photoplethysmography (PPG) and opens up a new direction of the utility of PPG in commodity wearable devices, especially in the domain of human computer interaction of fine-grained gesture recognition. We demonstrate that it is possible to leverage the widely deployed PPG sensors in wrist-worn wearable devices to enable finger-level gesture recognition, which could facilitate many emerging human-computer interactions (e.g., sign-language interpretation and virtual reality). While prior solutions in gesture recognition require dedicated devices (e.g., video cameras or IR sensors) or leverage various signals in the environments (e.g., sound, RF or ambient light), this paper introduces the first PPG-based gesture recognition system that can differentiate fine-grained hand gestures at finger level using commodity wearables. Our innovative system harnesses the unique blood flow changes in a user's wrist area to distinguish the user's finger and hand movements. The insight is that hand gestures involve a series of muscle and tendon movements that compress the arterial geometry with different degrees, resulting in significant motion artifacts to the blood flow with different intensity and time duration. By leveraging the unique characteristics of the motion artifacts to PPG, our system can accurately extract the gesture-related signals from the significant background noise (i.e., pulses), and identify different minute finger-level gestures. Extensive experiments are conducted with over 3600 gestures collected from 10 adults. Our prototype study using two commodity PPG sensors can differentiate nine finger-level gestures from American Sign Language with an average recognition accuracy over 88%, suggesting that our PPG-based finger-level gesture recognition system is promising to be one of the most critical components in sign language translation using wearables.", u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'1457-1465', u'year': u'2018', u'keywords': u'blood vessels;gesture recognition;human computer interaction;image motion analysis;medical image processing;photoplethysmography;sensors;commodity wearable devices;human-computer interactions;blood flow;finger-level gesture recognition leveraging wearables;minute finger-level gestures;arterial geometry;motion artifacts;American sign language;PPG-based finger-level gesture recognition system;commodity PPG sensors;gesture-related signals;fine-grained hand gestures;wrist-worn wearable devices;fine-grained gesture recognition;human computer interaction;Gesture recognition;Wearable sensors;Assistive technology;Wrist;Blood;Biomedical monitoring', 'ID': u'8486006', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
