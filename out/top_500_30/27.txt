{u'doi': u'10.1109/INFOCOM.2016.7524400', u'author': u'Y. Yin and Q. Li and L. Xie and S. Yi and E. Novak and S. Lu', u'title': u'CamK: A camera-based keyboard for small mobile devices', 'ENTRYTYPE': u'inproceedings', u'abstract': u"Due to the smaller size of mobile devices, on-screen keyboards become inefficient for text entry. In this paper, we present CamK, a camera-based text-entry method, which uses an arbitrary panel (e.g., a piece of paper) with a keyboard layout to input text into small devices. CamK captures the images during the typing process and uses the image processing technique to recognize the typing behavior. The principle of CamK is to extract the keys, track the user's fingertips, detect and localize the keystroke. To achieve high accuracy of keystroke localization and low false positive rate of keystroke detection, CamK introduces the initial training and online calibration. Additionally, CamK optimizes computation-intensive modules to reduce the time latency. We implement CamK on a mobile device running Android. Our experiment results show that CamK can achieve above 95% accuracy of keystroke localization, with only 4.8% false positive keystrokes. When compared to on-screen keyboards, CamK can achieve 1.25X typing speedup for regular text input and 2.5X for random character input.", u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'1-9', u'year': u'2016', u'keywords': u'cameras;image capture;keyboards;mobile computing;screens (display);camera-based keyboard;small-mobile devices;on-screen keyboards;text entry;camera-based text-entry method;CamK captures;image processing technique;user fingertips;keystroke localization accuracy;keystroke detection;online calibration;computation-intensive modules;Android;text input;random character input;Keyboards;Cameras;Image edge detection;Image segmentation;Smart phones', 'ID': u'7524400', u'booktitle': u'IEEE INFOCOM 2016 - The 35th Annual IEEE International Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8485933', u'author': u'N. Xiao and P. Yang and Y. Yan and H. Zhou and X. Li', u'title': u'Motion-Fi: Recognizing and Counting Repetitive Motions with Passive Wireless Backscattering', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Recently several ground-breaking RF-based motion-recognition systems were proposed to detect and/or recognize macro/micro human movements. These systems often suffer from various interferences caused by multiple-users moving simultaneously, resulting in extremely low recognition accuracy. To tackle this challenge, we propose a novel system, called Motion-Fi, which marries battery-free wireless backscattering and device-free sensing. Motion-Fi is an accurate, interference tolerable motion-recognition system, which counts repetitive motions without using scenario-dependent templates or profiles and enables multi-users performing certain motions simultaneously because of the relatively short transmission range of backscattered signals. Although the repetitive motions are fairly well detectable through the backscattering signals in theory, in reality they get blended into various other system noises during the motion. Moreover, irregular motion patterns among users will lead to expensive computation cost for motion recognition. We build a backscattering wireless platform to validate our design in various scenarios for over 6 months when different persons, distances and orientations are incorporated. In our experiments, the periodicity in motions could be recognized without any learning or training process, and the accuracy of counting such motions can be achieved within 5% count error. With little efforts in learning the patterns, our method could achieve 93.1% motion-recognition accuracy for a variety of motions. Moreover, by leveraging the periodicity of motions, the recognition accuracy could be further improved to nearly 100% with only 3 repetitions. Our experiments also show that the motions of multiple persons separated by around 2 meters cause little accuracy reduction in the counting process.', u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'2024-2032', u'year': u'2018', u'keywords': u'backscatter;feature extraction;image motion analysis;learning (artificial intelligence);radiofrequency interference;backscattering signals;irregular motion patterns;motion recognition;backscattering wireless platform;ground-breaking RF-based motion-recognition systems;extremely low recognition accuracy;battery-free wireless backscattering;interference tolerable motion-recognition system;Backscatter;Wireless communication;Wireless sensor networks;Antennas;Wireless fidelity;Impedance;Interference', 'ID': u'8485933', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8485933', u'author': u'N. Xiao and P. Yang and Y. Yan and H. Zhou and X. Li', u'title': u'Motion-Fi: Recognizing and Counting Repetitive Motions with Passive Wireless Backscattering', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Recently several ground-breaking RF-based motion-recognition systems were proposed to detect and/or recognize macro/micro human movements. These systems often suffer from various interferences caused by multiple-users moving simultaneously, resulting in extremely low recognition accuracy. To tackle this challenge, we propose a novel system, called Motion-Fi, which marries battery-free wireless backscattering and device-free sensing. Motion-Fi is an accurate, interference tolerable motion-recognition system, which counts repetitive motions without using scenario-dependent templates or profiles and enables multi-users performing certain motions simultaneously because of the relatively short transmission range of backscattered signals. Although the repetitive motions are fairly well detectable through the backscattering signals in theory, in reality they get blended into various other system noises during the motion. Moreover, irregular motion patterns among users will lead to expensive computation cost for motion recognition. We build a backscattering wireless platform to validate our design in various scenarios for over 6 months when different persons, distances and orientations are incorporated. In our experiments, the periodicity in motions could be recognized without any learning or training process, and the accuracy of counting such motions can be achieved within 5% count error. With little efforts in learning the patterns, our method could achieve 93.1% motion-recognition accuracy for a variety of motions. Moreover, by leveraging the periodicity of motions, the recognition accuracy could be further improved to nearly 100% with only 3 repetitions. Our experiments also show that the motions of multiple persons separated by around 2 meters cause little accuracy reduction in the counting process.', u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'2024-2032', u'year': u'2018', u'keywords': u'backscatter;feature extraction;image motion analysis;learning (artificial intelligence);radiofrequency interference;backscattering signals;irregular motion patterns;motion recognition;backscattering wireless platform;ground-breaking RF-based motion-recognition systems;extremely low recognition accuracy;battery-free wireless backscattering;interference tolerable motion-recognition system;Backscatter;Wireless communication;Wireless sensor networks;Antennas;Wireless fidelity;Impedance;Interference', 'ID': u'8485933', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8486285', u'author': u'H. Du and P. Li and H. Zhou and W. Gong and G. Luo and P. Yang', u'title': u'WordRecorder: Accurate Acoustic-based Handwriting Recognition Using Deep Learning', 'ENTRYTYPE': u'inproceedings', u'abstract': u'This paper presents WordRecorder, an efficient and accurate handwriting recognition system that identifies words using acoustic signals generated by pens and paper, thus enabling ubiquitous handwriting recognition. To achieve this, we carefully craft a new deep-learning based acoustic sensing framework with three major components, i.e., segmentation, classification, and word suggestion. First, we design a dual-window approach to segment the raw acoustic signal into a series of words and letters by exploiting subtle acoustic signal features of handwriting. Then we integrate a set of simple yet effective signal processing techniques to further refine raw acoustic signals into normalized spectrograms which are suitable for deep-learning classification. After that, we customize a deep neural network that is suitable for smart devices. Finally, we incorporate a word suggestion module to enhance the recognition performance. Our framework achieves both computation efficiency and desirable classification accuracy simultaneously. We prototype our design using off-the-shelf smartwatches and conduct extensive evaluations. Our results demonstrate that WordRecorder robustly archives 81% accuracy rate for trained users, and 75% for users without training, across a range of different environment, users, and writing habits.', u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'1448-1456', u'year': u'2018', u'keywords': u'acoustic signal processing;handwriting recognition;image classification;image segmentation;learning (artificial intelligence);neural nets;ubiquitous computing;deep learning;ubiquitous handwriting recognition;dual-window approach;raw acoustic signal;deep-learning classification;deep neural network;WordRecorder;handwriting recognition system;acoustic sensing;word suggestion;raw acoustic signals;segmentation;smart devices;Acoustics;Handwriting recognition;Writing;Microsoft Windows;Feature extraction;Conferences;Machine learning', 'ID': u'8486285', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8486285', u'author': u'H. Du and P. Li and H. Zhou and W. Gong and G. Luo and P. Yang', u'title': u'WordRecorder: Accurate Acoustic-based Handwriting Recognition Using Deep Learning', 'ENTRYTYPE': u'inproceedings', u'abstract': u'This paper presents WordRecorder, an efficient and accurate handwriting recognition system that identifies words using acoustic signals generated by pens and paper, thus enabling ubiquitous handwriting recognition. To achieve this, we carefully craft a new deep-learning based acoustic sensing framework with three major components, i.e., segmentation, classification, and word suggestion. First, we design a dual-window approach to segment the raw acoustic signal into a series of words and letters by exploiting subtle acoustic signal features of handwriting. Then we integrate a set of simple yet effective signal processing techniques to further refine raw acoustic signals into normalized spectrograms which are suitable for deep-learning classification. After that, we customize a deep neural network that is suitable for smart devices. Finally, we incorporate a word suggestion module to enhance the recognition performance. Our framework achieves both computation efficiency and desirable classification accuracy simultaneously. We prototype our design using off-the-shelf smartwatches and conduct extensive evaluations. Our results demonstrate that WordRecorder robustly archives 81% accuracy rate for trained users, and 75% for users without training, across a range of different environment, users, and writing habits.', u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'1448-1456', u'year': u'2018', u'keywords': u'acoustic signal processing;handwriting recognition;image classification;image segmentation;learning (artificial intelligence);neural nets;ubiquitous computing;deep learning;ubiquitous handwriting recognition;dual-window approach;raw acoustic signal;deep-learning classification;deep neural network;WordRecorder;handwriting recognition system;acoustic sensing;word suggestion;raw acoustic signals;segmentation;smart devices;Acoustics;Handwriting recognition;Writing;Microsoft Windows;Feature extraction;Conferences;Machine learning', 'ID': u'8486285', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2015.7218440', u'author': u'Q. Zhai and S. Ding and X. Li and F. Yang and J. Teng and J. Zhu and D. Xuan and Y. F. Zheng and W. Zhao', u'title': u'VM-tracking: Visual-motion sensing integration for real-time human tracking', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Human tracking in video has many practical applications such as visual guided navigation, assisted living, etc. In such applications, it is necessary to accurately track multiple humans across multiple cameras, subject to real-time constraints. Despite recent advances in visual tracking research, the tracking systems purely relying on visual information fail to meet the accuracy and real-time requirements at the same time. In this paper, we present a novel accurate and real-time human tracking system called VM-Tracking. The system aggregates the information of motion (M) sensor on human, and integrates it with visual (V) data based on physical locations. The system has two key features, i.e. location-based VM fusion and appearance-free tracking, which significantly distinguish itself from other existing human tracking systems. We have implemented the VM-Tracking system and conducted comprehensive experiments on challenging scenarios.', u'issn': u'0743-166X', u'number': '', u'month': u'April', u'volume': '', u'pages': u'711-719', u'year': u'2015', u'keywords': u'image sensors;object tracking;real-time systems;video signal processing;VM-tracking;visual motion sensing integration;real-time human tracking;visual guided navigation;assisted living;track multiple humans across multiple cameras;real-time constraints;visual tracking research;visual information;real-time requirements;real-time human tracking system;multiple cameras;Visualization;Tracking;Real-time systems;Cameras;Accuracy;Trajectory;Acceleration', 'ID': u'7218440', u'booktitle': u'2015 IEEE Conference on Computer Communications (INFOCOM)'}
{u'doi': u'10.1109/INFOCOM.2014.6848007', u'author': u'L. Zhang and K. Liu and Y. Jiang and X. Li and Y. Liu and P. Yang', u'title': u'Montage: Combine frames with movement continuity for realtime multi-user tracking', 'ENTRYTYPE': u'inproceedings', u'abstract': u'In this work we design and develop Montage for real-time multi-user formation tracking and localization by off-the-shelf smartphones. Montage achieves submeter-level tracking accuracy by integrating temporal and spatial constraints from user movement vector estimation and distance measuring. In Montage we designed a suite of novel techniques to surmount a variety of challenges in real-time tracking, without infrastructure and fingerprints, and without any a priori user-specific (e.g., stride-length and phone-placement) or site-specific (e.g., digitalized map) knowledge. We implemented, deployed and evaluated Montage in both outdoor and indoor environment. Our experimental results (847 traces from 15 users) show that the stride-length estimated by Montage over all users has error within 9cm, and the moving-direction estimated by Montage is within 20\xb0. For realtime tracking, Montage provides meter-second-level formation tracking accuracy with off-the-shelf mobile phones.', u'issn': u'0743-166X', u'number': '', u'month': u'April', u'volume': '', u'pages': u'799-807', u'year': u'2014', u'keywords': u'smart phones;target tracking;movement continuity;real-time multiuser formation tracking;smartphones;submeter-level tracking;temporal constraints;spatial constraints;user movement vector estimation;moving-direction estimation;meter-second-level formation tracking accuracy;mobile phones;Vectors;Distance measurement;Tracking;Acoustics;Earth;Topology;Acceleration', 'ID': u'6848007', u'booktitle': u'IEEE INFOCOM 2014 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8485910', u'author': u'C. Zhang and F. Yang and G. Li and Q. Zhai and Y. Jiang and D. Xuan', u'title': u'MV-Sports: A Motion and Vision Sensor Integration-Based Sports Analysis System', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Recently, intelligent sports analytics is becoming a hot area in both industry and academia for coaching, practicing tactic and technical analysis. With the growing trend of bringing sports analytics to live broadcasting, sports robots and common playfield, a low cost system that is easy to deploy and performs real-time and accurate sports analytics is very desirable. However, existing systems, such as Hawk-Eye, cannot satisfy these requirements due to various factors. In this paper, we present MV-Sports, a cost-effective system for real-time sports analysis based on motion and vision sensor integration. Taking tennis as a case study, we aim to recognize player shot types and measure ball states. For fine-grained player action recognition, we leverage motion signal for fast action highlighting and propose a long short term memory (LSTM)-based framework to integrate MV data for training and classification. For ball state measurement, we compute the initial ball state via motion sensing and devise an extended kalman filter (EKF)-based approach to combine ball motion physics-based tracking and vision positioning-based tracking to get more accurate ball state. We implement MV-Sports on commercial off-the-shelf (COTS) devices and conduct real-world experiments to evaluate the performance of our system. The results show our approach can achieve accurate player action recognition and ball state measurement with sub-second latency.', u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'1070-1078', u'year': u'2018', u'keywords': u'computer vision;data integration;image motion analysis;image sensors;Kalman filters;learning (artificial intelligence);neural nets;nonlinear filters;object recognition;object tracking;pattern classification;sensor fusion;sport;video signal processing;MV-Sports;intelligent sports analytics;tactic analysis;technical analysis;sports robots;cost-effective system;real-time sports analysis;fine-grained player action recognition;long short term memory-based framework;ball state measurement;motion sensing;ball motion physics-based tracking;vision positioning;motion signal;extended Kalman filter-based approach;player action recognition;coaching;tactic practicing;live broadcasting;motion-vision sensor integration;LSTM-based framework;MV data integration;classification;EKF-based approach;vision positioning-based tracking;Tracking;Real-time systems;Sensors;Cameras;Position measurement;Motion segmentation', 'ID': u'8485910', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8485910', u'author': u'C. Zhang and F. Yang and G. Li and Q. Zhai and Y. Jiang and D. Xuan', u'title': u'MV-Sports: A Motion and Vision Sensor Integration-Based Sports Analysis System', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Recently, intelligent sports analytics is becoming a hot area in both industry and academia for coaching, practicing tactic and technical analysis. With the growing trend of bringing sports analytics to live broadcasting, sports robots and common playfield, a low cost system that is easy to deploy and performs real-time and accurate sports analytics is very desirable. However, existing systems, such as Hawk-Eye, cannot satisfy these requirements due to various factors. In this paper, we present MV-Sports, a cost-effective system for real-time sports analysis based on motion and vision sensor integration. Taking tennis as a case study, we aim to recognize player shot types and measure ball states. For fine-grained player action recognition, we leverage motion signal for fast action highlighting and propose a long short term memory (LSTM)-based framework to integrate MV data for training and classification. For ball state measurement, we compute the initial ball state via motion sensing and devise an extended kalman filter (EKF)-based approach to combine ball motion physics-based tracking and vision positioning-based tracking to get more accurate ball state. We implement MV-Sports on commercial off-the-shelf (COTS) devices and conduct real-world experiments to evaluate the performance of our system. The results show our approach can achieve accurate player action recognition and ball state measurement with sub-second latency.', u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'1070-1078', u'year': u'2018', u'keywords': u'computer vision;data integration;image motion analysis;image sensors;Kalman filters;learning (artificial intelligence);neural nets;nonlinear filters;object recognition;object tracking;pattern classification;sensor fusion;sport;video signal processing;MV-Sports;intelligent sports analytics;tactic analysis;technical analysis;sports robots;cost-effective system;real-time sports analysis;fine-grained player action recognition;long short term memory-based framework;ball state measurement;motion sensing;ball motion physics-based tracking;vision positioning;motion signal;extended Kalman filter-based approach;player action recognition;coaching;tactic practicing;live broadcasting;motion-vision sensor integration;LSTM-based framework;MV data integration;classification;EKF-based approach;vision positioning-based tracking;Tracking;Real-time systems;Sensors;Cameras;Position measurement;Motion segmentation', 'ID': u'8485910', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8485867', u'author': u'V. Nguyen and M. Ibrahim and S. Rupavatharam and M. Jawahar and M. Gruteser and R. Howard', u'title': u'Eyelight: Light-and-Shadow-Based Occupancy Estimation and Room Activity Recognition', 'ENTRYTYPE': u'inproceedings', u'abstract': u'This paper explores the feasibility of localizing and detecting activities of building occupants using visible light sensing across a mesh of light bulbs. Existing Visible Light activity sensing (VLS) techniques require either light sensors to be deployed on the floor or a person to carry a device. Our approach integrates photosensors with light bulbs and exploits the light reflected off the floor to achieve an entirely device-free and light source based system. This forms a mesh of virtual light barriers across networked lights to track shadows cast by occupants. The design employs a synchronization circuit that implements a time division signaling scheme to differentiate between light sources and a sensitive sensing circuit to detect small changes in weak reflections. Sensor readings are fed into indoor supervised tracking algorithms as well as occupancy and activity recognition classifiers. Our prototype uses modified off-the-shelf LED flood light bulbs and is installed in a typical office conference room. We evaluate the performance of our system in terms of localization, occupancy estimation and activity classification, and find a 0.89m median localization error as well as 93.7% and 93.78% occupancy and activity classification accuracy, respectively.', u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'351-359', u'year': u'2018', u'keywords': u'cameras;estimation theory;image classification;light emitting diodes;light sources;lighting;object tracking;sensors;room activity recognition;light sensors;floor;virtual light barriers;networked lights;light sources;sensitive sensing circuit;activity recognition classifiers;flood light bulbs;building occupants;activity classification;Light-and-shadow-based occupancy estimation;detecting activities localization;Visible Light activity sensing techniques;VLS techniques;Sensors;Light emitting diodes;Light sources;Lighting;Tracking;Receivers;Buildings', 'ID': u'8485867', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
