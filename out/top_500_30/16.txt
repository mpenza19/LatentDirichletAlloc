{u'doi': u'10.1109/INFOCOM.2018.8485970', u'author': u'L. Wang and W. Wang and B. Li', u'title': u'Utopia: Near-optimal Coflow Scheduling with Isolation Guarantee', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Performance and service isolation come as two top objectives for coflow scheduling. However, the common wisdom is that these two objectives are often conflicting with each other and cannot be achieved simultaneously. Existing coflow scheduling frameworks either focus only on minimizing the average coflow completion time (CCT) (e.g., Varys), or providing optimal isolation between contending coflows by means of fair network sharing (e.g., HUG). In this paper, we make an attempt to achieve the best of both worlds through a novel coflow scheduler, Utopia, to attain near-optimal performance with provable isolation guarantee. This is particularly challenging given the correlation of bandwidth demands across multiple links from coflows. We show that Utopia is capable of reducing the average CCT dramatically, while still guaranteeing that no coflow will ever be delayed beyond a constant time than its CCT in a fair scheme. Both trace-driven simulation and EC2 deployment confirm that Utopia outperforms the fair sharing policy by 1.8 \xd7 in terms of average CCT, while producing no completion time delay for a single coflow. Even compared with performance-optimal Varys, Utopia speeds up average coflow completion by 9%.', u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'891-899', u'year': u'2018', u'keywords': u'delays;telecommunication scheduling;fair network;Utopia;near-optimal performance;provable isolation guarantee;average CCT;constant time;fair scheme;fair sharing policy;completion time delay;single coflow;performance-optimal Varys;near-optimal coflow scheduling;service isolation;common wisdom;coflow scheduling frameworks;average coflow completion time;optimal isolation;contending coflows;Bandwidth;Resource management;Fabrics;Delays;Conferences;Correlation;Production', 'ID': u'8485970', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8485970', u'author': u'L. Wang and W. Wang and B. Li', u'title': u'Utopia: Near-optimal Coflow Scheduling with Isolation Guarantee', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Performance and service isolation come as two top objectives for coflow scheduling. However, the common wisdom is that these two objectives are often conflicting with each other and cannot be achieved simultaneously. Existing coflow scheduling frameworks either focus only on minimizing the average coflow completion time (CCT) (e.g., Varys), or providing optimal isolation between contending coflows by means of fair network sharing (e.g., HUG). In this paper, we make an attempt to achieve the best of both worlds through a novel coflow scheduler, Utopia, to attain near-optimal performance with provable isolation guarantee. This is particularly challenging given the correlation of bandwidth demands across multiple links from coflows. We show that Utopia is capable of reducing the average CCT dramatically, while still guaranteeing that no coflow will ever be delayed beyond a constant time than its CCT in a fair scheme. Both trace-driven simulation and EC2 deployment confirm that Utopia outperforms the fair sharing policy by 1.8 \xd7 in terms of average CCT, while producing no completion time delay for a single coflow. Even compared with performance-optimal Varys, Utopia speeds up average coflow completion by 9%.', u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'891-899', u'year': u'2018', u'keywords': u'delays;telecommunication scheduling;fair network;Utopia;near-optimal performance;provable isolation guarantee;average CCT;constant time;fair scheme;fair sharing policy;completion time delay;single coflow;performance-optimal Varys;near-optimal coflow scheduling;service isolation;common wisdom;coflow scheduling frameworks;average coflow completion time;optimal isolation;contending coflows;Bandwidth;Resource management;Fabrics;Delays;Conferences;Correlation;Production', 'ID': u'8485970', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8485981', u'author': u'X. Zhang and Z. Qian and S. Zhang and X. Li and X. Wang and S. Lu', u'title': u'COBRA: Toward Provably Efficient Semi-Clairvoyant Scheduling in Data Analytics Systems', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Typical data analytics systems abstract jobs as directed acyclic graphs (DAGs). It is crucial to maximize throughput and speedup completions for DAG jobs in practice. Existing works propose clairvoyant schedulers optimizing these goals, however, they assume complete job information as a prior knowledge which limits their applicability. Instead, we remove the complete prior knowledge assumption and rely solely on a partial prior information, which is more practical. And we design a semi-clairvoyant task scheduler Cobra working within each job. Cobra adaptively adjusts its resource desires in a multiplicative-increase multiplicative-decrease (MIMD) manner according to nearly past resource utilizations and the current waiting tasks. On the other hand, Cobra seeks to satisfy task locality preferences by allowing each task to wait for some time that is bounded by a parameterized threshold. Surprisingly, even with the partial prior job information, we theoretically prove, Cobra, when working with the widely used fair job scheduler, is O(1)-competitive with respect to both makespan and average job response time. We experimentally validate that the performance promotion of Cobra in both real system deployment and trace-driven simulations.', u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'513-521', u'year': u'2018', u'keywords': u'data analysis;directed graphs;processor scheduling;resource allocation;complete prior knowledge assumption;semiclairvoyant task scheduler Cobra;multiplicative-increase multiplicative-decrease manner;task locality preferences;partial prior job information;average job response time;directed acyclic graphs;speedup completions;DAG jobs;fair job scheduler;real system deployment;semiclairvoyant scheduling;trace-driven simulations;data analytics systems abstract jobs;Task analysis;Containers;Time factors;Dynamic scheduling;Data analysis;Processor scheduling', 'ID': u'8485981', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8485981', u'author': u'X. Zhang and Z. Qian and S. Zhang and X. Li and X. Wang and S. Lu', u'title': u'COBRA: Toward Provably Efficient Semi-Clairvoyant Scheduling in Data Analytics Systems', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Typical data analytics systems abstract jobs as directed acyclic graphs (DAGs). It is crucial to maximize throughput and speedup completions for DAG jobs in practice. Existing works propose clairvoyant schedulers optimizing these goals, however, they assume complete job information as a prior knowledge which limits their applicability. Instead, we remove the complete prior knowledge assumption and rely solely on a partial prior information, which is more practical. And we design a semi-clairvoyant task scheduler Cobra working within each job. Cobra adaptively adjusts its resource desires in a multiplicative-increase multiplicative-decrease (MIMD) manner according to nearly past resource utilizations and the current waiting tasks. On the other hand, Cobra seeks to satisfy task locality preferences by allowing each task to wait for some time that is bounded by a parameterized threshold. Surprisingly, even with the partial prior job information, we theoretically prove, Cobra, when working with the widely used fair job scheduler, is O(1)-competitive with respect to both makespan and average job response time. We experimentally validate that the performance promotion of Cobra in both real system deployment and trace-driven simulations.', u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'513-521', u'year': u'2018', u'keywords': u'data analysis;directed graphs;processor scheduling;resource allocation;complete prior knowledge assumption;semiclairvoyant task scheduler Cobra;multiplicative-increase multiplicative-decrease manner;task locality preferences;partial prior job information;average job response time;directed acyclic graphs;speedup completions;DAG jobs;fair job scheduler;real system deployment;semiclairvoyant scheduling;trace-driven simulations;data analytics systems abstract jobs;Task analysis;Containers;Time factors;Dynamic scheduling;Data analysis;Processor scheduling', 'ID': u'8485981', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8486026', u'author': u'C. Chen and W. Wang and B. Li', u'title': u'Performance-Aware Fair Scheduling: Exploiting Demand Elasticity of Data Analytics Jobs', 'ENTRYTYPE': u'inproceedings', u'abstract': u"Efficient resource management is of paramount importance in today's production clusters. In this paper, we identify the demand elasticity of data-parallel jobs. Demand elasticity allows jobs to run with a significantly less amount of resources than they ideally need, at the expense of only a modest performance penalty. Our EC2 experiment using popular Spark benchmark suites confirms that running a job using 50% of demanded slots is sufficient to achieve at least 75% of the ideal performance. We show that such an elasticity is an intrinsic property of data-parallel jobs and can be exploited to speed up average job completion. In this regard, we propose Performance-Aware Fair (PAF) scheduler to identify the demand elasticity and use it to improve the average job performance, while still attaining near-optimal isolation guarantee close to fair sharing. PAF starts with a fair allocation and iteratively adjusts it by transferring resources from one job to another, improving the performance of resource-taker without penalizing resource-giver by a noticeable amount. We implemented PAF in Spark and evaluated its effectiveness through both EC2 experiments and large-scale simulations. Evaluation results show that compared with fair allocation, PAF improves the average job performance by 13%, while penalizing resource-givers by no more than 1%.", u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'504-512', u'year': u'2018', u'keywords': u'cloud computing;cluster computing;data analysis;parallel processing;resource allocation;scheduling;Performance-Aware Fair scheduling;demand elasticity;data analytics jobs;data-parallel jobs;Performance-Aware Fair scheduler;PAF;fair allocation;resource management;Spark benchmark suites;Task analysis;Elasticity;Resource management;Sparks;Data analysis;Runtime;Parallel processing', 'ID': u'8486026', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8486026', u'author': u'C. Chen and W. Wang and B. Li', u'title': u'Performance-Aware Fair Scheduling: Exploiting Demand Elasticity of Data Analytics Jobs', 'ENTRYTYPE': u'inproceedings', u'abstract': u"Efficient resource management is of paramount importance in today's production clusters. In this paper, we identify the demand elasticity of data-parallel jobs. Demand elasticity allows jobs to run with a significantly less amount of resources than they ideally need, at the expense of only a modest performance penalty. Our EC2 experiment using popular Spark benchmark suites confirms that running a job using 50% of demanded slots is sufficient to achieve at least 75% of the ideal performance. We show that such an elasticity is an intrinsic property of data-parallel jobs and can be exploited to speed up average job completion. In this regard, we propose Performance-Aware Fair (PAF) scheduler to identify the demand elasticity and use it to improve the average job performance, while still attaining near-optimal isolation guarantee close to fair sharing. PAF starts with a fair allocation and iteratively adjusts it by transferring resources from one job to another, improving the performance of resource-taker without penalizing resource-giver by a noticeable amount. We implemented PAF in Spark and evaluated its effectiveness through both EC2 experiments and large-scale simulations. Evaluation results show that compared with fair allocation, PAF improves the average job performance by 13%, while penalizing resource-givers by no more than 1%.", u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'504-512', u'year': u'2018', u'keywords': u'cloud computing;cluster computing;data analysis;parallel processing;resource allocation;scheduling;Performance-Aware Fair scheduling;demand elasticity;data analytics jobs;data-parallel jobs;Performance-Aware Fair scheduler;PAF;fair allocation;resource management;Spark benchmark suites;Task analysis;Elasticity;Resource management;Sparks;Data analysis;Runtime;Parallel processing', 'ID': u'8486026', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8486323', u'author': u'S. Im and M. Shadloo and Z. Zheng', u'title': u'Online Partial Throughput Maximization for Multidimensional Coflow', 'ENTRYTYPE': u'inproceedings', u'abstract': u"Coflow has recently been introduced to capture communication patterns that are widely observed in the cloud and massively parallel computing. Coflow consists of a number of flows that each represents data communication from one machine to another. A coflow is completed when all of its flows are completed. Due to its elegant abstraction of the complicated communication processes found in various parallel computing platforms, it has received significant attention. In this paper, we consider coflow for the objective of maximizing partial throughput. This objective seeks to measure the progress made for partially completed coflows before their deadline. Partially processed coflows still could be useful when their flows send out useful data that can be used for the next round computation. In our measure, a coflow is processed by a certain fraction when all of its flows are processed by the same fraction or more. We consider a natural class of greedy algorithms, which we call myopic concurrent. The algorithms seek to maximize the marginal increase of the partial throughput objective at each time. We analyze the performance of our algorithm against the optimal scheduler. In fact, our result is more general as a flow could be extended to demand various heterogeneous resources. Our experiment demonstrates our algorithm's superior performance.", u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'2042-2050', u'year': u'2018', u'keywords': u'data communication;parallel programming;scheduling;parallel computing platforms;partially completed coflows;partially processed coflows;partial throughput objective;online partial throughput maximization;multidimensional coflow;communication patterns;massively parallel computing;data communication;Task analysis;Throughput;Computational modeling;Schedules;Optimal scheduling;Cloud computing;Processor scheduling', 'ID': u'8486323', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8486323', u'author': u'S. Im and M. Shadloo and Z. Zheng', u'title': u'Online Partial Throughput Maximization for Multidimensional Coflow', 'ENTRYTYPE': u'inproceedings', u'abstract': u"Coflow has recently been introduced to capture communication patterns that are widely observed in the cloud and massively parallel computing. Coflow consists of a number of flows that each represents data communication from one machine to another. A coflow is completed when all of its flows are completed. Due to its elegant abstraction of the complicated communication processes found in various parallel computing platforms, it has received significant attention. In this paper, we consider coflow for the objective of maximizing partial throughput. This objective seeks to measure the progress made for partially completed coflows before their deadline. Partially processed coflows still could be useful when their flows send out useful data that can be used for the next round computation. In our measure, a coflow is processed by a certain fraction when all of its flows are processed by the same fraction or more. We consider a natural class of greedy algorithms, which we call myopic concurrent. The algorithms seek to maximize the marginal increase of the partial throughput objective at each time. We analyze the performance of our algorithm against the optimal scheduler. In fact, our result is more general as a flow could be extended to demand various heterogeneous resources. Our experiment demonstrates our algorithm's superior performance.", u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'2042-2050', u'year': u'2018', u'keywords': u'data communication;parallel programming;scheduling;parallel computing platforms;partially completed coflows;partially processed coflows;partial throughput objective;online partial throughput maximization;multidimensional coflow;communication patterns;massively parallel computing;data communication;Task analysis;Throughput;Computational modeling;Schedules;Optimal scheduling;Cloud computing;Processor scheduling', 'ID': u'8486323', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFCOM.2013.6566958', u'author': u'J. Tan and X. Meng and L. Zhang', u'title': u'Coupling task progress for MapReduce resource-aware scheduling', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Schedulers are critical in enhancing the performance of MapReduce/Hadoop in presence of multiple jobs with different characteristics and performance goals. Though current schedulers for Hadoop are quite successful, they still have room for improvement: map tasks (MapTasks) and reduce tasks (ReduceTasks) are not jointly optimized, albeit there is a strong dependence between them. This can cause job starvation and unfavorable data locality. In this paper, we design and implement a resource-aware scheduler for Hadoop. It couples the progresses of MapTasks and ReduceTasks, utilizing Wait Scheduling for ReduceTasks and Random Peeking Scheduling for MapTasks to jointly optimize the task placement. This mitigates the starvation problem and improves the overall data locality. Our extensive experiments demonstrate significant improvements in job response times.', u'issn': u'0743-166X', u'number': '', u'month': u'April', u'volume': '', u'pages': u'1618-1626', u'year': u'2013', u'keywords': u'resource allocation;task placement;random peeking scheduling;wait scheduling;ReduceTasks;MapTasks;Hadoop;MapReduce resource-aware scheduling;coupling task progress;Couplings;Delays;Heart beat;Processor scheduling;Synchronization;Time factors;Instruction sets', 'ID': u'6566958', u'booktitle': u'2013 Proceedings IEEE INFOCOM'}
{u'doi': u'10.1109/INFOCOM.2017.8056948', u'author': u'C. Chen and W. Wang and S. Zhang and B. Li', u'title': u'Cluster fair queueing: Speeding up data-parallel jobs with delay guarantees', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Cluster scheduler serves as a critical component to data-parallel systems in datacenters. Ideally, a scheduler should provide predictable performance with guarantees on the maximal job completion delay, while at the same time ensuring the minimal mean response time. Practically however, performance predictability and optimality are often conflicting with each other. The results often are a plethora of scheduling policies that either achieve predictable performance at the expense of long response times (e.g., max-min fairness), or run the risk of starving some jobs to obtain the minimal mean response time (e.g., Shortest Remaining Processing Time First). To address these problems, we develop a new scheduler, Cluster Fair Queueing (CFQ), which preferentially offers resources to jobs that complete the earliest under a fair sharing policy. We show that CFQ is able to minimize the mean response time while at the same time ensuring jobs to finish within a constant time after their completion under fair sharing. Our Spark deployment on a 100-node EC2 cluster demonstrates that compared to the built-in fair scheduler, CFQ can decrease the mean response time by 40%, which speeds up more than 40% of jobs by over 75% on average.', u'issn': '', u'number': '', u'month': u'May', u'volume': '', u'pages': u'1-9', u'year': u'2017', u'keywords': u'queueing theory;scheduling;CFQ;fair sharing policy;constant time;fair scheduler;Cluster fair queueing;data-parallel jobs;delay guarantees;Cluster scheduler;data-parallel systems;performance predictability;scheduling policies;max-min fairness;Shortest Remaining Processing Time First;Cluster Fair Queueing;mean response time;job completion delay;Spark deployment;EC2 cluster;Time factors;Sparks;Resource management;Delays;Global Positioning System;Prediction algorithms', 'ID': u'8056948', u'booktitle': u'IEEE INFOCOM 2017 - IEEE Conference on Computer Communications'}
