{u'doi': u'10.1109/INFOCOM.2018.8485969', u'author': u'T. Zhou and B. Xiao and Z. Cai and M. Xu and X. Liu', u'title': u'From Uncertain Photos to Certain Coverage: a Novel Photo Selection Approach to Mobile Crowdsensing', 'ENTRYTYPE': u'inproceedings', u'abstract': u"Traditional mobile crowdsensing photo selection process focuses on selecting photos from participants to a server. The server may contain tons of photos for a certain area. A new problem is how to select a set of photos from the server to a smartphone user when the user requests to view an area (e.g., a hot spot). The challenge of the new problem is that the photo set should attain both photo coverage and view quality (e.g., with clear Points of Interest). However, contributions of these geo-tagged photos could be uncertain for a target area due to unavailable information of photo shooting direction and no reference photos. In this paper, we propose a novel and generic server-to-requester photo selection approach. Our approach leverages a utility measure to quantify the contribution of a photo set, where photos' spatial distribution and visual correlation are jointly exploited to evaluate their performance on photo coverage and view quality. Finding the photo set with the maximum utility is proven to be NP-hard. We then propose an approximation algorithm based on a greedy strategy with rigorous theoretical analysis. The effectiveness of our approach is demonstrated with real-world datasets. The results show that the proposal outperforms other approaches with much higher photo coverage and better view quality.", u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'1979-1987', u'year': u'2018', u'keywords': u'approximation theory;greedy algorithms;information retrieval;mobile computing;smart phones;photo set;geo-tagged photos;photo shooting direction;generic server-to-requester photo selection approach;photo selection approach;mobile crowdsensing photo selection process;photo coverage;approximation algorithm;greedy strategy;Servers;Sensors;Correlation;Visualization;Mobile handsets;Uncertainty;Cameras', 'ID': u'8485969', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8485969', u'author': u'T. Zhou and B. Xiao and Z. Cai and M. Xu and X. Liu', u'title': u'From Uncertain Photos to Certain Coverage: a Novel Photo Selection Approach to Mobile Crowdsensing', 'ENTRYTYPE': u'inproceedings', u'abstract': u"Traditional mobile crowdsensing photo selection process focuses on selecting photos from participants to a server. The server may contain tons of photos for a certain area. A new problem is how to select a set of photos from the server to a smartphone user when the user requests to view an area (e.g., a hot spot). The challenge of the new problem is that the photo set should attain both photo coverage and view quality (e.g., with clear Points of Interest). However, contributions of these geo-tagged photos could be uncertain for a target area due to unavailable information of photo shooting direction and no reference photos. In this paper, we propose a novel and generic server-to-requester photo selection approach. Our approach leverages a utility measure to quantify the contribution of a photo set, where photos' spatial distribution and visual correlation are jointly exploited to evaluate their performance on photo coverage and view quality. Finding the photo set with the maximum utility is proven to be NP-hard. We then propose an approximation algorithm based on a greedy strategy with rigorous theoretical analysis. The effectiveness of our approach is demonstrated with real-world datasets. The results show that the proposal outperforms other approaches with much higher photo coverage and better view quality.", u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'1979-1987', u'year': u'2018', u'keywords': u'approximation theory;greedy algorithms;information retrieval;mobile computing;smart phones;photo set;geo-tagged photos;photo shooting direction;generic server-to-requester photo selection approach;photo selection approach;mobile crowdsensing photo selection process;photo coverage;approximation algorithm;greedy strategy;Servers;Sensors;Correlation;Visualization;Mobile handsets;Uncertainty;Cameras', 'ID': u'8485969', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8486394', u'author': u'R. Hou and I. Jahja and L. Luu and P. Saxena and H. Yu', u'title': u'Randomized View Reconciliation in Permissionless Distributed Systems', 'ENTRYTYPE': u'inproceedings', u'abstract': u"In a sybil attack, an adversary creates a large number of fake identities/nodes and have them join the system. Computational puzzles have long been investigated as a possible sybil defense: If a node fails to solve the puzzle in a timely fashion, it will no longer be accepted by other nodes. However, it is still possible for a malicious node to behave in such a way that it is accepted by some honest nodes but not other honest nodes. This results in different honest nodes having different views on which set of nodes should form the system. Such view divergence, unfortunately, breaks the overarching assumption required by many existing security protocols. Partly spurred by the growing popularity of Bitcoin, researchers have recently formalized the above view divergence problem and proposed interesting solutions (which we call view reconciliation protocols). For example, in CRYPTO 2015, Andrychowicz and Dziembowski proposed a view reconciliation protocol with \u0398(N) time complexity, with N being the number of honest nodes in the system. All existing view reconciliation protocols so far have a similar \u0398(N) time complexity. As this paper's main contribution, we propose a novel view reconciliation protocol with a time complexity of only \u0398([ln N/ln ln N]). To achieve such an exponential improvement, we aggressively exploit randomization.", u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'2528-2536', u'year': u'2018', u'keywords': u'computational complexity;peer-to-peer computing;protocols;security of data;randomized view reconciliation;permissionless distributed systems;sybil attack;malicious node;view divergence problem;view reconciliation protocol;fake identities;honest nodes;security protocols;time complexity;fake nodes;sybil defense;Bitcoin;randomization;Protocols;Bitcoin;Time complexity;Error probability;Computer science', 'ID': u'8486394', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8486394', u'author': u'R. Hou and I. Jahja and L. Luu and P. Saxena and H. Yu', u'title': u'Randomized View Reconciliation in Permissionless Distributed Systems', 'ENTRYTYPE': u'inproceedings', u'abstract': u"In a sybil attack, an adversary creates a large number of fake identities/nodes and have them join the system. Computational puzzles have long been investigated as a possible sybil defense: If a node fails to solve the puzzle in a timely fashion, it will no longer be accepted by other nodes. However, it is still possible for a malicious node to behave in such a way that it is accepted by some honest nodes but not other honest nodes. This results in different honest nodes having different views on which set of nodes should form the system. Such view divergence, unfortunately, breaks the overarching assumption required by many existing security protocols. Partly spurred by the growing popularity of Bitcoin, researchers have recently formalized the above view divergence problem and proposed interesting solutions (which we call view reconciliation protocols). For example, in CRYPTO 2015, Andrychowicz and Dziembowski proposed a view reconciliation protocol with \u0398(N) time complexity, with N being the number of honest nodes in the system. All existing view reconciliation protocols so far have a similar \u0398(N) time complexity. As this paper's main contribution, we propose a novel view reconciliation protocol with a time complexity of only \u0398([ln N/ln ln N]). To achieve such an exponential improvement, we aggressively exploit randomization.", u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'2528-2536', u'year': u'2018', u'keywords': u'computational complexity;peer-to-peer computing;protocols;security of data;randomized view reconciliation;permissionless distributed systems;sybil attack;malicious node;view divergence problem;view reconciliation protocol;fake identities;honest nodes;security protocols;time complexity;fake nodes;sybil defense;Bitcoin;randomization;Protocols;Bitcoin;Time complexity;Error probability;Computer science', 'ID': u'8486394', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8485860', u'author': u'K. Ji and G. Quan and J. Tan', u'title': u'Asymptotic Miss Ratio of LRU Caching with Consistent Hashing', 'ENTRYTYPE': u'inproceedings', u'abstract': u'To efficiently scale data caching infrastructure to support emerging big data applications, many caching systems rely on consistent hashing to group a large number of servers to form a cooperative cluster. These servers are organized together according to a random hash function. They jointly provide a unified but distributed hash table to serve swift and voluminous data item requests. Different from the single least-recently-used (LRU) server that has already been extensively studied, theoretically characterizing a cluster that consists of multiple LRU servers remains yet to be explored. These servers are not simply added together; the random hashing complicates the behavior. To this end, we derive the asymptotic miss ratio of data item requests on a LRU cluster with consistent hashing. We show that these individual cache spaces on different servers can be effectively viewed as if they could be pooled together to form a single virtual LRU cache space parametrized by an appropriate cache size. This equivalence can be established rigorously under the condition that the cache sizes of the individual servers are large. For typical data caching systems this condition is common. Our theoretical framework provides a convenient abstraction that can directly apply the results from the simpler single LRU cache to the more complex LRU cluster with consistent hashing.', u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'450-458', u'year': u'2018', u'keywords': u'Big Data;cache storage;file organisation;file servers;asymptotic miss ratio;LRU caching;consistent hashing;data caching infrastructure;big data applications;random hash function;swift data item requests;voluminous data item requests;multiple LRU servers;random hashing;individual cache spaces;single virtual LRU cache space;appropriate cache size;cache sizes;individual servers;typical data caching systems;simpler single LRU cache;complex LRU cluster;distributed hash table;Servers;Distributed databases;Partitioning algorithms;Conferences;Clustering algorithms;Random variables;Electronic mail', 'ID': u'8485860', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8485860', u'author': u'K. Ji and G. Quan and J. Tan', u'title': u'Asymptotic Miss Ratio of LRU Caching with Consistent Hashing', 'ENTRYTYPE': u'inproceedings', u'abstract': u'To efficiently scale data caching infrastructure to support emerging big data applications, many caching systems rely on consistent hashing to group a large number of servers to form a cooperative cluster. These servers are organized together according to a random hash function. They jointly provide a unified but distributed hash table to serve swift and voluminous data item requests. Different from the single least-recently-used (LRU) server that has already been extensively studied, theoretically characterizing a cluster that consists of multiple LRU servers remains yet to be explored. These servers are not simply added together; the random hashing complicates the behavior. To this end, we derive the asymptotic miss ratio of data item requests on a LRU cluster with consistent hashing. We show that these individual cache spaces on different servers can be effectively viewed as if they could be pooled together to form a single virtual LRU cache space parametrized by an appropriate cache size. This equivalence can be established rigorously under the condition that the cache sizes of the individual servers are large. For typical data caching systems this condition is common. Our theoretical framework provides a convenient abstraction that can directly apply the results from the simpler single LRU cache to the more complex LRU cluster with consistent hashing.', u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'450-458', u'year': u'2018', u'keywords': u'Big Data;cache storage;file organisation;file servers;asymptotic miss ratio;LRU caching;consistent hashing;data caching infrastructure;big data applications;random hash function;swift data item requests;voluminous data item requests;multiple LRU servers;random hashing;individual cache spaces;single virtual LRU cache space;appropriate cache size;cache sizes;individual servers;typical data caching systems;simpler single LRU cache;complex LRU cluster;distributed hash table;Servers;Distributed databases;Partitioning algorithms;Conferences;Clustering algorithms;Random variables;Electronic mail', 'ID': u'8485860', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFCOM.2009.5062044', u'author': u'A. Bremler-Barr and D. Hay and D. Hendler and R. M. Roth', u'title': u'PEDS: A Parallel Error Detection Scheme for TCAM Devices', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Ternary content-addressable memory (TCAM) devices are increasingly used for performing high-speed packet classification. A TCAM consists of an associative memory that compares a search key in parallel against all entries. TCAMs may suffer from error events that cause ternary cells to change their value to any symbol in the ternary alphabet "0","1","*". Due to their parallel access feature, standard error detection schemes are not directly applicable to TCAMs; an additional difficulty is posed by the special semantic of the "*" symbol. This paper introduces PEDS, a novel parallel error detection scheme that locates the erroneous entries in a TCAM device. PEDS is based on applying an error-detection code to each TCAM entry, and utilizing the parallel capabilities of the TCAM, by simultaneously checking the correctness of multiple TCAM entries. A key feature of PEDS is that the number of TCAM lookup operations required to locate all errors depends on the number of symbols per entry rather than the (orders-of-magnitude larger) number of TCAM entries. For large TCAM devices, a specific instance of PEDS requires only 200 lookups for 100-symbol entries, while a naive approach may need hundreds of thousands lookups. PEDS allows flexible and dynamic selection of trade-off points between robustness, space complexity, and number of lookups.', u'issn': u'0743-166X', u'number': '', u'month': u'April', u'volume': '', u'pages': u'1296-1304', u'year': u'2009', u'keywords': u'computational complexity;content-addressable storage;error detection;PEDS;parallel error detection scheme;TCAM devices;ternary content-addressable memory devices;high-speed packet classification;associative memory;ternary cells;space complexity;Random access memory;Associative memory;Peer to peer computing;Communications Society;Computer vision;Error correction codes;Robustness;Routing;Monitoring;Data security', 'ID': u'5062044', u'booktitle': u'IEEE INFOCOM 2009'}
{u'doi': u'10.1109/INFOCOM.2018.8485848', u'author': u'C. He and X. Feng', u'title': u'POMP: Protocol Oblivious SDN Programming with Automatic Multi-Table Pipelining', 'ENTRYTYPE': u'inproceedings', u'abstract': u'SDN programming has been challenging because programmers have to not only implement the control logic, but also handle low-level details such as the generation of flow tables and the communication between the controller and switches. New generation of SDN with protocol oblivious forwarding and multi-table pipelining introduces even more low-level details to consider. We propose POMP, the first SDN programming environment supporting both protocol oblivious forwarding and automatic multi-table pipelining. POMP applies the static taint analysis technique to automatically infer compact and efficient multi-table pipelines from a data-plane agnostic network policy written by the programmer. The runtime system tracks the execution of the network policy, and automatically generates table entries. POMP also introduces a novel notion of dependent labels in the taint analysis, which, combined with the runtime information of the network policy, can further reduce the number of table entries. Like P4, POMP supports protocol-oblivious programming by providing a network protocol specification language. Parsers of packets can be automatically generated based on the protocol specification. POMP supports two main emerging SDN platforms, POF and P4, therefore network policies written in POMP are portable over any switches supporting POF or P4.', u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'998-1006', u'year': u'2018', u'keywords': u'formal logic;protocols;software defined networking;specification languages;table entries;POMP;network protocol specification language;protocol oblivious SDN programming;control logic;flow tables;SDN programming environment;static taint analysis technique;data-plane agnostic network policy;SDN platforms;automatic multi-table pipelining;Protocols;Pipeline processing;Runtime;Programming;Optical fibers;Control systems;Pipelines', 'ID': u'8485848', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8485848', u'author': u'C. He and X. Feng', u'title': u'POMP: Protocol Oblivious SDN Programming with Automatic Multi-Table Pipelining', 'ENTRYTYPE': u'inproceedings', u'abstract': u'SDN programming has been challenging because programmers have to not only implement the control logic, but also handle low-level details such as the generation of flow tables and the communication between the controller and switches. New generation of SDN with protocol oblivious forwarding and multi-table pipelining introduces even more low-level details to consider. We propose POMP, the first SDN programming environment supporting both protocol oblivious forwarding and automatic multi-table pipelining. POMP applies the static taint analysis technique to automatically infer compact and efficient multi-table pipelines from a data-plane agnostic network policy written by the programmer. The runtime system tracks the execution of the network policy, and automatically generates table entries. POMP also introduces a novel notion of dependent labels in the taint analysis, which, combined with the runtime information of the network policy, can further reduce the number of table entries. Like P4, POMP supports protocol-oblivious programming by providing a network protocol specification language. Parsers of packets can be automatically generated based on the protocol specification. POMP supports two main emerging SDN platforms, POF and P4, therefore network policies written in POMP are portable over any switches supporting POF or P4.', u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'998-1006', u'year': u'2018', u'keywords': u'formal logic;protocols;software defined networking;specification languages;table entries;POMP;network protocol specification language;protocol oblivious SDN programming;control logic;flow tables;SDN programming environment;static taint analysis technique;data-plane agnostic network policy;SDN platforms;automatic multi-table pipelining;Protocols;Pipeline processing;Runtime;Programming;Optical fibers;Control systems;Pipelines', 'ID': u'8485848', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8485948', u'author': u'G. Grigoryan and Y. Liu and M. Leczinsky and J. Li', u'title': u'VeriTable: Fast Equivalence Verification of Multiple Large Forwarding Tables', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Due to network practices such as traffic engineering and multi-homing, the number of routes-also known as IP prefixes-in the global forwarding tables has been increasing significantly in the last decade and continues growing in a super linear trend. One of the most promising solutions is to use smart Forwarding Information Base (FIB) aggregation algorithms to aggregate the prefixes and convert a large table into a small one. Doing so poses a research question, however, i.e., how can we quickly verify that the original table yields the same forwarding behaviors as the aggregated one? We answer this question in this paper, including addressing the challenges caused by the longest prefix matching (LPM) lookups. In particular, we propose the VeriTable algorithm that can employ a single tree/trie traversal to quickly check if multiple forwarding tables are forwarding equivalent, as well as if they could result in routing loops or black holes. The VeriTable algorithm significantly outperforms the state-of-the-art work for both IPv4 and IPv6 tables in every aspect, including the total running time, memory access times and memory consumption.', u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'621-629', u'year': u'2018', u'keywords': u'computer networks;IP networks;telecommunication network routing;telecommunication traffic;trees (mathematics);fast equivalence verification;multiple large Forwarding tables;network practices;traffic engineering;multihoming;VeriTable algorithm;IPv6 tables;Forwarding Information Base aggregation algorithms;Routing;IP networks;Routing protocols;Binary trees;Memory management;Engines;Heuristic algorithms', 'ID': u'8485948', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
