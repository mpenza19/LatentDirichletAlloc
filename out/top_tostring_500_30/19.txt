Design and performance of a Web server accelerator. search engines; cache storage; application program interfaces; operating systems (computers); Web server accelerator; performance; server design; embedded operating system; data caching; PowerPC 604; throughput; optimized communications stack; hit rates; updated caches; API; application programs; static data; dynamic data; SPECweb96 benchmark; hit ratios; 200 MHz; Web server; Acceleration; Operating systems; Application software; Web pages; Hardware; Throughput; Data analysis; Performance analysis; Software performance. We describe the design, implementation and performance of a Web server accelerator which runs on an embedded operating system and improves Web server performance by caching data. The accelerator resides in front of one or more Web servers. Our accelerator can serve up to 5000 pages/second from its cache on a 200 MHz PowerPC 604. This throughput is an order of magnitude higher than that which would be achieved by a high-performance Web server running on similar hardware under a conventional operating system such as Unix or NT. The superior performance of our system results in part from its highly optimized communications stack. In order to maximize hit rates and maintain updated caches, our accelerator provides an API which allows application programs to explicitly add, delete, and update cached data. The API allows our accelerator to cache dynamic as well as static data, analyze the SPECweb96 benchmark, and show that the accelerator can provide high hit ratios and excellent performance for workloads similar to this benchmark.

DotSlash: Handling Web Hotspots at Dynamic Content Web Sites. Web server; Databases; Lamps; Java; Information retrieval; Prototypes; Benchmark testing; Service oriented architecture; Web pages; Computer science. 

A resilient replication method in distributed database systems. concurrency control; data integrity; distributed databases; recovery methods; resilient replication method; distributed database systems; multiversion technique; concurrency; storage; communication overhead; read-only sites; Database systems; Availability; Fault tolerant systems; Concurrent computing; Delay; Computer science; Computer networks; Distributed computing; Communication networks; Computer errors. A scheme is presented for maintaining consistency and improving resiliency of replicated data in distributed systems. A multiversion technique is used to increase the degree of concurrency. To reduce storage requirement and communication overhead, multiple versions are maintained only at read-only sites. Recovery methods for replicated data in distributed systems are also discussed.

DotSlash: handling Web hotspots at dynamic content Web sites. electronic mail; hobby computing; data handling; Web sites; computer network reliability; file servers; safety systems; content management; DotSlash; Web hotspot handling; scalable rescue system; dynamic content Web site; load migration; database server; LAMP configuration; RUBBoS bulletin board benchmark; Web server; Databases; Lamps; Java; Information retrieval; Prototypes; Benchmark testing; Web pages; Service oriented architecture; Computer science. We propose DotSlash, a self-configuring and scalable rescue system, for handling web hotspots at dynamic content Web sites. To support load migration for dynamic content, an origin Web server sets up needed rescue servers drafted from other Web sites on the fly, and those rescue servers retrieve the scripts dynamically from the origin Web server, cache the scripts locally, and access the corresponding database server directly. We have implemented a prototype of DotSlash for the LAMP configuration, and tested our implementation using the RUBBoS bulletin board benchmark. Experiments show that by using DotSlash a dynamic content web site can completely remove its web server bottleneck, and can support a request rate constrained only by the capacity of its database server.

A scalable system for consistently caching dynamic Web data. information resources; search engines; cache storage; sport; entertainment; graph theory; scalable system; dynamic Web data caching; data update propagation; data dependence information; cached objects; graph traversal algorithms; Web site; 1998 Olympic Winter Games; cache hit rates; Web servers; Web server; Central Processing Unit; Web pages; File servers; Content management; Databases. This paper presents a new approach for consistently caching dynamic Web data in order to improve performance. Our algorithm, which we call data update propagation (DUP), maintains data dependence information between cached objects and the underlying data which affect their values in a graph. When the system becomes aware of a change to underlying data, graph traversal algorithms are applied to determine which cached objects are affected by the change. Cached objects which are found to be highly obsolete are then either invalidated or updated. The DUP was a critical component at the official Web site for the 1998 Olympic Winter Games. By using DUP, we were able to achieve cache hit rates close to 100% compared with 80% for an earlier version of our system which did not employ DUP. As a result of the high cache hit rates, the Olympic Games Web site was able to serve data quickly even during peak request periods.

Refreshment policies for Web content caches. Internet; cache storage; client-server systems; refreshment policies; Web content caches; expiration times; cache hits; stale copies; message size; latency; cache misses; functionality; latency-reducing mechanism; content availability; freshness; protocols; request patterns; frequency; recency; trace-based simulations; Network servers; Delay; Costs; Bandwidth; Cache storage; Web server; Prefetching; Protocols; Frequency; Web and internet services. Web content caches are often placed between end-users and origin servers as a mean to reduce server load, network usage, and ultimately, user-perceived latency. Cached objects typically have associated expiration times, after which they are considered stale and must be validated with a remote server (origin or another cache) before they can be sent to a client. A considerable fraction of cache hits involve stale copies that turned out to be current. These validations of current objects have small message size, but nonetheless, often induce latency comparable to full-fledged cache misses. Thus, the functionality of caches as a latency-reducing mechanism highly depends not only on content availability but also on its freshness. We propose policies for caches to preactively validate selected objects as they become stale, and thus allow for more client requests to be processed locally. Our policies operate within the existing protocols and exploit natural properties of request patterns such as frequency and recency. We evaluated and compared different policies using trace-based simulations.

Characterizing caching workload of a large commercial Content Delivery Network. cache storage; content management; Internet; large commercial content delivery network; Internet; CDN cache performance; performance metrics; disk load; network load; cache servers; caching workload analysis; geographically distributed CDN cache servers; cache replacement algorithms; CDN workload analysis; Streaming media; Servers; Measurement; Image coding; Internet; Aggregates; Social network services. Content Delivery Networks (CDNs) have emerged as a dominant mechanism to deliver content over the Internet. Despite their importance, to our best knowledge, large-scale analysis of CDN cache performance is lacking in prior literature. A CDN serves many content publishers simultaneously and thus has unique workload characteristics; it typically deals with extremely large content volume and high content diversity from multiple content publishers. CDNs also have unique performance metrics; other than hit ratio, CDNs also need to minimize network and disk load on cache servers. In this paper, we present measurement and analysis of caching workload at a large commercial CDN. Using detailed logs from four geographically distributed CDN cache servers, we analyze over 600 million content requests accounting for more than 1.3 petabytes worth of traffic. We analyze CDN workload from a wide range of perspectives, including request composition, size, popularity, and temporal dynamics. Using real-world logs, we also evaluate cache replacement algorithms, including two enhancements designed based on our CDN workload analysis: N-hit and content-aware caching. The results show that these enhancements achieve substantial performance gains in terms of cache hit ratio, disk load, and origin traffic volume.

Class-based cache management for dynamic Web content. Internet; cache storage; file servers; data structures; client-server systems; class-based cache management; dynamic Web content; server site; server resource demands; dynamic page caching; proxy sites; coarse-grain cache management; dynamic pages; URL patterns; page identification; data dependence; lazy invalidation; slow disk accesses; digest format; precomputing; stale pages; load peaks; data structure; eager invalidation; Cachuma; server response times; Content management; Uniform resource locators; Intrusion detection; Network servers; Web server; Prefetching; Pattern matching; Computer science; Data structures; Web page design. Caching dynamic pages at a server site is beneficial in reducing server resource demands and it also helps dynamic page caching at proxy sites. Previous work has used fine-grain dependence graphs among individual dynamic pages and underlying data sets to enforce result consistency. This paper proposes a complementary solution for applications that require coarse-grain cache management. The key idea is to partition dynamic pages into classes based on URL patterns so that an application can specify page identification and data dependence, and invoke invalidation for a class of dynamic pages. To make this scheme time-efficient with small space requirement, lazy invalidation is used to minimize slow disk accesses when IDs of dynamic pages are stored in memory with a digest format. Selective precomputing is further proposed to refresh stale pages and smoothen load peaks. A data structure is developed for efficient URL class searching during lazy or eager invalidation. This paper also presents design and implementation of a caching system called Cachuma which integrates the above techniques, runs in tandem with standard Web servers, and allows Web sites to add dynamic page caching capability with minimal changes. The experimental results show that the proposed techniques are effective in supporting coarse-grain cache management and reducing server response times for tested applications.

Estimation of DNS Source and Cache Dynamics under Interval-Censored Age Sampling. cache storage; peer-to-peer computing; probability; query processing; data-churn rates; interval-censored age sampling; query latency; inter-update distribution; query arrival rate; DNS records; general update/download processes; integral performance measure; freshness; cache models; dynamic DNS; retrieved records; cache hit rate; source bandwidth; client caches; record eviction; authoritative domain server; TTL-based replication scheme; cache dynamics; DNS source; Servers; Internet; Delays; Conferences; Random variables; Frequency control. Since inception, DNS has used a TTL-based replication scheme that allows the source (i.e., an authoritative domain server) to control the frequency of record eviction from client caches. Existing studies of DNS predominantly focus on reducing query latency and source bandwidth, both of which are optimized by increasing the cache hit rate. However, this causes less-frequent contacts with the source and results in higher staleness of retrieved records. Given high data-churn rates at certain providers (e.g., dynamic DNS, CDNs) and importance of consistency to their clients, we propose that cache models include the probability of freshness as an integral performance measure. We derive this metric under general update/download processes and present a novel framework for measuring its value using remote observation (i.e., without access to the source or the cache). Besides freshness, our methods can estimate the inter-update distribution of DNS records, cache hit rate, distribution of TTL, and query arrival rate from other clients. Furthermore, these algorithms do not require any changes to the existing infrastructure/protocols.

Estimation of DNS Source and Cache Dynamics under Interval-Censored Age Sampling. cache storage; peer-to-peer computing; probability; query processing; data-churn rates; interval-censored age sampling; query latency; inter-update distribution; query arrival rate; DNS records; general update/download processes; integral performance measure; freshness; cache models; dynamic DNS; retrieved records; cache hit rate; source bandwidth; client caches; record eviction; authoritative domain server; TTL-based replication scheme; cache dynamics; DNS source; Servers; Internet; Delays; Conferences; Random variables; Frequency control. Since inception, DNS has used a TTL-based replication scheme that allows the source (i.e., an authoritative domain server) to control the frequency of record eviction from client caches. Existing studies of DNS predominantly focus on reducing query latency and source bandwidth, both of which are optimized by increasing the cache hit rate. However, this causes less-frequent contacts with the source and results in higher staleness of retrieved records. Given high data-churn rates at certain providers (e.g., dynamic DNS, CDNs) and importance of consistency to their clients, we propose that cache models include the probability of freshness as an integral performance measure. We derive this metric under general update/download processes and present a novel framework for measuring its value using remote observation (i.e., without access to the source or the cache). Besides freshness, our methods can estimate the inter-update distribution of DNS records, cache hit rate, distribution of TTL, and query arrival rate from other clients. Furthermore, these algorithms do not require any changes to the existing infrastructure/protocols.

