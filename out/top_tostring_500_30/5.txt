Email as a Master Key: Analyzing Account Recovery in the Wild. authorisation; data protection; electronic mail; message authentication; Web sites; registered email account; user passwords; email-based account recovery attack; account authentication; recovery protocols; email service providers; user email accounts; compromised email accounts; Secure Email Account Recovery; email security; websites; account recovery email protection; Electronic mail; Password; Protocols; Authentication; Tools; Google. Account recovery (usually through a password reset) on many websites has mainly relied on accessibility to a registered email due to its favorable deployability and usability. However, it makes a user's online accounts vulnerable to a single point of failure when the registered email account is compromised. While previous research focuses on strengthening user passwords, the security risk imposed by email-based account recovery has not yet been well studied. In this paper, we investigate the possibility of mounting an email-based account recovery attack. Specifically, we examine the account authentication and recovery protocols in 239 traffic-heavy websites, confirming that most of them use emails for account recovery. We further scrutinize the security policy of major email service providers and show that a significant portion of them take no or marginal effort to protect user email accounts, leaving compromised email accounts readily available for mounting account recovery attacks. Then, we conduct case studies to assess potential losses caused by such attacks. Finally, we propose a lightweight email security enhancement called Secure Email Account Recovery (SEAR) to defend against account recovery attacks as an extra layer of protection to account recovery emails.

Email as a Master Key: Analyzing Account Recovery in the Wild. authorisation; data protection; electronic mail; message authentication; Web sites; registered email account; user passwords; email-based account recovery attack; account authentication; recovery protocols; email service providers; user email accounts; compromised email accounts; Secure Email Account Recovery; email security; websites; account recovery email protection; Electronic mail; Password; Protocols; Authentication; Tools; Google. Account recovery (usually through a password reset) on many websites has mainly relied on accessibility to a registered email due to its favorable deployability and usability. However, it makes a user's online accounts vulnerable to a single point of failure when the registered email account is compromised. While previous research focuses on strengthening user passwords, the security risk imposed by email-based account recovery has not yet been well studied. In this paper, we investigate the possibility of mounting an email-based account recovery attack. Specifically, we examine the account authentication and recovery protocols in 239 traffic-heavy websites, confirming that most of them use emails for account recovery. We further scrutinize the security policy of major email service providers and show that a significant portion of them take no or marginal effort to protect user email accounts, leaving compromised email accounts readily available for mounting account recovery attacks. Then, we conduct case studies to assess potential losses caused by such attacks. Finally, we propose a lightweight email security enhancement called Secure Email Account Recovery (SEAR) to defend against account recovery attacks as an extra layer of protection to account recovery emails.

Curbing Timeouts for TCP-Incast in Data Centers via A Cross-Layer Faster Recovery Mechanism. computer centres; transport protocols; curbing timeouts; cross-layer faster recovery mechanism; TCP-incast performance; TCP minimum RTO mechanism; index terms-data center networks; congestion window; Internet scale; timely retransmitted ACK; T-RACK; end-host NIC; FCT distribution; index terms-data center; kernel module; virtualized multitenant public data centers; long retransmission timeout; packet losses; Data centers; Packet loss; Microsoft Windows; Delays; Probes; Computational efficiency. We first study, at a microscopic level, the effects of various types of packet losses on TCP performance in a small data center. Then based on the findings we propose a simple recovery mechanism to combat the drawbacks of the long retransmission timeout. We emphasize through our empirical study that packet losses that occur at the tail of short-lived flows a nd/or bursty losses that span a large fraction of the congestion window are frequent in data center networks; and, in most cases, especially for short-lived flows, they result in a loss recovery that incurs waiting for a long retransmission timeout (RTO). The negative effect of frequent RTOs on the FCT is dramatic, yet recovery via RTO is merely a symptom of the pathological design of TCP's minimum RTO mechanism (set by default to the Internet scale). We propose the so-called Timely Retransmitted ACKs (T-RACKs), a very simple recovery mechanism for data centers, implemented as a shim layer between the virtual machines layer and the end-host NIC, to bridge the gap between TCP's huge RTO and the actual round trip times experienced in the data center. Compared to alternative solutions such as DCTCP, our T-RACKS has the virtue of not requiring any modification to TCP, which makes it readily deployable in virtualized multi-tenant public data centers. Experimental results show considerable improvements in the FCT distribution. Index Terms-Data Center, Cross Layer, Fast Recovery, Kernel Module, TCP-Incast, Timeouts.

Curbing Timeouts for TCP-Incast in Data Centers via A Cross-Layer Faster Recovery Mechanism. computer centres; transport protocols; curbing timeouts; cross-layer faster recovery mechanism; TCP-incast performance; TCP minimum RTO mechanism; index terms-data center networks; congestion window; Internet scale; timely retransmitted ACK; T-RACK; end-host NIC; FCT distribution; index terms-data center; kernel module; virtualized multitenant public data centers; long retransmission timeout; packet losses; Data centers; Packet loss; Microsoft Windows; Delays; Probes; Computational efficiency. We first study, at a microscopic level, the effects of various types of packet losses on TCP performance in a small data center. Then based on the findings we propose a simple recovery mechanism to combat the drawbacks of the long retransmission timeout. We emphasize through our empirical study that packet losses that occur at the tail of short-lived flows a nd/or bursty losses that span a large fraction of the congestion window are frequent in data center networks; and, in most cases, especially for short-lived flows, they result in a loss recovery that incurs waiting for a long retransmission timeout (RTO). The negative effect of frequent RTOs on the FCT is dramatic, yet recovery via RTO is merely a symptom of the pathological design of TCP's minimum RTO mechanism (set by default to the Internet scale). We propose the so-called Timely Retransmitted ACKs (T-RACKs), a very simple recovery mechanism for data centers, implemented as a shim layer between the virtual machines layer and the end-host NIC, to bridge the gap between TCP's huge RTO and the actual round trip times experienced in the data center. Compared to alternative solutions such as DCTCP, our T-RACKS has the virtue of not requiring any modification to TCP, which makes it readily deployable in virtualized multi-tenant public data centers. Experimental results show considerable improvements in the FCT distribution. Index Terms-Data Center, Cross Layer, Fast Recovery, Kernel Module, TCP-Incast, Timeouts.

TCP Vegas revisited. transport protocols; software performance evaluation; telecommunication congestion control; telecommunication traffic; TCP Vegas; TCP Reno; performance evaluation; slow start; congestion recovery; congestion avoidance mechanism; throughput; fairness problems; round trip time; Throughput; Performance gain; Computer science; Protocols; Time measurement; Technological innovation; Bandwidth; Loss measurement. The innovative techniques of TCP Vegas have been the subject of much debate in recent years. Several studies have reported that TCP Vegas provides better performance than TCP Reno. However, the question of which of the new techniques are responsible for the impressive performance gains remains unanswered so far. This paper presents a detailed performance evaluation of TCP Vegas. By decomposing TCP Vegas into the various novel mechanisms proposed and assessing the effect of each of these mechanisms on performance, we show that the reported performance gains are achieved primarily by TCP Vegas's new techniques for slow start and congestion recovery. TCP Vegas's innovative congestion avoidance mechanism is shown to have only a minor influence on throughput. Furthermore, we find that the congestion avoidance mechanism exhibits fairness problems even if all competing connections operate with the same round trip time.

Techniques for optimizing CORBA middleware for distributed embedded systems. distributed object management; client-server systems; embedded systems; telecommunication computing; program compilers; network operating systems; protocols; land mobile radio; distributed embedded systems; CORBA middleware optimisation; real-time operating systems; Inferno Windows CE; EPOC; Palm OS; mobile communication applications; electronic mail; Internet browsing; network management; standard middleware components; cost reduction; cycle time reduction; protocol engine; IDL compiler; TAO; stubs; interpretive marshalling; performance; skeletons; IDL data types; small footprint; efficiency; object services; code size; Middleware; Skeleton; Embedded system; Communication industry; Electronics industry; Real time systems; Operating systems; Mobile communication; Electronic mail; IP networks. The distributed embedded systems industry is poised to leverage emerging real-time operating systems, such as Inferno Windows CE, EPOC, and Palm OS to support mobile communication applications, such as electronic mail, Internet browsing, and network management. Ideally, these applications can be developed using standard middleware components like CORBA to improve their quality and reduce their cost and cycle time. However, stringent constraints on memory available in embedded systems imposes a severe limit on the footprint of CORBA middleware. This paper provides three contributions to the study and design of small footprint, embedded CORBA middleware. First, we describe the optimizations used to develop the protocol engine and CORBA IDL compiler provided by TAO, which is our real-time CORBA implementation. TAO's IDL compiler produces stubs that can use either compiled and/or interpretive marshalling. Second, we compare the performance and footprint of TAO IDL compiler-generated stubs and skeletons that use compiled and/or interpretive marshalling for a wide range of IDL data types. Third, we illustrate the benefits of the small footprint and efficiency of TAO IDL compiler-generated stubs and skeletons for CORBA object services implemented using TAO. The results comparing the performance of the compiled and interpretive stubs and skeletons indicate that the interpretive stubs and skeletons perform between 75-100% of the compiled stubs and skeletons for a wide range of data types. However the code size for the interpreted stubs and skeletons was between 26-45% and 50-80% of the compiled stubs and skeletons, respectively. These results indicate a positive step towards implementing high performance, small footprint middleware for distributed embedded systems.

Measurement and analysis of global IP-usage patterns of fast-flux botnets. IP networks; global IP-usage patterns; fast-flux botnets; malicious domains; benign domains; DNS probing engine; DIGGER; IP management strategy; IP networks; Servers; Continents; Recruitment; Monitoring; Computers; Indexes. This paper considers the global IP-usage patterns exhibited by different types of malicious and benign domains, with a focus on single and double fast-flux domains. We have developed and deployed a lightweight DNS probing engine, called DIGGER, on 240 PlanetLab nodes spanning 4 continents. Collecting DNS data for over 3.5 months on a plethora of domains, our global vantage points enabled us to identify distinguishing behavioral features between them based on their DNS-query results. To help us analyze the enormous amount of data, we have quantified these features and designed an effective classifier capable of accurately discriminating between different types of domains. Applying the classifier on the 3.5-month DNS data allows us to reveal the relative prevalence of different fast-flux domains and conduct detailed studies on them separately. These results provide insight into the current global state of fast-flux botnets and their range in implementation, revealing potential trends for botnet-based services. We also uncover previously-unseen domains whose name servers alone demonstrate fast-flux behavior and a new, cautious IP management strategy currently employed by criminals to evade detection.

An analysis of Internet inter-domain topology and route stability. telecommunication network routing; network topology; Internet; stability; Internet inter-domain topology; route stability; Internet routing fabric; single commercial entity; inter-domain routing traces; inter-domain topology; Internet growth; four-level hierarchy; Internet domains; connectivity; distribution of paths; route availability; mean reachability duration; Topology; Stability analysis; Fabrics; Routing protocols; Spine; Availability; Degradation; Access protocols; Web and internet services. The Internet routing fabric is partitioned into several domains. Each domain represents a region of the fabric administered by a single commercial entity. Over the past two years, the routing fabric has experienced significant growth. From more than a year's worth of inter-domain routing traces, we analyze the Internet inter-domain topology, its route stability behavior, and the effect of growth on these characteristics. Our analysis reveals several interesting results. Despite growth, the degree distribution and the diameter of the inter-domain topology have remained relatively unchanged. Furthermore, there exists a four-level hierarchy of Internet domains classified by degree. However, connectivity between domains is significantly non-hierarchical. Despite increased connectivity at higher levels in the topology, the distribution of paths to prefixes from the backbone remained relatively unchanged. There is evidence that both route availability and the mean reachability duration have degraded with Internet growth.

Unveiling the adoption and cascading process of OSN-based gifting applications. social networking (online); over online social networks; OSN-based gifting applications; adoption process; cascading process; user invitations; detailed large-scale dataset; popular Facebook gifting application; iHeart; good predictors; ultimate cascade sizes; Hugged; Facebook; Predictive models; Twitter; Conferences; Computers; Electronic mail; Analytical models. This paper demystifies the adoption and cascading process of OSN-based applications that grow via user invitations. We analyze a detailed large-scale dataset of a popular Facebook gifting application, iHeart, that contains more than 2 billion entries of user activities generated by 190 million users during a span of 64 weeks. We investigate: (1) how users invite their friends to an OSN-based application, (2) what factors drive the cascading process of application adoptions, and (3) what are the good predictors of the ultimate cascade sizes. We find that sending or receiving a large number of invitations does not necessarily help to recruit new users to iHeart. We also identify a set of distinctive features that are good predictors of the growth of the application adoptions in terms of final population size. Finally, based on the insights learned from our analyses, we propose a prediction model to infer whether a cascade of application adoption will continue to grow in the future based on observing the initial adoption process. Results show our proposed model can achieve high precision (over 80%) in iHeart as well as in another OSN-based gifting application, Hugged.

Inferring TCP connection characteristics through passive measurements. Internet; telecommunication congestion control; transport protocols; TCP connection; passive measurements methodology; senders congestion window; round trip time; end-user-perceived network; concurrent active measurements; Tier-1 network provider; Internet; Time measurement; IP networks; Throughput; Communication system traffic control; Transport protocols; TCPIP; Web and internet services; Peer to peer computing; Monitoring; Spine. We propose a passive measurement methodology to infer and keep track of the values of two important variables associated with a TCP connection: the sender's congestion window (cwnd) and the connection round trip time (RTT). Together, these variables provide a valuable diagnostic of end-user-perceived network performance. Our methodology is validated via both simulation and concurrent active measurements, and is shown to be able to handle various flavors of TCP. Given our passive approach and measurement points within a Tier-1 network provider, we are able to analyze more than 10 million connections, with senders located in more than 45% of the autonomous systems in today's Internet. Our results indicate that sender throughput is frequently limited by a lack of data to send, that the TCP congestion control flavor often has minimal impact on throughput, and that the vast majority of connections do not experience significant variations in RTT during their lifetime

