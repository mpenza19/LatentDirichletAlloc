{u'doi': u'10.1109/INFOCOM.2006.268', u'author': u'M. Karsten', u'title': u'SI-WF lt;sup gt;2 lt;/sup gt;Q: WF lt;sup gt;2 lt;/sup gt;Q Approximation with Small Constant Execution Overhead', 'ENTRYTYPE': u'inproceedings', u'abstract': '', u'issn': u'0743-166X', u'number': '', u'month': u'April', u'volume': '', u'pages': u'1-12', u'year': u'2006', u'keywords': u'Scheduling algorithm;Global Positioning System;Delay;Data structures;Processor scheduling;Costs;Traffic control;Approximation algorithms;Computer science;Telecommunication traffic', 'ID': u'4146921', u'booktitle': u'Proceedings IEEE INFOCOM 2006. 25TH IEEE International Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8485969', u'author': u'T. Zhou and B. Xiao and Z. Cai and M. Xu and X. Liu', u'title': u'From Uncertain Photos to Certain Coverage: a Novel Photo Selection Approach to Mobile Crowdsensing', 'ENTRYTYPE': u'inproceedings', u'abstract': u"Traditional mobile crowdsensing photo selection process focuses on selecting photos from participants to a server. The server may contain tons of photos for a certain area. A new problem is how to select a set of photos from the server to a smartphone user when the user requests to view an area (e.g., a hot spot). The challenge of the new problem is that the photo set should attain both photo coverage and view quality (e.g., with clear Points of Interest). However, contributions of these geo-tagged photos could be uncertain for a target area due to unavailable information of photo shooting direction and no reference photos. In this paper, we propose a novel and generic server-to-requester photo selection approach. Our approach leverages a utility measure to quantify the contribution of a photo set, where photos' spatial distribution and visual correlation are jointly exploited to evaluate their performance on photo coverage and view quality. Finding the photo set with the maximum utility is proven to be NP-hard. We then propose an approximation algorithm based on a greedy strategy with rigorous theoretical analysis. The effectiveness of our approach is demonstrated with real-world datasets. The results show that the proposal outperforms other approaches with much higher photo coverage and better view quality.", u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'1979-1987', u'year': u'2018', u'keywords': u'approximation theory;greedy algorithms;information retrieval;mobile computing;smart phones;photo set;geo-tagged photos;photo shooting direction;generic server-to-requester photo selection approach;photo selection approach;mobile crowdsensing photo selection process;photo coverage;approximation algorithm;greedy strategy;Servers;Sensors;Correlation;Visualization;Mobile handsets;Uncertainty;Cameras', 'ID': u'8485969', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2018.8485969', u'author': u'T. Zhou and B. Xiao and Z. Cai and M. Xu and X. Liu', u'title': u'From Uncertain Photos to Certain Coverage: a Novel Photo Selection Approach to Mobile Crowdsensing', 'ENTRYTYPE': u'inproceedings', u'abstract': u"Traditional mobile crowdsensing photo selection process focuses on selecting photos from participants to a server. The server may contain tons of photos for a certain area. A new problem is how to select a set of photos from the server to a smartphone user when the user requests to view an area (e.g., a hot spot). The challenge of the new problem is that the photo set should attain both photo coverage and view quality (e.g., with clear Points of Interest). However, contributions of these geo-tagged photos could be uncertain for a target area due to unavailable information of photo shooting direction and no reference photos. In this paper, we propose a novel and generic server-to-requester photo selection approach. Our approach leverages a utility measure to quantify the contribution of a photo set, where photos' spatial distribution and visual correlation are jointly exploited to evaluate their performance on photo coverage and view quality. Finding the photo set with the maximum utility is proven to be NP-hard. We then propose an approximation algorithm based on a greedy strategy with rigorous theoretical analysis. The effectiveness of our approach is demonstrated with real-world datasets. The results show that the proposal outperforms other approaches with much higher photo coverage and better view quality.", u'issn': '', u'number': '', u'month': u'April', u'volume': '', u'pages': u'1979-1987', u'year': u'2018', u'keywords': u'approximation theory;greedy algorithms;information retrieval;mobile computing;smart phones;photo set;geo-tagged photos;photo shooting direction;generic server-to-requester photo selection approach;photo selection approach;mobile crowdsensing photo selection process;photo coverage;approximation algorithm;greedy strategy;Servers;Sensors;Correlation;Visualization;Mobile handsets;Uncertainty;Cameras', 'ID': u'8485969', u'booktitle': u'IEEE INFOCOM 2018 - IEEE Conference on Computer Communications'}
{u'doi': u'10.1109/INFCOM.2009.5061911', u'author': u'E. Alessandria and M. Gallo and E. Leonardi and M. Mellia and M. Meo', u'title': u'P2P-TV Systems under Adverse Network Conditions: A Measurement Study', 'ENTRYTYPE': u'inproceedings', u'abstract': u'In this paper we define a simple experimental setup to analyze the behavior of commercial P2P-TV applications under adverse network conditions. Our goal is to reveal the ability of different P2P-TV applications to adapt to dynamically changing conditions, such as delay, loss and available capacity, e.g., checking whether such systems implement some form of congestion control. We apply our methodology to four popular commercial P2P-TV applications: PPLive, SOPCast, TVants and TVUPlayer. Our results show that all the considered applications are in general capable to cope with packet losses and to react to congestion arising in the network core. Indeed, all applications keep trying to download data by avoiding bad paths and carefully selecting good peers. However, when the bottleneck affects all peers, e.g., it is at the access link, their behavior results rather aggressive, and potentially harmful for both other applications and the network.', u'issn': u'0743-166X', u'number': '', u'month': u'April', u'volume': '', u'pages': u'100-108', u'year': u'2009', u'keywords': u'digital television;peer-to-peer computing;ubiquitous computing;video streaming;P2P-TV system;adverse network condition;commercial P2P-TV application;congestion control;PPLive;SOPCast;TVants;TVUPlayer;Streaming media;Internet;Costs;Bandwidth;Delay;Peer to peer computing;Network servers;Web server;Telecommunication traffic;Testing', 'ID': u'5061911', u'booktitle': u'IEEE INFOCOM 2009'}
{u'doi': u'10.1109/INFOCOM.2008.300', u'author': u'P. K. Manna and S. Ranka and S. Chen', u'title': u'DAWN: A Novel Strategy for Detecting ASCII Worms in Networks', 'ENTRYTYPE': u'inproceedings', u'abstract': u'While a considerable amount of research has been done for detecting the binary worms exploiting the vulnerability of buffer overflow, very little effort has been spent in detecting worms that consist of only text, Le., printable ASCII characters. We show that the existing worm detectors often either do not examine the ASCII stream or are not well suited to efficiently detect worms in the ASCII stream due to the structural properties of the ASCII payload. In this paper, we analyze the potentials and constraints of the ASCII worms vis-a-vis their binary counterpart, and devise a detection technique that would exploit those limitations. We introduce DAWN, a novel ASCII worm detection strategy that is fast, easily deployable, and has very little overhead. Unlike many signature-based detection methods, DAWN is completely signature-free and therefore capable of detecting zero-day outbreak of ASCII worms.', u'issn': u'0743-166X', u'number': '', u'month': u'April', u'volume': '', u'pages': u'2315-2323', u'year': u'2008', u'keywords': u'computer networks;digital signatures;security of data;telecommunication security;ASCII worm detection;binary worm;DAWN;signature-based detection method;zero-day outbreak detection;computer networks;Computer worms;Detectors;Payloads;Web server;Frequency;Communications Society;Computer networks;Information science;Buffer overflow;Computer security', 'ID': u'4509895', u'booktitle': u'IEEE INFOCOM 2008 - The 27th Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2008.242', u'author': u'H. Yu and R. Mahapatra', u'title': u'A Memory-Efficient Hashing by Multi-Predicate Bloom Filters for Packet Classification', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Hash tables (HTs) are poorly designed for multiple off-chip memory accesses during packet classification and critically affect throughput in high-speed routers. Therefore, an HT with fast on-chip memory and high-capacity off-chip memory for predictable lookup-throughput is desirable. Both a legacy HT (LHT) and a recently proposed fast HT (FHT) have the disadvantage of memory overhead due to pointers and duplicate items in linked lists. Also, memory usage for an FHT did not consider the bits in counters for fair comparison with an LHT. In this paper, we propose a novel hash architecture called a Multi-predicate Bloom-filtered HT (MBHT) using parallel Bloom filters and generating off-chip memory addresses in the base- 2<sup>x</sup> number system, xisin{1,2,hellip}, which removes the overhead of pointers. Using a larger base of number system, an MBHT reduces on-chip memory size by a factor of log<sub>2</sub> b<sub>2</sub>/ log<sub>2</sub> b<sub>1</sub> where b<sub>1</sub> and b<sub>2</sub> are bases of number system (b<sub>2</sub>&gt;b<sub>1</sub>). Compared to an FHT, the MBHT is approximately x(log<sub>2</sub> n + 4)/(2 log<sub>2</sub> n) times more efficient for on-chip memory, where n is the number of keys. This results in a significant reduction in the number of off- chip memory accesses. A simulation with a dataset of packets from NLANR shows the on-chip memory reductions by 1.7 and 2 times over an LHT and an FHT are made. Besides, an MBHT of base-16 needs less off-chip memory accesses by 2117 in total URL queries of NLANR, compared to an FHT.', u'issn': u'0743-166X', u'number': '', u'month': u'April', u'volume': '', u'pages': u'1795-1803', u'year': u'2008', u'keywords': u'data structures;memory architecture;pattern classification;storage allocation;table lookup;telecommunication network routing;memory-efficient hashing;multipredicate bloom-filtered HT;packet classification;hash tables;off-chip memory accesses;high-speed routers;parallel Bloom filters;number system;lookup-throughput;Filters;Uniform resource locators;Large-scale systems;Costs;Communications Society;Computer science;Throughput;Counting circuits;System-on-a-chip;Access control', 'ID': u'4509837', u'booktitle': u'IEEE INFOCOM 2008 - The 27th Conference on Computer Communications'}
{u'doi': u'10.1109/INFOCOM.2008.319', u'author': u'J. Bi and J. Wu and W. Zhang', u'title': u'A Trust and Reputation based Anti-SPIM Method', 'ENTRYTYPE': u'inproceedings', u'abstract': u"Instant Messaging (IM) service is a killer application in the Internet. Due to the problem of IM spam (SPIM), building an effective anti-spim method is an important research topic. At present, most of anti-spim solutions are based on email-spam prevention techniques, which are not directly applicable to anti-spim. In this paper, we present a new anti-spim method SpimRank, which integrates trust and reputation mechanisms with black-list technique. SpimRank also tracks user's historical action to deal with spim attacks in nearly real time, which is applicable to IM environment.", u'issn': u'0743-166X', u'number': '', u'month': u'April', u'volume': '', u'pages': u'2485-2493', u'year': u'2008', u'keywords': u'authorisation;information retrieval;Internet;unsolicited e-mail;instant messaging service;Internet;IM spam;reputation based anti-SPIM method;email-spam prevention techniques;SpimRank;trust mechanisms;black-list technique;Web and internet services;Filters;Communications Society;Bismuth;IP networks;Privacy;Web pages;Damping;Convergence;Algorithm design and analysis', 'ID': u'4509914', u'booktitle': u'IEEE INFOCOM 2008 - The 27th Conference on Computer Communications'}
{u'doi': u'10.1109/INFCOM.2009.5061978', u'author': u'K. -. Chen and C. -. Tu and W. -. Xiao', u'title': u'OneClick: A Framework for Measuring Network Quality of Experience', 'ENTRYTYPE': u'inproceedings', u'abstract': u"As the service requirements of network applications shift from high throughput to high media quality, interactivity, and responsiveness, the definition of QoE (Quality of Experience) has become multidimensional. Although it may not be difficult to measure individual dimensions of the QoE, how to capture users' overall perceptions when they are using network applications remains an open question. In this paper, we propose a framework called OneClick to capture users' perceptions when they are using network applications. The framework only requires a subject to click a dedicated key whenever he/she feels dissatisfied with the quality of the application in use. OneClick is particularly effective because it is intuitive, lightweight, efficient, time-aware, and application-independent. We use two objective quality assessment methods, PESQ and VQM, to validate OneClick's ability to evaluate the quality of audio and video clips. To demonstrate the proposed framework's efficiency and effectiveness in assessing user experiences, we implement it on two applications, one for instant messaging applications, and the other for first- person shooter games. A Flash implementation of the proposed framework is also presented.", u'issn': u'0743-166X', u'number': '', u'month': u'April', u'volume': '', u'pages': u'702-710', u'year': u'2009', u'keywords': u'computer games;electronic messaging;quality of service;OneClick;network experience quality;high throughput;high media quality;interactivity;responsiveness;QoE;quality of experience;objective quality assessment methods;PESQ;VQM;instant messaging applications;first-person shooter games;Flash;perceptual evaluation of speech quality model;video quality measurement model;Testing;Delay;Throughput;Multidimensional systems;Feedback;Communications Society;Information science;Computer science;Application software;Quality assessment', 'ID': u'5061978', u'booktitle': u'IEEE INFOCOM 2009'}
{u'doi': u'10.1109/INFCOM.2005.1498369', u'author': u'W. Chen and L. Clarke and J. Kurose and D. Towsley', u'title': u'Optimizing cost-sensitive trust-negotiation protocols', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Trust negotiation is a process that establishes mutual trust by the exchange of digital credentials and/or guiding policies among entities who may have no pre-existing knowledge about each other. Motivated by the desire to disclose as little sensitive information as possible in practice, this paper investigates the problem of minimizing the "cost" of the credentials exchanged by a trust-negotiation protocol. A credential or a policy is assigned a weighted cost, referred to as its sensitivity cost. We formalize an optimization problem, namely the minimum sensitivity cost problem, whose objective is to minimize the total sensitivity costs of the credentials and policies disclosed during trust negotiation. We study the complexity of the minimal sensitivity cost problem and propose algorithms to solve the problem efficiently, for the cases that policies are cost-sensitive and cost-insensitive. A simple finite state machine model of trust-negotiation protocols is presented to model various trust-negotiation protocols, and to provide a quantitative evaluation of the number of exchange rounds needed to achieve a successful negotiation, and the probability of achieving a successful negotiation under various credential disclosure strategies.', u'issn': u'0743-166X', u'number': '', u'month': u'March', u'volume': u'2', u'pages': u'1431-1442 vol. 2', u'year': u'2005', u'keywords': u'protocols;minimisation;finite state machines;authorisation;Internet;probability;digital credentials;guiding policies;trust-negotiation protocol;optimization problem;minimum sensitivity cost problem;minimization;finite state machine model;quantitative evaluation;probability;Protocols;Automata;Security;Telephony;Computer science;Cost function;Internet;Electronic commerce;Government;Employment', 'ID': u'1498369', u'booktitle': u'Proceedings IEEE 24th Annual Joint Conference of the IEEE Computer and Communications Societies.'}
{u'doi': u'10.1109/INFOCOM.2008.258', u'author': u'A. Banerjee and D. Barman and M. Faloutsos and L. N. Bhuyan', u'title': u'Cyber-Fraud is One Typo Away', 'ENTRYTYPE': u'inproceedings', u'abstract': u'Spelling errors when typing a URL can be exploited by website-squatters: users are led to phony sites in a phenomenon we call parasitic URL naming. These phony sites imitate popular websites and try to extract personal information from unsuspecting users, or simply advertise and sell products to users. In this paper, we conduct a massive study in order to quantify the extent of this parasitic URL naming We start with a corpus of 900 popular websites, which we refer to as original URLs, and generate roughly 3 million URLs by varying the original names systematically and exhaustively. Over a period of 60 days, we analyze how many sites have URLs very similar to our original URLs. We find that parasitic URL naming is a wide-spread problem and quantify the extent of this issue. We believe that this work will provide the first step towards research and tools to combat web-fraud.', u'issn': u'0743-166X', u'number': '', u'month': u'April', u'volume': '', u'pages': u'1939-1947', u'year': u'2008', u'keywords': u'computer crime;fraud;Web sites;cyber-fraud;URL;Website-squatters;phony sites;Uniform resource locators;Internet;Communications Society;Computer science;Computer errors;Data mining;Credit cards;Security;Portals;Costs', 'ID': u'4509853', u'booktitle': u'IEEE INFOCOM 2008 - The 27th Conference on Computer Communications'}
