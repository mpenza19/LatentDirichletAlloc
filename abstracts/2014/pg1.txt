@INPROCEEDINGS{6847918,
author={L. Gong and Y. Wen and Z. Zhu and T. Lee},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Toward profit-seeking virtual network embedding algorithm via global resource capacity},
year={2014},
volume={},
number={},
pages={1-9},
abstract={In this paper, after proposing a novel metric, i.e., global resource capacity (GRC), to quantify the embedding potential of each substrate node, we propose an efficient heuristic virtual network embedding (VNE) algorithm, called as GRC-VNE. The proposed algorithm aims to maximize the revenue and to minimize the cost of the infrastructure provider (InP). Based on GRC, the proposed algorithm applies a greedy load-balance manner to embed each virtual node sequentially, and then adopts the shortest path routing to embed each virtual link. Simulation results demonstrate that our proposed GRC-VNE algorithm achieves lower request blocking probability and higher revenue due to the more appropriate consideration of the resource distribution of the entire network, when compared to the two lastest VNE algorithms that also consider the resources of entire substrate network. Then, we introduce a classical reserved cloud revenue model, which consists of fixed revenue and variable one. Based on this revenue model, we design a novel admission control policy selectively accepting the VNR with high revenue-to-cost ratio to maximize the InP's profit based on an empirical threshold. Through extensive simulations, we observe that the optimal empirical threshold is proportional to the ratio of variable revenue to the fixed one.},
keywords={cloud computing;cost reduction;greedy algorithms;profitability;resource allocation;telecommunication network routing;virtual private networks;profit-seeking virtual network embedding algorithm;global resource capacity;heuristic virtual network embedding algorithm;GRC-VNE algorithm;infrastructure provider cost minimization;InP;greedy load-balancing;shortest path routing;virtual link;request blocking probability;resource distribution;substrate network;reserved cloud revenue model;admission control policy;revenue-to-cost ratio;Substrates;Indium phosphide;Computational modeling;Algorithm design and analysis;Vectors;Bandwidth;Computers;Network virtualization;Virtual network embedding (VNE);Global resource capacity (GRC);GRC-VNE;Reserved cloud revenue model;Admission policy},
doi={10.1109/INFOCOM.2014.6847918},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847919,
author={F. Hao and M. Kodialam and T. V. Lakshman and S. Mukherjee},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Online allocation of virtual machines in a distributed cloud},
year={2014},
volume={},
number={},
pages={10-18},
abstract={One of the primary functions of a cloud service provider is to allocate cloud resources to users upon request. Requests arrive in real-time and resource placement decisions must be made as and when a request arrives, without any prior knowledge of future arrivals. In addition, when a cloud service provider operates a geographically diversified cloud that consists of large number of small data centers, the resource allocation problem becomes even more complex. This is due to the fact that resource request can have additional constraints on data center location, service delay guarantee, etc. In this paper, we propose a generalized resource placement methodology that can work across different cloud architectures, resource request constraints, with real-time request arrivals and departures. The proposed algorithms are online in the sense that allocations are made without any knowledge of resource requests that arrive in the future, and the current resource allocations are made in such a manner as to permit the acceptance of as many future arrivals as possible. We derive worst case competitive ratio for the algorithms. We show through experiments and case studies the superior performance of the algorithms in practice.},
keywords={cloud computing;real-time systems;resource allocation;virtual machines;online virtual machine allocation;distributed cloud;cloud service provider;cloud resource allocation problem;resource placement decisions;geographically diversified cloud;data center location;resource placement methodology;cloud architectures;resource request constraints;real-time request arrivals;Resource management;Distributed databases;Upper bound;Conferences;Computers;Approximation algorithms;Approximation methods},
doi={10.1109/INFOCOM.2014.6847919},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847920,
author={Y. Peng and K. Chen and G. Wang and W. Bai and Z. Ma and L. Gu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={HadoopWatch: A first step towards comprehensive traffic forecasting in cloud computing},
year={2014},
volume={},
number={},
pages={19-27},
abstract={This paper presents our effort towards comprehensive traffic forecasting for big data applications using external, light-weighted file system monitoring. Our idea is motivated by the key observations that rich traffic demand information already exists in the log and meta-data files of many big data applications, and that such information can be readily extracted through run-time file system monitoring. As the first step, we use Hadoop as a concrete example to explore our methodology and develop a system called HadoopWatch to predict traffic demand of Hadoop applications. We further implement HadoopWatch in our real small-scale testbed with 10 physical servers and 30 virtual machines. Our experiments over a series of MapReduce applications demonstrate that HadoopWatch can forecast the traffic demand with almost 100% accuracy and time advance. Furthermore, it makes no modification of the Hadoop framework, and introduces little overhead to the application performance.},
keywords={Big Data;cloud computing;meta data;parallel programming;public domain software;telecommunication traffic;virtual machines;HadoopWatch;comprehensive traffic demand forecasting;cloud computing;Big Data applications;external-light-weighted file system monitoring;log files;meta-data files;information extraction;traffic demand prediction;real-small-scale testbed;physical servers;virtual machines;MapReduce applications;Monitoring;Forecasting;Big data;Writing;Conferences;Computers;Pipelines},
doi={10.1109/INFOCOM.2014.6847920},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847921,
author={L. Jiao and J. Lit and W. Du and X. Fu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Multi-objective data placement for multi-cloud socially aware services},
year={2014},
volume={},
number={},
pages={28-36},
abstract={Socially aware services often have a large user base and data of users have to be partitioned and replicated over multiple geographically distributed clouds. Choosing in which cloud to place data, however, is difficult. Effective data placements entail meeting multiple system objectives, including reducing the usage of cloud resources, providing good service quality to users, and even minimizing the carbon footprint, while facing critical challenges such as the interconnection of social data, the conflicting requirements of different objectives, and the customized multi-cloud data access policies. In this paper, we study multi-objective optimization for placing users' data over multiple clouds for socially aware services. We build a model framework that can accommodate a range of different objectives, and based on this model we formulate the optimization problem. Leveraging graph cuts, we propose an optimization approach that decomposes our original problem into two simpler subproblems and solves them alternately in multiple rounds. We carry out evaluations using a large group of real-world geographically distributed users with realistic interactions, and place users' data over 10 clouds all across the US. We demonstrate results that are significantly superior to standard and de facto methods in all objectives, and also show that our approach is capable of exploring trade-offs among objectives, converges fast and scales to a huge user base.},
keywords={cloud computing;graph theory;optimisation;resource allocation;social networking (online);multiobjective data placement;multicloud socially aware services;geographically distributed clouds;cloud resources;service quality;carbon footprint minimization;social data;customized multicloud data access policies;multiobjective optimization;graph cuts;real-world geographically distributed users;US;Carbon;Optimization;Data models;Distributed databases;Communities;Conferences;Computers},
doi={10.1109/INFOCOM.2014.6847921},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847922,
author={E. Novak and Q. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Near-pri: Private, proximity based location sharing},
year={2014},
volume={},
number={},
pages={37-45},
abstract={As the ubiquity of smartphones increases we see an increase in the popularity of location based services. Specifically, online social networks provide services such as alerting the user of friend co-location, and finding a user's k nearest neighbors. Location information is sensitive, which makes privacy a strong concern for location based systems like these. We have built one such service that allows two parties to share location information privately and securely. Our system allows every user to maintain and enforce their own policy. When one party, (Alice), queries the location of another party, (Bob), our system uses homomorphic encryption to test if Alice is within Bob's policy. If she is, Bob's location is shared with Alice only. If she is not, no user location information is shared with anyone. Due to the importance and sensitivity of location information, and the easily deployable design of our system, we offer a useful, practical, and important system to users. Our main contribution is a flexible, practical protocol for private proximity testing, a useful and efficient technique for representing location values, and a working implementation of the system we design in this paper. It is implemented as an Android application with the Facebook online social network used for communication between users.},
keywords={cryptography;mobile computing;smart phones;social networking (online);private proximity based location sharing;Near-Pri;smartphone ubiquity;location based services;location based systems;homomorphic encryption;user location information privacy;location information sensitivity;private proximity testing;location value representation;Android application;Facebook online social network;Lead;Protocols;Facebook;Polynomials;Privacy;Cryptography;Vegetation},
doi={10.1109/INFOCOM.2014.6847922},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847923,
author={Y. Yang and M. Jin and H. Wu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={3D surface localization with terrain model},
year={2014},
volume={},
number={},
pages={46-54},
abstract={The majority of current research on sensor network localization focuses on wireless sensor networks deployed on two dimensional (2D) plane or in three dimensional (3D) space, very few on 3D surface. However, many real world applications require large-scale sensor networks deployed on the surface of a complex 3D terrain. Compared with planar and 3D network localizations, surface network localization generates unique and fundamental hardness. In this research, we explore 3D surface network localization with terrain model. A digital terrain model (DTM), available to public with a variable resolution up to one meter, is a 3D representation of a terrain's surface. It is commonly built using remote sensing technology or from land surveying and can be easily converted to a triangular mesh. Given a sensor network deployed on the surface of a 3D terrain with one-hop distance information available, we can extract a triangular mesh from the connectivity graph of the network. The constraint that the sensors must be on the known 3D terrain's surface ensures that the triangular meshes of the network and the DTM of the terrain's surface approximate the same geometric shape and overlap. We propose a fully distributed algorithm to construct a well-aligned mapping between the two triangular meshes. Based on this mapping, each sensor node of the network can easily locate reference grid points from the DTM to calculate its own geographic location. We carry out extensive simulations under various scenarios to evaluate the overall performance of the proposed localization algorithm. We also discuss the possibility of 3D surface network localization with mere connectivity and the results are promising.},
keywords={terrain mapping;wireless sensor networks;3D surface localization;sensor network localization;wireless sensor networks;complex 3D terrain;3D network localizations;planar localizations;digital terrain model;DTM;variable resolution;terrain surface;remote sensing technology;triangular mesh;connectivity graph;distributed algorithm;Three-dimensional displays;Conformal mapping;Measurement;Surface topography;Shape;Face;Computers},
doi={10.1109/INFOCOM.2014.6847923},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847924,
author={Y. Tian and R. Gao and K. Bian and F. Ye and T. Wang and Y. Wang and X. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Towards ubiquitous indoor localization service leveraging environmental physical features},
year={2014},
volume={},
number={},
pages={55-63},
abstract={Mainstream indoor localization technologies rely on RF signatures that require extensive human efforts to measure and periodically re-calibrate. Although recent crowdsourcing based work has started to address the issue, incentives are still lacking for wide user adoption. Thus the progress to ubiquitous localization remains slow. In this paper, we explore an alternative approach that leverages environmental physical features such as store logos or wall posters. A user uses a smartphone to obtain relative position measurements to such static reference points for the system to triangulate the user location. We study the principle of such localization, determine the suitable sensor, and devise guidelines for the user to choose reference points for better accuracy. To enable fast deployment, we propose a lightweight site survey method for service providers to quickly estimate the coordinates of reference points. We incorporate and enhance image matching algorithms with a heuristic technique to automatically identify chosen reference points at high accuracy. Extensive experiments have shown that the prototype achieves 4-5m accuracy at 80-percentile, comparable to the industry state-of-the-art, while covering a 150×75m mall and 300×200m train station requires a one time investment of only 2-3 man-hours from service providers.},
keywords={image matching;indoor radio;smart phones;ubiquitous indoor localization service;environmental physical features;RF signature;smartphone;lightweight site survey method;image matching algorithm;heuristic technique;Accuracy;Gyroscopes;Compass;Position measurement;Conferences;Computers;IEEE 802.11 Standards},
doi={10.1109/INFOCOM.2014.6847924},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847925,
author={J. Liu and Z. Wang and Z. Peng and J. Cui and L. Fiondella},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Suave: Swarm underwater autonomous vehicle localization},
year={2014},
volume={},
number={},
pages={64-72},
abstract={Swarms of autonomous underwater vehicles (AUVs) forming mobile underwater networks often operate in moving currents, which introduce severe turbulence that interferes with coordinated and stealthy navigation of fleet. Therefore, individual AUV must adjust their heading whenever needed to ensure it can reach a pre-determined destination. To achieve accurate navigation, AUVs must maintain precise knowledge of their locations. This paper develops the “Suave” (Swarm underwater autonomous vehicle localization) algorithm to localize swarms of AUVs operating in rough waters. The purpose of Suave is to ensure that all AUVs arrive at their destinations by preserving localization throughout the entire mission. Suave lowers the probability that an AUV swarm is detected by reducing the number of occasions that vehicles must surface to obtain accurate location information from external sources such as satellites. The Suave algorithm also achieves better energy conservation through improved control of localization reference messages. Simulations show Suave significantly improves localization accuracy, lowers energy consumption, and the probability of swarm detection.},
keywords={autonomous underwater vehicles;probability;swarm intelligence;autonomous underwater vehicles;mobile underwater networks;moving currents;turbulence;coordinated navigation;stealthy navigation;predetermined destination;Suave algorithm;swarm underwater autonomous vehicle localization algorithm;rough waters;AUV swarm;location information;localization reference messages;swarm detection probability;Vehicles;Uncertainty;Accuracy;Computers;Navigation;Conferences;Acoustics},
doi={10.1109/INFOCOM.2014.6847925},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847926,
author={H. Cui and C. Luo and C. W. Chen and F. Wu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Robust uncoded video transmission over wireless fast fading channel},
year={2014},
volume={},
number={},
pages={73-81},
abstract={This research studies robust uncoded video transmission over wireless fast fading channel, where only statistical channel state information (CSI) is available at the transmitter. We observe that increasing channel diversity for high priority (HP) data is essential to improving the robustness of video transmission in fading channels. By utilizing the noise and loss resilient nature of video, we find it possible to design a more robust system by re-allocating the power and channel uses among HP and LP (low priority) data. With total power and channel use constraints, we derive an optimal resource allocation scheme under the squared error distortion criterion. In particular, we first propose a new power allocation algorithm at given channel allocation. Second, based on the proposed power allocation algorithm, we design a channel allocation algorithm to strike the tradeoff between the diversity increase of HP data and the information loss of LP data. Third, under known noise power distribution, we derive the optimal resource allocation for uncoded video multicast. Simulations show that the proposed system achieves 2dB and 5dB gain in average and outage PSNR over Softcast in video unicast, and around 1.4dB and 4dB gain in multicast.},
keywords={channel allocation;fading channels;resource allocation;statistical analysis;video communication;robust uncoded video transmission;wireless fast fading channel;statistical channel state information;CSI;transmitter;high priority data;low priority data;channel use constraints;total power constraints;optimal resource allocation scheme;squared error distortion criterion;power allocation algorithm;channel allocation algorithm;LP data;information loss;HP data;noise power distribution;uncoded video multicast transmission;Softcast;PSNR;gain 2 dB;gain 5 dB;Resource management;Channel allocation;Noise;Discrete cosine transforms;Rayleigh channels;Conferences},
doi={10.1109/INFOCOM.2014.6847926},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847927,
author={V. Joseph and G. de Veciana},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={NOVA: QoE-driven optimization of DASH-based video delivery in networks},
year={2014},
volume={},
number={},
pages={82-90},
abstract={We consider the problem of optimizing video delivery for a network supporting video clients streaming stored video. Specifically, we consider the joint optimization of network resource allocation and video quality adaptation. Our objective is to fairly maximize video clients' Quality of Experience (QoE) realizing tradeoffs among the mean quality, temporal variability in quality, and fairness, incorporating user preferences on rebuffering and cost of video delivery. We present a simple asymptotically optimal online algorithm, NOVA, to solve the problem. NOVA is asynchronous, and using minimal communication, distributes the tasks of resource allocation to network controller, and quality adaptation to respective video clients. Video quality adaptation in NOVA is also optimal for standalone video clients, and is well suited for use in the DASH framework. Further, NOVA can be extended for use with more general QoE models, networks shared with other traffic loads and networks using fixed/legacy resource allocation.},
keywords={hypermedia;optimisation;quality of experience;radio networks;resource allocation;telecommunication traffic;video streaming;dynamic adaptive streaming over HTTP;supporting video clients streaming;network optimization for video adaptation;fixed-legacy resource allocation;traffic loads;DASH framework;network controller;minimal communication;simple asymptotically optimal online algorithm;video delivery cost;rebuffering;temporal variability;mean quality;tradeoffs;quality of experience;video client maximization;video quality adaptation optimization;network resource allocation optimization;DASH-based video delivery optimization;QoE-driven optimization;NOVA;Streaming media;Resource management;Optimization;Computers;Conferences;Video recording;Quality assessment},
doi={10.1109/INFOCOM.2014.6847927},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847928,
author={Z. Wang and L. Sun and C. Wu and W. Zhu and S. Yang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Joint online transcoding and geo-distributed delivery for dynamic adaptive streaming},
year={2014},
volume={},
number={},
pages={91-99},
abstract={Dynamic adaptive video streaming has emerged as a popular approach for video streaming in today's Internet. To date the two important components in dynamic adaptive streaming, video transcoding which generates the adaptive bitrates of a video and video delivery which streams the videos to users, have been separately studied, resulting in a huge waste of computation and storage resource due to transcoding useless videos and suboptimal streaming quality due to homogeneous video replication. In this paper, we propose to jointly perform video transcoding and video delivery for adaptive streaming in an online manner. We conduct extensive measurement studies of a video sharing system and a CDN to motivate our design. We formulate and solve optimization problems to enable high streaming quality for the users, and low computation and replication costs for the system. In particular, our design connects video transcoding and video delivery based on users' preferences of CDN regions and regional preferences of video versions. Extensive trace-driven experiments further confirm the superiority of our design.},
keywords={Internet;optimisation;video coding;video streaming;optimization problems;CDN;video sharing system;suboptimal streaming quality;storage resource;video transcoding;Internet;dynamic adaptive video streaming;geodistributed video delivery;joint online transcoding;Servers;Streaming media;Transcoding;Bandwidth;Bit rate;Joints;Availability},
doi={10.1109/INFOCOM.2014.6847928},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847929,
author={R. Bhatia and T. V. Lakshman and A. Netravali and K. Sabnani},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Improving mobile video streaming with link aware scheduling and client caches},
year={2014},
volume={},
number={},
pages={100-108},
abstract={The rapid growth in multimedia traffic is straining mobile networks thus necessitating the need for efficient content delivery mechanisms. In this paper we present the design and analysis of a scheme for streaming non-live, pre-recorded content (e.g. Video on Demand) that opportunistically takes advantage of the “slow fading” variations in the wireless link quality. The proposed scheme works by selectively sending more content to sessions at times when they have better link quality while providing sufficient rate guarantees to keep their buffers from under-flowing. We establish analytically that the performance of such scheme is within two times that of any optimal scheme and that it results in throughput gains, per user and aggregate, that increase in proportion to the number of streaming users. Our performance evaluations indicate that by exploiting slow time-varying channels the streaming capacity can more than double with significant benefits to the users at the edge of the cell.},
keywords={cache storage;mobile radio;radio links;scheduling;telecommunication traffic;time-varying channels;video streaming;mobile video streaming;link aware scheduling;client caches;multimedia traffic;mobile networks;content delivery mechanisms;wireless link quality;time-varying channels;Fading;Streaming media;Data transfer;Throughput;Mobile communication;Aggregates;Computer architecture},
doi={10.1109/INFOCOM.2014.6847929},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847930,
author={D. Yang and X. Zhang and G. Xue},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={PROMISE: A framework for truthful and profit maximizing spectrum double auctions},
year={2014},
volume={},
number={},
pages={109-117},
abstract={Auctions provide a platform for licensed spectrum users to trade their underutilized spectrum with unlicensed users. Existing spectrum auctions either do not apply to the scenarios where multiple sellers and buyers both make offers, or assume the knowledge of the users' valuation distribution for maximizing the profit of the auction. To fill this void, we design PROMISE, a framework for spectrum double auctions, which jointly considers spectrum reusability, truthfulness, and profit maximization without the distribution knowledge. We propose a novel technique, called cross extraction, to compute the bid representing a group of secondary users, who can share a common channel. We prove that PROMISE is computationally efficient, individual-rational, and truthful. In addition, PROMISE is guaranteed to achieve an approximate profit of the optimal auction.},
keywords={radio spectrum management;PROMISE framework;licensed spectrum users;spectrum double auctions;spectrum reusability;truthfulness;profit maximization;cross extraction;optimal auction;Vectors;Cost accounting;Resource management;Silicon;Approximation methods;Conferences;Computers},
doi={10.1109/INFOCOM.2014.6847930},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847931,
author={J. Zhao and H. Li and C. Wu and Z. Li and Z. Zhang and F. C. M. Lau},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Dynamic pricing and profit maximization for the cloud with geo-distributed data centers},
year={2014},
volume={},
number={},
pages={118-126},
abstract={Cloud providers often choose to operate datacenters over a large geographic span, in order that users may be served by resources in their proximity. Due to time and spatial diversities in utility prices and operational costs, different datacenters typically have disparate charges for the same services. Cloud users are free to choose the datacenters to run their jobs, based on a joint consideration of monetary charges and quality of service. A fundamental problem with significant economic implications is how the cloud should price its datacenter resources at different locations, such that its overall profit is maximized. The challenge escalates when dynamic resource pricing is allowed and long-term profit maximization is pursued. We design an efficient online algorithm for dynamic pricing of VM resources across datacenters in a geo-distributed cloud, together with job scheduling and server provisioning in each datacenter, to maximize the profit of the cloud provider over a long run. Theoretical analysis shows that our algorithm can schedule jobs within their respective deadlines, while achieving a time-average overall profit closely approaching the offline maximum, which is computed by assuming that perfect information on future job arrivals are freely available. Empirical studies further verify the efficacy of our online profit maximizing algorithm.},
keywords={cloud computing;computer centres;optimisation;pricing;profitability;quality of service;scheduling;dynamic pricing;geo-distributed data centers;cloud providers;spatial diversities;time diversities;utility prices;operational costs;cloud users;monetary charges;quality of service;datacenter resources;dynamic resource pricing;long-term profit maximization;online algorithm;dynamic VM resource pricing;geo-distributed cloud;job scheduling;online profit maximizing algorithm;Servers;Heuristic algorithms;Delays;Optimization;Algorithm design and analysis;Pricing;Dynamic scheduling},
doi={10.1109/INFOCOM.2014.6847931},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847932,
author={T. Luo and H. Tan and L. Xia},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Profit-maximizing incentive for participatory sensing},
year={2014},
volume={},
number={},
pages={127-135},
abstract={We design an incentive mechanism based on all-pay auctions for participatory sensing. The organizer (principal) aims to attract a high amount of contribution from participating users (agents) while at the same time lowering his payout, which we formulate as a profit-maximization problem. We use a contribution-dependent prize function in an environment that is specifically tailored to participatory sensing, namely incomplete information (with information asymmetry), risk-averse agents, and stochastic population. We derive the optimal prize function that induces the maximum profit for the principal, while satisfying strict individual rationality (i.e., strictly have incentive to participate at equilibrium) for both risk-neutral and weakly risk-averse agents. The thus induced profit is demonstrated to be higher than the maximum profit induced by constant (yet optimized) prize. We also show that our results are readily extensible to cases of risk-neutral agents and deterministic populations.},
keywords={electronic commerce;incentive schemes;optimisation;profit-maximizing incentive;participatory sensing;all-pay auctions;contribution-dependent prize function;information asymmetry;stochastic population;risk-neutral agents;weakly risk-averse agents;Sensors;Sociology;Statistics;Bayes methods;Games;Standards;Conferences;Mechanism design;Bayesian game;all-pay auction;perturbation analysis;network economics;crowdsensing},
doi={10.1109/INFOCOM.2014.6847932},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847933,
author={X. Zou and H. Li and Y. Sui and W. Peng and F. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Assurable, transparent, and mutual restraining e-voting involving multiple conflicting parties},
year={2014},
volume={},
number={},
pages={136-144},
abstract={E-voting techniques and systems have not been widely accepted and deployed by society due to various concerns and problems. One particular issue associated with many existing e-voting techniques is the lack of transparency, leading to the failure to deliver voter assurance. In this work, we propose an assurable, transparent, and mutual restraining e-voting protocol that exploits the existing two-party political dynamics in the US. The proposed e-voting protocol consists of three original technical contributions - universal verifiable voting vector, forward and backward mutual lock voting, and in-process check and enforcement - that, in combination, resolves the apparent conflicts in voting such as anonymity vs. accountability and privacy vs. verifiability. Especially, the trust is split equally among tallying authorities who have conflicting interests and will technically restrain each other. The voting and tallying processes are transparent to voters and any third party, which allow any voter to verify that his vote is indeed counted and also allow any third party to audit the tally.},
keywords={data privacy;government data processing;protocols;E-voting techniques;E-voting systems;universal verifiable voting vector;anonymity;privacy verifiability;transparency;mutual restraining voting;voter assurance;voting processes;tallying processes;backward mutual lock voting;forward mutual lock voting;political Parties;mutual restraining e-voting protocol;multiple conflicting parties;Protocols;Vectors;Cryptography;Electronic voting;Privacy;Computers;Casting;E-voting;Verifiability;Anonymity;Privacy;Assurance;Transparency;Mutual restraining voting},
doi={10.1109/INFOCOM.2014.6847933},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847934,
author={L. Xie and H. Han and Q. Li and J. Wu and S. Lu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Efficiently collecting histograms over RFID tags},
year={2014},
volume={},
number={},
pages={145-153},
abstract={Collecting histograms over RFID tags is an essential premise for effective aggregate queries and analysis in large-scale RFID-based applications. In this paper we consider efficient collection of histograms from the massive number of RFID tags without the need to read all tag data. We first consider the problem of basic histogram collection and propose an efficient algorithm based on the idea of ensemble sampling. We further consider the problems of advanced histogram collection, respectively, with an iceberg query and a top-k query. Efficient algorithms are proposed to tackle the above problems such that the qualified/unqualified categories can be quickly identified. Experiment results indicate that our ensemble sampling-based solutions can achieve a much better performance than the basic estimation/identification schemes.},
keywords={radiofrequency identification;sampling methods;sea ice;histogram efficient collection;RFID tags;effective aggregate queries;large-scale RFID-based applications;tag data;iceberg query;top-k query;qualified-unqualified categories;ensemble sampling-based solutions;estimation-identification schemes;efficient algorithm;Histograms;Accuracy;Sociology;RFID tags;Protocols;Algorithms;RFID;Time efficiency;Histogram},
doi={10.1109/INFOCOM.2014.6847934},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847935,
author={L. Kong and L. He and Y. Gu and M. Wu and T. He},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A Parallel Identification Protocol for RFID systems},
year={2014},
volume={},
number={},
pages={154-162},
abstract={Nowadays, RFID systems have been widely deployed for applications such as supply chain management and inventory control. One of their most essential operations is to swiftly identify individual tags to distinguish their associated objects. Most existing solutions identify tags sequentially in the temporal dimension to avoid signal collisions, whose performance degrades significantly as the system scale increases. In this paper, we propose a <i>Parallel Identification Protocol</i> (PIP) for RFID systems, which achieves the parallel identification paradigm and is compatible with current RFID devices. Uniquely, PIP encodes the tag ID into a specially designed pattern and thus greatly facilitates the reader to correctly and effectively recover them from collisions. Furthermore, we analytically investigate its performance and provide guidance on determining its optimal settings. Extensive simulations show that PIP reduces the identification delay by about 25%-50% when compared with the standard method in EPC C1G2 and the state-of-the-art solutions.},
keywords={encoding;protocols;radiofrequency identification;telecommunication congestion control;parallel identification protocol;supply chain management;inventory control;individual tag identification;temporal dimension;signal collision avoidance;PIP;RFID device compatibility;optimal setting;identification delay reduction;EPC C1G2;radio frequency identification;Radiofrequency identification;Protocols;Encoding;Delays;Conferences;Computers;Standards},
doi={10.1109/INFOCOM.2014.6847935},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847936,
author={J. Liu and B. Xiao and K. Bu and L. Chen},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Efficient distributed query processing in large RFID-enabled supply chains},
year={2014},
volume={},
number={},
pages={163-171},
abstract={Radio Frequency Identification (RFID) has dramatically streamlined supply chain management by automatically monitoring and tracking commodities. Considering the proliferation of RFID data volume, distributed storage is more applicable and scalable than centralized storage for distributed query processing. Traditional distributed RFID data storage requires each distribution center to locally store raw RFID data, leading to data redundancy, storage and query inefficiency. In this paper, we design an efficient distributed storage model by leveraging Bloom filters to save storage space and improve query efficiency. Meanwhile, we establish corresponding query processing schemes to locally support existence queries and path queries, which are two kinds of most popular queries in the supply chain management. A local query can be completed with constant time complexity regardless of data volume. Experiments demonstrate that our storage model outperforms the traditional one in terms of both space and time efficiency.},
keywords={data structures;monitoring;query processing;radiofrequency identification;supply chain management;distributed query processing;RFID;radio frequency identification;supply chain management;commodities monitoring;commodities tracking;distributed storage;Bloom filters;Supply chains;Radiofrequency identification;Query processing;Accuracy;Data models;Distributed databases;Memory},
doi={10.1109/INFOCOM.2014.6847936},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847937,
author={M. Chen and S. Chen and Q. Xiao},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Pandaka: A lightweight cipher for RFID systems},
year={2014},
volume={},
number={},
pages={172-180},
abstract={The ubiquitous use of RFID tags raises concern about potential security risks in RFID systems. Because low-cost tags are extremely resource-constrained devices, common security mechanisms adopted in resource-rich equipment such as computers are no longer applicable to them. Hence, one challenging research topic is to design a lightweight cipher that is suitable for low-cost RFID tags. Traditional cryptography generally assumes that the two communicating parties are equipotent entities. In contrast, there is a large capability gap between readers and tags in RFID systems. We observe that the readers, which are much more powerful, should take more responsibility in RFID cryptographic protocols. In this paper, we make a radical shift from traditional cryptography, and design a novel cipher called Pandaka<sup>1</sup>, in which most workload is pushed to the readers. As a result, Pandaka is particularly hardware-efficient for tags. We perform extensive simulations to evaluate the effectiveness of Pandaka. In addition, we present security analysis of Pandaka facing different attacks.},
keywords={cryptographic protocols;radiofrequency identification;telecommunication security;Pandaka security analysis;lightweight cipher;RFID systems;security risks;resource-constrained devices;security mechanisms;resource-rich equipment;low-cost RFID tags;RFID cryptographic protocols;Ciphers;Radiofrequency identification;Computers;Servers;Indexes},
doi={10.1109/INFOCOM.2014.6847937},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847938,
author={G. Rossini and D. Rossi and M. Garetto and E. Leonardi},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Multi-Terabyte and multi-Gbps information centric routers},
year={2014},
volume={},
number={},
pages={181-189},
abstract={One of the main research directions along which the future Internet is evolving can be identified in the paradigmatic shift from a network of hosts toward a network of caches. Yet, several questions remain concerning the scalability of individual algorithms (e.g., name based lookup and routing) and components (e.g., caches) of these novel Information Centric Networking (ICN) architectures. Exploiting a peculiar characteristics of ICN (i.e., the fact that contents are split in chunks), and the nature of video streaming (which dominates Internet traffic), this paper proposes a novel two-layers caching scheme that allows multi-Terabyte caches to sustain content streaming at multi-Gbps speed. We model the system as an extension, to the case of chunked contents, of the well known Che approximation, that has the advantage of being very simple and accurate at the same time. Simulations under synthetic and realistic trace-driven traffic confirm the accuracy of the analysis and the feasibility of the proposed architecture.},
keywords={cache storage;computer networks;telecommunication network routing;video streaming;multiterabyte router;multigigabit per second router;information centric router;future Internet;paradigmatic shift;information centric networking;video streaming;two layer caching;multiterabyte cache;content streaming;Che approximation;trace driven traffic;Streaming media;Random access memory;Indexes;Prefetching;Catalogs;Computer architecture;Aggregates},
doi={10.1109/INFOCOM.2014.6847938},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847939,
author={T. Pan and T. Zhang and J. Shi and Y. Li and L. Jin and F. Li and J. Yang and B. Zhang and B. Liu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Towards zero-time wakeup of line cards in power-aware routers},
year={2014},
volume={},
number={},
pages={190-198},
abstract={As the network infrastructure has been consuming more and more power, various schemes have been proposed to improve power efficiency of network devices. Many schemes put links to sleep when idle and wake them up when needed. A presumption in these schemes, though, is that router's line cards can be waken up quickly. However, through systematic measurement of a major vender's high-end router, we find that it takes minutes to get a line card ready under the current implementation. To address this issue, we propose a new line card design that (1) keeps the host processor in a line card always up, which only consumes a small fraction of power, and (2) downloads a slim slot of popular prefixes with higher priority, so that the line card will be ready for forwarding most of the traffic much earlier. We design algorithms that ensure fast and correct longest prefix match lookup during prioritized routing prefix download. Experiments on real hardware show that the wakeup time can be reduced to 127.27ms, which is 0.3% of the original line card wakeup time, well supporting many power-saving schemes.},
keywords={telecommunication network routing;telecommunication power management;zero-time wakeup;power-aware routers;network infrastructure;network devices;systematic measurement;high-end router;line card design;traffic;routing prefix download;power-saving schemes;line card wakeup time;Routing;Routing protocols;Hardware;Conferences;Computers;Power supplies;Fabrics},
doi={10.1109/INFOCOM.2014.6847939},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847940,
author={S. Vissicchio and L. Vanbever and L. Cittadini and G. G. Xie and O. Bonaventure},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Safe routing reconfigurations with route redistribution},
year={2014},
volume={},
number={},
pages={199-207},
abstract={Simultaneously providing flexibility, evolvability and correctness of routing is one of the basic and still unsolved problems in networking. Route redistribution provides a tool, used in many enterprise networks, to either partition a network into multiple routing domains or merge previously independent networks. However, no general technique exists for changing a live network's route redistribution configuration without incurring packet losses and service disruptions. In this paper, we study the problem of how to safely transition between route redistribution configurations. We investigate what anomalies may occur in the reconfiguration process, showing that many long-lasting forwarding loops can and do occur if naive techniques are applied. We devise new sufficient conditions for anomaly-free reconfigurations, and we leverage them to build provably safe and practical reconfiguration procedures. Our procedures enable seamless network re-organizations to accomplish both short-term objectives, such as local repair or traffic engineering, and long-term requirement changes.},
keywords={telecommunication network routing;traffic engineering computing;safe routing reconfigurations;route redistribution;packet losses;service disruptions;traffic engineering;Routing;Routing protocols;Network topology;Topology;Conferences;Computers},
doi={10.1109/INFOCOM.2014.6847940},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847941,
author={S. Yang and M. Xu and D. Wang and G. Bayzelon and J. Wu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Scalable forwarding tables for supporting flexible policies in enterprise networks},
year={2014},
volume={},
number={},
pages={208-216},
abstract={With increasing demands for more flexible services, the routing policies in enterprise network becomes much richer. This has placed a heavy burden to the current router forwarding plane to support the increasing number of policies, primarily due to the limited capacity in TCAM. This hinders the development of new network services. In this paper, we present the design and implementation of a new forwarding table structure. It separates the functions of TCAM and SRAM and maximally utilizes the large & flexible SRAM. We progressively design a set of schemes, to maintain correctness, compress storage, and achieve line-card speeds. We also design incremental update algorithms that bring less accesses to memory. We present implementation designs and evaluate our scheme with a real implementation on a commercial router using real data sets. Our design does not require new devices. The evaluation results show that the performance of our forwarding tables is promising.},
keywords={business communication;content-addressable storage;data compression;SRAM chips;telecommunication network routing;scalable forwarding table structure;flexible policies;enterprise networks;routing policies;router forwarding plane;TCAM;SRAM;correctness maintenance;storage compression;line-card speeds;incremental update algorithms;Indexes;Random access memory;Routing;Hardware;Pipeline processing;Field programmable gate arrays;Conferences},
doi={10.1109/INFOCOM.2014.6847941},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847942,
author={Q. Wang and M. Liu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Learning in hide-and-seek},
year={2014},
volume={},
number={},
pages={217-225},
abstract={Existing work on pursuit-evasion problems typically either assumes stationary or heuristic behavior of one side and examines countermeasures of the other, or assumes both sides to be strategic which leads to a game theoretical framework. Results from the former may lack robustness against changes in the adversarial behavior, while those from the latter are often difficult to justify due to the implied full information (either as realizations or as distributions) and rationality, both of which may be limited in practice. In this paper, we take a different approach by assuming an intelligent pursuer/evader that is adaptive to the information available to it and is capable of learning over time with performance guarantee. Within this context we investigate two cases. In the first case we assume either the evader or the pursuer is aware of the type of learning algorithm used by the opponent, while in the second case neither side has such information and thus must try to learn. We show that the optimal policies in the first case have a greedy nature, hiding/seeking in the location that the opponent is the least/most likely to appear. This result is then used to assess the performance of the learning algorithms that both sides employ in the second case, which is shown to be mutually optimal and there is no loss for either side compared to the case when it completely knows the adaptive pattern used by the adversary and responses optimally.},
keywords={game theory;learning (artificial intelligence);hide-and-seek;pursuit-evasion problems;game theoretical framework;adversarial behavior;intelligent pursuer-evader;performance guarantee;learning algorithm;optimal policies;greedy nature;adaptive pattern;Adaptation models;Conferences;Computers;Games;Probabilistic logic;Context;Computational modeling},
doi={10.1109/INFOCOM.2014.6847942},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847943,
author={W. Sun and S. Yu and W. Lou and Y. T. Hou and H. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Protecting your right: Attribute-based keyword search with fine-grained owner-enforced search authorization in the cloud},
year={2014},
volume={},
number={},
pages={226-234},
abstract={Search over encrypted data is a critically important enabling technique in cloud computing, where encryption-before-outsourcing is a fundamental solution to protecting user data privacy in the untrusted cloud server environment. Many secure search schemes have been focusing on the single-contributor scenario, where the outsourced dataset or the secure searchable index of the dataset are encrypted and managed by a single owner, typically based on symmetric cryptography. In this paper, we focus on a different yet more challenging scenario where the outsourced dataset can be contributed from multiple owners and are searchable by multiple users, i.e. multi-user multi-contributor case. Inspired by attribute-based encryption (ABE), we present the first attribute-based keyword search scheme with efficient user revocation (ABKS-UR) that enables scalable fine-grained (i.e. file-level) search authorization. Our scheme allows multiple owners to encrypt and outsource their data to the cloud server independently. Users can generate their own search capabilities without relying on an always online trusted authority. Fine-grained search authorization is also implemented by the owner-enforced access policy on the index of each file. Further, by incorporating proxy re-encryption and lazy re-encryption techniques, we are able to delegate heavy system update workload during user revocation to the resourceful semi-trusted cloud server. We formalize the security definition and prove the proposed ABKS-UR scheme selectively secure against chosen-keyword attack. Finally, performance evaluation shows the efficiency of our scheme.},
keywords={authorisation;cloud computing;cryptography;data privacy;information retrieval;trusted computing;searchable index;encryption;symmetric cryptography;attribute-based encryption;ABE;user revocation;ABKS-UR scheme;always online trusted authority;owner-enforced access policy;proxy re-encryption technique;lazy re-encryption technique;resourceful semi-trusted cloud server;security definition;chosen-keyword attack;single-contributor search scenario;cloud server environment;data privacy;encryption-before-outsourcing;cloud computing;fine-grained owner-enforced search authorization;attribute-based keyword search;Indexes;Authorization;Keyword search;Encryption;Servers;Data privacy},
doi={10.1109/INFOCOM.2014.6847943},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847944,
author={R. Pal and L. Golubchik and K. Psounis and P. Hui},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Will cyber-insurance improve network security? A market analysis},
year={2014},
volume={},
number={},
pages={235-243},
abstract={Recent work in security has illustrated that solutions aimed at detection and elimination of security threats alone are unlikely to result in a robust cyberspace. As an orthogonal approach to mitigating security problems, some have pursued the use of cyber-insurance as a suitable risk management technique. Such an approach has the potential to jointly align with the incentives of security vendors (e.g., Symantec, Microsoft, etc.), cyber-insurers (e.g., ISPs, cloud providers, security vendors, etc.), regulatory agencies (e.g., government), and network users (individuals and organizations), in turn paving the way for comprehensive and robust cyber-security mechanisms. To this end, in this work, we are motivated by the following important question: can cyber-insurance really improve the security in a network? To address this question, we adopt a market-based approach. Specifically, we analyze regulated monopolistic and competitive cyber-insurance markets, where the market elements consist of risk-averse cyber-insurers, risk-averse network users, a regulatory agency, and security vendors. Our results show that (i) without contract discrimination amongst users, there always exists a unique market equilibrium for both market types, but the equilibrium is inefficient and does not improve network security, and (ii) in monopoly markets, contract discrimination amongst users results in a unique market equilibrium that is efficient, which in turn results in network security improvement - however, the cyber-insurer can make zero expected profits. The latter fact is often sufficient to de-incentivize the insurer to be a part of a market, and will eventually lead to its collapse. This fact also emphasizes the need for designing mechanisms that incentivize the insurer to permanently be part of the market.},
keywords={computer network security;insurance;monopoly;risk management;cyber-insurance improve network security problem;market analysis;security threats;robust cyberspace;orthogonal approach;risk management technique;security vendor incentives;cyber-insurers;ISPs;cloud providers;security vendors;regulatory agencies;network users;robust cyber-security mechanisms;market-based approach;regulated monopolistic cyber-insurance markets;competitive cyber-insurance markets;risk-averse cyber-insurers;risk-averse network users;monopoly markets;market equilibrium;Security;Insurance;Investment;Communication networks;Contracts;Equations;Nash equilibrium;security;cyber-insurance;market;equilibrium},
doi={10.1109/INFOCOM.2014.6847944},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847945,
author={J. Shao and R. Lu and X. Lin},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={FINE: A fine-grained privacy-preserving location-based service framework for mobile devices},
year={2014},
volume={},
number={},
pages={244-252},
abstract={In this paper, we propose a fine-grained privacy-preserving location-based service (LBS) framework, called FINE, for mobile devices. It adopts the data-as-a-service (DaaS) model, where the LBS provider publishes its data to a third party (e.g., cloud server) who executes users' LBS queries. The proposed FINE framework employs a ciphertext-policy anonymous attribute-based encryption technique to achieve fine-grained access control, location privacy, confidentiality of the LBS data and its access policy, and accurate LBS query result while without involving any trusted third party. Moreover, the proposed FINE framework also integrates the transformation key and proxy re-encryption to migrate most of computation-intensive tasks from the LBS provider and users to the cloud server. This property keeps mobile devices away from massive resource-consuming operations. Extensive analysis shows that our proposed FINE framework is secure and highly efficient for mobile devices in terms of computation and communication cost.},
keywords={authorisation;cryptography;mobile computing;fine-grained privacy-preserving location-based service framework;mobile devices;LBS framework;ciphertext-policy anonymous attribute-based encryption technique;fine-grained access control;location privacy;LBS data;FINE framework;data-as-a-service model;DaaS model;Servers;Mobile handsets;Encryption;Mobile radio mobility management;Access control},
doi={10.1109/INFOCOM.2014.6847945},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847946,
author={X. Xie and X. Zhang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Does full-duplex double the capacity of wireless networks?},
year={2014},
volume={},
number={},
pages={253-261},
abstract={Full-duplex has emerged as a new communication paradigm and is anticipated to double wireless capacity. Existing studies of full-duplex mainly focused on its PHY layer design, which enables bidirectional transmission between a single pair of nodes. In this paper, we establish an analytical framework to quantify the network-level capacity gain of full-duplex over half-duplex. Our analysis reveals that inter-link interference and spatial reuse substantially reduces full-duplex gain, rendering it well below 2 in common cases. More remarkably, the asymptotic gain approaches 1 when interference range approaches transmission range. Through a comparison between optimal half- and full-duplex MAC algorithms, we find that full-duplex's gain is further reduced when it is applied to CSMA based wireless networks. Our analysis provides important guidelines for designing full-duplex networks. In particular, network-level mechanisms such as spatial reuse and asynchronous contention must be carefully addressed in full-duplex based protocols, in order to translate full-duplex's PHY layer capacity gain into network throughput improvement.},
keywords={carrier sense multiple access;interference suppression;radiocommunication;wireless networks;communication paradigm;wireless capacity;PHY layer design;bidirectional transmission;network-level capacity;interlink interference;spatial reuse;asymptotic gain;interference range;transmission range;MAC algorithms;CSMA;full-duplex networks;network-level mechanisms;asynchronous contention;Interference;Throughput;Receivers;Lattices;Wireless networks;Radio transmitters;Multiaccess communication},
doi={10.1109/INFOCOM.2014.6847946},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847947,
author={Y. Yang and B. Chen and K. Srinivasan and N. B. Shroff},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Characterizing the achievable throughput in wireless networks with two active RF chains},
year={2014},
volume={},
number={},
pages={262-270},
abstract={Recent breakthroughs in wireless communication show that by using new signal processing techniques, a wireless node is capable of transmitting and receiving simultaneously on the same frequency band by activating both of its RF chains, thus achieving full-duplex communication and potentially doubling the link throughput. However, with two sets of RF chains, one can build a half-duplex multi-input and multi-output (MIMO) system that achieves the same gain. While this gain is the same between a pair of nodes, the gains are unclear when multiple nodes are involved, as in a general network. The key reason is that MIMO and full-duplex have different interference patterns. A MIMO transmission blocks transmissions around its receiver and receptions around its transmitter. A full-duplex bidirectional transmission blocks any transmission around the two communicating nodes, but allows a reception on one RF chain. Thus, in a general network, the requirements for the two technologies could result in potentially different achievable throughput regions. This work investigates the achievable throughput performance of MIMO, full-duplex and their variants that allow simultaneous activation of two RF chains. It is the first work of its kind to precisely characterize the conditions under which these technologies outperform each other for a general network topology under a binary interference model. The analytical results in this paper are validated using software-defined radios.},
keywords={MIMO communication;radio networks;software radio;achievable throughput;wireless network;active RF chain;wireless communication;signal processing technique;full duplex communication;multiple input multiple output system;MIMO system;full duplex bidirectional transmission;software defined radio;Radio frequency;MIMO;Throughput;Interference;Multiplexing;Wireless communication;Schedules},
doi={10.1109/INFOCOM.2014.6847947},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847948,
author={C. Han and K. Wu and Y. Wang and L. M. Ni},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={WiFall: Device-free fall detection by wireless networks},
year={2014},
volume={},
number={},
pages={271-279},
abstract={The world population is in the midst of a unique and irreversible process of aging. Fall, which is one of the major health threats and obstacles to independent living of elders, will aggravate the global pressure in elders' health care and injury rescue. Thus, automatic fall detection is highly in need. Current proposed fall detection systems either need hardware installation or disrupt people's daily life. These limitations make it hard to widely deploy fall detection systems in residential settings. In this work, we analyze the wireless signal propagation model considering human activities influence. We then propose a novel and truly unobtrusive detection method based on the advanced wireless technologies, which we call as WiFall. WiFall employs the time variability and special diversity of Channel State Information (CSI) as the indicator of human activities. As CSI is readily available in prevalent in-use wireless infrastructures, WiFall withdraws the need for hardware modification, environmental setup and worn or taken devices. We implement WiFall on laptops equipped with commercial 802.11n NICs. Two typical indoor scenarios and several layout schemes are examined. As demonstrated by the experimental results, WiFall yielded 87% detection precision with false alarm rate of 18% in average.},
keywords={biomedical communication;diversity reception;wireless LAN;WiFall;device-free fall detection;wireless networks;residential settings;wireless signal propagation model;channel state information;commercial 802.11n NIC;CSI;automatic fall detection;unobtrusive detection method;Wireless communication;Wireless sensor networks;Sensors;Noise;Indoor environments;Receivers;Conferences},
doi={10.1109/INFOCOM.2014.6847948},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847949,
author={H. Jang and S. Yun and J. Shin and Y. Yi},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Distributed learning for utility maximization over CSMA-based wireless multihop networks},
year={2014},
volume={},
number={},
pages={280-288},
abstract={Game-theoretic modeling and equilibrium analysis have provided valuable insights into the design of robust local control rules for the individual agents in multi-agent systems, e.g., Internet congestion control, road transportation networks, etc. In this paper, we introduce a non-cooperative MAC (Medium Access Control) game for wireless networks and propose new fully-distributed CSMA (Carrier Sense Multiple Access) learning algorithms that are probably optimal in the sense that their long-term throughputs converge to the optimal solution of a utility maximization problem over the maximum throughput region. The most significant part of our approach lies in introducing a novel cost function in agents' utilities so that the proposed game admits an ordinal potential function with (asymptotically) no price-of-anarchy. The game formulation naturally leads to known game-based learning rules to find a Nash equilibrium, but they are computationally inefficient and often require global information. Towards our goal of fully-distributed operation, we propose new fully-distributed learning algorithms by utilizing a unique property of CSMA that enables each link to estimate its temporary link throughput without message passing for the applied CSMA parameters. The proposed algorithms can be thought as `stochastic approximations' to the standard learning rules, which is a new feature in our work, not prevalent in other traditional game-theoretic approaches. We show that they converge to a Nash equilibrium, which is a utility-optimal point, numerically evaluate their performance to support our theoretical findings and further examine various features such as convergence speed and its tradeoff with efficiency.},
keywords={carrier sense multiple access;game theory;learning (artificial intelligence);multi-agent systems;wireless LAN;game-theoretic modeling;equilibrium analysis;robust local control rules;individual agents;multiagent systems;noncooperative MAC game;noncooperative medium access control game;wireless multihop networks;CSMA learning algorithms;carrier sense multiple access learning algorithms;utility maximization problem;cost function;ordinal potential function;game formulation;game-based learning rules;Nash equilibrium;fully-distributed learning algorithms;temporary link throughput;CSMA parameters;stochastic approximations;utility-optimal point;Games;Throughput;Multiaccess communication;Heuristic algorithms;Schedules;Message passing;Interference},
doi={10.1109/INFOCOM.2014.6847949},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847950,
author={Q. Zhang and M. F. Zhani and M. Jabri and R. Boutaba},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Venice: Reliable virtual data center embedding in clouds},
year={2014},
volume={},
number={},
pages={289-297},
abstract={Cloud computing has become a cost-effective model for deploying online services in recent years. To improve the Quality-of-Service (QoS) of the provisioned services, recently a number of proposals have advocated to provision both guaranteed server and network resources in the form of Virtual Data Centers (VDCs). However, existing VDC scheduling algorithms have not fully considered the reliability aspect of the allocations in terms of (1) hardware failure characteristics on which the service is hosted, and (2) the impact of individual failures on service availability, given the dependencies among the virtual components. To address this limitation, in this paper we present a technique for computing VDC availability that considers heterogeneous hardware failure rates and dependencies among virtual components. We then propose Venice, an availability-aware VDC embedding framework for achieving high VDC availability and low operational costs. Experiments show Venice can significantly improve VDC availability while achieving higher income compared to availability-oblivious solutions.},
keywords={cloud computing;computer centres;reliability;virtualisation;reliable virtual data center embedding;cloud computing;quality-of-service;online services;QoS;network resources;VDC scheduling algorithms;reliability;hardware failure characteristics;heterogeneous hardware failure rates;Venice;operational costs;Availability;Servers;Bandwidth;Hardware;Maintenance engineering;Conferences},
doi={10.1109/INFOCOM.2014.6847950},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847951,
author={Y. Zhao and H. Jiang and K. Zhou and Z. Huang and P. Huang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Meeting service level agreement cost-effectively for video-on-demand applications in the cloud},
year={2014},
volume={},
number={},
pages={298-306},
abstract={Increasingly video-on-demand (VoD) applications have been ported to cloud platforms. Leveraging the elastic resource provisioning of the cloud, it is believed that VoD applications should attain high performance cost-effectively. In this paper we propose an approach that aims to solve the fundamental resource reservation and scheduling problem of configuring the cloud utility to meet SLAs for VoD applications at a modest cost. First, we devise a constraint-based model that describes the relationship among channel placement, user groups' bandwidth allocation, operating costs and QoS constraints. Second, we present a distributed heuristic algorithm, called DREAM, that solves the model and produces a budget solution that reserves and allocates cloud bandwidth, and determines the channel layout among datacenters. Simulations driven by data traces collected from a commercial VoD system demonstrate that DREAM provides much better access locality and data availability than and comparable streaming quality to state-of-the-art solutions at lower cloud operating costs.},
keywords={cloud computing;contracts;cost reduction;quality of service;video on demand;service level agreement;video-on-demand;VoD;cloud platforms;QoS;distributed heuristic algorithm;DREAM;operating costs;Bandwidth;Availability;Streaming media;Optimization;Quality of service;Conferences;Computers},
doi={10.1109/INFOCOM.2014.6847951},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847952,
author={J. Li and T. Li and J. Ren},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Beyond the MDS bound in distributed cloud storage},
year={2014},
volume={},
number={},
pages={307-315},
abstract={Distributed storage plays a crucial role in the current cloud computing framework. After the theoretical bound for distributed storage was derived by the pioneer work of the regenerating code, Reed-Solomon code based regenerating codes were developed. The RS code based minimum storage regeneration code (RS-MSR) and the RS code based minimum bandwidth regeneration code (RS-MBR) can achieve the theoretical bounds on the MSR point and the MBR point respectively in code regeneration. They can also maintain the MDS property in code reconstruction. However, in the hostile network where the storage nodes can be compromised and the packets can be tampered with, the storage capacity of the network can be significantly affected. In this paper, we propose a Hermitian code based regenerating (H-MSR) code. We first prove that this code can achieve the theoretical MSR bound. We then propose data regeneration and reconstruction algorithms for the H-MSR code in both error-free network and hostile network. Theoretical evaluation shows that our proposed schemes can detect the erroneous decodings and correct more errors in the hostile network than the RS-MSR code with the same code rate. Our analysis also demonstrates that the proposed H-MSR code has a lower complexity than the RS-MSR code in both code regeneration and code reconstruction.},
keywords={cloud computing;Reed-Solomon codes;storage management;MDS bound;distributed cloud storage;cloud computing framework;RS code based minimum storage regeneration code;RS-MSR;RS code based minimum bandwidth regeneration code;RS-MBR;MSR point;MBR point;code reconstruction;Hermitian code based regenerating code;H-MSR code;error-free network;hostile network;Reed-Solomon code;Bandwidth;Equations;Cloud computing;Encoding;Conferences;Computers;Distributed databases;regenerating code;Reed-Solomon code;error-correction;Hermitian code},
doi={10.1109/INFOCOM.2014.6847952},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847953,
author={J. Li and B. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Cooperative repair with minimum-storage regenerating codes for distributed storage},
year={2014},
volume={},
number={},
pages={316-324},
abstract={Distributed storage systems store redundant data to tolerate failures of storage nodes and lost data should be repaired when storage nodes fail. A class of MDS codes, called minimum-storage regenerating (MSR) codes, has been designed to optimize bandwidth consumption when repairing one single failure. Compared with repairing failures individually, the cooperative repair of multiple failures can help to further save bandwidth consumption when multiple failures are being repaired. In this paper, we present a new construction of minimum-storage cooperative regenerating (MSCR) codes that repair two failures cooperatively and exactly. We show that given a valid instance of linear exact MSR codes, we are able to construct a corresponding repair procedure to repair any two failures cooperatively with optimal bandwidth consumption, i.e., to construct an instance of exact MSCR codes directly from exact MSR codes. With this connection, we are also able to repair any single failure exactly with MSCR codes.},
keywords={codes;storage management;cooperative repair;minimum-storage regenerating codes;distributed storage systems;redundant data;MDS codes;MSR codes;minimum-storage cooperative regenerating codes;MSCR codes;optimal bandwidth consumption;Maintenance engineering;Bandwidth;Computers;Vectors;Conferences;Distributed databases;Encoding},
doi={10.1109/INFOCOM.2014.6847953},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847954,
author={R. Banirazi and E. Jonckheere and B. Krishnamachari},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Heat-Diffusion: Pareto optimal dynamic routing for time-varying wireless networks},
year={2014},
volume={},
number={},
pages={325-333},
abstract={A new routing policy, named Heat-Diffusion (HD), is developed for multihop wireless networks subject to stochastic arrivals, time-varying topology, and inter-channel interference, using only current queue congestion and current channel states, without requiring the knowledge of topology and arrivals. Besides throughput optimality, HD minimizes a quadratic routing cost defined by endowing each channel with a cost-factor. It also minimizes average total queue congestion, and so average network delay, within the class of routing policies that base decision only on current queue lengths and current channel states. Further, within this class, HD provides a Pareto optimal tradeoff between average delay and average routing cost, meaning that no policy can improve either one without detriment to the other. Finally, HD fluid limit follows graph combinatorial heat equation that opens a new way to study wireless networks using heat calculus, a very active area of pure mathematics.},
keywords={Pareto optimisation;radio networks;radiofrequency interference;stochastic processes;telecommunication network routing;heat-diffusion;Pareto optimal dynamic routing;time-varying wireless networks;routing policy;multihop wireless networks;stochastic arrivals;time-varying topology;inter-channel interference;current queue congestion;current channel states;quadratic routing;routing policies;graph combinatorial heat equation;heat calculus;High definition video;Routing;Delays;Heating;Vectors;Wireless networks;Minimization},
doi={10.1109/INFOCOM.2014.6847954},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847955,
author={P. Wan and Z. Wang and L. Wang and Z. Wan and S. Ji},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={From least interference-cost paths to maximum (Concurrent) multiflow in MC-MR wireless networks},
year={2014},
volume={},
number={},
pages={334-342},
abstract={Maximum multiflow and maximum concurrent mul-tiflow in multi-channel multi-radio (MC-MR) wireless networks have been well-studied in the literature. They are NP-hard even in single-channel single-radio (SC-SR) wireless networks when all nodes have uniform (and fixed) interference radii and the positions of all nodes are available. While they admit a polynomial-time approximation scheme (PTAS) when the number of channels is bounded by a constant, such PTAS is quite infeasible practically. Other than the PTAS, all other known approximation algorithms, in both SC-SR wireless networks and MC-MR wireless networks, resorted to solve a polynomial-sized linear program (LP) exactly. The scalability of their running time is fundamentally limited by the general-purposed LP solvers. In this paper, we first introduce the concept of interference costs and prices of a path and explore their relations with the maximum (concurrent) multiflow. Then we develop purely combinatorial approximation algorithms which compute a sequence of least interference-cost routing paths along which the flows are routed. These algorithms are faster and simpler, and achieve nearly the same approximation bounds known in the literature.},
keywords={approximation theory;combinatorial mathematics;computational complexity;linear programming;radio networks;radiofrequency interference;MC-MR wireless networks;maximum multiflow;concurrent multiflow;least interference-cost paths;multichannel multiradio wireless networks;single-channel single-radio wireless networks;NP-hard problem;uniform interference radii;polynomial-time approximation scheme;polynomial-sized linear program;combinatorial approximation algorithm;Interference;Approximation algorithms;Approximation methods;Wireless networks;Schedules;Protocols;Computers},
doi={10.1109/INFOCOM.2014.6847955},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847956,
author={G. S. Paschos and C. Li and E. Modiano and K. Choumas and T. Korakis},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Multirate multicast: Optimal algorithms and implementation},
year={2014},
volume={},
number={},
pages={343-351},
abstract={Multirate multicast improves user quality but complicates network optimization. This paper introduces a novel control scheme to dynamically optimize multirate multicast. We present MMT, an adaptive policy which combines differential backlog scheduling and intelligent packet dropping, both based on local information. MMT is shown to maximize network throughput by adapting to changing conditions such as channel quality, network congestion, and device capabilities. Then, we study the problem of per-receiver network utility maximization. To maximize sum utility we propose the MMU policy, an extension of MMT with receiver-end flow control. Under the operation of both policies backlog sizes are deterministically bounded, which provides delay guarantees on delivered packets. An important feature of the proposed scheme is that it does not require source cooperation or centralized calculations. To illustrate its practicality, we present a prototype implementation in the NITOS wireless testbed. Experimental results verify the optimality of the scheme and its low complexity.},
keywords={multicast communication;optimisation;packet radio networks;radio networks;radio receivers;network optimization;multirate multicast optimization;MMT;differential backlog scheduling;intelligent packet dropping;network throughput maximization;perreceiver network utility maximization;MMU policy;receiver-end flow control;NITOS wireless testbed;Throughput;Receivers;Conferences;Computers;Streaming media;Optimization;Wireless communication},
doi={10.1109/INFOCOM.2014.6847956},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847957,
author={T. Harks and M. Hoefer and K. Schewior and A. Skopalik},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Routing games with progressive filling},
year={2014},
volume={},
number={},
pages={352-360},
abstract={Max-min fairness (MMF) is a widely known approach to a fair allocation of bandwidth to each of the users in a network. This allocation can be computed by uniformly raising the bandwidths of all users without violating capacity constraints. We consider an extension of these allocations by raising the bandwidth with arbitrary and not necessarily uniform time-depending velocities (allocation rates). These allocations are used in a game-theoretic context for routing choices, which we formalize in progressive filling games (PFGs). We present a variety of results for equilibria in PFGs. We show that these games possess pure Nash and strong equilibria. While computation in general is NP-hard, there are polynomial-time algorithms for prominent classes of Max-Min-Fair Games (MMFG), including the case when all users have the same source-destination pair. We characterize prices of anarchy and stability for pure Nash and strong equilibria in PFGs and MMFGs when players have different or the same source-destination pairs. In addition, we show that when a designer can adjust allocation rates, it is possible to design games with optimal strong equilibria. Some initial results on polynomial-time algorithms in this direction are also derived.},
keywords={bandwidth allocation;computational complexity;game theory;minimax techniques;polynomials;telecommunication network routing;routing game;fair bandwidth allocation;capacity constraint;time-depending velocity;game-theoretic context;progressive filling game;PFG;Nash equilibria;NP-hard computation;polynomial- time algorithm;max-min-fair game;MMFG;source-destination pair;stability;optimal strong equilibria;Bandwidth;Games;Resource management;Routing;Protocols;Polynomials;Silicon;routing;congestion control;existence of strong and Nash equilibrium;complexity and convergence;price of anarchy},
doi={10.1109/INFOCOM.2014.6847957},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847958,
author={W. Xi and J. Zhao and X. Li and K. Zhao and S. Tang and X. Liu and Z. Jiang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Electronic frog eye: Counting crowd using WiFi},
year={2014},
volume={},
number={},
pages={361-369},
abstract={Crowd counting, which count or accurately estimate the number of human beings within a region, is critical in many applications, such as guided tour, crowd control and marketing research and analysis. A crowd counting solution should be scalable and be minimally intrusive (i.e., device-free) to users. Image-based solutions are device-free, but cannot work well in a dim or dark environment. Non-image based solutions usually require every human being carrying device, and are inaccurate and unreliable in practice. In this paper, we present FCC, a device-Free Crowd Counting approach based on Channel State Information (CSI). Our design is motivated by our observation that CSI is highly sensitive to environment variation, like a frog eye. We theoretically discuss the relationship between the number of moving people and the variation of wireless channel state. A major challenge in our design of FCC is to find a stable monotonic function to characterize the relationship between the crowd number and various features of CSI. To this end, we propose a metric, the Percentage of nonzero Elements (PEM), in the dilated CSI Matrix. The monotonic relationship can be explicitly formulated by the Grey Verhulst Model, which is used for crowd counting without a labor-intensive site survey. We implement FCC using off-the-shelf IEEE 802.11n devices and evaluate its performance via extensive experiments in typical real-world scenarios. Our results demonstrate that FCC outperforms the state-of-art approaches with much better accuracy, scalability and reliability.},
keywords={image processing;matrix algebra;wireless channels;wireless LAN;electronic frog eye;Wi-Fi;crowd counting;image-based solutions;channel state information;wireless channel state;percentage of nonzero elements;PEM;dilated CSI matrix;Grey Verhulst model;IEEE 802.11n devices;Computers;Conferences;Computer science;Educational institutions;FCC;Performance evaluation;Cameras},
doi={10.1109/INFOCOM.2014.6847958},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847959,
author={W. Huang and Y. Xiong and X. Li and H. Lin and X. Mao and P. Yang and Y. Liu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Shake and walk: Acoustic direction finding and fine-grained indoor localization using smartphones},
year={2014},
volume={},
number={},
pages={370-378},
abstract={We propose an accurate acoustic direction finding scheme, Swadloon, according to the arbitrary pattern of phone shaking in rough horizontal plane. Swadloon tracks the displacement of smartphone relative to the acoustic direction with the resolution less than 1 millimeter. The direction is then obtained by combining the velocity from the displacement with the one from the inertial sensors. Major challenges in implementing Swadloon are to measure the displacement precisely and to estimate the shaking velocity accurately when the speed of phone-shaking is low and changes arbitrarily. We propose rigorous methods to address these challenges, and apply Swadloon to several case studies: Phone-to-Phone direction finding, indoor localization and tracking. Our extensive experiments show that the mean error of direction finding is around 2.1° within the range of 32 m. For indoor localization, the 90-percentile errors are under 0.92 m. For real-time tracking, the errors are within 0.4 m for walks of 51 m.},
keywords={acoustic signal processing;indoor radio;radio direction-finding;radio tracking;sensors;smart phones;acoustic direction finding scheme;fine-grained indoor localization;Swadloon;phone shaking arbitrary pattern;rough horizontal plane;smartphone displacement;inertial sensors;shaking velocity estimation;phone-shaking speed;phone-to-phone direction finding;indoor tracking;real-time tracking;Acoustics;Phase locked loops;Sensors;Band-pass filters;Acoustic measurements;Real-time systems;Doppler effect},
doi={10.1109/INFOCOM.2014.6847959},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847960,
author={T. Liu and L. Yang and Q. Lin and Y. Guo and Y. Liu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Anchor-free backscatter positioning for RFID tags with high accuracy},
year={2014},
volume={},
number={},
pages={379-387},
abstract={RFID technology has been widely adopted in a variety of applications from logistics to access control. Many applications gain benefits from knowing the exact position of an RFID-tagged object. Existing localization algorithms in wireless network, however, can hardly be directly employed due to tag's limited capabilities in terms of energy and memory. For example, the RSS based methods are vulnerable to both distance and tag orientation, while AOA based methods put a strict constraint on the antennas' spacing that reader's directional antennas are too large to meet. In this paper, we propose BackPos, a fine-grained backscatter positioning technique using the COTS RFID products with detected phase. Our study shows that the phase is indeed a stable indicator highly related to tag's position and preserved over frequency or tag orientation, but challenged by its periodicity and tag's diversity. We attempt to infer the distance differences from phases detected by antennas under triangle constraint. Further, hyperbolic positioning using the distance differences is employed to shrink the tag's candidate positions until filtering out the real one. In combination with interrogation zone, we finally relax the triangle constraint and allow arbitrary deployment of antennas by sacrificing the feasible region. We implement a prototype of BackPos with COTS RFID products and evaluate this design in various scenarios. The results show that BackPos achieves the mean accuracy of 12.8cm with variance of 3.8cm.},
keywords={backscatter;hyperbolic equations;radio networks;radiofrequency identification;anchor-free backscatter positioning;RFID tag;localization algorithm;wireless network;RSS based method;distance orientation;tag orientation;AOA based method;directional antenna;BackPos;fine-grained backscatter positioning technique;COTS RFID product;detected phase;triangle constraint;hyperbolic positioning;candidate position;interrogation zone;antenna arbitrary deployment;commercial off-the-shelf radiofrequency identification;received signal strength based method;Antennas;Equations;Backscatter;Radiofrequency identification;Uncertainty;Conferences;Computers;RFID;hyperbolic positioning;BackPos;feasible region},
doi={10.1109/INFOCOM.2014.6847960},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847961,
author={Y. Shu and P. Cheng and Y. Gu and J. Chen and T. He},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={TOC: Localizing wireless rechargeable sensors with time of charge},
year={2014},
volume={},
number={},
pages={388-396},
abstract={Wireless rechargeable sensor network is a promising platform for long-term applications such as inventory management, supply chain monitoring and so on. For these applications, sensor localization is one of the most fundamental challenges. Different from traditional sensor node, wireless rechargeable sensor has to be charged above a voltage level by the wireless charger in order to support its sensing, computation and communication operations. In this work, we consider the scenario where a mobile charger stops at different positions to charge sensors, and propose a novel localization design that utilizes the unique Time of Charge (TOC) sequences among wireless rechargeable sensors. Specifically, we introduce two efficient region dividing methods, Inter-node Division and Inter-area Division, to exploit TOC differences from both temporal and spatial dimensions to localize individual sensor nodes. To further optimize the system performance, we introduce both an optimal charger stop planning algorithm for single sensor case and a suboptimal charger stop planning algorithm for the generic multisensor scenario with a provable performance bound. We have extensively evaluated our design by both testbed experiments and large-scale simulations. The experiment and simulation results show that by as less as 5 stops, our design can achieve sub-meter accuracy and the performance is robust under various system conditions.},
keywords={battery chargers;optimisation;sensor fusion;sensor placement;telecommunication network planning;wireless sensor networks;wireless rechargeable sensor localization;time of charge;wireless rechargeable sensor network;wireless charger;mobile charger;region dividing method;internode division;interarea division;TOC;temporal dimension;spatial dimension;system performance optimization;suboptimal charger stop planning algorithm;multisensor;Sensors;Wireless sensor networks;Wireless communication;Algorithm design and analysis;Peer-to-peer computing;Approximation algorithms;Conferences},
doi={10.1109/INFOCOM.2014.6847961},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847962,
author={Z. Li and G. Xie and J. Lin and Y. Jin and M. Kaafar and K. Salamatian},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={On the geographic patterns of a large-scale mobile video-on-demand system},
year={2014},
volume={},
number={},
pages={397-405},
abstract={The widespread availability of smart mobile terminals along with the ever increasing bandwidth capabilities has promoted the popularity of mobile Internet video systems. Understanding the geographic features of mobile content consumption is of an extreme importance for the design and the performance optimization of a mobile video delivery system. This paper is a first step towards characterization of the geographic patterns of a large-scale commercial mobile video-on-demand (VoD) system, by measuring both uniformity and intensity of geographic interests on videos. In particular, we identify a geographical concentration effect of views for individual videos, which is however dependent on video popularity. We also analyze the temporal evolution trends of the geographic popularity which reveal distinct behavior of popular and non-popular videos. While the set of locations that contribute to most of the views of non-popular videos largely varies, the daily geographic popularity distribution of popular videos closely follows the distribution of global traffic and remains stable. We also examine the impact of content type and viewing sources on the geographic features of mobile videos consumption, and the correlation between content similarity and geographic locality. Finally, we provide insights into the implications of our findings.},
keywords={mobile radio;video on demand;geographic pattern;large scale mobile video-on-demand system;smart mobile terminal;mobile content consumption;performance optimization;mobile video delivery system;video popularity;Mobile communication;Entropy;TV;Geology;Portals;Conferences;Computers},
doi={10.1109/INFOCOM.2014.6847962},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847963,
author={V. Joseph and S. Borst and M. I. Reiman},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Optimal rate allocation for adaptive wireless video streaming in networks with user dynamics},
year={2014},
volume={},
number={},
pages={406-414},
abstract={We consider the problem of optimal rate allocation and admission control for adaptive video streaming sessions in wireless networks with user dynamics. The central aim is to achieve an optimal tradeoff between several key objectives: maximizing the average rate utility per user, minimizing the temporal rate variability, and maximizing the number of users supported. We identify the structure of algorithms that achieve asymptotically optimal performance in large-capacity systems, and exploit the insight into this structure to devise parsimonious and robust online algorithms. Extensive simulation experiments demonstrate that the proposed online algorithms perform well, even in systems with relatively small capacity.},
keywords={radio networks;resource allocation;telecommunication congestion control;video streaming;optimal rate allocation;adaptive wireless video streaming;user dynamics;admission control;wireless networks;temporal rate variability;Streaming media;Wireless communication;Dynamic scheduling;Algorithm design and analysis;Bandwidth;Aggregates;Upper bound},
doi={10.1109/INFOCOM.2014.6847963},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847964,
author={Y. Cao and X. Chen and T. Jiang and J. Zhang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={SoCast: Social ties based cooperative video multicast},
year={2014},
volume={},
number={},
pages={415-423},
abstract={In this paper, we propose SoCast - a cooperative video multicast framework to stimulate effective cooperation among mobile users (clients), by leveraging two types of important social ties, i.e., social trust and social reciprocity. By using SoCast, clients can form groups to restore incomplete video frames by obtaining missing packets from other clients, according to the unique video encoding structure. In return, the user perception video quality of mobile video multicast can be improved. Specifically, we first cast the problem of social ties based group formation among clients as a coalitional game, and then devise a distributed algorithm to obtain the core solution (group formation) for the formulated coalitional game. Further, a resource allocation mechanism is proposed for the base station to handle radio resource requests from client groups. Extensive numerical studies with real video traces corroborate the significant performance gain by using the SoCast.},
keywords={game theory;mobile computing;video signal processing;SoCast;social ties;cooperative video multicast;video multicast framework;mobile users;social reciprocity;social trust;video frames;missing packets;video encoding structure;video quality;group formation;coalitional game;distributed algorithm;resource allocation mechanism;base station;radio resource;Streaming media;Mobile communication;Encoding;Games;Resource management;Engines;Video recording},
doi={10.1109/INFOCOM.2014.6847964},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847965,
author={M. Choi and W. Sun and J. Koo and S. Choi and K. G. Shin},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Reliable video multicast over Wi-Fi networks with coordinated multiple APs},
year={2014},
volume={},
number={},
pages={424-432},
abstract={Forward Error Correction (FEC) can be exploited to realize reliable video multicast over Wi-Fi with high video quality. We propose reliable video multicast over Wi-Fi networks with coordinated multiple Access Points (APs) to enhance video quality. By coordinating multiple APs, each AP can transmit entirely or partially different FEC-encoded packets so that a multicast receiver can benefit from both spatial and time diversities. The proposed schemes can enlarge the satisfactory video multicast region by exploiting the AP diversity, thus serving more multicast receivers located at cell edge with satisfactory video quality. We propose a resource-allocation algorithm for FEC-code rate adaptation, utilizing the limited wireless resource more efficiently while enhancing video quality. We also introduce the method for estimating the video packet delivery ratio after FEC decoding. The effectiveness of the proposed schemes is comparatively evaluated via extensive simulation and experimentation. The proposed schemes are observed to enhance the ratio of satisfied users by up to 37.1% compared to the conventional single AP multicast scheme.},
keywords={cellular radio;decoding;diversity reception;forward error correction;image enhancement;multimedia communication;video coding;video communication;wireless LAN;video multicast reliability;Wi-Fi networks;coordinated multiple-AP;forward error correction;coordinated multiple-access points;video quality enhancement;FEC-encoded packets;multicast receiver;spatial diversity;time diversity;AP diversity;multicast receivers;cell edge;resource-allocation algorithm;FEC-code rate adaptation;limited wireless resource;video packet delivery ratio;FEC decoding;Streaming media;Forward error correction;Decoding;Reliability;Wireless communication;IEEE 802.11 Standards;Video recording},
doi={10.1109/INFOCOM.2014.6847965},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847966,
author={L. Zhang and Z. Li and C. Wu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Dynamic resource provisioning in cloud computing: A randomized auction approach},
year={2014},
volume={},
number={},
pages={433-441},
abstract={This work studies resource allocation in a cloud market through the auction of Virtual Machine (VM) instances. It generalizes the existing literature by introducing combinatorial auctions of heterogeneous VMs, and models dynamic VM provisioning. Social welfare maximization under dynamic resource provisioning is proven NP-hard, and modeled with a linear integer program. An efficient α-approximation algorithm is designed, with α ~ 2.72 in typical scenarios. We then employ this algorithm as a building block for designing a randomized combinatorial auction that is computationally efficient, truthful in expectation, and guarantees the same social welfare approximation factor α. A key technique in the design is to utilize a pair of tailored primal and dual LPs for exploiting the underlying packing structure of the social welfare maximization problem, to decompose its fractional solution into a convex combination of integral solutions. Empirical studies driven by Google Cluster traces verify the efficacy of the randomized auction.},
keywords={approximation theory;cloud computing;convex programming;electronic commerce;integer programming;linear programming;public administration;randomised algorithms;resource allocation;virtual machines;dynamic resource provisioning;cloud computing;randomized combinatorial auction approach;resource allocation;cloud market;virtual machine;heterogeneous VM;dynamic VM provisioning model;NP-hard problem;linear integer program;α-approximation algorithm;social welfare approximation;packing structure;social welfare maximization problem;convex combination;fractional solution decomposition;integral solutions;Google Cluster traces;Approximation algorithms;Approximation methods;Algorithm design and analysis;IP networks;Resource management;Heuristic algorithms;Bismuth},
doi={10.1109/INFOCOM.2014.6847966},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847967,
author={W. Wu and R. T. B. Ma and J. C. S. Lui},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Exploring bundling sale strategy in online service markets with network effects},
year={2014},
volume={},
number={},
pages={442-450},
abstract={In recent years, we have witnessed a growing trend for online service companies to offer “bundling sales” to increase revenue. Bundling sale means that a company groups a set of its products/services and charges this bundle at a fixed price, which is usually less than the total price of individual items. In this paper, our goal is to understand the underlying dynamics of bundling, in particular, what is the optimal bundling sale strategy and under what situations it will be more attractive than the separate sales. We focus on online service markets that exhibit network effect. We provide mathematical models to capture the interactions between buyers and sellers, analyze the market equilibrium and its stability, and formulate an optimization framework to determine the optimal sale strategy for the service provider. We analyze the impact of the key factors, including the network effects and operating costs, on the profitability of bundling. We show that bundling is more profitable than separate sale in most cases; however, the heterogeneity of services and the asymmetry of operating costs reduce the advantage of bundling. These findings provide important insights in designing proper sale strategies for online services.},
keywords={information services;optimisation;sales management;online service markets;network effects;online service companies;optimal bundling sale strategy;mathematical models;buyers;sellers;optimization framework;operating cost asymmetry;sale strategies;Cost accounting;Silicon;Optimization;Companies;Mathematical model;Pricing;Conferences},
doi={10.1109/INFOCOM.2014.6847967},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847968,
author={G. Iosifidis and L. Gao and J. Huang and L. Tassiulas},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Enabling crowd-sourced mobile Internet access},
year={2014},
volume={},
number={},
pages={451-459},
abstract={Crowd-sourced mobile Internet access services enable mobile users to connect with each other and share their Internet connections. This is a promising solution for addressing users' increasing needs for ubiquitous connectivity and alleviating network congestion. The success of such services heavily depends on users' willingness to contribute their resources. In this paper, we consider a general model for such services, and design a distributed incentive mechanism for encouraging users' participation. This bargaining based scheme ensures that the contribution of user resources, in terms of Internet access bandwidths and battery energy, and the allocation of service capacity, measured in the delivered mobile data, are Pareto efficient and proportionally fair. The numerical results verify that the service always improves users' performance and that these benefits depend on the diversity of the users' resources.},
keywords={Internet;mobile communication;mobile computing;Pareto optimisation;crowd sourced mobile Internet access;mobile users;Internet connections;ubiquitous connectivity;network congestion;distributed incentive mechanism;battery energy;Pareto efficient;Internet;Online banking;Mobile communication;IEEE 802.11 Standards;Routing;NIST;Logic gates},
doi={10.1109/INFOCOM.2014.6847968},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847969,
author={S. Hasan and S. Gorinsky and C. Dovrolis and R. K. Sitaraman},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Trade-offs in optimizing the cache deployments of CDNs},
year={2014},
volume={},
number={},
pages={460-468},
abstract={Content delivery networks (CDNs) deploy globally distributed systems of caches in a large number of autonomous systems (ASes). It is important for a CDN operator to satisfy the performance requirements of end users, while minimizing the cache deployment cost. In this paper, we study the cache deployment optimization (CaDeOp) problem of determining how much server, energy, and bandwidth resources to provision in each cache AS, i.e., each AS chosen for cache deployment. The CaDeOp objective is to minimize the total cost incurred by the CDN, subject to meeting the end-user performance requirements. We formulate the CaDeOp problem as a mixed integer program (MIP) and solve it for realistic AS-level topologies, traffic demands, and non-linear energy and bandwidth costs. We also evaluate the sensitivity of the results to our parametric assumptions. When the end-user performance requirements become more stringent, the CDN footprint rapidly expands, requiring cache deployments in additional ASes and geographical regions. Also, the CDN cost increases several times, with the cost balance shifting toward bandwidth and energy costs. On the other hand, the traffic distribution among the cache ASes stays relatively even, with the top 20% of the cache ASes serving around 30% of the overall traffic.},
keywords={cache storage;cost reduction;integer programming;Internet;telecommunication network topology;telecommunication traffic;cache deployment optimization;CDN;content delivery networks;distributed systems;autonomous systems;cache deployment cost minimization;CaDeOp problem;end-user performance requirements;mixed integer program;MIP;AS-level topologies;traffic demands;nonlinear energy;bandwidth cost;traffic distribution;Internet data dissemination;Topology;Internet;Bandwidth;Vectors;Pricing;Servers;Approximation methods},
doi={10.1109/INFOCOM.2014.6847969},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847970,
author={J. Han and C. Qian and X. Wang and D. Ma and J. Zhao and P. Zhang and W. Xi and Z. Jiang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Twins: Device-free object tracking using passive tags},
year={2014},
volume={},
number={},
pages={469-476},
abstract={Device-free based object tracking provides a promising solution for many localization and tracking systems to monitor non-cooperative objects which do not carry any transceiver such as intruders. However, existing device-free solutions mainly use sensors and active RFID tags, which are much more expensive compared to passive tags. In this paper, we propose a novel motion detection and tracking method using passive RFID tags, named Twins. The method leverages a phenomenon called critical state caused by interference among passive tags. We theoretically explain this phenomenon via an interference model and conduct extensive experiment to validate it. We design a practical Twins based intrusion detection system and implement a real prototype with commercial off-the-shelf reader and tags. Experimental results show that Twins is effective in detecting the moving object, with low location errors of 0.75m in average.},
keywords={motion estimation;object detection;object tracking;radiofrequency identification;radiofrequency interference;safety systems;device-free based object tracking;tracking systems;noncooperative objects;device-free solutions;active RFID tags;motion detection method;tracking method;passive RFID tags;critical state;interference model;Twins based intrusion detection system;commercial off-the-shelf reader;location errors;Couplings;Antennas;Tracking;Motion detection;Radiofrequency identification;Particle filters;Prototypes;Device-free;Passive RFID tag;Tracking;Critical State},
doi={10.1109/INFOCOM.2014.6847970},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847971,
author={W. Gong and K. Liu and X. Miao and H. Liu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Arbitrarily accurate approximation scheme for large-scale RFID cardinality estimation},
year={2014},
volume={},
number={},
pages={477-485},
abstract={One important issue of RFID applications is to estimate the cardinality of large-scale RFID tags in the interested region. From a practical perspective, we require: (i) the estimate can be arbitrarily accurate, and (ii) its time cost should be scalable with the tags size, regardless of the tags distribution. Existing solutions, however, either assume the use of hash functions with ideal random properties, or impose unacceptable computation/storage overhead for tags. More importantly, those approaches only give asymptotic results and fail to provide rigorous bounds for the rate of convergence. In this paper, we propose a new scheme, Arbitrarily Accurate Approximation (A3), to reliably estimate the number of tags with any desired accuracy. In particular, for a given requirement of (ε,δ), we show that A3achieves O((log log n+ε-2) log δ-1) time efficiency. Results show that A3significantly outperforms previous designs under various distributions of tags.},
keywords={approximation theory;computational complexity;radiofrequency identification;large-scale RFID cardinality estimation;large-scale RFID tags;time cost;tag distribution;tag size;hash functions;random property;convergence rate;arbitrarily accurate approximation scheme;Approximation methods;Estimation;Accuracy;Approximation algorithms;RFID tags;Error probability},
doi={10.1109/INFOCOM.2014.6847971},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847972,
author={H. Liu and W. Gong and X. Miao and K. Liu and W. He},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Towards adaptive continuous scanning in large-scale RFID systems},
year={2014},
volume={},
number={},
pages={486-494},
abstract={Radio Frequency Identification (RFID) technology plays an important role in supply chain logistics and inventory control. In these applications, a series of scanning operations at different locations are often needed to cover the entire inventory (tags). In such continuous scanning scenario, adjacent scans inevitably read overlapping tags multiple times. Most existing methods suffer from low scanning efficiency when the overlap is small, since they do not distinguish the size of overlap which is an important factor of scanning performance. In this paper, we analytically unveil the fundamental relationship between the performance of continuous scanning and the size of overlap, deriving a critical threshold for the selection of scanning strategy. Further, we design an accurate estimator to approximate the overlap. Combining the estimate and a compact data structure, an adaptive scanning scheme is introduced to achieve low communication time. Through detailed analysis and extensive simulations, we demonstrate that the proposed scheme significantly outperforms previous approach in total scanning time.},
keywords={data structures;radiofrequency identification;adaptive continuous scanning;large-scale RFID systems;radiofrequency identification technology;supply chain logistics;inventory control;compact data structure;adaptive scanning scheme;low communication time;Servers;Radiofrequency identification;Protocols;Estimation;Algorithm design and analysis;Conferences;Computers},
doi={10.1109/INFOCOM.2014.6847972},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847973,
author={Y. Zheng and M. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Read bulk data from computational RFIDs},
year={2014},
volume={},
number={},
pages={495-503},
abstract={Without the need of local energy supply, computational RFID (CRFID) sensors are emerging as important platforms enabling a variety of sensing and computing applications. Nevertheless, the data throughput of CRFIDs is very low. This paper aims at efficiently transferring bulk data from CRFIDs to commodity RFID readers. We first investigate the problem of low data throughput of CRFIDs. We then propose several simple yet effective techniques to allow CRFIDs to meet stringent timing requirement of commodity RFID readers and achieve efficient data transfer. We implement a prototype system based on the WISP CRFIDs and commercial off-the-self RFID reader. The experiment results show that our approach provides better compatibility with EPCglobal C1G2 compliant RFID devices and works perfectly with the commodity RFID readers.},
keywords={radiofrequency identification;sensors;read bulk data;computational RFID sensors;CRFID sensors;CRFID data throughput;commodity RFID readers;data transfer efficiency;WISP CRFID;commercial off-the-self RFID reader;EPCglobal C1G2 compliant RFID devices;Radiofrequency identification;Data transfer;Standards;Timing;Microcontrollers;Computers},
doi={10.1109/INFOCOM.2014.6847973},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847974,
author={J. Zhao and T. Jung and Y. Wang and X. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Achieving differential privacy of data disclosure in the smart grid},
year={2014},
volume={},
number={},
pages={504-512},
abstract={The smart grid introduces new privacy implications to individuals and their family due to the fine-grained usage data collection. For example, smart metering data could reveal highly accurate real-time home appliance energy load, which may be used to infer the human activities inside the houses. One effective way to hide actual appliance loads from the outsiders is Battery-based Load Hiding (BLH), in which a battery is installed for each household and smartly controlled to store and supply power to the appliances. Even though such technique has been demonstrated useful and can prevent certain types of attacks, none of existing BLH works can provide probably privacy-preserving mechanisms. In this paper, we investigate the privacy of smart meters via differential privacy. We first analyze the current existing BLH methods and show that they cannot guarantee differential privacy in the BLH problem. We then propose a novel randomized BLH algorithm which successfully assures differential privacy, and further propose the Multitasking-BLH-Exp3 algorithm which adaptively updates the BLH algorithm based on the context and the constraints. Results from extensive simulations show the efficiency and effectiveness of the proposed method over existing BLH methods.},
keywords={data acquisition;domestic appliances;smart meters;smart power grids;data disclosure;smart grid;fine-grained usage data collection;smart metering data;real-time home appliance energy load;battery-based load hiding;BLH methods;privacy-preserving mechanisms;smart meters via differential privacy;multitasking-BLH-Exp3 algorithm;Privacy;Batteries;Home appliances;Smart meters;Data privacy;Noise;Energy consumption;Smart Grid;Smart Meter;Privacy;Differential Privacy;Data Disclosure},
doi={10.1109/INFOCOM.2014.6847974},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847975,
author={L. Yang and X. Chen and J. Zhang and H. V. Poor},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Optimal privacy-preserving energy management for smart meters},
year={2014},
volume={},
number={},
pages={513-521},
abstract={Smart meters, designed for information collection and system monitoring in smart grid, report fine-grained power consumption to utility providers. With these highly accurate profiles of energy usage, however, it is possible to identify consumers' specific activity or behavior patterns, thereby giving rise to serious privacy concerns. In this paper, this concern is addressed by using battery energy storage. Beyond privacy protection, batteries can also be used to cut down the electricity bill. From a holistic perspective, a dynamic optimization framework is designed for consumers to strike a tradeoff between the smart meter data privacy and the electricity bill. In general, a major challenge in solving dynamic optimization problems lies in the need of the knowledge of the future electricity consumption events. By exploring the underlying structure of the original problem, an equivalent problem is derived, which can be solved by using only the current observations. An online control algorithm is then developed to solve the equivalent problem based on the Lyapunov optimization technique. To overcome the difficulty of solving a mixed-integer nonlinear program involved in the online control algorithm, the problem is further decomposed into multiple cases and the closed-form solution to each case is derived accordingly. It is shown that the proposed online control algorithm can optimally control the battery operations to protect the smart meter data privacy and cut down the electricity bill, without the knowledge of the statistics of the time-varying load requirement and the electricity price processes. The efficacy of the proposed algorithm is demonstrated through extensive numerical evaluations using real data.},
keywords={data privacy;integer programming;nonlinear programming;power consumption;smart meters;smart power grids;optimal privacy-preserving energy management;information collection;smart grid;power consumption;battery energy storage;privacy protection;electricity bill;dynamic optimization framework;smart meter data privacy;dynamic optimization problems;electricity consumption;Lyapunov optimization technique;mixed-integer nonlinear program;online control algorithm;time-varying load requirement;Batteries;Electricity;Smart meters;Data privacy;Privacy;Optimization;Algorithm design and analysis;Smart Meter;Smart Grid;Data Privacy;Load Monitor;Cost Saving;Battery},
doi={10.1109/INFOCOM.2014.6847975},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847976,
author={Q. Zheng and S. Xu and G. Ateniese},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={VABKS: Verifiable attribute-based keyword search over outsourced encrypted data},
year={2014},
volume={},
number={},
pages={522-530},
abstract={It is common nowadays for data owners to outsource their data to the cloud. Since the cloud cannot be fully trusted, the outsourced data should be encrypted. This however brings a range of problems, such as: How should a data owner grant search capabilities to the data users? How can the authorized data users search over a data owner's outsourced encrypted data? How can the data users be assured that the cloud faithfully executed the search operations on their behalf? Motivated by these questions, we propose a novel cryptographic solution, called verifiable attribute-based keyword search (VABKS). The solution allows a data user, whose credentials satisfy a data owner's access control policy, to (i) search over the data owner's outsourced encrypted data, (ii) outsource the tedious search operations to the cloud, and (iii) verify whether the cloud has faithfully executed the search operations. We formally define the security requirements of VA B K S and describe a construction that satisfies them. Performance evaluation shows that the proposed schemes are practical and deployable.},
keywords={authorisation;cloud computing;cryptography;information retrieval;outsourcing;trusted computing;VABKS;verifiable attribute-based keyword search;outsourced encrypted data;data owners;cloud;search capabilities;authorized data users;search operations;cryptographic solution;data owner access control policy;performance evaluation;Access control;Keyword search;Encryption;TV;Indexes},
doi={10.1109/INFOCOM.2014.6847976},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847977,
author={D. Luchaup and L. De Carli and S. Jha and E. Bach},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Deep packet inspection with DFA-trees and parametrized language overapproximation},
year={2014},
volume={},
number={},
pages={531-539},
abstract={IPSs determine whether incoming traffic matches a database of vulnerability signatures defined as regular expressions. DFA representations are popular, but suffer from the state-explosion problem. We introduce a new matching structure: a tree of DFAs where the DFA associated with a node over-approximates those at its children, and the DFAs at the leaves represent the signature set. Matching works top-down, starting at the root of the tree and stopping at the first node whose DFA does not match. In the common case (benign traffic) matching does not reach the leaves. DFA-trees are built using Compact Overapproximate DFAs (CODFAs). A CODFA D' for D over-approximates the language accepted by D, has a smaller number of states than D, and has a low false-match rate. Although built from approximate DFAs, DFA-trees perform exact matching faster than a commonly used method, have a low memory overhead and a guaranteed good worst case performance.},
keywords={computational complexity;deterministic automata;digital signatures;finite automata;formal languages;pattern matching;tree data structures;deterministic finite automata;NP-hard problem;low memory overhead;low false-match rate;CODFAs;compact overapproximate DFAs;benign traffic matching;matching structure;state-explosion problem;regular expressions;vulnerability signatures;intrusion prevention system;IPSs;parametrized language overapproximation;DFA-trees;deep packet inspection;Payloads;Training;Automata;Conferences;Computers;DH-HEMTs;Approximation error},
doi={10.1109/INFOCOM.2014.6847977},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847978,
author={W. Sun and Z. Yang and K. Wang and Y. Liu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Hello: A generic flexible protocol for neighbor discovery},
year={2014},
volume={},
number={},
pages={540-548},
abstract={Neighbor discovery is crucial for both wireless sensor networks and mobile computing applications. The crux of the problem is to achieve energy efficiency, which has been demonstrated to be difficult by prior work. In this paper we propose Hello, a generic flexible protocol for neighbor discovery. With an unrestricted parameter, it serves as a generic framework that incorporates existing deterministic protocols. Under the framework, we expose optimal parameters for either symmetric or asymmetric duty cycles, which is the first to our knowledge. As a result, it exhibits great flexibility by adjusting itself to any configuration that best meets application demands. Besides, several techniques are applied to removing redundant discoveries without sacrificing the worst-case latency. We evaluate Hello through extensive simulation and real-world sensor experiments. The results show that Hello is highly energy-efficient under symmetric and asymmetric duty cycles. In particular, it is two times better than the state of the art in terms of the worst-case latency under asymmetric duty cycles.},
keywords={mobile computing;protocols;wireless sensor networks;Hello;generic flexible protocol;neighbor discovery;wireless sensor networks;mobile computing applications;energy efficiency;deterministic protocols;symmetric duty cycles;asymmetric duty cycles;redundant discovery removal;Protocols;Wireless sensor networks;Schedules;Conferences;Computers;Mobile computing;Probabilistic logic},
doi={10.1109/INFOCOM.2014.6847978},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847979,
author={L. Zhang and Z. Guan and T. Melodia},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Cooperative anti-jamming for infrastructure-less wireless networks with stochastic relaying},
year={2014},
volume={},
number={},
pages={549-557},
abstract={Denial-of-service (DoS) attacks launched by malicious jammers can pose significant threats to infrastructure-less wireless networks where a centralized controller may not be available. While significant recent research efforts have dealt with such attacks and several possible countermeasures have been proposed, little attention has been paid to the idea of cooperative anti-jamming. Inspired by this observation, we propose and study a cooperative anti-jamming scheme designed to enhance the quality of links degraded by jammers. To achieve this objective, users are allowed to cooperate at two levels. First, they cooperate to optimally regulate their channel access probabilities so that jammed users gain a higher share of channel utilization. Second, users leverage multiple-input single-output cooperative communication techniques to enhance the throughput of jammed links. We formulate the problem of optimal cooperative anti-jamming as a distributed pricing-based optimization problem and propose a best response algorithm to solve it in a distributed way. Simulations demonstrate that the proposed algorithm achieves considerable gains (compared to traditional noncooperative antijamming) especially under heavy traffic or high jamming power. Furthermore, by comparing the proposed algorithm with a provably-optimal centralized algorithm, we show that it achieves close-to-global optimality under moderate traffic load.},
keywords={cooperative communication;interference suppression;jamming;MIMO communication;pricing;probability;radio networks;relay networks (telecommunication);stochastic processes;telecommunication industry;telecommunication network reliability;telecommunication traffic;wireless channels;infrastructureless wireless network;stochastic relaying;denial-of-service attack;DoS attack;malicious jammer;cooperative antijamming scheme;channel access probability;channel utilization;users leverage multiple-input single-output cooperative communication technique;optimal cooperative anti-jamming problem;distributed pricing-based optimization problem;provably-optimal centralized algorithm;traffic load moderation;Jamming;Relays;Algorithm design and analysis;Sensors;Resource management;OFDM;Physical layer},
doi={10.1109/INFOCOM.2014.6847979},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847980,
author={X. Xu and M. Song},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Restricted coverage in wireless networks},
year={2014},
volume={},
number={},
pages={558-564},
abstract={For wireless networks, coverage with different restrictions that can capture the practical requirements have received great research interests. We will study several restricted coverage problems. The first problem is aboutK-coverage, i.e., how to deploy wireless nodes such that each target is covered by at leastKwireless nodes. We study the problem restricted to linear-K-coverage where there is a line, all targets lie in one side of this line and all wireless nodes lie in the other side. Assume each wireless node is associated with a weight, the objective is to select a minimum weighted subset of nodes such that each target isK-covered. We propose a 3-approximation for this problem by exploring geometric properties. The second problem is calledK-road-coverage. Given a road map in a two-dimensional area which contains a set of paths and a set of wireless nodes, the locations of nodes can either be arbitrary or fixed, the objective is to select a minimum number of wireless nodes such that each path can beK-covered. We will reduce the problem toK-coverage and apply the algorithmic results forK-coverage to solve it. Another line of this work is to investigate a well-motivated problem called strongly dominating set, which is intrinsically related to coverage. Given a wireless networking system represented by a digraph G = (V, E⃗). Each wireless node u has a covering disk centering at u with its radius equal to the transmission range of u. We then draw a directed edge uv⃗ in G if u's corresponding covering disk contains v. A subset U ⊆ V of wireless nodes is a strongly dominating set if every wireless node in V \ U has both an in-neighbor in U and an out-neighbor in U. The objective is to find a minimum size strongly dominating set. Our method can achieve an approximation factor of (2 + ε).},
keywords={approximation theory;geometry;radio networks;restricted coverage problems;out-neighbor;in-neighbor;wireless networking system;road-coverage;geometric properties;3-approximation;linear-K-coverage;wireless nodes;Wireless communication;Wireless sensor networks;Polynomials;Conferences;Computers;Approximation methods;Approximation algorithms},
doi={10.1109/INFOCOM.2014.6847980},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847981,
author={C. Boyacı and Y. Xia},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Optimal delay bound for maximum weight scheduling policy in wireless networks},
year={2014},
volume={},
number={},
pages={565-573},
abstract={We provide an improved bound for the expectation of the stationary delay under the maximum weight scheduling (MWS) policy in one-hop wireless networks. In the model, the interference of the links is characterized by an interference graph G=(V, E). For a vector μ ϵ R|V|+, let χf(G, μ) be the weighted fractional coloring number for the graph G under the weight vector μ. For an arrival rate vector λ in the capacity region, we define a quantity ε(λ)to be the unique value satisfying the condition χf(G, λ+ε(λ)e) = 1, where e = (1, 1,..., 1)T. We show that the stationary delay is upper-bounded by B(2(Σi=1|V|λi) ε(λ)), where B is a constant depending on the arrival process. We show that the new bound is the tightest single-parameter bound obtainable with an often-used analytical framework. Generalizing the above, we also provide the tightest bounds for all MWS-w policies.},
keywords={graph theory;radio links;scheduling;vectors;stationary delay;maximum weight scheduling policy;MWS policy;one-hop wireless networks;interference graph;weighted fractional coloring number;weight vector;arrival rate vector;capacity region;single-parameter bound;MWS-w policies;Vectors;Interference;Delays;Schedules;Wireless communication;Computers;Conferences;wireless link scheduling;delay;maximum weight schedule;stability;queue size},
doi={10.1109/INFOCOM.2014.6847981},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847982,
author={J. Zhang and F. Ren and C. Lin},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Delay guaranteed live migration of Virtual Machines},
year={2014},
volume={},
number={},
pages={574-582},
abstract={The proliferation of cloud services makes virtualization technology more important. One important feature of virtualization is live Virtual Machine (VM) migration, which can be employed to facilitate load balancing, fault management and server maintenance etc. Two main metrics of evaluating a live VM migration mechanism are total migration time and downtime. The existing literature on live VM migration mainly focus on designing migration mechanisms to shorten these two metrics or making a tradeoff between them. Few of them can be applied to the applications with delay requirements, such as, delay-sensitive web services or a VM backup process that needs to be done in a specific time. This will not only negatively impact the user experiences, but also reduce the profit of cloud service providers. Besides, the frequently varied bandwidth required by the widely used pre-copy mechanism is difficult to be provided by current network technologies. In this work, we theoretically analyze how much bandwidth is required to guarantee the total migration time and downtime of a live VM migration. We first propose a deterministic-based model as a simple example, then assume that the dirtying frequency of each page obeys the bernoulli distribution. At last, we analyze the statistic features of the typical workload running in a VM and build a reciprocal-based workload model, and theoretically give the required bandwidth value to satisfy the performance metrics of a live VM migration. The experimental results demonstrate that the bandwidth obtained from the reciprocal-based model can guarantee the expected total migration time and downtime.},
keywords={cloud computing;fault diagnosis;resource allocation;software maintenance;virtual machines;virtualisation;Web services;delay guaranteed live migration;virtual machines;virtualization technology;live virtual machine migration;load balancing;fault management;server maintenance;VM migration mechanism;migration mechanisms;delay requirement;delay-sensitive Web services;VM backup process;cloud service provider;precopy mechanism;current network technology;live VM migration;deterministic-based model;dirtying frequency;Bernoulli distribution;reciprocal-based workload model;bandwidth value;performance metrics;Bandwidth;Kernel;Distribution functions;Nickel;Delays;Servers;Conferences;Virtualization;Live Migration;Delay;Bandwidth},
doi={10.1109/INFOCOM.2014.6847982},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847983,
author={W. Wang and B. Li and B. Liang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Dominant resource fairness in cloud computing systems with heterogeneous servers},
year={2014},
volume={},
number={},
pages={583-591},
abstract={We study the multi-resource allocation problem in cloud computing systems where the resource pool is constructed from a large number of heterogeneous servers, representing different points in the configuration space of resources such as processing, memory, and storage. We design a multi-resource allocation mechanism, called DRFH, that generalizes the notion of Dominant Resource Fairness (DRF) from a single server to multiple heterogeneous servers. DRFH provides a number of highly desirable properties. With DRFH, no user prefers the allocation of another user; no one can improve its allocation without decreasing that of the others; and more importantly, no user has an incentive to lie about its resource demand. As a direct application, we design a simple heuristic that implements DRFH in real-world systems. Large-scale simulations driven by Google cluster traces show that DRFH significantly outperforms the traditional slot-based scheduler, leading to much higher resource utilization with substantially shorter job completion times.},
keywords={cloud computing;resource allocation;dominant resource fairness;cloud computing systems;heterogeneous servers;multiresource allocation problem;resource configuration space;processing configuration;memory configuration;storage configuration;DRFH multiresource allocation mechanism;DRF notion;Google cluster traces;slot-based scheduler;resource utilization;job completion times;Resource management;Servers;Cloud computing;Schedules;Computational modeling;Vectors;Computers},
doi={10.1109/INFOCOM.2014.6847983},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847984,
author={B. Balasubramanian and T. Lan and M. Chiang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={SAP: Similarity-aware partitioning for efficient cloud storage},
year={2014},
volume={},
number={},
pages={592-600},
abstract={Given a set of files that show a certain degree of similarity, we consider a novel problem of deduplicating them (eliminating redundant chunks) across a set of distributed servers in a manner that is: (i) space-efficient: the total space needed to deduplicate and store the files is minimized and, (ii) access-efficient: each file can be accessed by communicating with a bounded number of servers, thereby minimizing network-access times in congested data center networks. A space-optimal solution in which we first deduplicate all the files and then distribute them across the servers (referred to as chunk-distribution), may require communication with many servers to access each file. On the other hand, an access-efficient solution in which we randomly partition the files cross the servers, and then store their unique chunks on each server may not exploit the similarities across files to reduce the space overhead. In this paper, we first show that finding an access-efficient, space optimal solution is an NP-Hard problem. Following this, we present the similarity-aware-partitioning (SAP) algorithms that find access-efficient solutions within polynomial time complexity and guarantees bounded space overhead for arbitrary files. Our experimental verification on files from Dropbox and CNN confirm that the SAP technique is much more space-efficient than random partitioning, while maintaining compression ratio close to the chunk-distribution solution.},
keywords={cloud computing;computational complexity;computer centres;data communication;file servers;storage management;similarity aware partitioning;cloud storage;distributed server;file storage;file access;network access time minimization;data center network congestion;file distribution;random partition;chunk storage;NP-hard problem;space optimal solution;efficient optimal solution access;polynomial time complexity;bounded space overhead;arbitrary files;Dropbox;CNN;SAP technique;chunk distribution;deduplicating problem;Servers;Partitioning algorithms;Upper bound;Conferences;Computers;Polynomials;Writing},
doi={10.1109/INFOCOM.2014.6847984},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847985,
author={S. Albagli-Kim and H. Shachnai and T. Tamir},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Scheduling jobs with dwindling resource requirements in clouds},
year={2014},
volume={},
number={},
pages={601-609},
abstract={We consider a job-scheduling problem arising on cloud systems and in broadcasting networks, where the goal is to optimally utilize a limited amount of a resource (e.g., cloud servers, bandwidth, or storage capacity) available along a given time interval. The resource is utilized by a set of weighted jobs. The processing of a job consists of several contiguous stages, each having a specific length and a specific resource-demand, such that the set of demands forms a decreasing sequence. Each job is associated with a release time and a deadline, defining the time interval in which it can be processed. Some notable applications for this scenario include progressive download, QuickStart and prefetching methods, hierarchical image reconstruction, and routine security and maintenance tasks. The goal is to find a feasible schedule of a maximum-weight subset of the jobs. In a feasible schedule, at any time, the total amount of resource allocated to the active jobs does not exceed the available amount of resource. Since this problem is NP-hard already for highly restricted inputs, we focus on obtaining approximation algorithms and heuristics and present a comparative study among them. Our main result, the first constant-factor approximation algorithm for the problem, generalizes the state of art for the fundamental problem of resource constrained real-time scheduling, to scenarios where jobs may have dwindling resource requirements. Our empirical study shows that this algorithm is in fact nearly optimal for realistic inputs.},
keywords={approximation theory;job shop scheduling;dwindling resource requirements;job scheduling problem;cloud systems;broadcasting networks;weighted jobs;prefetching methods;image reconstruction;routine security;approximation algorithms;constant-factor approximation algorithm;Color;Approximation methods;Approximation algorithms;Schedules;Processor scheduling;Servers;Linear programming},
doi={10.1109/INFOCOM.2014.6847985},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847986,
author={T. Kimura and K. Ishibashi and T. Mori and H. Sawada and T. Toyono and K. Nishimatsu and A. Watanabe and A. Shimoda and K. Shiomoto},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Spatio-temporal factorization of log data for understanding network events},
year={2014},
volume={},
number={},
pages={610-618},
abstract={Understanding the impacts and patterns of network events such as link flaps or hardware errors is crucial for diagnosing network anomalies. In large production networks, analyzing the log messages that record network events has become a challenging task due to the following two reasons. First, the log messages are composed of unstructured text messages generated by vendor-specific rules. Second, network equipment such as routers, switches, and RADIUS severs generate various log messages induced by network events that span across several geographical locations, network layers, protocols, and services. In this paper, we have tackled these obstacles by building two novel techniques: statistical template extraction (STE) and log tensor factorization (LTF). STE leverages a statistical clustering technique to automatically extract primary templates from unstructured log messages. LTF aims to build a statistical model that captures spatial-temporal patterns of log messages. Such spatial-temporal patterns provide useful insights into understanding the impacts and root cause of hidden network events. This paper first formulates our problem in a mathematical way. We then validate our techniques using massive amount of network log messages collected from a large operating network. We also demonstrate several case studies that validate the usefulness of our technique.},
keywords={electronic messaging;IP networks;matrix decomposition;protocols;spatiotemporal phenomena;statistics;telecommunication network management;telecommunication network routing;tensors;spatio-temporal factorization;log data;network events;link flaps;hardware errors;network anomalies;production networks;unstructured text messages;vendor-specific rules;network equipment;RADIUS severs;geographical locations;network layers;protocols;statistical template extraction;STE;log tensor factorization;LTF;statistical clustering technique;statistical model;spatial-temporal patterns;network log messages;Tensile stress;Data mining;Conferences;Computers;Mathematical model;Protocols;Hidden Markov models},
doi={10.1109/INFOCOM.2014.6847986},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847987,
author={H. Liu and Y. Zhang and Y. Zhou and D. Zhang and X. Fu and K. K. Ramakrishnan},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Mining checkins from location-sharing services for client-independent IP geolocation},
year={2014},
volume={},
number={},
pages={619-627},
abstract={Accurately determining the geographic location of an Internet host is important for location-aware applications such as location-based advertising and network diagnostics. Despite their fast response time, widely used database-driven geolocation approaches provide only inaccurate locations. Delay measurement based approaches improve the estimation accuracy but still suffer from a limited precision (about 10 km) and a long response time (tens of seconds) to localize a single PC, which cannot meet the demand of precise and real-time geolocation for location-aware applications. In this paper, we propose a new geolocation approach, Checkin-Geo, which exploits geolocation resources fundamentally different from existing database-driven (using DNS, Whois, etc.) or network delay measurement based approaches. In particular, we leverage the location data that users are willing to share in location-sharing services and logs of user logins from PCs for real-time and accurate geolocation. Experimental results show that compared to existing geolocation techniques, Checkin-Geo achieves 1) a median estimation error of 799 meters (an order of magnitude smaller than existing approaches), and 2) a negligible response time, which are promising for accurate location-aware applications.},
keywords={data mining;Internet;IP networks;checkins mining;location-sharing services;client-independent IP geolocation;geographic location;Internet host;location-aware applications;location-based advertising;network diagnostics;database-driven geolocation;real-time geolocation;Checkin-Geo;network delay measurement;user logins;median estimation error;IP networks;Geology;Delays;Time factors;History;Global Positioning System;Advertising},
doi={10.1109/INFOCOM.2014.6847987},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847988,
author={A. X. Liu and C. R. Meiners and E. Torng},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Packet classification using binary Content Addressable Memory},
year={2014},
volume={},
number={},
pages={628-636},
abstract={Packet classification is the core mechanism that enables many networking devices. Although using Ternary Content Addressable Memories (TCAMs) to perform high speed packet classification has become the widely adopted solution, TCAMs are very expensive, have limited capacity, consume large amounts of power, and generate tremendous amounts of heat because of their extremely dense and parallel circuitry. In this paper, we propose the first packet classification scheme that uses Binary Content Addressable Memories (BCAMs). BCAMs are similar to TCAMs except that in BCAMs, every bit has only two possible states: 0 or 1; in contrast, in TCAMs, every bit has three possible states: 0, 1, or * (don't care). Because of the high complexity in implementing the extra “don't care” state, TCAMs have much higher circuit density than BCAMs. As the power consumption, heat generation, and price grow non-linearly with circuit density, BCAMs consume much less power, generate much less heat, and cost much less money than TCAMs. Our BCAM based packet classification scheme is built on two key ideas. First, we break a multi-dimensional lookup into a series of one-dimensional lookups. Second, for each one-dimensional lookup, we convert the ternary matching problem into a binary string exact matching problem. To speed up the lookup process, we propose a number of optimization techniques including skip lists, free expansion, minimizing maximum lookup time, minimizing average lookup time, and lookup short circuiting. We evaluated our BCAM scheme on 17 real-life packet classifiers. On these classifiers, our BCAM scheme requires roughly 5 times fewer CAM bits than the traditional TCAM based scheme. The penalty is a throughput that is roughly 4 times less.},
keywords={circuit optimisation;content-addressable storage;pattern classification;power consumption;string matching;packet classification scheme;binary content addressable memory;networking devices;TCAMs;high speed packet classification;parallel circuitry;BCAMs;power consumption;don't care state;circuit density;heat generation;price;multidimensional lookup;ternary matching problem;binary string exact matching problem;optimization techniques;skip lists;free expansion;maximum lookup time minimization;average lookup time minimization;Transistors;Computer aided manufacturing;Throughput;Conferences;Computers;Associative memory;Ports (Computers)},
doi={10.1109/INFOCOM.2014.6847988},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847989,
author={A. Wang and A. J. T. Gurney and X. Han and J. Cao and B. T. Loo and C. Talcott and A. Scedrov},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A reduction-based approach towards scaling up formal analysis of internet configurations},
year={2014},
volume={},
number={},
pages={637-645},
abstract={The Border Gateway Protocol (BGP) is the single inter-domain routing protocol that enables network operators within each autonomous system (AS) to influence routing decisions by independently setting local policies on route filtering and selection. This independence leads to fragile networking and makes analysis of policy configurations very complex. To aid the systematic and efficient study of the policy configuration space, this paper presents network reduction, a scalability technique for policy-based routing systems. In network reduction, we provide two types of reduction rules that transform policy configurations by merging duplicate and complementary router configurations to simplify analysis. We show that the reductions are sound, dual of each other and are locally complete. The reductions are also computationally attractive, requiring only local configuration information and modification. We have developed a prototype of network reduction and demonstrated that it is applicable on various BGP systems and enables significant savings in analysis time. In addition to making possible safety analysis on large networks that would otherwise not complete within reasonable time, network reduction is also a useful tool for discovering possible redundancies in BGP systems.},
keywords={Internet;routing protocols;reduction based approach;formal analysis;Internet configurations;border gateway protocol;single interdomain routing protocol;autonomous system;AS;route filtering;route selection;policy configurations;scalability technique;policy based routing systems;BGP systems;safety analysis;network reduction;Merging;Safety;Routing;Redundancy;Conferences;Computers;Protocols},
doi={10.1109/INFOCOM.2014.6847989},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847990,
author={X. Yin and Z. Li and X. Wang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A matroid theory approach to multicast network coding},
year={2014},
volume={},
number={},
pages={646-654},
abstract={Network coding encourages the mixing of information flows at intermediate nodes of a network for enhanced network capacity, especially for one-to-many multicast applications. A fundamental problem in multicast network coding is to construct a feasible solution such that encoding and decoding are performed over a finite field of size as small as possible. Coding operations over very small finite fields (e.g., F<sub>2</sub>) enable low computational complexity in theory and ease of implementation in practice. In this work, we propose a new approach based on matroid theory to study multicast network coding and its minimum field size requirements. Applying this new approach that translates multicast networks into matroids, we derive the first upper-bounds on the field size requirement based on the number of relay nodes in the network, and make new progresses along the direction of proving that coding over very small fields (F<sub>2</sub> and F<sub>3</sub>) suffices for multicast network coding in planar networks.},
keywords={combinatorial mathematics;matrix algebra;multicast communication;network coding;information flows;intermediate nodes;enhanced network capacity;one-to-many multicast applications;multicast network coding;encoding;decoding;coding operations;matroid theory;minimum field size requirements;multicast networks;relay nodes;planar networks;Encoding;Network coding;Vectors;Relays;Receivers;Throughput;Multicast communication},
doi={10.1109/INFOCOM.2014.6847990},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847991,
author={W. Kuo and C. Wang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Robust and optimal opportunistic scheduling for downlink 2-flow inter-session network coding with varying channel quality},
year={2014},
volume={},
number={},
pages={655-663},
abstract={This paper considers the downlink traffic from a base station to two different clients. Assuming infinite backlog, it is known that inter-session network coding (INC) can significantly increase the throughput of each flow. However, the corresponding scheduling solution (assuming dynamic arrivals and requiring bounded delay) is still nascent. For the 2-flow downlink scenario, we propose the first opportunistic INC + scheduling solution that is provably optimal for time-varying channels, i.e., the corresponding stability region matches the optimal linear-INC capacity. To that end, we first introduce a new binary INC operation, which is distinctly different from the traditional wisdom of XORing two overheard packets. We then develop a queue-length-based scheduling scheme, which, with the help of the new INC operation, can robustly and optimally adapt to time-varying channel quality. A byproduct of our results is a scheduling scheme for stochastic processing networks (SPNs) with random departure. The new SPN results relax the previous assumption of deterministic departure, a major limitation of the existing SPN model, by considering stochastic packet departure behavior, and could further broaden the applications of SPN scheduling to other real-world scenarios.},
keywords={network coding;scheduling;telecommunication traffic;optimal opportunistic scheduling;robust opportunistic scheduling;downlink 2-flow intersession network coding;downlink traffic;infinite backlog;INC;dynamic arrivals;bounded delay;time-varying channels;stability region;queue length based scheduling scheme;time-varying channel quality;stochastic processing networks;SPN scheduling application;Vectors;Time-varying channels;Dynamic scheduling;Computers;Downlink;Throughput},
doi={10.1109/INFOCOM.2014.6847991},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847992,
author={S. Ji and T. Chen and S. Zhong and S. Kak},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={DAWN: Defending against wormhole attacks in wireless network coding systems},
year={2014},
volume={},
number={},
pages={664-672},
abstract={Network coding has been shown to be an effective approach to improve the wireless system performance. However, many security issues impede its wide deployment in practice. Besides the well-studied pollution attacks, there is another severe threat, that of wormhole attacks, which undermines the performance gain of network coding. Since the underlying characteristics of network coding systems are distinctly different from traditional wireless networks, the impact of wormhole attacks and countermeasures are generally unknown. In this paper, we quantify wormholes' devastating harmful impact on network coding system performance through experiments. Then we propose DAWN, a Distributed detection Algorithm against Wormhole in wireless Network coding systems, by exploring the change of the flow directions of the innovative packets caused by wormholes. We rigorously prove that DAWN guarantees a good lower bound of successful detection rate. We perform analysis on the resistance of DAWN against collusion attacks. We find that the robustness depends on the node density in the network, and prove a necessary condition to achieve collusion-resistance. DAWN does not rely on any location information, global synchronization assumptions or special hardware/middleware. It is only based on the local information that can be obtained from regular network coding protocols, and thus does not introduce any overhead by extra test messages. Extensive experimental results have verified the effectiveness and the efficiency of DAWN.},
keywords={network coding;radio networks;synchronisation;telecommunication security;DAWN;wormhole attacks;wireless network coding systems;wireless system performance;pollution attacks;distributed detection algorithm;flow directions;detection rate;collusion attacks;node density;collusion-resistance;location information;global synchronization assumptions;regular network coding protocols;test messages;Network coding;Wireless networks;Encoding;Routing;Protocols;Probability;Throughput},
doi={10.1109/INFOCOM.2014.6847992},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847993,
author={F. Chen and T. Xiang and Y. Yang and S. S. M. Chow},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Secure cloud storage meets with secure network coding},
year={2014},
volume={},
number={},
pages={673-681},
abstract={This paper investigates the intrinsic relationship between secure cloud storage and secure network coding for the first time. Secure cloud storage was proposed only recently while secure network coding has been studied for more than ten years. We show in general how to construct a secure cloud storage protocol given any secure network coding protocol. Our construction suggests a systematic way to construct various secure cloud storage protocols. We also show that it is secure under a definition which captures the real world uses of the cloud storage. From our general construction, we propose a secure cloud storage protocol based on a recent secure network coding protocol. The protocol is the first publicly verifiable secure cloud storage protocol in the standard model, while the previous work is either not publicly verifiable, or security argument is only argued heuristically in the random oracle model. We also enhance the proposed protocol to support third-party public auditing, which has received considerable attention recently. Finally, we prototype the proposed protocol and evaluate its performance. Experimental results validate the effectiveness of the protocol.},
keywords={cloud computing;network coding;security of data;intrinsic relationship;secure cloud storage protocols;general construction;secure network coding protocol;random oracle model;public auditing;Cloud computing;Secure storage;Network coding;Authentication;Routing protocols;Cloud storage auditing;network coding;security;third-party public auditing},
doi={10.1109/INFOCOM.2014.6847993},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847994,
author={L. Gao and G. Iosifidis and J. Huang and L. Tassiulas},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Hybrid data pricing for network-assisted user-provided connectivity},
year={2014},
volume={},
number={},
pages={682-690},
abstract={User-provided connectivity (UPC) is a promising paradigm to achieve a low-cost ubiquitous connectivity. In this paper, we study a network-assisted UPC service model, where a mobile virtual network operator (MVNO) enables its subscribers to operate as mobile WiFi hotspots (hosts) and provide Internet connectivity for others. A unique aspect of this service model is that the MVNO offers some free data quota to hosts as reimbursements (incentives) for connectivity sharing. This reimbursing scheme, together with a usage-based pricing, constitute a revolutionary hybrid data pricing-reimbursing scheme, which has not been considered before. We analyze the different impacts of data price and reimbursement on the host's connectivity sharing decision systematically. Based on this analysis, we further derive the optimal hybrid pricing-reimbursing policy that maximizes the MVNO's revenue. Our numerical result indicates that by using the proposed hybrid pricing policy, the MVNO can increase its revenue by 20% to 135% under an elastic client demand, and by 20% to 550% under an inelastic client demand, comparing to those achieved under a pricing-only policy.},
keywords={Internet;mobile computing;pricing;share prices;wireless LAN;network-assisted user-provided connectivity;low-cost ubiquitous connectivity;network-assisted UPC service model;mobile virtual network operator;mobile WiFi hotspots;Internet connectivity sharing;free data quota;usage-based pricing;revolutionary hybrid data pricing-reimbursing scheme;MVNO revenue maximization;elastic client demand;inelastic client demand;optimal hybrid pricing-reimbursing policy;Optimal scheduling;Pricing;IEEE 802.11 Standards;Schedules;Mobile communication;Games;Internet},
doi={10.1109/INFOCOM.2014.6847994},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847995,
author={Z. Lu and P. Sinha and R. Srikant},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={EasyBid: Enabling cellular offloading via small players},
year={2014},
volume={},
number={},
pages={691-699},
abstract={Data offloading is an increasingly popular mechanism for meeting the rising demands of cellular users. In order to enable the small players, such as businesses and individual owners to make their services available to the bigger wireless service providers (WSPs) to help offload data, a simple, practical and easy-to-use payment machinery needs to be devised. Existing auction mechanisms usually assume that bidders can precisely estimate their true valuations, and they ignore the significant overhead to sellers incurred for obtaining a precise estimation. Such assumption is unrealistic in femtocell networks. To allow imprecise valuations, we introduce the novel concept of perceived valuation, which is a value that can be acquired by the seller at little or no cost. We further propose two novel metrics: partial truthfulness, and imprecision loss, to measure the quality of a truthful auction that accepts perceived valuations. Based on this, we propose EasyBid, a new auction model that provides guarantees for truthfulness even when considering a system with imprecise valuations. Finally, we design a dynamic programming based algorithm which aims to maximize the WSP's utility while satisfying any given constraints on partial truthfulness and imprecision loss. Through simulations, we show that the utility achieved by EasyBid with imprecise valuations can be close to the optimal solution that assumes precise valuations.},
keywords={dynamic programming;femtocellular radio;cellular offloading;EasyBid auction model;data offloading;wireless service providers;WSPs;easy-to-use payment machinery;auction mechanisms;femtocell networks;partial truthfulness;imprecision loss;truthful auction quality;dynamic programming based algorithm;Cost accounting;Silicon;Mobile communication;Conferences;Computers;Wireless communication;Estimation},
doi={10.1109/INFOCOM.2014.6847995},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847996,
author={L. Zhang and W. Wu and D. Wang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Time dependent pricing in wireless data networks: Flat-rate vs. usage-based schemes},
year={2014},
volume={},
number={},
pages={700-708},
abstract={With the advances of bandwidth-intensive mobile devices, we see severe congestion problems in wireless data networks. Recently, research emerges to solve this problem from a pricing point of view. Time dependent pricing has been introduced, and initial investigations have shown its advantages over the conventional time independent pricing. Nevertheless, much is unknown in how a practical and effective time dependent pricing scheme can be designed. In this paper, we explore the design space of time dependent pricing. In particular, we focus on a number of schemes, e.g., the usage-based scheme, the flat-rate scheme, and a mixture of them which we called a cap scheme. Our findings include: 1) the ISP obtains a higher profit with usage-based (or flat-rate) scheme if the capacity is insufficient (or sufficient); 2) the usage-based scheme usually achieves a higher consumer surplus and more efficient traffic utilization than the flat-rate scheme; and 3) the cap scheme is strongly preferred by the ISP to further increase its revenue. We believe our findings provide important insights for ISPs to design effective pricing schemes.},
keywords={mobile computing;pricing;time dependent pricing;wireless data network;flat rate pricing;usage based pricing;bandwidth intensive mobile device;design space;ISP;Pricing;Cost accounting;Games;Wireless communication;Sensitivity;Bandwidth;Monopoly},
doi={10.1109/INFOCOM.2014.6847996},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847997,
author={W. Dong and S. Rallapalli and L. Qiu and K. K. Ramakrishnan and Y. Zhang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Double auctions for dynamic spectrum allocation},
year={2014},
volume={},
number={},
pages={709-717},
abstract={Wireless spectrum is a precious resource and must be allocated and used efficiently. The conventional spectrum allocation lets a government (e.g., FCC) sell a given portion of spectrum to one provider. This is not only restrictive, but also limits spectrum reuse and may lead to significant under-utilization of spectrum. In this paper, we develop a novel truthful double auction scheme to let any resource owner (e.g., a cellular provider), who has spare spectrum at a given time, sell to one or more providers that need additional spectrum at that time. Spectrum auction is fundamentally different from conventional auction problems since spectrum can be re-used and competition pattern is complex due to wireless interference. We propose the first double auction design for spectrum allocation that explicitly decouples the buyer side and seller side auction design while achieving (i) truthfulness, (ii) individual rationality, and (iii) budget balance. To accurately capture wireless interference and support spectrum reuse, we partition the conflict graph so that buyers with strong direct and indirect interference are put into the same subgraph and buyers with no or weak interference are put into separate subgraphs and then compute pricing independently within each subgraph. We develop a merge scheme to combine spectrum allocation results from different subgraphs and resolve potential conflicts. Using conflict graphs generated from real cell tower locations, we extensively evaluate our approach and demonstrate that it achieves high efficiency, revenue, and utilization.},
keywords={electronic commerce;radio spectrum management;radiofrequency interference;resource allocation;double auctions;dynamic spectrum allocation;wireless spectrum;FCC;cellular provider;spectrum auction;wireless interference;buyer side auction design;seller side auction design;spectrum reuse;indirect interference;Interference;Resource management;Wireless communication;Pricing;Clustering algorithms;Partitioning algorithms;Conferences},
doi={10.1109/INFOCOM.2014.6847997},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847998,
author={X. Liu and J. Cao and S. Tang and P. Guo},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A generalized coverage-preserving scheduling in WSNs: A case study in structural health monitoring},
year={2014},
volume={},
number={},
pages={718-726},
abstract={Wireless sensor networks (WSNs) are generally used to monitor, in an area, certain phenomena which can be events or targets that users are interested. To extend the system lifetime, a widely used technique is `Energy-Efficient Coverage-Preserving Scheduling(EECPS)', in which at any time, only part of the nodes are activated to fulfill the function. To determine which nodes should be activated at a certain time is the key for the EECPS and this problem has been studied extensively. Existing solutions are based on the assumption that each node has a fixed coverage area, and once the event/target occurs in this area, it can be detected by this sensor. However, this coverage model is not always valid. In some applications such as structural health monitoring (SHM) and volcano monitoring, to fulfill a required function always requires low level collaboration from multiple sensors. The coverage area for individual sensor node therefore cannot be defined explicitly since single sensor is not able to fulfill the function alone, even it is close to the event or target to be monitored. In this paper, using an example of SHM, we illustrate how to support EECPS in some special applications of WSNs. We re-define the `coverage' and based on the new coverage model, two methods are proposed to partition the deployed sensor nodes into qualified cover sets such that the system lifetime can be maximized by letting these sets work by turns. The performance of the methods is demonstrated through extensive simulation and experiment.},
keywords={condition monitoring;structural engineering;telecommunication power management;wireless sensor networks;generalized coverage-preserving scheduling;WSN;wireless sensor networks;EECPS;structural health monitoring;volcano monitoring;multiple sensors;SHM;sensor nodes deployment;Wireless sensor networks;Shape;Monitoring;Genetic algorithms;Time-frequency analysis;Conferences;Computers},
doi={10.1109/INFOCOM.2014.6847998},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6847999,
author={H. Han and J. Yu and H. Zhu and Y. Chen and J. Yang and Y. Zhu and G. Xue and M. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={SenSpeed: Sensing driving conditions to estimate vehicle speed in urban environments},
year={2014},
volume={},
number={},
pages={727-735},
abstract={Acquiring instant vehicle speed is desirable and a corner stone to many important vehicular applications. This paper utilizes smartphone sensors to estimate the vehicle speed, especially when GPS is unavailable or inaccurate in urban environments. In particular, we estimate the vehicle speed by integrating the accelerometer's readings over time and find the acceleration errors can lead to large deviations between the estimated speed and the real one. Further analysis shows that the changes of acceleration errors are very small over time which can be corrected at some points, called reference points, where the true vehicle speed is known. Recognizing this observation, we propose an accurate vehicle speed estimation system, SenSpeed, which senses natural driving conditions in urban environments including making turns, stopping and passing through uneven road surfaces, to derive reference points and further eliminates the speed estimation deviations caused by acceleration errors. Extensive experiments demonstrate that SenSpeed is accurate and robust in real driving environments. On average, the real-time speed estimation error on local road is 1.32mph, and the offline speed estimation error is as low as 0.75mph. Whereas the average error of GPS is 3.1mph and 2.8mph respectively.},
keywords={acceleration measurement;accelerometers;Global Positioning System;road vehicles;sensors;smart phones;velocity measurement;SenSpeed;sensing driving condition;urban environment;smartphone sensor;GPS;accelerometer;vehicle speed estimation system;acceleration error;velocity 1.32 mph;velocity 3.1 mph;velocity 2.8 mph;Vehicles;Acceleration;Estimation;Sensors;Accelerometers;Roads;Global Positioning System},
doi={10.1109/INFOCOM.2014.6847999},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848000,
author={A. E. A. A. Abdulla and Z. M. Fadlullah and H. Nishiyama and N. Kato and F. Ono and R. Miura},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={An optimal data collection technique for improved utility in UAS-aided networks},
year={2014},
volume={},
number={},
pages={736-744},
abstract={Recent technological advances in electronics, sensors, and communications devices have facilitated the proliferation of Unmanned Aircraft System (UAS)-aided applications. However, the UAS-aided communications networks are yet to receive sufficient research endeavor. In this paper, we address one of the most important research challenges pertaining to UAS-aided networks comprising adaptive modulation-capable nodes, namely how to fairly maximize the energy efficiency (throughput per energy). For the mobility pattern innate to the UAS, we demonstrate how the adaptive modulation behaves. Furthermore, we formulate the problem as a potential game that is played between the UAS and the network-nodes, and prove its stability, optimality, and convergence. Based upon the potential game, a data collection method is envisioned to maximize the energy efficiency with the fairness constraint. Additionally, we analyze the Price of Anarchy (PoA) of our proposed game. Extensive simulations exhibit the effectiveness of our proposal under varying environments.},
keywords={autonomous aerial vehicles;game theory;mobile robots;mobility management (mobile radio);optimal data collection technique;utility improvement;UAS-aided communications networks;unmanned aircraft system-aided applications;adaptive modulation-capable nodes;energy efficiency maximization;mobility pattern;stability;optimality;convergence;fairness constraint;price-of-anarchy;PoA;potential game;Games;Signal to noise ratio;Modulation;Data collection;Convergence;Bit error rate;Game theory},
doi={10.1109/INFOCOM.2014.6848000},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848001,
author={S. He and D. Shin and J. Zhang and J. Chen},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Toward optimal allocation of location dependent tasks in crowdsensing},
year={2014},
volume={},
number={},
pages={745-753},
abstract={Crowdsensing offers an efficient approach to meet the demand in large scale sensing applications. In crowdsensing, it is of great interest to find the optimal task allocation, which is challenging since sensing tasks with different requirements of quality of sensing are typically associated with specific locations and mobile users are constrained by time budgets. We show that the allocation problem is NP hard. We then focus on approximation algorithms, and devise an efficient local ratio based algorithm (LRBA). Our analysis shows that the approximation ratio of the aggregate rewards obtained by the optimal allocation to those by LRBA is 5. This reveals that LRBA is efficient, since a lower (but not tight) bound on the approximation ratio is 4. We also discuss about how to decide the fair prices of sensing tasks to provide incentives since mobile users tend to decline the tasks with low incentives. We design a pricing mechanism based on bargaining theory, in which the price of each task is determined by the performing cost and market demand (i.e., the number of mobile users who intend to perform the task). Extensive simulation results are provided to demonstrate the advantages of our proposed scheme.},
keywords={computational complexity;game theory;mobile computing;pricing;resource allocation;wireless sensor networks;location dependent task optimal allocation;crowdsensing;time budgets;NP hard problem;approximation algorithms;local ratio based algorithm;LRBA;pricing mechanism;bargaining theory;lower bound;approximation ratio;Sensors;Materials requirements planning;Mobile communication;Resource management;Algorithm design and analysis;Approximation methods;Computers;Crowdsensing Applications;Location Dependent Task Allocation;Approximation Ratio},
doi={10.1109/INFOCOM.2014.6848001},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848002,
author={B. Niu and Q. Li and X. Zhu and G. Cao and H. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Achieving k-anonymity in privacy-aware location-based services},
year={2014},
volume={},
number={},
pages={754-762},
abstract={Location-Based Service (LBS) has become a vital part of our daily life. While enjoying the convenience provided by LBS, users may lose privacy since the untrusted LBS server has all the information about users in LBS and it may track them in various ways or release their personal data to third parties. To address the privacy issue, we propose a Dummy-Location Selection (DLS) algorithm to achieve k-anonymity for users in LBS. Different from existing approaches, the DLS algorithm carefully selects dummy locations considering that side information may be exploited by adversaries. We first choose these dummy locations based on the entropy metric, and then propose an enhanced-DLS algorithm, to make sure that the selected dummy locations are spread as far as possible. Evaluation results show that the proposed DLS algorithm can significantly improve the privacy level in terms of entropy. The enhanced-DLS algorithm can enlarge the cloaking region while keeping similar privacy level as the DLS algorithm.},
keywords={data privacy;mobile computing;k-anonymity;privacy-aware location-based services;untrusted LBS server;dummy-location selection algorithm;DLS algorithm;entropy metric;user information;cloaking region;Privacy;Entropy;Servers;Algorithm design and analysis;Measurement;Computers;Conferences},
doi={10.1109/INFOCOM.2014.6848002},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848003,
author={R. Lu and X. Lin and Z. Shi and J. Shao},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={PLAM: A privacy-preserving framework for local-area mobile social networks},
year={2014},
volume={},
number={},
pages={763-771},
abstract={In this paper, we propose a privacy-preserving framework, called PLAM, for local-area mobile social networks. The proposed PLAM framework employs a privacy-preserving request aggregation protocol with k-Anonymity and l-Diversity properties while without involving a trusted anonymizer server to keep user preference privacy when querying location-based service (LBS), and integrates unlinkable pseudo-ID technique to achieve user identity privacy, location privacy. Moreover, the proposed PLAM framework also introduces the privacy-preserving and verifiable polynomial computation to keep LBS provider's functions private while preventing the provider from cheating in computation. Detailed security analysis shows that the proposed PLAM framework can not only achieve desirable privacy requirements but also resist outside attacks on source authentication, data integrity and availability. In addition, extensive simulations are also conducted, and simulation results guide us on how to set proper thresholds for k-anonymity, l-diversity to make a tradeoff between the desirable user preference privacy level and the request delay in different scenarios.},
keywords={cryptography;data integrity;data privacy;local area networks;mobile computing;polynomials;protocols;social networking (online);trusted computing;local-area mobile social networks;PLAM framework;privacy-preserving request aggregation protocol;trusted anonymizer server;location-based service;LBS;unlinkable pseudo-ID technique;user identity privacy;location privacy;verifiable polynomial computation;LBS provider functions;security analysis;source authentication;data integrity;data availability;k-anonymity;l-diversity;user preference privacy level;request delay;Privacy;Mobile communication;Polynomials;Mobile computing;Security;Data privacy;Protocols;Privacy-preserving;preference privacy;location-based services;mobile social network},
doi={10.1109/INFOCOM.2014.6848003},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848004,
author={Z. Qin and S. Yi and Q. Li and D. Zamkov},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Preserving secondary users' privacy in cognitive radio networks},
year={2014},
volume={},
number={},
pages={772-780},
abstract={Cognitive radio plays an important role in improving spectrum utilization in wireless services. In the cognitive radio paradigm, secondary users (SUs) are allowed to utilize licensed spectrum opportunistically without interfering with primary users (PUs). To motivate PU to share licensed spectrum with SU, it is reasonable for SU to pay PU a fee whenever the former is utilizing the latter's licensed spectrum. SU's detailed usage information, such as when and how long the licensed spectrum is utilized, is needed for PU to calculate payment. Providing usage information to PU, however, may compromise SU's privacy. To solve this dilemma, we are the first to propose a novel privacy-preserving mechanism for cognitive radio transactions through commitment scheme and zero-knowledge proof. This mechanism, on one hand, only allows PU to know the total payment to SU for a billing period, plus a little portion of SU's usage information. On the other hand, it guarantees PU that the payment is correctly calculated. We have implemented our mechanism and evaluated its performance.},
keywords={cognitive radio;data privacy;radio spectrum management;telecommunication security;secondary users privacy;cognitive radio networks;licensed spectrum;usage information;privacy preserving mechanism;cognitive radio transactions;commitment scheme;zero knowledge proof;Monitoring;Privacy;Cognitive radio;Pricing;Protocols;Conferences;Computers},
doi={10.1109/INFOCOM.2014.6848004},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848005,
author={M. Korczyński and A. Duda},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Markov chain fingerprinting to classify encrypted traffic},
year={2014},
volume={},
number={},
pages={781-789},
abstract={In this paper, we propose stochastic fingerprints for application traffic flows conveyed in Secure Socket Layer/Transport Layer Security (SSL/TLS) sessions. The fingerprints are based on first-order homogeneous Markov chains for which we identify the parameters from observed training application traces. As the fingerprint parameters of chosen applications considerably differ, the method results in a very good accuracy of application discrimination and provides a possibility of detecting abnormal SSL/TLS sessions. Our analysis of the results reveals that obtaining application discrimination mainly comes from incorrect implementation practice, the misuse of the SSL/TLS protocol, various server configurations, and the application nature.},
keywords={computer network security;cryptographic protocols;fingerprint identification;Internet;Markov processes;Markov chain fingerprinting;encrypted traffic classification;stochastic fingerprints;secure socket layer/transport layer security;SSL/TLS sessions;fingerprint parameters;SSL/TLS protocol;Protocols;Servers;Markov processes;Ciphers;Twitter},
doi={10.1109/INFOCOM.2014.6848005},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848006,
author={S. Lv and X. Dong and Y. Lu and X. Du and X. Wang and Y. Dou and X. Zhou},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={3D pipeline contention: Asymmetric full duplex in wireless networks},
year={2014},
volume={},
number={},
pages={790-798},
abstract={Coordination among users is an indispensable part in wireless networks for efficient medium access. Alone with the rapid increase of transmission rate, however, coordination time becomes insufferable. We present AFD, namely asymmetric full duplex, to achieve high coordination efficiency at nearly zero overhead. In AFD, channel contention is performed simultaneously with data transmission. We propose a 3D pipeline contention scheme where the contention process is divided into several parallel stages and executed in a pipelined manner in a 3D domain specified by time, frequency and spatial antenna. To mitigate the interference between the data packet and the contention signal, we adopt a singleton PN sequence as a contention pilot. AFD provides a novel network-scale full duplex capability. The performance is evaluated by both simulations and measurements in a testbed. AFD outperforms IEEE 802.11 significantly, i.e., the Jain's fairness index is around 0.95 with a throughput gain up to 120%.},
keywords={antennas;carrier sense multiple access;data communication;pipeline processing;radio networks;time-frequency analysis;wireless channels;wireless networks;medium access;AFD;asymmetric full duplex;channel contention;data transmission;3D pipeline contention scheme;spatial antenna;interference mitigation;data packet;singleton PN sequence;network-scale full duplex capability;Jain fairness index;3D domain;time domain;frequency domain;Pipelines;Data communication;Antennas;Three-dimensional displays;Interference;IEEE 802.11 Standards;Time-frequency analysis},
doi={10.1109/INFOCOM.2014.6848006},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848007,
author={L. Zhang and K. Liu and Y. Jiang and X. Li and Y. Liu and P. Yang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Montage: Combine frames with movement continuity for realtime multi-user tracking},
year={2014},
volume={},
number={},
pages={799-807},
abstract={In this work we design and develop Montage for real-time multi-user formation tracking and localization by off-the-shelf smartphones. Montage achieves submeter-level tracking accuracy by integrating temporal and spatial constraints from user movement vector estimation and distance measuring. In Montage we designed a suite of novel techniques to surmount a variety of challenges in real-time tracking, without infrastructure and fingerprints, and without any a priori user-specific (e.g., stride-length and phone-placement) or site-specific (e.g., digitalized map) knowledge. We implemented, deployed and evaluated Montage in both outdoor and indoor environment. Our experimental results (847 traces from 15 users) show that the stride-length estimated by Montage over all users has error within 9cm, and the moving-direction estimated by Montage is within 20°. For realtime tracking, Montage provides meter-second-level formation tracking accuracy with off-the-shelf mobile phones.},
keywords={smart phones;target tracking;movement continuity;real-time multiuser formation tracking;smartphones;submeter-level tracking;temporal constraints;spatial constraints;user movement vector estimation;moving-direction estimation;meter-second-level formation tracking accuracy;mobile phones;Vectors;Distance measurement;Tracking;Acoustics;Earth;Topology;Acceleration},
doi={10.1109/INFOCOM.2014.6848007},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848008,
author={X. Xie and X. Zhang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Scalable user selection for MU-MIMO networks},
year={2014},
volume={},
number={},
pages={808-816},
abstract={In a multi-user MIMO (MU-MIMO) network, an AP with M antennas can only serve up to M users out of a large user population. The M users' rates are inter-coupled and depend on their channel orthogonality. Substantial theoretical studies focused on selecting users to maximize capacity, but they require feedback of channel state information (CSI) from all users. The resulting overhead can easily overwhelm useful data in large scale networks. In this paper, we propose a scalable user selection mechanism called orthogonality probing based user selection (OPUS). OPUS only requires up to M rounds of CSI feedback. In each round, it employs a novel probing mechanism that enables a user to evaluate its orthogonality with existing users, and a distributed contention mechanism that singles out the best user to feedback its CSI. Software-radio based implementation and experimentation shows that OPUS significantly outperforms traditional user selection schemes in both throughput and fairness.},
keywords={antennas;feedback;MIMO communication;software radio;wireless channels;MU-MIMO networks;AP;antennas;channel orthogonality;channel state information;CSI feedback;large scale networks;scalable user selection mechanism;orthogonality probing based user selection;OPUS;distributed contention mechanism;software-radio based implementation;access point;multiuser MIMO network;Measurement;Downlink;Vectors;Interference;Signal to noise ratio;Antennas;MIMO},
doi={10.1109/INFOCOM.2014.6848008},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848009,
author={Y. Du and E. Aryafar and J. Camp and M. Chiang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={iBeam: Intelligent client-side multi-user beamforming in wireless networks},
year={2014},
volume={},
number={},
pages={817-825},
abstract={Frequently, client-side wireless devices have a view of multiple WiFi access points, whether from open residential and commercial networks, corporate networks, or mesh networks. Given the increasing number of radios and antennas in today's wireless devices, residual capacity from these multiple APs could be leveraged if client devices communicate with multiple APs simultaneously. In this paper, we exploit multi-user multi-input multi-output (MU-MIMO) technology to improve throughput and reliability in both directions of a wireless connection. For uplink, we use multi-user beamforming to enable the client devices to send multiple data streams to multiple APs simultaneously. For downlink, we leverage interference nulling technology to allow the client devices to decode parallel packets from multiple APs. This iBeam system requires no changes to existing APs or backhaul networks and is compatible with the IEEE 802.11 standards. We experimentally evaluate iBeam and show significant throughput improvements over both single-AP connections and multi-AP connections in a time division mode. The client's reliability and stability are also significantly improved due to the multi-AP diversity gain.},
keywords={array signal processing;diversity reception;interference suppression;MIMO communication;multi-access systems;telecommunication network reliability;wireless LAN;iBeam;intelligent client side beamforming;multiuser beamforming;wireless networks;multiple WiFi access point;multiple input multiple output technology;MIMO technology;interference nulling technology;parallel packet decoding;IEEE 802.11 standard;diversity gain;Interference;Array signal processing;Antennas;Uplink;IEEE 802.11 Standards;Throughput;Wireless communication;Multi-user;Multi-AP;Beamforming;Interference Nulling;Channel Reciprocity;WiFi Networks;Rate Adaptation},
doi={10.1109/INFOCOM.2014.6848009},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848010,
author={G. Liang and U. C. Kozat},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={TOFEC: Achieving optimal throughput-delay trade-off of cloud storage using erasure codes},
year={2014},
volume={},
number={},
pages={826-834},
abstract={Our paper presents solutions using erasure coding, parallel connections to storage cloud and limited chunking (i.e., dividing the object into a few smaller segments) together to significantly improve the delay performance of uploading and downloading data in and out of cloud storage. TOFEC is a strategy that helps front-end proxy adapt to level of workload by treating scalable cloud storage (e.g. Amazon S3) as a shared resource requiring admission control. Under light workloads, TOFEC creates more smaller chunks and uses more parallel connections per file, minimizing service delay. Under heavy workloads, TOFEC automatically reduces the level of chunking (fewer chunks with increased size) and uses fewer parallel connections to reduce overhead, resulting in higher throughput and preventing queueing delay. Our trace-driven simulation results show that TOFEC's adaptation mechanism converges to an appropriate code that provides the optimal delay-throughput trade-off without reducing system capacity. Compared to a non-adaptive strategy optimized for throughput, TOFEC delivers 2.5× lower latency under light workloads; compared to a non-adaptive strategy optimized for latency, TOFEC can scale to support over 3× as many requests.},
keywords={cloud computing;queueing theory;resource allocation;optimal throughput-delay trade-off;erasure coding;parallel connections;delay performance improvement;data uploading;data downloading;TOFEC;front-end proxy;workload level;scalable cloud storage;Amazon S3;resource sharing;admission control;service delay minimization;heavy-workloads;light-workloads;chunking level;overhead reduction;throughput;queueing delay prevention;trace-driven simulation;TOFEC adaptation mechanism;latency reduction;Delays;Cloud computing;Strips;Standards;Correlation;Encoding;Throughput;FEC;Cloud storage;Queueing;Delay},
doi={10.1109/INFOCOM.2014.6848010},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848011,
author={H. Shen and Z. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={New bandwidth sharing and pricing policies to achieve a win-win situation for cloud provider and tenants},
year={2014},
volume={},
number={},
pages={835-843},
abstract={For predictable application performance or fairness in network sharing in clouds, many bandwidth allocation policies have been proposed. However, with these policies, tenants are not incentivized to use idle bandwidth or prevent link congestion, and may even take advantage of the policies to gain unfair bandwidth allocation. Increasing network utilization while avoiding congestion not only benefits cloud provider but also the tenants by improving application performance. In this paper, we propose a new pricing model that sets different unit prices for reserved bandwidth, the bandwidth on congested links and on uncongested links, and makes the unit price for congested links proportional to their congestion degrees. We use game theory model to analyze tenants' behaviors in our model and the current pricing models, which shows the effectiveness of our model in providing the incentives. With the pricing model, we propose a network sharing policy to achieve both min-guarantee and proportionality, while prevent tenants from earning unfair bandwidth. We further propose methods for each virtual machine to arrange its traffic to maximize its utility. As a result, our solution creates a win-win situation, where tenants strive to increase their benefits in bandwidth sharing, which also concurrently increases the utilities of cloud provider and other tenants. Our simulation and trace-driven experimental results show the effectiveness of our solution in creating the win-win situation.},
keywords={cloud computing;game theory;operating systems (computers);virtual machines;bandwidth sharing;pricing policies;win-win situation;cloud provider;cloud tenants;network sharing;bandwidth allocation policies;link congestion;bandwidth allocation;game theory model;pricing models;network sharing policy;virtual machine;Bandwidth;Pricing;Channel allocation;Resource management;Barium;Analytical models;Computers},
doi={10.1109/INFOCOM.2014.6848011},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848012,
author={Y. Hua and X. Liu and D. Feng},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Neptune: Efficient remote communication services for cloud backups},
year={2014},
volume={},
number={},
pages={844-852},
abstract={In order to efficiently achieve fault tolerance in cloud computing, large-scale data centers generally leverage remote backups to improve system reliability. Due to longdistance and expensive network transmission, the backups incur heavy communication overheads and potential errors. To address this important problem, we propose an efficient remote communication service, called Neptune. Neptune efficiently transmits massive data between long-distance data centers via a cost-effective filtration scheme. The filtration in Neptune is interpreted as eliminating redundancy and compressing similarity of files, which are generally studied independently in existing work. In order to bridge the gap between them, Neptune leverages chunk-level deduplication to eliminates duplicate files, and approximate delta compression to compresses similar files. Moreover, in order to reduce the complexity and overheads, Neptune uses a locality-aware hashing to group similar files and proposes shortcut delta chains for fast remote recovery. We have really implemented Neptune. We examine the Neptune performance by using real-world traces of LANL, HP, MSN and Google. Compared with state-of-the-art work, experimental results demonstrate the efficiency and efficacy of Neptune.},
keywords={cloud computing;computer centres;file organisation;software fault tolerance;Neptune;remote communication services;cloud backups;fault tolerance;cloud computing;large-scale data centers;system reliability;long-distance data centers;duplicate files;locality-aware hashing;LANL;HP;MSN;Google;Servers;Indexes;Containers;Computers;Throughput;Vectors;Bandwidth},
doi={10.1109/INFOCOM.2014.6848012},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848013,
author={G. Dán and N. Carlsson},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Dynamic content allocation for cloud-assisted service of periodic workloads},
year={2014},
volume={},
number={},
pages={853-861},
abstract={Motivated by improved models for content workload prediction, in this paper we consider the problem of dynamic content allocation for a hybrid content delivery system that combines cloud-based storage with low cost dedicated servers that have limited storage and unmetered upload bandwidth. We formulate the problem of allocating contents to the dedicated storage as a finite horizon dynamic decision problem, and show that a discrete time decision problem is a good approximation for piecewise stationary workloads. We provide an exact solution to the discrete time decision problem in the form of a mixed integer linear programming problem, propose computationally feasible approximations, and give bounds on their approximation ratios. Finally, we evaluate the algorithms using synthetic and measured traces from a commercial music on-demand service and give insight into their performance as a function of the workload characteristics.},
keywords={cloud computing;integer programming;linear programming;dynamic content allocation;cloud-assisted service;content workload prediction;hybrid content delivery system;cloud-based storage;finite horizon dynamic decision problem;discrete time decision problem;mixed integer linear programming problem;approximation ratios;commercial music on-demand service;Bandwidth;Servers;Approximation methods;Resource management;Steady-state;Optimization;Aggregates},
doi={10.1109/INFOCOM.2014.6848013},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848014,
author={K. Kim and H. Nam and H. Schulzrinne},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={WiSlow: A Wi-Fi network performance troubleshooting tool for end users},
year={2014},
volume={},
number={},
pages={862-870},
abstract={Slow Internet connectivity is often caused by poor Wi-Fi performance. The main reasons of such performance degradation include channel contention and non-Wi-Fi interference. Although these problem sources can be easily removed in many cases once they are discovered, it is difficult for end users to identify the sources of such interference. We investigated the characteristics of different sources that can degrade Wi-Fi performance, and developed WiSlow, a software tool that diagnoses the root causes of poor Wi-Fi performance using user-level network probes, and leveraging peer collaboration to identify the physical location of these causes. WiSlow uses two main methods: packet loss analysis and 802.11 ACK number analysis. The accuracy of WiSlow exceeds 90% when the sources are close to Wi-Fi devices. Also, our experiment proves that the collaborative approach is feasible for determining the relative location of an interfering device.},
keywords={computer network performance evaluation;Internet;packet radio networks;radiofrequency interference;wireless LAN;Wi-Fi network performance;troubleshooting tool;Internet connectivity;channel contention;nonWi-Fi interference;WiSlow;user-level network probes;packet loss analysis;802.11 ACK number analysis;Interference;IEEE 802.11 Standards;Packet loss;Monitoring;Pediatrics;Bit rate},
doi={10.1109/INFOCOM.2014.6848014},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848015,
author={R. Jia and J. Zhang and X. Wang and X. Tian and Q. Zhang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Scaling laws for heterogeneous cognitive radio networks with cooperative secondary users},
year={2014},
volume={},
number={},
pages={871-879},
abstract={Cognitive radio (CR) technique is considered an effective mechanism to relieve the spectrum scarcity issue, where the secondary users (SUs) can utilize the idle spectrum of the primary users (PUs). How the performance of the wireless network will be influenced by the introduction of CR technique has been attracting much attention in past years. While many efforts have been made to study the cognitive radio network, where the data source and the destination (S-D) is homogeneously distributed, the research on cognitive radio networks (CRN) with heterogeneous S-D distribution is still very limited. In this paper, we investigate the throughput and delay scaling law in the heterogeneous cognitive radio network (HCRN), where the S-D pair follows a rank based model and SUs provide relay service for PUs in reciprocating the utilization of PUs' idle spectrum. By applying a cellular TDMA scheduling scheme, we show that the primary network throughput is the same for different heterogeneous extents of S-D distribution owing to the flexible assistance of SUs, while the throughput of secondary networks is proven to be changing with respect to the S-D heterogeneity exponent denoted by α. In addition, the delay scaling are derived for both primary and secondary networks and shown to be altering in accordance with α. Further, we reveal that the density of SUs required to assist PUs can be dramatically reduced when considering the S-D heterogeneity, while achieving the same primary network throughput.},
keywords={cellular radio;cognitive radio;radio spectrum management;relay networks (telecommunication);scheduling;time division multiple access;primary network throughput;secondary networks;S-D heterogeneity exponent;cellular TDMA scheduling scheme;PU idle spectrum;relay service;rank based model;HCRN;delay scaling law;heterogeneous S-D distribution;data sourc-destination;wireless network;spectrum scarcity;SUs;primary users;CR technique;cooperative secondary users;heterogeneous cognitive radio networks;throughput scaling laws;Throughput;Delays;Cognitive radio;Mathematical model;Protocols;Relays;Equations},
doi={10.1109/INFOCOM.2014.6848015},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848016,
author={Y. Hou and M. Li and X. Yuan and Y. T. Hou and W. Lou},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Cooperative cross-technology interference mitigation for heterogeneous multi-hop networks},
year={2014},
volume={},
number={},
pages={880-888},
abstract={This paper explores a new paradigm for the coexistence among heterogeneous multi-hop networks in unplanned deployment settings, called cooperative interference mitigation (CIM). CIM exploits recent advancements in physical layer technologies such as technology-independent multiple output (TIMO), making it possible for disparate networks to cooperatively mitigate the interference to each other to enhance everyone's performance, even if they possess different wireless technologies. This paper offers a thorough study of the CIM paradigm for unplanned multi-hop networks. We first show the feasibility of CIM among heterogeneous multi-hop networks by exploiting only channel ratio information, and then establish a tractable model to accurately characterize the CIM behaviors of both networks. We also develop a bi-criteria optimization formulation to maximize both networks' throughput, and propose a new methodology to compute the Pareto-optimal throughput curve as performance bound. Simulation results show that CIM provides significant performance gains to both networks compared with the traditional interference-avoidance paradigm.},
keywords={cooperative communication;interference suppression;Pareto optimisation;radio networks;cooperative cross-technology interference mitigation;heterogeneous multihop networks;CIM;technology-independent multiple output;TIMO;physical layer technology;wireless technology;unplanned multihop networks;channel ratio information;bi-criteria optimization formulation;network throughput;Pareto-optimal throughput curve;performance bound;interference-avoidance paradigm;Integrated circuits;Interference;MIMO;Computer integrated manufacturing;Spread spectrum communication;Throughput;Receivers},
doi={10.1109/INFOCOM.2014.6848016},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848017,
author={A. Rahman and N. Abu-Ghazaleh},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={On the expected size of minimum-energy path-preserving topologies for wireless multi-hop networks},
year={2014},
volume={},
number={},
pages={889-897},
abstract={Topology Control (TC) algorithms for multi-hop wireless networks create a connected communication subgraph that satisfies some topological properties by assigning appropriate transmission power to each node. A topology is said to be minimum-energy path-preserving if it preserves minimum energy paths between every pair of nodes. Creating minimum-energy path-preserving sparse topologies is a fundamental research problem in TC that has been addressed in several recent research works. Although sparseness is a key metric in comparing the performance of such algorithms, none of these prior works provides analytical models to determine the sparseness. In this paper, we provide a generic analytical model for evaluating sparseness of such topologies. The derived analytical expressions are useful in determining topology size without running simulations or prior to the deployment of real systems. Moreover, we demonstrate how to analytically couple sparseness of topologies with the radio transceiver parameters. The analytical expressions are validated through extensive simulation experiments.},
keywords={graph theory;network theory (graphs);radio networks;telecommunication network topology;wireless multihop networks;topology control algorithms;TC algorithm;connected communication subgraph;transmission power;minimum-energy path-preserving sparse topology;generic analytical model;expected topology size;radio transceiver parameters;Topology;Network topology;Relays;Analytical models;Wireless networks;Conferences;Computers},
doi={10.1109/INFOCOM.2014.6848017},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848018,
author={H. Li and C. Huang and S. Cui and J. Zhang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Distributed opportunistic scheduling for wireless networks powered by renewable energy sources},
year={2014},
volume={},
number={},
pages={898-906},
abstract={This paper considers an ad hoc network with multiple transmitter-receiver pairs, in which all transmitters are capable of harvesting renewable energy from the environment and compete for the same channel by random access. To quantify the roles of both the energy state information (ESI) and the channel state information (CSI), a distributed opportunistic scheduling (DOS) framework with a save-then-transmit scheme is proposed. First, in the channel probing stage, each transmitter probes the CSI via channel contention; next, in the data transmission stage, the successful transmitter decides to either give up the channel (if the expected reward calculated over the CSI and ESI is small) or hold and utilize the channel by optimally exploring the energy harvesting and data transmission tradeoff. With a constant energy arrival model, i.e., the energy harvesting rate keeps identical over the time of interest, the expected throughput maximization problem is formulated as an optimal stopping problem, whose solution is shown to exist and have a threshold-based structure, for both the homogeneous and heterogenous cases. Furthermore, we prove that there exists a steady-state distribution for the stored energy level at each transmitter, and propose an efficient iterative algorithm for its computation. Finally, we show via numerical results that the proposed scheme can achieve a potential 175% throughput gain compared with the method of best-effort delivery.},
keywords={ad hoc networks;data communication;energy harvesting;radio receivers;radio transmitters;renewable energy sources;scheduling;distributed opportunistic scheduling;DOS framework;wireless networks;ad hoc network;multiple transmitter-receiver pairs;harvesting renewable energy sources;energy state information;ESI;channel state information;CSI;save-then-transmit scheme;channel probing stage;channel contention;data transmission;steady-state distribution;iterative algorithm;Transmitters;Throughput;Data communication;Energy states;Steady-state;Batteries;Bismuth},
doi={10.1109/INFOCOM.2014.6848018},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848019,
author={C. Gao and W. Zhang and J. Tang and C. Wang and S. Zou and S. Su},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Relax, but Do Not Sleep: A new perspective on Green Wireless Networking},
year={2014},
volume={},
number={},
pages={907-915},
abstract={Saving power on base stations (BS) becomes a critical issue in wireless cellular networks. Many existing work has proposed to schedule BS into sleep to save energy. However, in reality, it is very difficult to shut down and reboot BSs frequently due to numerous technical issues and performance requirements. In this work, we propose a much more practical solution and offer a new perspective on implementing Green Wireless Networking by embracing the hot-trended small cell network idea. Instead of putting BSs into sleep, we tactically reduce the coverage (and the power usage) of each BS, and strategically place microcells (relay stations) to offload the traffic transmitted to/from BSs in order to save total power consumption. We propose approximation algorithms for various network design scenarios, with different wireless network setups and different power saving optimization objectives. Extensive numerical results are presented to confirm our theoretical analysis.},
keywords={cellular radio;energy conservation;power consumption;relay networks (telecommunication);telecommunication power management;telecommunication traffic;green wireless networking;power saving;base stations;wireless cellular networks;microcells;relay stations;power consumption;Power demand;Energy consumption;Green products;Base stations;Relays;Wireless networks},
doi={10.1109/INFOCOM.2014.6848019},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848020,
author={W. Hu and G. Cao},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Energy optimization through traffic aggregation in wireless networks},
year={2014},
volume={},
number={},
pages={916-924},
abstract={Cellular networks can provide pervasive data access for smartphones, but also consume lots of energy, because the cellular interface has to stay in high power state for a long time (called long tail problem) after a data transmission. In this paper, we propose to reduce the tail energy by aggregating the data traffic of multiple nodes using their P2P interfaces. This traffic aggregation problem is formalized as finding the best task schedule to minimize energy. We first propose an A* search algorithm, which can reduce the search space for finding the optimal schedule offline, and then introduce an online traffic aggregation algorithm. We have implemented the online traffic aggregation algorithm on Android smartphones, and have built a small testbed. Trace-driven simulations and Experimental results show that our traffic aggregation algorithm can significantly reduce the energy and delay.},
keywords={cellular radio;optimisation;smart phones;telecommunication power management;energy optimization;wireless network;cellular network;pervasive data;long tail problem;tail energy reduction;P2P interface;A* search algorithm;search space reduction;online traffic aggregation algorithm;Android smart phone;Conferences;Computers;Delays;Computational modeling;Abstracts;Schedules;Energy measurement},
doi={10.1109/INFOCOM.2014.6848020},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848021,
author={A. K. Das and P. H. Pathak and C. Chuah and P. Mohapatra},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Contextual localization through network traffic analysis},
year={2014},
volume={},
number={},
pages={925-933},
abstract={The rise of location-based services has enabled many opportunities for content service providers to optimize the content delivery based on user's location. Since sharing precise location remains a major privacy concern among the users, many location-based services rely on contextual location (e.g. residence, cafe etc.) as opposed to acquiring user's exact physical location. In this paper, we present PACL (Privacy-Aware Contextual Localizer), which can learn user's contextual location just by passively monitoring user's network traffic. PACL can discern a set of vital attributes (statistical and application-based) from user's network traffic, and predict user's contextual location with a very high accuracy. We design and evaluate PACL using real-world network traces of over 1700 users with over 100 gigabytes of total data. Our results show that PACL (built using decision tree) can predict user's contextual location with the accuracy of around 87%.},
keywords={data privacy;mobile computing;mobility management (mobile radio);contextual localization;network traffic analysis;location based services;user location;user exact physical location;privacy aware contextual localizer;user network traffic;vital attribute;IP networks;Monitoring;Privacy;IEEE 802.11 Standards;Airports;Mobile radio mobility management;Predictive models},
doi={10.1109/INFOCOM.2014.6848021},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848022,
author={M. Malboubi and L. Wang and C. Chuah and P. Sharma},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Intelligent SDN based traffic (de)Aggregation and Measurement Paradigm (iSTAMP)},
year={2014},
volume={},
number={},
pages={934-942},
abstract={Fine-grained traffic flow measurement, which provides useful information for network management tasks and security analysis, can be challenging to obtain due to monitoring resource constraints. The alternate approach of inferring flow statistics from partial measurement data has to be robust against dynamic temporal/spatial fluctuations of network traffic. In this paper, we propose an intelligent Traffic (de)Aggregation and Measurement Paradigm (iSTAMP), which partitions TCAM entries of switches/routers into two parts to: 1) optimally aggregate part of incoming flows for aggregate measurements, and 2) de-aggregate and directly measure the most informative flows for per-flow measurements. iSTAMP then processes these aggregate and per-flow measurements to effectively estimate network flows using a variety of optimization techniques. With the advent of Software-Defined-Networking (SDN), such real-time rule (re)configuration can be achieved via OpenFlow or other similar SDN APIs. We first show how to design the optimal aggregation matrix for minimizing the flow-size estimation error. Moreover, we propose a method for designing an efficient-compressive flow aggregation matrix under hard resource constraints of limited TCAM sizes. In addition, we propose an intelligent Multi-Armed Bandit based algorithm to adaptively sample the most “rewarding” flows, whose accurate measurements have the highest impact on the overall flow measurement and estimation performance. We evaluate the performance of iSTAMP using real traffic traces from a variety of network environments and by considering two applications: traffic matrix estimation and heavy hitter detection. Also, we have implemented a prototype of iSTAMP and demonstrated its feasibility and effectiveness in Mininet environment.},
keywords={content-addressable storage;optimisation;software radio;telecommunication network management;telecommunication traffic recording;software defined networking;intelligent SDN traffic aggregation and measurement paradigm;iSTAMP;traffic flow measurement;network management;security analysis;network traffic;per-flow measurements;OpenFlow;flow aggregation matrix;flow-size estimation error;intelligent multiarmed bandit based algorithm;traffic matrix estimation;heavy hitter detection;Mininet environment;TCAM;Monitoring;Estimation;Optimization;Aggregates;Routing;Accuracy;Heuristic algorithms},
doi={10.1109/INFOCOM.2014.6848022},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848023,
author={A. Lutu and M. Bagnulo and J. Cid-Sueiro and O. Maennel},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Separating wheat from chaff: Winnowing unintended prefixes using machine learning},
year={2014},
volume={},
number={},
pages={943-951},
abstract={In this paper, we propose the use of prefix visibility at the interdomain level as an early symptom of anomalous events in the Internet. We focus on detecting anomalies which, despite their significant impact on the routing system, remain concealed from state of the art tools. We design a machine learning system to winnow the prefixes with unintended limited visibility - symptomatic of anomalous events - from the prefixes with intended limited visibility - resulting from legitimate routing operations. We train a winnowing algorithm with ground-truth data on 20,000 operational limited visibility prefixes (LVPs) already classified by the operators of the origin networks. The ground-truth was collected using the BGP Visibility Scanner, a tool we developed to provide operators with a multi-angle view on the efficacy of their routing policies. We build a dataset with the pre-classified prefixes and the features describing their visibility status dynamics. We further use this dataset to derive a boosted decision tree which winnows unintended LVPs with an accuracy of 95%.},
keywords={computer network security;decision trees;Internet;internetworking;learning (artificial intelligence);routing protocols;interdomain level;anomalous events;Internet;anomaly detection;routing system;machine learning system;unintended limited visibility;intended limited visibility;legitimate routing operations;winnowing algorithm;ground-truth data;operational limited visibility prefixes;BGP visibility scanner;routing policies;preclassified prefixes;visibility status dynamics;boosted decision tree;unintended LVP;border gateway protocol;Routing;Internet;Monitoring;Training;Feeds;Algorithm design and analysis;Machine learning algorithms},
doi={10.1109/INFOCOM.2014.6848023},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848024,
author={A. X. Liu and E. Torng},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={An overlay automata approach to regular expression matching},
year={2014},
volume={},
number={},
pages={952-960},
abstract={Regular expression (RegEx) matching, the core operation of intrusion detection and prevention systems, remains a fundamentally challenging problem. A desired RegEx matching scheme should satisfy four requirements: DFA speed, NFA size, automated construction, and scalable construction. Despite lots of work on RegEx matching, no prior scheme satisfies all four of these requirements. In this paper, we approach this holy grail by proposing OverlayCAM, a RegEx matching scheme that satisfies all four requirements. The theoretical underpinning of our scheme is OD<sup>2</sup>FA, a new automata model proposed in this paper that captures both state and transition replication inherent in DFAs. Our RegEx matching solution processes one input character per lookup like a DFA, requires only the space of an NFA, is grounded in sound automata models, is easy to deploy in existing network devices, and comes with scalable and automated construction algorithms.},
keywords={automata theory;pattern matching;security of data;overlay automata approach;regular expression matching;RegEx matching;intrusion detection and prevention systems;DFA speed requirement;NFA size requirement;automated construction requirement;scalable construction requirement;OverlayCAM scheme;OD2FA automata model;Automata;Explosions;Conferences;Computers;Memory management;Random access memory;Security},
doi={10.1109/INFOCOM.2014.6848024},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848025,
author={X. Ji and Y. He and J. Wang and W. Dong and X. Wu and Y. Liu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Walking down the STAIRS: Efficient collision resolution for wireless sensor networks},
year={2014},
volume={},
number={},
pages={961-969},
abstract={Collision resolution is a crucial issue in wireless sensor networks. The existing approaches of collision resolution have drawbacks with respect to energy efficiency and processing latency. In this paper, we propose <i>ST AIRS</i>, a time and energy efficient collision resolution mechanism for wireless sensor networks. STAIRS incorporates the constructive interference technique in its design and explicitly forms superimposed colliding signals. Through extensive observations and theoretical analysis, we show that the RSSI of the superimposed signals exhibit stairs-like phenomenon with different number of contenders. That principle offers an attractive feature to efficiently distinguish multiple contenders and in turn makes collision-free schedules for channel access. In the design and implementation of STAIRS, we address practical challenges such as contenders alignment, online detection of RSSI change points, and fast channel assignment. The experiments on real testbed show that STARIS realizes fast and effective collision resolution, which significantly improves the network performance in terms of both latency and throughput.},
keywords={channel allocation;radiofrequency interference;wireless sensor networks;STAIRS;wireless sensor networks;energy efficient collision resolution mechanism;constructive interference technique;collision-free schedules;contenders alignment;RSSI change points;fast channel assignment;network performance;network throughput;Receivers;Wireless sensor networks;Image edge detection;Interference;Schedules;Synchronization;Signal resolution},
doi={10.1109/INFOCOM.2014.6848025},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848026,
author={Y. Hu and X. Wang and X. Gan},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Critical sensing range for mobile heterogeneous camera sensor networks},
year={2014},
volume={},
number={},
pages={970-978},
abstract={In camera sensor networks (CSNs), full view coverage, in which any direction of any point in the operational region is covered by at least one camera sensor, is of great significance since image shot at the frontal viewpoint considerably increases the possibility to recognize the object. However, finding the critical condition to achieve full view coverage in mobile heterogeneous CSNs remains an open question. In this paper, we analyze both the static and mobile random deployed camera sensor networks. A centralized parameter - equivalent sensing radius (ESR) - is defined to evaluate the critical requirement for asymptotic full view coverage in heterogeneous CSNs. We derive the critical sensing range for full view coverage under static model, 2-dimensional random walk mobility model, 1-dimensional random walk mobility model and random rotating model. We then discuss the impact of various mobility patterns on sensing energy consumption and study the relationship between ESR and percentage of full view coverage, and show that random walk mobility model can decrease the sensing energy consumption under certain delay tolerance. To our knowledge, our work is the very first that derive the critical condition to achieve full view coverage in mobile heterogeneous CSNs.},
keywords={mobility management (mobile radio);object recognition;random processes;sensor placement;video cameras;visual communication;wireless sensor networks;critical sensing range;mobile heterogeneous camera sensor network;object recognition;mobile heterogeneous CSN;static random deployed camera sensor network;mobile random deployed camera sensor network;centralized parameter;equivalent sensing radius;asymptotic full view coverage;random rotating model;mobility patterns;ESR;random walk mobility model;Cameras;Robot sensing systems;Mobile communication;Computers;Mobile computing;Numerical models},
doi={10.1109/INFOCOM.2014.6848026},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848027,
author={K. Huang and C. Ni and R. Sarkar and J. Gao and J. S. B. Mitchell},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Bounded stretch geographic homotopic routing in sensor networks},
year={2014},
volume={},
number={},
pages={979-987},
abstract={Homotopic routing asks for a path going around holes according to a given “threading”. Paths of different homo-topy types can be used to improve load balancing and routing resilience. We propose the first lightweight homotopic routing scheme that generates constant bounded stretch compared to the shortest path of the same homotopy type. Our main insight is that in a sequence of triangles to traverse, a message always routed to the nearest point on the next triangle in the sequence travels at most a constant times the length of any shortest path going through the same sequence of triangles. Our routing scheme operates on two levels enabled by a coarse triangulation. The top level is used to specify and represent the requested homotopy type, while the bottom level executes the local greedy routing on a triangle sequence. After a preprocessing step that triangulates the given region and creates a minimum-size auxiliary structure, routing operates greedily at two different resolutions. We also present simulation analysis in a variety of settings and show that the paths indeed have small stretch in practice, considerably shorter than the bounds guaranteed by the theory.},
keywords={telecommunication network routing;wireless sensor networks;wireless sensor network;auxiliary structure;triangle sequence;greedy routing;coarse triangulation;lightweight homotopic routing scheme;routing resilience;load balancing;sensor networks;bounded stretch geographic homotopic routing;Routing;Spirals;Robot sensing systems;Conferences;Computers;Joining processes},
doi={10.1109/INFOCOM.2014.6848027},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848028,
author={S. Basagni and L. Bölöni and P. Gjanci and C. Petrioli and C. A. Phillips and D. Turgut},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Maximizing the value of sensed information in underwater wireless sensor networks via an autonomous underwater vehicle},
year={2014},
volume={},
number={},
pages={988-996},
abstract={This paper considers underwater wireless sensor networks (UWSNs) for submarine surveillance and monitoring. Nodes produce data with an associated value, decaying in time. An autonomous underwater vehicle (AUV) is sent to retrieve information from the nodes, through optical communication, and periodically emerges to deliver the collected data to a sink, located on the surface or onshore. Our objective is to determine a collection path for the AUV so that the Value of Information (VoI) of the data delivered to the sink is maximized. To this purpose, we first define an Integer Linear Programming (ILP) model for path planning that considers realistic data communication rates, distances, and surfacing constraints. We then define the first heuristic for path finding that is adaptive to the occurrence of new events, relying only on acoustic communication for exchanging short control messages. Our Greedy and Adaptive AUV Path-finding (GAAP) heuristic drives the AUV to collect packets from nodes to maximize the VoI of the delivered data. We compare the VoI of data obtained by running the optimum solution derived by the ILP model to that obtained from running GAAP over UWSNs with realistic and desirable size. In our experiments GAAP consistently delivers more than 80% of the theoretical maximum VoI determined by the ILP model.},
keywords={autonomous underwater vehicles;information retrieval;integer programming;linear programming;path planning;underwater acoustic communication;video surveillance;underwater wireless sensor networks;autonomous underwater vehicle;sensed information;UWSN;submarine surveillance;submarine monitoring;information retrieval;optical communication;Value of Information;VoI;integer linear programming;ILP;path planning;data communication;greedy and adaptive AUV path finding;GAAP;Silicon;Optimized production technology;Acoustics;Path planning;Monitoring;Videos;Underwater vehicles},
doi={10.1109/INFOCOM.2014.6848028},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848029,
author={J. Sun and X. Chen and J. Zhang and Y. Zhang and J. Zhang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={SYNERGY: A game-theoretical approach for cooperative key generation in wireless networks},
year={2014},
volume={},
number={},
pages={997-1005},
abstract={This paper studies secret key establishment between two adjacent mobile nodes, which is crucial for securing emerging device-to-device (D2D) communication. As a promising method, cooperative key generation allows two mobile nodes to select some common neighbors as relays and directly extract a secret key from the wireless channels among them. A challenging issue that has been overlooked is that mobile nodes are often self-interested and reluctant to act as relays without adequate reward in return. We propose SYNERGY, a game-theoretical approach for stimulating cooperative key generation. The underlying idea of SYNERGY is to partition a group of mobile nodes into disjoint coalitions such that the nodes in each coalition fully collaborate on cooperative key generation. We formulate the group partitioning as a coalitional game and design centralized and also distributed protocols for obtaining the core solution to the game. The performance of SYNERGY is evaluated by extensive simulations.},
keywords={cooperative communication;game theory;private key cryptography;radio networks;relay networks (telecommunication);telecommunication security;wireless channels;synergy;game-theoretical approach;cooperative key generation;wireless network;secret key establishment;adjacent mobile node;device-to-device communication;D2D communication;wireless channel;secret key extraction;relay channel;mobile node partition;disjoint coalition;group partitioning;distributed protocol;Peer-to-peer computing;Relays;Mobile nodes;Games;Wireless communication;Servers},
doi={10.1109/INFOCOM.2014.6848029},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848030,
author={G. Yunchuan and Y. Lihua and L. Licai and F. Binxing},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Utility-based cooperative decision in cooperative authentication},
year={2014},
volume={},
number={},
pages={1006-1014},
abstract={In mobile networks, cooperative authentication is an efficient way to recognize false identities and messages. However, an attacker can track the location of cooperative mobile nodes by monitoring their communications. Moreover, mobile nodes consume their own resources when cooperating with other nodes in the process of authentication. These two factors cause selfish mobile nodes not to actively participate in authentication. In this paper, a bargaining-based game for cooperative authentication is proposed to help nodes decide whether to participate in authentication or not, and our strategy guarantees that mobile nodes participating in cooperative authentication can obtain the maximum utility, all at an acceptable cost. We obtain Nash equilibrium in static complete information games. To address the problem of nodes not knowing the utility of other nodes, incomplete information games for cooperative authentication are established. We also develop an algorithm based on incomplete information games to maximize every node's utility. The simulation results demonstrate that our strategy has the ability to guarantee authentication probability and increase the number of successful authentications.},
keywords={game theory;mobile ad hoc networks;probability;telecommunication security;utility based cooperative decision;cooperative authentication;mobile networks;cooperative mobile nodes;authentication process;mobile nodes;Nash equilibrium;information games;authentication probability;MANET;mobile ad hoc network;Conferences;Computers;Human computer interaction;High definition video;Bismuth;Cooperative authentication;location privacy;games},
doi={10.1109/INFOCOM.2014.6848030},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848031,
author={H. Rahbari and M. Krunz and L. Lazos},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Security vulnerability and countermeasures of frequency offset correction in 802.11a systems},
year={2014},
volume={},
number={},
pages={1015-1023},
abstract={Frequency offset (FO) is an inherent feature of wireless communications. It results from differences in the operating frequency of different radio oscillators. Failure to compensate for the FO may lead to a decoding failure, particularly in OFDM systems. IEEE 802.11a/g systems use a globally known preamble to deal with this issue. In this paper, we demonstrate how an adversary can exploit the structure and publicity of 802.11a's frame preamble to launch a low-power reactive jamming attack against the FO estimation mechanism. In this attack, the adversary will need to quickly detect a PHY frame and subsequently distort the FO estimation mechanism, irrespective of the channel conditions. By employing a fast frame detection technique, and optimizing the energy and structure of the jamming signal, we show the feasibility of such an attack. Furthermore, we propose some mitigation techniques and evaluate one of them through simulations and USRP testbed experimentation.},
keywords={jamming;OFDM modulation;radiofrequency oscillators;software radio;telecommunication security;telecommunication standards;wireless channels;wireless LAN;security vulnerability;countermeasures;frequency offset correction;FO correction;wireless communications;radio oscillators;decoding failure;OFDM systems;IEEE 802.11a/g systems;low-power reactive jamming attack;FO estimation mechanism;PHY frame;channel conditions;fast frame detection technique;jamming signal;USRP testbed experimentation;Jamming;Estimation;OFDM;Receivers;Channel estimation;Noise;Timing},
doi={10.1109/INFOCOM.2014.6848031},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848032,
author={F. Huo and G. Gong},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A new efficient physical layer OFDM encryption scheme},
year={2014},
volume={},
number={},
pages={1024-1032},
abstract={In this paper, we propose a new encryption scheme for OFDM systems. The reason for physical layer approach is that it has the least impact on the system and is the fastest among all layers. This scheme is computationally secure against the adversary. It requires less key streams compared with other approaches. The idea comes from the importance of orthogonality in OFDM symbols. Destroying the orthogonality create intercarrier interferences. This in turn cause higher bit and symbol decoding error rate. The encryption is performed on the time domain OFDM symbols, which is equivalent to performing nonlinear masking in the frequency domain. Various attacks are explored in this paper. These include known plaintext and ciphertext attack, frequency domain attack, time domain attack, statistical attack and random guessing attack. We show our scheme is resistant against these attacks. Finally, simulations are conducted to compare the new scheme with the conventional cipher encryption.},
keywords={cryptography;decoding;intercarrier interference;OFDM modulation;efficient physical layer OFDM encryption scheme;OFDM systems;physical layer approach;OFDM symbols;orthogonality;intercarrier interferences;symbol decoding error rate;time domain OFDM symbols;nonlinear masking;frequency domain;ciphertext attack;plaintext attack;frequency domain attack;time domain attack;statistical attack;random guessing attack;cipher encryption;OFDM;Encryption;Ciphers;Time-domain analysis;Receivers;Frequency-domain analysis},
doi={10.1109/INFOCOM.2014.6848032},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848033,
author={L. Chen and H. Shen},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Consolidating complementary VMs with spatial/temporal-awareness in cloud datacenters},
year={2014},
volume={},
number={},
pages={1033-1041},
abstract={In cloud datacenters, effective resource provisioning is needed to maximize energy efficiency and utilization of cloud resources while guaranteeing the Service Level Agreement (SLA) for tenants. Previous resource provisioning strategies either allocate physical resources to virtual machines (VMs) based on static VM resource demands or dynamically handle the variations in VM resource requirements through live VM migrations. However, the former fail to maximize energy efficiency and resource utilization while the latter produce high migration overhead. To handle these problems, we propose an initial VM allocation mechanism that consolidates complementary VMs with spatial/temporal-awareness. Complementary VMs are the VMs whose total demand of each resource dimension (in the spatial space) nearly reaches their host's capacity during VM lifetime period (in the temporal space). Based on our observation of the existence of VM resource utilization patterns, the mechanism predicts the lifetime resource utilization patterns of short-term VMs or periodical resource utilization patterns of long-term VMs. Based on the predicted patterns, it coordinates the requirements of different resources and consolidates complementary VMs in the same physical machine (PM). This mechanism reduces the number of PMs needed to provide VM service hence increases energy efficiency and resource utilization and also reduces the number of VM migrations and SLA violations. Simulation based on two real traces and real-world testbed experiments show that our initial VM allocation mechanism significantly reduces the number of PMs used, SLA violations and VM migrations of the previous resource provisioning strategies.},
keywords={cloud computing;computer centres;contracts;resource allocation;virtual machines;virtual machines;cloud datacenters;resource provisioning;service level agreement;SLA;physical resource allocation;VM allocation mechanism;VM resource utilization patterns;physical machine;PM;complementary VM consolidation;Resource management;Computers;Google;Conferences;Radio access networks;Virtual machining;Bandwidth},
doi={10.1109/INFOCOM.2014.6848033},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848034,
author={S. Chen and Y. Sun and U. C. Kozat and L. Huang and P. Sinha and G. Liang and X. Liu and N. B. Shroff},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={When queueing meets coding: Optimal-latency data retrieving scheme in storage clouds},
year={2014},
volume={},
number={},
pages={1042-1050},
abstract={Storage clouds, such as Amazon S3, are being widely used for web services and Internet applications. It has been observed that the delay for retrieving data from and placing data into the clouds is quite random, and exhibits weak correlations between different read/write requests. This inspires us to investigate a key problem: can we reduce the delay by transmitting data replications in parallel or using powerful erasure codes? In this paper, we study the problem of reducing the delay of downloading data from cloud storage systems by leveraging multiple parallel threads, assuming that the data has been encoded and stored in the clouds using fixed rate forward error correction (FEC) codes with parameters (n, k). That is., each file is divided into k equal-sized chunks, which are then expanded into n chunks such that any k chunks out of the n are sufficient to successfully restore the original file. The model can be depicted as a multiple-server queue with arrivals of data retrieving requests and a server corresponding to a thread. However, this is not a typical queueing model because a server can terminate its operation, depending on when other servers complete their service (due to the redundancy that is spread across the threads). Hence, to the best of our knowledge, the analysis of this queueing model remains quite uncharted. Real traces from Amazon S3 show that the time to retrieve a fixed size chunk is random and can be accurately approximated as an i.i.d. exponentially distributed random variable. We show that any work-conserving scheme is delay-optimal when k = 1. When k > 1, we find that a simple greedy scheme, which allocates all available threads to the head of line request, is delay optimal, which appears surprising.},
keywords={cloud computing;forward error correction;storage management;Web services;Web services;Internet applications;powerful erasure codes;downloading data;cloud storage systems;multiple parallel threads;forward error correction codes;FEC codes;multiple-server queue;data retrieving requests;queueing model;read/write requests;optimal-latency data retrieving scheme;parallel data replication transmission;exponentially distributed random variable;Amazon S3;work-conserving scheme;greedy scheme;Delays;Cloud computing;Encoding;Instruction sets;Servers;Redundancy;Resource management},
doi={10.1109/INFOCOM.2014.6848034},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848035,
author={R. Shea and H. Wang and J. Liu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Power consumption of virtual machines with network transactions: Measurement and improvements},
year={2014},
volume={},
number={},
pages={1051-1059},
abstract={There have been significant studies on virtual machines (VMs), including their power consumption in performing different types of tasks. The VM's power consumption with network transactions, however, has seldom been examined. This paper presents an empirical study on the power consumption of typical virtualization packages while performing network tasks. We find that both Hardware Virtualization and Paravirtualization add considerable energy overhead, affecting both sending and receiving, and a busy virtualized web-server may consume 40% more energy than its non-virtualized counterparts. Our detailed profiling on packet path reveals that a VM can take 5 times more cycles to deliver a packet than a bare-metal machine, and is also much less efficient on caching. Without fundamental changes to the hypervisor-based VM architecture, we show that the use of adaptive packet buffering potentially reduces the overhead. Its practicality and effectiveness in power saving are validated through driver-level implementation and experiments.},
keywords={power consumption;virtual machines;virtual machines;network transactions;VM power consumption;hardware virtualization packages;paravirtualization;energy overhead;virtualized Web server;bare-metal machine;caching;hypervisor-based VM architecture;adaptive packet buffering;power saving;Power demand;Virtualization;Virtual machining;Hardware;Kernel;Power measurement;Virtual machine monitors},
doi={10.1109/INFOCOM.2014.6848035},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848036,
author={Y. Li and W. Wang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Can mobile cloudlets support mobile applications?},
year={2014},
volume={},
number={},
pages={1060-1068},
abstract={A mobile cloudlet is a set of resource-rich mobile devices - referred to as cloudlet nodes - that an initiator mobile device can connect to for services. In this paper, we examine the fundamental mobile cloudlet properties that unfold whether and when a mobile cloudlet can provide mobile application service. Specifically, we investigate the cloudlet size, cloudlet node's lifetime and reachable time. Traces and mathematical analysis demonstrate that 1) the more frequently mobile devices meet, the larger the pool of computing resources an initiator can access; 2) intermittent connection between devices has little adverse effect on the optimal computing performance of mobile cloudlet in the long run; 3) the ratio E(T<sub>C</sub>)/[E(T<sub>I</sub>)+E(T<sub>C</sub>)] indicates the connection likelihood of an initiator and a cloudlet node (i.e., reachability of the cloudlet node), where T<sub>C</sub> and T<sub>I</sub> are their contact and inter-contact time. We further derive upper and lower bounds on computing capacity and computing speed of a mobile cloudlet. An initiator can use both bounds to decide whether to offload its task to remote clouds or local mobile cloudlets for better mobile application services.},
keywords={cloud computing;mobile computing;mobile applications;resource-rich mobile devices;initiator mobile device;mobile cloudlet properties;mobile application service;cloudlet size;cloudlet node lifetime;reachable time;computing resources;intermittent connection;optimal computing performance;connection likelihood;intercontact time;computing capacity;computing speed;remote clouds;Mobile communication;Mobile handsets;Cloud computing;Mobile computing;Conferences;Computers;Peer-to-peer computing},
doi={10.1109/INFOCOM.2014.6848036},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848037,
author={W. Bao and B. Liang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Structured spectrum allocation and user association in heterogeneous cellular networks},
year={2014},
volume={},
number={},
pages={1069-1077},
abstract={We study joint spectrum allocation and user association in heterogeneous cellular networks with multiple tiers of base stations. A stochastic geometric approach is applied as the basis to derive the average downlink user data rate in a closed-form expression. Then, the expression is employed as the objective function in jointly optimizing spectrum allocation and user association, which is of non-convex programming in nature. A computationally efficient Structured Spectrum Allocation and User Association (SSAUA) approach is proposed, solving the optimization problem optimally when the density of users is low, and near-optimally with a guaranteed performance bound when the density of users is high. A Surcharge Pricing Scheme (SPS) is also presented, such that the designed association bias values can be achieved in Nash equilibrium. Simulations and numerical studies are conducted to validate the accuracy and efficiency of the proposed SSAUA approach and SPS.},
keywords={cellular radio;concave programming;frequency allocation;geometric programming;stochastic programming;heterogeneous cellular networks;stochastic geometric approach;multiple base stations tiers;average downlink user data rate;closed-form expression;objective function;nonconvex programming;structured spectrum allocation and user association approach;SSAUA approach;optimization problem;guaranteed performance bound;surcharge pricing scheme;SPS;Nash equilibrium;association bias values;Resource management;Optimization;Interference;Downlink;Stochastic processes;Computers;Joints},
doi={10.1109/INFOCOM.2014.6848037},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848038,
author={K. Poularakis and G. Iosifidis and A. Argyriou and L. Tassiulas},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Video delivery over heterogeneous cellular networks: Optimizing cost and performance},
year={2014},
volume={},
number={},
pages={1078-1086},
abstract={Video delivery to mobile users is one of the largest challenges that network operators face today. In this work we consider a heterogeneous cellular network with storage capable small-cell base stations, and study this problem for pre-stored video files that can be encoded with two different schemes, namely versions or layers, in various qualities. We introduce a framework for the joint derivation of video caching and routing policies for users with different quality requirements. This allows the operator to optimize a balanced objective of incurred servicing cost, and users experienced delay, according to his priorities. The numerical results indicate that versions and layers may have different impact on the delay and servicing cost, depending on the diversity of users' demand, and that the cost-delay trade off is affected by the network's load.},
keywords={cellular radio;telecommunication network routing;video delivery;heterogeneous cellular networks;mobile users;network operators;small-cell base stations;pre-stored video files;video caching;routing policies;quality requirements;user demand;cost-delay trade off;network load;Delays;Base stations;Encoding;Routing;Streaming media;Computers;Mobile communication},
doi={10.1109/INFOCOM.2014.6848038},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848039,
author={W. Zhao and S. Wang and C. Wang and X. Wu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Cell planning for heterogeneous networks: An approximation algorithm},
year={2014},
volume={},
number={},
pages={1087-1095},
abstract={Low-power access points, such as pico base stations (BSs), femto BSs, and relays are introduced to the next generation cellular systems to enhance coverage and improve system capacity. Deploying low-power access points to offload the conventional macro BSs is deemed as a spectrum- and cost-efficient way to meet the sharp increase of traffic requirements of cellular networks. However, it also leads to heterogeneous network framework and raises new challenges for cell planning. In this paper, we study the minimum cost cell planning problem in such a heterogeneous network. Our optimization task is to select a subset of candidate sites to lay BSs, including macro BSs, pico BSs and relays, to minimize the total deployment cost while satisfying the rate requirements of the demand nodes (DNs) served by the cellular network. We prove that the general case of the formulated problem is APX-hard, where a DN is constrained to be associated with only one BS. However, if each DN can be served by multiple BSs, which is a reasonable case for practical cellular systems, we show it is not APX-hard and develop an approximation algorithm to work out promising solutions. Our proposed algorithm guarantees an approximation ratio of O(logR) to the global optimum, where R is the maximum achievable capacity of the BSs. Numerical results indicate that our proposal can significantly reduce the deployment cost of the cellular network with given rate requirements of DNs compared to other cell planning schemes.},
keywords={approximation theory;computational complexity;cost reduction;femtocellular radio;next generation networks;optimisation;picocellular radio;radio spectrum management;relay networks (telecommunication);telecommunication network planning;telecommunication traffic;minimum cost cell planning problem;heterogeneous network framework;approximation algorithm;low-power access points;pico base stations;femto BS;relays;next generation cellular systems;coverage enhancement;system capacity improvement;traffic requirements;spectrum-efficient way;cost-efficient way;optimization task;total deployment cost minimization;demand nodes;APX-hard problem;Relays;Approximation algorithms;Planning;Approximation methods;Bandwidth;Manganese;Optimization},
doi={10.1109/INFOCOM.2014.6848039},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848040,
author={W. Saad and Z. Han and R. Zheng and M. Debbah and H. V. Poor},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A college admissions game for uplink user association in wireless small cell networks},
year={2014},
volume={},
number={},
pages={1096-1104},
abstract={In this paper, the problem of uplink user association in small cell networks, which involves interactions between users, small cell base stations, and macro-cell stations, having often conflicting objectives, is considered. The problem is formulated as a college admissions game with transfers in which a number of colleges, i.e., small cell and macro-cell stations seek to recruit a number of students, i.e., users. In this game, the users and access points (small cells and macro-cells) rank one another based on preference functions that capture the users' need to optimize their utilities which are functions of packet success rate (PSR) and delay as well as the small cells' incentive to extend the macro-cell coverage (e.g., via cell biasing/range expansion) while maintaining the users' quality-of-service. A distributed algorithm that combines notions from matching theory and coalitional games is proposed to solve the game. The convergence of the algorithm is shown and the properties of the resulting assignments are discussed. Simulation results show that the proposed approach yields a performance improvement, in terms of the average utility per user, reaching up to 23% relative to a conventional, best-PSR algorithm.},
keywords={cellular radio;game theory;quality of service;college admissions game;uplink user association;wireless small cell networks;cell base stations;macrocell stations;preference functions;packet success rate;cell biasing-range expansion;distributed algorithm;matching theory;coalitional games;PSR algorithm;quality-of-service;QoS;Games;Educational institutions;Delays;Uplink;Wireless communication;Quality of service;Computers},
doi={10.1109/INFOCOM.2014.6848040},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848041,
author={H. Dai and Y. Liu and G. Chen and X. Wu and T. He},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Safe Charging for wireless power transfer},
year={2014},
volume={},
number={},
pages={1105-1113},
abstract={As battery-powered mobile devices become more popular and energy hungry, wireless power transfer technology receives intensive interests, as it allows the power to be transferred from a charger to ambient devices wirelessly. The existing studies mainly focus on the power transfer efficiency but overlook the health impairments caused by RF exposure. In this paper, we study the Safe Charging Problem (SCP) of scheduling power chargers so that more energy can be received while no location in the field has electromagnetic radiation (EMR) exceeding a given threshold Rt. We prove that SCP is NP-hard and propose a solution which provably outperforms the optimal solution to SCP with a relaxed EMR threshold (1 - ε)Rt. Testbed results based on 8 Powercast TX91501 chargers validate our results. Extensive simulation results show that the gap between our solution and the optimal one is only 6.7% when ε = 0.1, while a naive greedy algorithm is 34.6% below our solution.},
keywords={battery chargers;biological effects of radiation;electromagnetic waves;greedy algorithms;mobile radio;optimisation;radiofrequency power transmission;battery-powered mobile devices;wireless power transfer technology;RF exposure;safe charging problem;SCP;power chargers;electromagnetic radiation;NP-hard;EMR threshold;Powercast TX91501 chargers;greedy algorithm;Approximation algorithms;Approximation methods;Safety;Wireless communication;Piecewise linear approximation;Radio frequency;Conferences},
doi={10.1109/INFOCOM.2014.6848041},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848042,
author={C. Qiu and H. Shen and L. Yu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Energy-efficient cooperative broadcast in fading wireless networks},
year={2014},
volume={},
number={},
pages={1114-1122},
abstract={Cooperative broadcast, in which receivers are allowed to combine received packet from different senders to combat transmission errors, has gained increasing attention. Previous studies showed that broadcast optimization solutions are sufficient in non-fading environments but may suffer a low delivery ratio under wireless channel fading. Though previous work analyzed the tradeoff between energy and delay in cooperative broadcast, no works investigated the tradeoff in a fading environment. Thus, in this paper, we study this tradeoff with the consideration of fading. We formulate this problem as a Fading-resistant Delay-constrained Minimum Energy Cooperative Broadcast (FDMECB) problem, and prove that it is NP-complete. We then propose an approximation algorithm for theoretical interests. We further propose a heuristic algorithm that makes approximately optimal local decision to achieve global optimization. Our experimental results show that our algorithms outperform a previous non-fading resistant algorithm.},
keywords={computational complexity;cooperative communication;fading channels;optimisation;radio networks;energy-efficient cooperative broadcast;fading wireless networks;broadcast optimization solutions;wireless channel fading;fading-resistant delay-constrained minimum energy cooperative broadcast problem;FDMECB problem;NP-complete;approximation algorithm;heuristic algorithm;optimal local decision;global optimization;nonfading resistant algorithm;Relays;Delays;Schedules;Fading;Receivers;Approximation methods;Approximation algorithms},
doi={10.1109/INFOCOM.2014.6848042},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848043,
author={Y. Cui and S. Xiao and X. Wang and M. Li and H. Wang and Z. Lai},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Performance-aware energy optimization on mobile devices in cellular network},
year={2014},
volume={},
number={},
pages={1123-1131},
abstract={In cellular networks, it is important to conserve energy while at the same time ensuring users to have good transmission experiences. The energy cost can result from tail energy due to the radio resource control strategies designed in cellular networks and data transmission. Existing efforts generally consider one of the energy issues, and also ignore the adverse impact on user transmission performance due to energy conservation. In addition, many existing algorithms are based on prediction and knowledge on future traffic, which are hard to apply in a practical wireless system with dynamic user traffic and channel condition. The goal of this work is to design an efficient online scheduling algorithm to minimize energy consumption both due to tail energy and transmissions while meeting user performance expectation. We prove the problem to be NP-hard, and design a practical online scheduling algorithm PerES to minimize the total energy cost of multiple mobile applications subject to user performance constraints. We propose a comprehensive performance cost metric to capture the impacts due to task delay, deadline violation, different application profiles and user preferences. We prove that our proposed scheduling algorithm can make the energy consumption arbitrarily close to that of the optimal scheduling solution. The evaluation results demonstrate the effectiveness of our scheme and its higher performance than peers. Moreover, by supporting dynamic performance requirement by mobile users, PerES can achieve 2 times faster convergence to both the performance degradation bound and optimal energy conversation bound than those of traditional static methods. Using 821 million traffic flows collected from a commercial cellular carrier, we verify our scheme could achieve on average 32%-56% energy savings with different levels of user experience.},
keywords={cellular radio;mobile radio;scheduling;performance aware energy optimization;mobile devices;cellular network;energy conservation;tail energy;radio resource control strategies;data transmission;wireless system;NP-hard problem;optimal scheduling;cellular carrier;Delays;Data communication;Optimization;Energy consumption;Mobile communication;Wireless communication;Resource management},
doi={10.1109/INFOCOM.2014.6848043},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848044,
author={B. Leng and P. Mansourifard and B. Krishnamachari},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Microeconomic analysis of base-station sharing in green cellular networks},
year={2014},
volume={},
number={},
pages={1132-1140},
abstract={Cellular networks can be operated more energy-efficiently if operators agree to share base-stations during off-peak hours. We apply a micro-economic analysis for a single-cell two-operator scenario to investigate the conditions under which self-interested operators would agree to share resources in this manner. Our analysis yields a comprehensive treatment of the existence and number of Nash Equilibria. We consider the cases when the payment rates are exogenous, as well as when they can be set strategically by the operators. Through numerical solutions we examine the quality of the best and worst Nash Equilibria in comparison with the globally optimized solution. Our results show that there is often a sensitive dependence on key parameters such as energy price, capacity, load, revenues, penalties and payments.},
keywords={game theory;microeconomics;mobile radio;telecommunication power management;green cellular network;base station sharing microeconomic analysis;single cell two operator scenario;self-interested operator;resource sharing;Nash equilibria quality;Base stations;Turning;Games;Conferences;Computers;Green products;Educational institutions},
doi={10.1109/INFOCOM.2014.6848044},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848045,
author={O. B. Karimi and J. Liu and J. Rexford},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Optimal collaborative access point association in wireless networks},
year={2014},
volume={},
number={},
pages={1141-1149},
abstract={The popularity of wireless local area networks has led to a dramatic increase in the density of access points, especially in urban areas. These access points are individually owned, placed, and power-tuned for their local users and are generally oblivious to others. On the other hand, the abundance of access points that mostly share the same upstream provider, offers opportunities for optimization of association to mitigate the negative impact of the overlapped coverage. We use this opportunity to enable collaboration by using a share of each access point's bandwidth to serve non-local users and gain access to their bandwidth in return. We extend the conventional proportional fair association through sharing and collaboration among individual networks, and present centrally optimized solutions. Our performance evaluation, based on data traces collected in 100 residential locations, demonstrate the superiority of our solution, outperforming the throughput of non-collaborative optimal access by up to 140%.},
keywords={multi-access systems;optimisation;wireless LAN;optimal collaborative access point association;wireless local area networks;access points;upstream provider;optimization;overlapped coverage;nonlocal users;data traces;residential locations;noncollaborative optimal access;Throughput;Collaboration;Wireless sensor networks;Wireless networks;Optimization;Bandwidth},
doi={10.1109/INFOCOM.2014.6848045},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848046,
author={S. Xiong and Y. Yao and Q. Cao and T. He},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={kBF: A Bloom Filter for key-value storage with an application on approximate state machines},
year={2014},
volume={},
number={},
pages={1150-1158},
abstract={Key-value (k-v) storage has been used as a crucial component for many network applications, such as social networks, online retailing, and cloud computing. Such storage usually provides support for operations on key-value pairs, and can be stored in memory to speed up responses to queries. So far, existing methods have been deterministic: they will faithfully return previously inserted key-value pairs. Providing such accuracy, however, comes at the cost of memory and CPU time. In contrast, in this paper, we present an approximate k-v storage that is more compact than existing methods. The tradeoff is that it may, theoretically, return a null value for a valid key with a low probability, or return a valid value for a key that was never inserted. Its design is based on the probabilistic data structure called the “Bloom Filter”, which was originally developed to test element membership in sets. In this paper, we extend the bloom filter concept to support key-value operations, and demonstrate that it still retains the compact nature of the original bloom filter. We call the resulting design as the kBF (key-value bloom filter), and systematically analyze its performance advantages and design tradeoffs. Finally, we apply the kBF to a practical problem of implementing a state machine in network intrusion detection to demonstrate how the kBF can be used as a building block for more complicated software infrastructures.},
keywords={computer network security;data structures;finite state machines;probability;Bloom filter;kBF;key-value storage;approximate state machines;key-value pairs;approximate k-v storage;probabilistic data structure;element membership test;key-value operations;network intrusion detection;software infrastructures;Encoding;Radiation detectors;Decoding;Compaction;Conferences;Computers;Data structures},
doi={10.1109/INFOCOM.2014.6848046},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848047,
author={H. Mekky and R. Torres and Z. Zhang and S. Saha and A. Nucci},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Detecting malicious HTTP redirections using trees of user browsing activity},
year={2014},
volume={},
number={},
pages={1159-1167},
abstract={The web has become a platform that attackers exploit to infect vulnerable hosts, or deceive victims into buying rogue software. To accomplish this, attackers either inject malicious scripts into popular web sites or manipulate content delivered by servers to exploit vulnerabilities in users' browsers. To hide malware distribution servers, attackers employ HTTP redirections, which automatically redirect users' requests through a series of intermediate web sites, before landing on the final distribution site. In this paper, we develop a methodology to identify malicious chains of HTTP redirections. We build per-user chains from passively collected traffic and extract novel statistical features from them, which capture inherent characteristics from malicious redirection cases. Then, we apply a supervised decision tree classifier to identify malicious chains. Using a large ISP dataset, with more than 15K clients, we demonstrate that our methodology is very effective in accurately identifying malicious chains, with recall and precision values over 90% and up to 98%.},
keywords={Internet;invasive software;trees (mathematics);Web sites;malicious HTTP redirections;user browsing activity;World Wide Web;rogue software;intermediate Web sites;user browsers;malware distribution servers;malicious redirection;supervised decision tree classifier;malicious chains;large ISP dataset;Web sites;Malware;Browsers;Feature extraction;Servers;Software;Search engines},
doi={10.1109/INFOCOM.2014.6848047},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848048,
author={Y. Zhang and X. Ruan and H. Wang and H. Wang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={What scale of audience a campaign can reach in what price on Twitter?},
year={2014},
volume={},
number={},
pages={1168-1176},
abstract={Campaigns with commercial and spam purposes have flooded the Twitter community. To understand what scale of audience a campaign could reach, we first perform a measurement study by collecting a dataset of about 10 million tweets via streaming API and one million search tweets for targeting topics, as well as 37,313 user accounts that are suspended by Twitter. From the dataset, we extract a spam campaign and a commercial promotion campaign accompanied by spamming activities. Then, we characterize the way in which a campaign can reach its audience, especially revealing the features that dominate the information diffusion. After identifying the accounts suspended by Twitter, we further inspect to what extent these features can help to weed out spam accounts. Also, the retrospective inspection is useful to uncover the tactics that malicious accounts utilize to avoid being suspended. Using the measurement results, we then develop a theoretical framework based on an epidemic model to investigate the dynamics of spammers and victims whom spammers reach in the spam campaign. With the theoretical framework, we conduct a benefit-cost analysis of the spam campaign, shedding lights on how to restrict the benefit of the spam campaign.},
keywords={application program interfaces;cost-benefit analysis;social networking (online);unsolicited e-mail;Twitter;campaign;tweets;streaming API;spam campaign;commercial promotion campaign;spamming activities;information diffusion;spam accounts;epidemic model;spammer dynamics;benefit-cost analysis;Games;Twitter;Time factors;Feature extraction;Conferences;Computers},
doi={10.1109/INFOCOM.2014.6848048},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848049,
author={A. Loukas and M. Zuniga and I. Protonotarios and J. Gao},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={How to identify global trends from local decisions? Event region detection on mobile networks},
year={2014},
volume={},
number={},
pages={1177-1185},
abstract={The decentralized detection of event regions is a fundamental building block for monitoring and reasoning about spatial phenomena. However, so far the problem has been studied almost exclusively for static networks. This study proposes a theoretical framework with which we can analyze event detection algorithms suitable for large-scale mobile networks. Our analysis builds on the following insight: the inherent trends of spatial events are well captured by the spectral domain of the network graph. Using this framework, we propose novel local algorithms that are location-free; that work with mobile nodes and dynamic events; that operate on 3D topologies; and that are simple to implement. We are not aware of event detection algorithms possessing all these traits. Simulations based on complex oil spill traces showcase the resilience and robustness of our methods. Additionally, we demonstrate their validity for practical scenarios by evaluating them on a 105 node testbed.},
keywords={mobile computing;signal detection;event region detection algorithm;large-scale mobile networks;spatial events;network graph;novel local algorithms;3D topologies;decentralized detection;static networks;Eigenvalues and eigenfunctions;Topology;Laplace equations;Kernel;Heating;Heuristic algorithms;Noise},
doi={10.1109/INFOCOM.2014.6848049},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848050,
author={J. Wang and Z. Cao and X. Mao and Y. Liu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Sleep in the Dins: Insomnia therapy for duty-cycled sensor networks},
year={2014},
volume={},
number={},
pages={1186-1194},
abstract={Duty cycling mode is widely adopted in wireless sensor networks to save energy. Existing duty-cycling protocols cannot well adapt to different data rates and dynamics, resulting in a high energy consumption in real networks. Improving those protocols may require global information or heavy computation and thus may not be practical, leading to empirical parameters in real protocols. To fill the gap between the application requirement and protocol performance, we design a light-weight adaptive duty-cycling protocol (LAD), which reduces the energy consumption under different data rates and protocol dynamics. We theoretically validate the performance improvement of the protocol. We implement the protocol in TinyOS and extensively evaluate it on 40 TelosB nodes. The evaluation results show the energy consumption can be reduced by 28.2%~40.1% compared with state-of-the-art protocols. Results based on data from a 1200-node operational network further show the effectiveness and scalability of the design.},
keywords={protocols;telecommunication power management;wireless sensor networks;insomnia therapy;duty cycling mode;wireless sensor networks;lightweight adaptive duty cycling protocol;LAD;energy consumption reduction;protocol dynamics;TinyOS;TelosB nodes;Protocols;Energy consumption;Receivers;Switching circuits;Wireless sensor networks;Sensors;Conferences},
doi={10.1109/INFOCOM.2014.6848050},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848051,
author={L. He and P. Cheng and Y. Gu and J. Pan and T. Zhu and C. Liu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Mobile-to-mobile energy replenishment in mission-critical robotic sensor networks},
year={2014},
volume={},
number={},
pages={1195-1203},
abstract={Recently, much research effort has been devoted to employing mobile chargers for energy replenishment of the robots in robotic sensor networks. Observing the discrepancy between the charging latency of robots and charger travel distance, we propose a novel tree-based charging schedule for the charger, which minimizes its travel distance without causing the robot energy depletion. We analytically evaluate its performance and show its closeness to the optimal solutions. Furthermore, through a queue-based approach, we provide theoretical guidance on the setting of the remaining energy threshold at which the robots request energy replenishment. This guided setting guarantees the feasibility of the tree-based schedule to return a depletion-free charging schedule. The performance of the tree-based charging schedule is evaluated through extensive simulations. The results show that the charger travel distance can be reduced by around 20%, when compared with the schedule that only considers the robot charging latency.},
keywords={distributed sensors;mobile robots;queueing theory;scheduling;depletion-free charging schedule;energy threshold;queue-based approach;robot energy depletion;tree-based charging schedule;charger travel distance;mobile chargers;mission-critical robotic sensor networks;mobile-to-mobile energy replenishment;Schedules;Robot sensing systems;Robot kinematics;Mobile communication;Trajectory;Mission critical systems},
doi={10.1109/INFOCOM.2014.6848051},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848052,
author={H. Huang and C. Ni and X. Ban and J. Gao and A. T. Schneider and S. Lin},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Connected wireless camera network deployment with visibility coverage},
year={2014},
volume={},
number={},
pages={1204-1212},
abstract={We consider the problem of deployment of cameras inside a complex indoor setting for surveillance applications. We formulate the problem of the minimum guarding network that places a minimum number of cameras satisfying both visual coverage of the domain and wireless network connectivity. We prove that finding the minimum guarding network in both the geometric setting and discrete setting is NP-hard. We also give a 2-approximation algorithm to the geometric minimum guarding network. Motivated by the connection of this problem with the watchman tour problem and the art gallery problem, we develop two algorithms that generate satisfactory results in a prototype testbed and in our simulations.},
keywords={approximation theory;cameras;computational complexity;optimisation;sensor placement;video surveillance;wireless sensor networks;art gallery problem;watchman tour problem;geometric minimum guarding network;2-approximation algorithm;NP-hard geometric setting;NP-hard discrete setting;surveillance application;visibility coverage;connected wireless camera network deployment;Cameras;Wireless communication;Wireless sensor networks;Relays;Computers;Art;Buildings;Visibility Coverage;Wireless Connectivity;Camera Networks},
doi={10.1109/INFOCOM.2014.6848052},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848053,
author={D. Zhao and X. Li and H. Ma},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={How to crowdsource tasks truthfully without sacrificing utility: Online incentive mechanisms with budget constraint},
year={2014},
volume={},
number={},
pages={1213-1221},
abstract={Mobile crowdsourced sensing (MCS) is a new paradigm which takes advantage of pervasive smartphones to efficiently collect data, enabling numerous novel applications. To achieve good service quality for a MCS application, incentive mechanisms are necessary to attract more user participation. Most of existing mechanisms apply only for the offline scenario where all users' information are known a priori. On the contrary, we focus on a more realistic scenario where users arrive one by one online in a random order. Based on the online auction model, we investigate the problem that users submit their private types to the crowdsourcer when arrive, and the crowdsourcer aims at selecting a subset of users before a specified deadline for maximizing the value of services (assumed to be a non-negative monotone submodular function) provided by selected users under a budget constraint. We design two online mechanisms, OMZ and OMG, satisfying the computational efficiency, individual rationality, budget feasibility, truthfulness, consumer sovereignty and constant competitiveness under the zero arrival-departure interval case and a more general case, respectively. Through extensive simulations, we evaluate the performance and validate the theoretical properties of our online mechanisms.},
keywords={incentive schemes;mobile computing;smart phones;trusted computing;task crowdsourcing;online incentive mechanisms;budget constraint;mobile crowdsourced sensing;MCS;pervasive smartphones;service quality;user participation;offline scenario;random order;online auction model;budget constraint;online mechanisms;OMZ;OMG;computational efficiency;individual rationality;budget feasibility;truthfulness;consumer sovereignty;constant competitiveness;zero arrival-departure interval case;online mechanisms;Sensors;Resource management;Smart phones;Mechanical factors;Conferences;Computers;Complexity theory},
doi={10.1109/INFOCOM.2014.6848053},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848054,
author={X. Qi and Y. Wang and Y. Wang and L. Xu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Compressive sensing over strongly connected digraph and its application in traffic monitoring},
year={2014},
volume={},
number={},
pages={1222-1230},
abstract={Compressive sensing over graphs has recently attracted great research attentions, which takes limited number end-to-end measurements along paths (walks) to recover sparse vectors representing link/node properties. Unlike traditional compressive sensing, the along-path measurements rule out the freedom of random sampling, which introduces path constraints to the measurement matrix. The constraint makes explicit analysis of recovery performance difficult Only for undirected graphs, early results showed that O(klog(n)) end-to-end measurements taken by random walks are sufficient to recover k-sparse edge vector. However, the problem becomes more difficult when directed graphs are considered, because of the easy state absorbing and the difficulty of evaluating the stationary distribution. But digraphs inherently model many network systems. In this paper, particularly for strongly connected digraphs with low node degrees, we presents bounds for the stationary distribution of random walks, and present deliberative proofs which put forward that O(klog(n)) path measurements are sufficient to recover k-sparse edge vectors. Further more, because urban road networks are exactly strongly connected, low degree digraphs, we designed efficient recovery methods to estimate road delays by a small number of probing cars. Although the road delay vector is actually not sparse, we leverage the empirical non-congested road delays as references and develop an algorithm which divide the problem to iteratively recover several k-sparse vectors. Simulation results show that when less than 10% edges are congested, more than 90% congestion states can be recovered correctly by 10% measurements.},
keywords={compressed sensing;delay estimation;directed graphs;matrix algebra;road traffic;signal sampling;traffic engineering computing;vectors;compressive sensing;strongly connected digraph;traffic monitoring;end-to-end measurements;sparse vectors;link-node property;along-path measurement rule;random sampling;path constraints;measurement matrix;undirected graphs;random walks;k-sparse edge vector recovery;stationary distribution;network systems;urban road networks;low degree digraphs;road delay estimation;probing cars;road delay vector;empirical noncongested road delays;Vectors;Roads;Delays;Compressed sensing;Monitoring;Sparse matrices},
doi={10.1109/INFOCOM.2014.6848054},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848055,
author={Z. Feng and Y. Zhu and Q. Zhang and L. M. Ni and A. V. Vasilakos},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={TRAC: Truthful auction for location-aware collaborative sensing in mobile crowdsourcing},
year={2014},
volume={},
number={},
pages={1231-1239},
abstract={In this paper, we tackle the problem of stimulating smartphone users to join mobile crowdsourcing applications with smartphones. Different from existing work of mechanism design, we uniquely take into consideration the crucial dimension of location information when assigning sensing tasks to smartphones. However, the location awareness largely increases the theoretical and computational complexity. In this paper, we introduce a reverse auction framework to model the interactions between the platform and the smartphones. We rigorously prove that optimally determining the winning bids is NP hard. In this paper we design a mechanism called TRAC which consists of two main components. The first component is a near-optimal approximate algorithm for determining the winning bids with polynomial-time computation complexity, which approximates the optimal solution within a factor of 1 + ln(n), where n is the maximum number of sensing tasks that a smartphone can accommodate. The second component is a critical payment scheme which, despite the approximation of determining winning bids, guarantees that submitted bids of smartphones reflect their real costs of performing sensing tasks. Through both rigid theoretical analysis and extensive simulations, we demonstrate that the proposed mechanism achieves truthfulness, individual rationality and high computation efficiency.},
keywords={computational complexity;mobile computing;polynomial approximation;smart phones;TRAC;truthful auction;location-aware collaborative sensing;smartphone users;mobile crowdsourcing applications;location information;sensing tasks;location awareness;reverse auction framework;NP hard;winning bids;near-optimal approximate algorithm;polynomial-time computation complexity;critical payment scheme;winning bids;submitted bids;rigid theoretical analysis;Sensors;Mobile communication;Crowdsourcing;Algorithm design and analysis;Approximation methods;Approximation algorithms;Collaboration},
doi={10.1109/INFOCOM.2014.6848055},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848056,
author={X. Guo and E. C. L. Chan and C. Liu and K. Wu and S. Liu and L. M. Ni},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={ShopProfiler: Profiling shops with crowdsourcing data},
year={2014},
volume={},
number={},
pages={1240-1248},
abstract={Sensing data from mobile phones provide us exciting and profitable applications. Recent research focuses on sensing indoor environment, but suffers from inaccuracy because of the limited reachability of human traces or requires human intervention to perform sophisticated tasks. In this paper, we present ShopProfiler, a shop profiling system on crowdsourcing data. First, we extract customer movement patterns from traces. Second, we improve accuracy of building floor plan by adopting a gradient-based approach and then localize shops through WiFi heat map. Third, we categorize shops by designing an SVM classifier in shop space to support multi-label classification. Finally, we infer brand name from SSID by applying string similarity measurement. Based on over five thousand traces in three big malls in two different countries, we conclude that ShopProfiler achieves better accuracy in building refined floor plan, and characterizes shops in terms of location, category and name with little human intervention.},
keywords={gradient methods;marketing;mobile computing;pattern classification;support vector machines;wireless LAN;ShopProfiler;crowdsourcing data;sensing data;mobile phones;profitable applications;indoor environment sensing;human traces;human intervention;shop profiling system;customer movement patterns;gradient-based approach;WiFi heat map;SVM classifier;shop space;multilabel classification;brand name;SSID;string similarity measurement;building refined floor plan;Legged locomotion;Mobile handsets;IEEE 802.11 Standards;Sociology;Statistics;Radiation detectors},
doi={10.1109/INFOCOM.2014.6848056},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848057,
author={Z. Chen and L. Huang and L. Li and W. Yang and H. Miao and M. Tian and F. Wang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={PS-TRUST: Provably secure solution for truthful double spectrum auctions},
year={2014},
volume={},
number={},
pages={1249-1257},
abstract={Truthful spectrum auctions have been extensively studied in recent years. Truthfulness makes bidders bid their true valuations, simplifying greatly the analysis of auctions. However, revealing one's true valuation causes severe privacy disclosure to the auctioneer and other bidders. To make things worse, previous work on secure spectrum auctions does not provide adequate security. In this paper, based on TRUST, we propose PS-TRUST, a provably secure solution for truthful double spectrum auctions. Besides maintaining the properties of truthfulness and special spectrum reuse of TRUST, PS-TRUST achieves provable security against semi-honest adversaries in the sense of cryptography. Specifically, PS-TRUST reveals nothing about the bids to anyone in the auction, except the auction result. To the best of our knowledge, PS-TRUST is the first provably secure solution for spectrum auctions. Furthermore, experimental results show that the computation and communication overhead of PS-TRUST is modest, and its practical applications are feasible.},
keywords={data privacy;radio spectrum management;telecommunication security;PS-TRUST;privacy disclosure;provably secure solution for truthful double spectrum auctions;TRUST spectrum reuse;semihonest adversary;communication overhead;Cryptography;Protocols;Algorithm design and analysis;Privacy;Cost accounting;Zinc},
doi={10.1109/INFOCOM.2014.6848057},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848058,
author={P. Hu and K. Xing and X. Cheng and H. Wei and H. Zhu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Information leaks out: Attacks and countermeasures on compressive data gathering in wireless sensor networks},
year={2014},
volume={},
number={},
pages={1258-1266},
abstract={Compressive sensing (CS) has been viewed as a promising technology to greatly improve the communication efficiency of data gathering in wireless sensor networks. However, this new data collection paradigm may bring in new threats but few study has paid attention to prevent information leakage during compressive data gathering. In this paper, we identify two statistical inference attacks and demonstrate that traditional compressive data gathering may suffer from serious information leakage under these attacks. In our theoretical analysis, we quantitatively analyze the estimation error of compressive data gathering through extensive statistical analysis, based on which we propose a new secure compressive data aggregation scheme by adaptively changing the measurement coefficients at each sensor and correspondingly at the sink without the need of time synchronization. In our analysis, we show that the proposed scheme could significantly improve data confidentiality at light computational and communication overhead.},
keywords={compressed sensing;data acquisition;data protection;statistical analysis;telecommunication security;wireless sensor networks;wireless sensor network;compressive data gathering;countermeasure;compressive sensing;data collection;information leakage prevention;statistical inference attack identification;estimation error analyze;statistical analysis;secure compressive data aggregation scheme;measurement coefficients;Wireless sensor networks;Vectors;Cryptography;Compressed sensing;Matching pursuit algorithms;Conferences},
doi={10.1109/INFOCOM.2014.6848058},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848059,
author={W. Wang and L. Chen and K. G. Shin and L. Duan},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Secure cooperative spectrum sensing and access against intelligent malicious behaviors},
year={2014},
volume={},
number={},
pages={1267-1275},
abstract={Sensing falsification is a key security problem in cooperative spectrum sensing for cognitive radio networks. Most previous approaches assume that malicious users only cheat in their sensing reports following a predefined rule. However, some malicious users usually act intelligently to strategically adjust their malicious behavior according to their objectives and the network's defense schemes. The existing schemes cannot resist the malicious behaviors of intelligent malicious users (IMUs) without long-term collection of information on their reputation. In this paper, we construct a moral hazard principal-agent framework and design an incentive compatible mechanism to thwart the malicious behaviors of rational and irrational IMUs. We find that neither spectrum sensing nor spectrum access alone can prevent the malicious behavior without any information on users' reputation. According to the analysis of malicious behavior resistance methods, we propose a joint spectrum sensing and access mechanism to optimally prevent the IMUs from sensing falsification. Our evaluation results show that the proposed mechanism achieves almost the same performance as the ideal case with perfect sensing.},
keywords={cognitive radio;radio spectrum management;signal detection;telecommunication security;secure cooperative spectrum sensing;secure cooperative spectrum access;intelligent malicious behaviors;sensing falsification;security problem;cognitive radio networks;malicious users;network defense;intelligent malicious users;IMU;principal agent framework;incentive compatible mechanism;Sensors;Resistance;Ethics;Hazards;Joints;Conferences;Computers},
doi={10.1109/INFOCOM.2014.6848059},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848060,
author={X. Du and D. Shan and K. Zeng and L. Huie},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Physical layer challenge-response authentication in wireless networks with relay},
year={2014},
volume={},
number={},
pages={1276-1284},
abstract={Exploiting physical layer characteristics to enhance or complement authentication strength in wireless networks has been attracting research attention recently. Existing physical layer authentication mechanisms mainly tackle single-hop communications. In this paper, we propose two physical layer challenge-response authentication mechanisms for wireless networks with relay. One mechanism, named PHY-CRAMR, is an extension of the existing PHY-CRAM protocol. It fully utilizes the randomness, reciprocity, and location decorrelation features of the wireless fading channel to hide/encrypt the challenge response messages at the physical layer, and is immune to outside attacks with a trusted relay. The other novel mechanism, named PHY-AUR, exploits randomness, coherence, and location decorrelation properties of wireless fading channel to securely convey the product of the channel state information on consecutive links and uses the fading channel to encrypt challenge and response messages. PHY-AUR is immune to both outside and inside attacks with an untrusted relay. Both PHY-CRAMR and PHY-AUR adopt OFDM technique to modulate the authentication key and challenge-response messages on subcarriers. Physical layer pilots and preambles are eliminated to prevent an attacker from gaining knowledge about the channel state information, and as a result prevent the authentication key from being revealed to untrusted attackers. We analyze the security strength of both mechanisms and conduct extensive simulations to evaluate them. It shows that both PHY-CRAMR and PHY-AUR can achieve both a high successful authentication rate and low false acceptance rate, and the performance improves as the signal to noise ratio (SNR) increases.},
keywords={cryptographic protocols;fading channels;message authentication;OFDM modulation;radio networks;relay networks (telecommunication);telecommunication security;physical layer challenge-response authentication mechanism;wireless networks;single-hop communications;PHY-CRAM protocol;location decorrelation features;wireless fading channel;outside attacks;trusted relay;channel state information;consecutive links;response messages;inside attacks;OFDM technique;preambles;PHY-AUR;high successful authentication rate;low false acceptance rate;signal to noise ratio;SNR;physical-layer security;Authentication;Relays;Physical layer;OFDM;Protocols;Wireless communication;Communication system security;Physical-layer security;challenge-response authentication;relay network;wireless fading channel;OFDM},
doi={10.1109/INFOCOM.2014.6848060},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848061,
author={R. Shea and F. Wang and H. Wang and J. Liu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A deep investigation into network performance in virtual machine based cloud environments},
year={2014},
volume={},
number={},
pages={1285-1293},
abstract={Existing research on cloud network (in)stability has primarily focused on communications between Virtual Machines (VMs) inside a cloud, leaving that of VM communications over higher-latency wide-area networks largely unexplored. Through measurement in real-world cloud platforms, we find that there are prevalent and significant degradation and variation for such VM communications with both TCP and UDP traffic, even over lightly utilized networks. Our in-depth measurement and detailed system analysis reveal that the performance variation and degradation are mainly due to the dual-role of the CPU in both computation and network communication in a VM, and they can be dramatically affected by the CPU's scheduling policy. We provide strong evidence that such issues can be addressed in the hypervisor level and present concrete solutions. Such remedies have been implemented and evaluated in our cloud testbed, showing noticeable improvement for long-haul network communications with VMs.},
keywords={cloud computing;scheduling;transport protocols;virtual machines;network performance;virtual machine based cloud environment;cloud network stability;VM;higher-latency wide-area networks;VM communications;TCP traffic;UDP traffic;transport control protocol;user defined protocol;CPU scheduling policy;Cloud computing;Bandwidth;Benchmark testing;Throughput;Virtual machining;Degradation;Computers},
doi={10.1109/INFOCOM.2014.6848061},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848062,
author={L. Chen and H. Shen and K. Sapra},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={RIAL: Resource Intensity Aware Load balancing in clouds},
year={2014},
volume={},
number={},
pages={1294-1302},
abstract={To provide robust infrastructure as a service (IaaS), clouds currently perform load balancing by migrating virtual machines (VMs) from heavily loaded physical machines (PMs) to lightly loaded PMs. The unique features of clouds pose formidable challenges to achieving effective and efficient load balancing. First, VMs in clouds use different resources (e.g., CPU, bandwidth, memory) to serve a variety of services (e.g., high performance computing, web services, file services), resulting in different overutilized resources in different PMs. Also, the overutilized resources in a PM may vary over time due to the time-varying heterogenous service requests. Second, there is intensive network communication between VMs. However, previous load balancing methods statically assign equal or predefined weights to different resources, which leads to degraded performance in terms of speed and cost to achieve load balance. Also, they do not strive to minimize the VM communications between PMs. We propose a Resource Intensity Aware Load balancing method (RIAL). For each PM, RIAL dynamically assigns different weights to different resources according to their usage intensity in the PM, which significantly reduces the time and cost to achieve load balance and avoids future load imbalance. It also tries to keep frequently communicating VMs in the same PM to reduce bandwidth cost, and migrate VMs to PMs with minimum VM performance degradation. Our extensive trace-driven simulation results and real-world experimental results show the superior performance of RIAL compared to other load balancing methods.},
keywords={cloud computing;cost reduction;digital simulation;resource allocation;virtual machines;RIAL;resource intensity aware load balancing;clouds;robust infrastructure as a service;IaaS;virtual machines;heavily loaded physical machines;PM;overutilized resources;time-varying heterogenous service requests;VM communications;load imbalance;bandwidth cost reduction;trace-driven simulation;Load management;Bandwidth;Degradation;Resource management;Servers;Computers;Convergence},
doi={10.1109/INFOCOM.2014.6848062},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848063,
author={J. Kuo and H. Yang and M. Tsai},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Optimal approximation algorithm of virtual machine placement for data latency minimization in cloud systems},
year={2014},
volume={},
number={},
pages={1303-1311},
abstract={The MapReduce/Hadoop architecture has become very important and effective in cloud systems because many data-intensive applications are usually required to process big data. In such environments, big data is partitioned and stored over several data nodes; thus, the total completion time of a task would be delayed if the maximum access latency among all pairs of a data node and its assigned computation node is not bounded. Moreover, the computation nodes usually need to communicate with each other for aggregating the computation results; therefore, the maximum access latency among all pairs of assigned computation nodes also needs to be bounded. In the literature, it has been proved that the placement problem of computation nodes (virtual machines) to minimize the maximum access latency among all pairs of a data node and its assigned computation node and among all pairs of assigned computation nodes does not admit any approximation algorithm with a factor smaller than two, whereas no approximation algorithms have been proposed so far. In this paper, we first propose a 3-approximation algorithm for resolving the problem. Subsequently, we close the gap by proposing a 2-approximation algorithm, that is, an optimal approximation algorithm, for resolving the problem in the price of higher time complexity. Finally, we conduct simulations for evaluating the performance of our algorithms.},
keywords={approximation theory;Big Data;cloud computing;computational complexity;minimisation;virtual machines;MapReduce-Hadoop architecture;cloud systems;data-intensive applications;Big Data processing;data access latency;data node;computation node placement problem;maximum access latency minimization;3-approximation algorithm;2-approximation algorithm;optimal approximation algorithm;time complexity;virtual machine placement;data latency minimization;Approximation algorithms;Approximation methods;Bipartite graph;Virtual machining;Big data;Three-dimensional displays;Conferences},
doi={10.1109/INFOCOM.2014.6848063},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848064,
author={P. J. Argibay-Losada and K. Nozhnina and G. Sahin and C. Qiao},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Using stop-and-wait to improve TCP throughput in fast optical switching (FOS) networks over short physical distances},
year={2014},
volume={},
number={},
pages={1312-1320},
abstract={Due to lack of optical RAM buffers in fast optical switches (FOS), statistical multiplexing technologies using FOS like Optical Burst Switching (OBS) or Optical Packet Switching (OPS) are expected to have higher data losses than conventional electronic networks. Consequently, applications transferring data by means of TCP can suffer from a lower throughput. In this paper, we show that this low-throughput problem is mainly an artifact caused by the conventional TCP congestion control algorithms, and can be remedied by using a simple yet effective stop-and-wait congestion control algorithm instead, as long as the propagation delay between TCP source and TCP destination is small compared to the transmission time of an optical packet. We show that such a condition holds for a wide range of scenarios, including fat-tree-based data center networks. We also show that the throughput achieved in a FOS network using stop-and-wait can be the same as, or higher than that in an equivalent, conventional electronic network.},
keywords={optical burst switching;optical switches;packet switching;statistical multiplexing;trees (mathematics);TCP throughput;optical RAM buffers;fast optical switching networks;FOS;statistical multiplexing technologies;optical burst switching;OBS;optical packet switching;OPS;low-throughput problem;TCP congestion control algorithms;effective stop-and-wait congestion control algorithm;propagation delay;fat-tree-based data center networks;Surface acoustic waves;Optical switches;Optical buffering;Throughput;Optical packet switching;Receivers;Servers},
doi={10.1109/INFOCOM.2014.6848064},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848065,
author={Y. Du and G. de Veciana},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={“Wireless networks without edges”: Dynamic radio resource clustering and user scheduling},
year={2014},
volume={},
number={},
pages={1321-1329},
abstract={Cellular systems using Coordinated Multi-Point (CoMP) transmissions leveraging clusters of spatially distributed radio antennas as Virtual Base Stations (VBSs) have the potential to realize overall throughput gains and, perhaps more importantly, can deliver substantial enhancement to poor performing “edge” users. In this paper we propose a novel framework aimed at fully exploiting the potential of such systems through dynamic radio resource clustering and user scheduling which maximize system utility. The dynamic clustering problem is modeled as a maximum weight clustering problem which is NP-hard, however, we show that by structuring the set of possible VBSs to be “2-decomposable” it can be efficiently computed. We also propose to optimize over a class of power allocation policies to radio resources, and thus VBSs, which allow dynamic user scheduling and flexible power allocations depending on instantaneous channel realizations. We use simulation to compare our approach with a state-of-the-art baseline which exploits dynamic frequency reuse and opportunistic user scheduling, but no clustering, and show edge users' throughput gains are as high as 80% without degrading the performance of others.},
keywords={cellular radio;computational complexity;mobile antennas;pattern clustering;scheduling;wireless networks;dynamic radio resource clustering problem;cellular systems;coordinated multipoint transmissions;CoMP transmissions;spatially distributed radio antennas;virtual base stations;system utility maximization;NP-hard problem;maximum weight clustering problem;power allocation policies;flexible power allocations;instantaneous channel realizations;dynamic frequency reuse;opportunistic user scheduling;edge user throughput gains;Dynamic scheduling;Resource management;Processor scheduling;Interference;Vectors;Optimized production technology;Heuristic algorithms},
doi={10.1109/INFOCOM.2014.6848065},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848066,
author={O. Gurewitz and Y. Sandomirsky and G. Scalosub},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Cellular multi-coverage with non-uniform rates},
year={2014},
volume={},
number={},
pages={1330-1338},
abstract={Recent advances in the standardization of 4G cellular networks introduce the notion of multi-coverage, where multiple base stations may collaboratively satisfy the demands of mobile users. We provide a theoretical model for studying such multi-coverage environments, in highly heterogeneous settings, where users demands and profits may vary, as can base stations' capacities and the rates with which they can service the users. Whereas previous works provided solutions that were only applicable to scenarios where rates are uniform throughout the network, or allowed a mobile user to be serviced by at most one base station, we present several algorithms for the multi-coverage problem in the presence of non-uniform rates, and analyze their performance. We complete our study by a simulation study that further validates our results and provides further insight into algorithm design, depending on the users' characteristics.},
keywords={4G mobile communication;cellular radio;cellular multicoverage;4G cellular networks;base stations;Base stations;Approximation algorithms;Mobile communication;Approximation methods;Conferences;Computers;Resource management},
doi={10.1109/INFOCOM.2014.6848066},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848067,
author={R. Margolies and A. Sridharan and V. Aggarwal and R. Jana and N. K. Shankaranarayanan and V. A. Vaishampayan and G. Zussman},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Exploiting mobility in proportional fair cellular scheduling: Measurements and algorithms},
year={2014},
volume={},
number={},
pages={1339-1347},
abstract={Proportional Fair (PF) scheduling algorithms are the de-facto standard in cellular networks. They exploit the users' channel state diversity (induced by fast-fading), and are optimal for stationary channel state distributions and an infinite time-horizon. However, mobile users experience a non-stationary channel, due to slow-fading (on the order of seconds), and are associated with basestations for short periods. Hence, we develop the Predictive Finite-horizon PF Scheduling ((PF)<sup>2</sup>S) Framework that exploits mobility. We present extensive channel measurement results from a 3G network and characterize mobility-induced channel state trends. We show that a user's channel state is highly reproducible and leverage that to develop a data rate prediction mechanism. We then present a few channel allocation estimation algorithms that rely on the prediction mechanism. Our trace-based simulations consider instances of the PF<sup>2</sup>S Framework composed of combinations of prediction and channel allocation estimation algorithms. They indicate that the framework can increase the throughput by 15%-55% compared to traditional PF schedulers, while improving fairness.},
keywords={3G mobile communication;cellular radio;channel allocation;channel estimation;diversity reception;fading channels;scheduling;proportional fair cellular scheduling;predictive finite horizon PF scheduling;cellular networks;channel state diversity;nonstationary channel;slow fading;base stations;channel measurement;3G network;channel allocation estimation algorithms;Mobile communication;Prediction algorithms;Market research;Scheduling;Correlation;Delays;Cellular networks;Mobility;Proportional fairness;Measurements;Channel state prediction;Slow-fading},
doi={10.1109/INFOCOM.2014.6848067},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848068,
author={Y. Zhang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={User mobility from the view of cellular data networks},
year={2014},
volume={},
number={},
pages={1348-1356},
abstract={Understanding the user mobility is essential to resource optimization and algorithm evaluation in mobile networks, such as network planning, content distribution, and evaluation of hand-over mechanisms. Existing human mobility models focus on extracting mobility patterns from Call Detail Records (CDRs) or WiFi traces. While the former only captures movements during phone calls, the latter does not provide direct answers to mobility of cellular network users in a large scale. In this paper, we take the first step to investigate if the mobility properties derived from cellular data traffic is different from the previous findings using other data source, especially the commonly used CDR based approach. We present a comprehensive characterization of the mobility patterns from the cellular data networks' perspective, using a set of systematic methods. We find that the data network records can provide finer granularity of location and movement information. Three different temporal movement patterns are identified. Furthermore, we propose a new method for predicting future application usage given the mobility patterns and show promising results.},
keywords={cellular radio;mobility management (mobile radio);telecommunication congestion control;user mobility;cellular data networks;resource optimization;algorithm evaluation;mobile networks;network planning;content distribution;hand-over mechanisms evaluation;mobility patterns;call detail records;CDR;WiFi traces;cellular data traffic;systematic methods;data network records;location information;movement information;temporal movement patterns;Mobile communication;Poles and towers;Oscillators;Entropy;Mobile computing;Conferences;Computers},
doi={10.1109/INFOCOM.2014.6848068},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848069,
author={K. Jung and Y. Qi and C. Yu and Y. Suh},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Energy efficient Wifi tethering on a smartphone},
year={2014},
volume={},
number={},
pages={1357-1365},
abstract={While numerous efforts have been made to save energy of “client” devices but it has not been addressed for access points (APs) as they are assumed to be supported by AC power. This paper proposes E-MAP, which is an energy saving algorithm for a tethering smartphone that plays a role of mobile AP (MAP) temporarily. It saves MAP's energy by introducing the sleep cycle as in power save mode (PSM) in 802.11 but successfully keeps clients from transmitting while it sleeps. One important design goal of E-MAP is backward compatibility, i.e., it requires no modification on the client side and supports PSM and adaptive PSM (A-PSM) as well as normal constant awake mode (CAM) clients. Experiments show that E-MAP reduces the energy consumption of a Wifi tethering smartphone by up to 54% with a little impact on packet delay under various traffic patterns derived from real-life traces.},
keywords={packet radio networks;smart phones;telecommunication power management;telecommunication traffic;wireless LAN;energy efficient Wi-Fi tethering;smartphone;client device;E-MAP;energy saving algorithm;mobile AP;MAP;sleep cycle;power save mode;IEEE 802.11;adaptive PSM;A-PSM;normal constant awake mode;CAM client;traffic patterns;Delays;Computer aided manufacturing;Uplink;Bismuth;IEEE 802.11 Standards;Computers;Protocols},
doi={10.1109/INFOCOM.2014.6848069},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848070,
author={Q. Zhao and Y. Zhu and H. Zhu and J. Cao and G. Xue and B. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Fair energy-efficient sensing task allocation in participatory sensing with smartphones},
year={2014},
volume={},
number={},
pages={1366-1374},
abstract={With the proliferation of smartphones, participatory sensing using smartphones provides unprecedented opportunities for collecting enormous sensing data. There are two crucial requirements in participatory sensing, fair task allocation and energy efficiency, which are particularly challenging given high combinatorial complexity, tradeoff between energy efficiency and fairness, and dynamic and unpredictable task arrivals. In this paper, we present a novel fair energy-efficient allocation framework whose objective is characterized by min-max aggregate sensing time. We rigorously prove that optimizing the min-max aggregate sensing time is NP hard even when the tasks are assumed as a priori. We consider two allocation models: offline allocation and online allocation. For the offline allocation model, we design an efficient approximation algorithm with the approximation ratio of 2 - 1/m, where m is the number of member smartphones in the system. For the online allocation model, we propose a greedy online algorithm which achieves a competitive ratio of at most m. The results demonstrate that the approximation algorithm reduces over 81% total sensing time, the greedy online algorithm reduces more than 73% total sensing time, and both algorithms achieve over 3x better min-max fairness.},
keywords={approximation theory;computational complexity;energy conservation;greedy algorithms;smart phones;novel fair energy-efficient sensing task allocation framework;participatory sensing;smartphones;combinatorial complexity;min-max aggregate sensing time;NP hard problem;offline allocation model;efficient approximation algorithm;online allocation model;greedy online algorithm;Sensors;Resource management;Smart phones;Approximation algorithms;Aggregates;Algorithm design and analysis;Approximation methods},
doi={10.1109/INFOCOM.2014.6848070},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848071,
author={J. Crowcroft and M. Segal and L. Levin},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Improved structures for data collection in wireless sensor networks},
year={2014},
volume={},
number={},
pages={1375-1383},
abstract={In this paper we consider the problem of efficient data gathering in sensor networks for arbitrary sensor node deployments. The efficiency of the solution is measured by a number of criteria: total energy consumption, total transport capacity, latency and quality of the transmissions. We present a number of different constructions with various tradeoffs between aforementioned parameters. We provide theoretical performance analysis for our approaches, present their distributed implementation and discuss the different aspects of using each. We show that in many cases our output-sensitive approximation solution performs better than the currently known best results for sensor networks. Our simulation results validate the theoretical findings.},
keywords={approximation theory;data communication;energy consumption;wireless sensor networks;data collection;wireless sensor networks;data gathering;arbitrary sensor node deployments;energy consumption;transport capacity;transmission quality;theoretical performance analysis;output-sensitive approximation solution;Data collection;Energy consumption;Wireless sensor networks;Base stations;Computers;Approximation methods;Conferences},
doi={10.1109/INFOCOM.2014.6848071},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848072,
author={L. Liu and X. Cao and Y. Cheng and L. Du and W. Song and Y. Wang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Energy-efficient capacity optimization in wireless networks},
year={2014},
volume={},
number={},
pages={1384-1392},
abstract={We study how to achieve optimal network capacity in the most energy-efficient manner over a general large-scale wireless network, say, a multi-hop multi-radio multi-channel (MR-MC) network. We develop a multi-objective optimization framework for computing the resource allocation that leads to optimal network capacity with minimal energy consumption. Our framework is based on a linear programming multi-commodity flow (MCF) formulation augmented with scheduling constraints over multi-dimensional conflict graph (MDCG). The optimization problem however involves finding all independent sets (ISs), which is NP-hard in general. Novel delayed column generation (DCG) based algorithms are developed to effectively solve the optimization problem. The DCG-based algorithms have significant advantages of low computation overhead and achieving high energy efficiency, compared to the common heuristic algorithm that randomly searches a large number of ISs to use. Extensive numerical results demonstrate the energy efficiency improvement by the proposed energy-efficient optimization techniques, over a wide range of networking scenarios.},
keywords={energy conservation;energy consumption;graph theory;linear programming;radio networks;resource allocation;telecommunication channels;energy-efficient capacity optimization;wireless networks;optimal network capacity;multi-radio multi-channel network;MR-MC network;multi-objective optimization framework;resource allocation;energy consumption;linear programming multi-commodity flow;MCF formulation;scheduling constraints;multi-dimensional conflict graph;MDCG;optimization problem;independent sets;NP-hard;delayed column generation;DCG-based algorithms;optimization problem;energy efficiency;heuristic algorithm;energy-efficient optimization techniques;Optimization;Energy consumption;Interference;Resource management;Linear programming;Throughput;Search problems;Multi-radio multi-channel network;capacity optimization;energy efficiency;multi-objective optimization},
doi={10.1109/INFOCOM.2014.6848072},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848073,
author={Z. Zhang and Z. Zhang and P. P. C. Lee and Y. Liu and G. Xie},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={ProWord: An unsupervised approach to protocol feature word extraction},
year={2014},
volume={},
number={},
pages={1393-1401},
abstract={Protocol feature words are byte subsequences within traffic payload that can distinguish application protocols, and they form the building blocks of many constructions of deep packet analysis rules in network management, measurement, and security systems. However, how to systematically and efficiently extract protocol feature words from network traffic remains a challenging issue. Existing n-gram approaches simply break pay-load into equal-length pieces and are ineffective in capturing the hidden statistical structure of the payload content. In this paper, we propose ProWord, an unsupervised approach that extracts protocol feature words from traffic traces. ProWord builds on two nontrivial algorithms. First, we propose an unsupervised segmentation algorithm based on the modified Voting Experts algorithm, such that we break payload into candidate words according to entropy information and provide more accurate segmentation than existing n-gram approaches. Second, we propose a ranking algorithm that incorporates different types of well-known feature word retrieval heuristics, such that we can build an ordered structure on the candidate words and select the highest ranked ones as protocol feature words. We compare ProWord and existing n-gram approaches via evaluation on real-world traffic traces. We show that ProWord captures true protocol feature words more accurately and performs significantly faster.},
keywords={computer network management;computer network security;feature extraction;information retrieval;protocols;telecommunication traffic;unsupervised learning;word processing;ProWord;protocol feature word extraction;byte subsequences;traffic payload;deep packet analysis;network management;security system;n-gram approach;statistical structure;nontrivial algorithm;voting expert algorithm;entropy information;ranking algorithm;feature word retrieval heuristics;traffic trace;unsupervised segmentation algorithm;Feature extraction;Protocols;Algorithm design and analysis;Payloads;Entropy;Partitioning algorithms;Computers},
doi={10.1109/INFOCOM.2014.6848073},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848074,
author={Z. Ling and J. Luo and K. Wu and W. Yu and X. Fu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={TorWard: Discovery of malicious traffic over Tor},
year={2014},
volume={},
number={},
pages={1402-1410},
abstract={Tor is a popular low-latency anonymous communication system. However, it is currently abused in various ways. Tor exit routers are frequently troubled by administrative and legal complaints. To gain an insight into such abuse, we design and implement a novel system, TorWard, for the discovery and systematic study of malicious traffic over Tor. The system can avoid legal and administrative complaints and allows the investigation to be performed in a sensitive environment such as a university campus. An IDS (Intrusion Detection System) is used to discover and classify malicious traffic. We performed comprehensive analysis and extensive real-world experiments to validate the feasibility and effectiveness of TorWard. Our data shows that around 10% Tor traffic can trigger IDS alerts. Malicious traffic includes P2P traffic, malware traffic (e.g., botnet traffic), DoS (Denial-of-Service) attack traffic, spam, and others. Around 200 known malware have been identified. To the best of our knowledge, we are the first to perform malicious traffic categorization over Tor.},
keywords={computer network security;peer-to-peer computing;telecommunication network routing;telecommunication traffic;malicious traffic discovery;low-latency anonymous communication system;Tor exit routers;IDS;intrusion detection system;IDS alerts;P2P traffic;DoS;denial-of-service attack traffic;spam;malicious traffic categorization;Servers;Malware;Bandwidth;Logic gates;Ports (Computers);Computers;Mobile handsets;Tor;Malicious Traffic;Intrusion Detection System},
doi={10.1109/INFOCOM.2014.6848074},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848075,
author={D. Kong and G. Yan},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Transductive malware label propagation: Find your lineage from your neighbors},
year={2014},
volume={},
number={},
pages={1411-1419},
abstract={The numerous malware variants existing in the cyberspace have posed severe threats to its security. Supervised learning techniques have been applied to automate the process of classifying malware variants. Supervised learning, however, suffers in situations where we have only scarce labeled malware samples. In this work, we propose a transductive malware classification framework, which propagates label information from labeled instances to unlabeled ones. We improve the existing Harmonic function approach based on the maximum confidence principle. We apply this framework on the structural information collected from malware programs, and propose a PageRank-like algorithm to evaluate the distance between two malware programs. We evaluate the performance of our method against the standard Harmonic function method as well as two popular supervised learning techniques. Experimental results suggest that our method outperforms these existing approaches in classifying malware variants when only a small number of labeled samples are available.},
keywords={Malware;Supervised learning;Harmonic analysis;Registers;Vectors;Support vector machines;Standards},
doi={10.1109/INFOCOM.2014.6848075},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848076,
author={Q. Huang and P. P. C. Lee},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={LD-Sketch: A distributed sketching design for accurate and scalable anomaly detection in network data streams},
year={2014},
volume={},
number={},
pages={1420-1428},
abstract={Real-time characterization of traffic anomalies, such as heavy hitters and heavy changers, is critical for the robustness of operational networks, but its accuracy and scalability are challenged by the ever-increasing volume and diversity of network traffic. We address this problem by leveraging parallelization. We propose LD-Sketch, a data structure designed for accurate and scalable traffic anomaly detection using distributed architectures. LD-Sketch combines the classical counter-based and sketch-based techniques, and performs detection in two phases: local detection, which guarantees zero false negatives, and distributed detection, which reduces false positives by aggregating multiple detection results. We derive the error bounds and the space and time complexity for LD-Sketch. We compare LD-Sketch with state-of-the-art sketch-based techniques by conducting experiments on traffic traces from a real-life 3G cellular data network. Our results demonstrate the accuracy and scalability of LD-Sketch over prior approaches.},
keywords={3G mobile communication;cellular radio;computational complexity;data structures;LD-sketch;distributed sketching design;network data streams;data structure;traffic anomaly detection;sketch-based techniques;counter-based technique;local detection;distributed detection;time complexity;space complexity;real-life 3G cellular data network;Radiation detectors;Arrays;Time complexity;Accuracy;Scalability},
doi={10.1109/INFOCOM.2014.6848076},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848077,
author={M. K. Yoon and J. Son and S. Shin},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Bloom tree: A search tree based on Bloom filters for multiple-set membership testing},
year={2014},
volume={},
number={},
pages={1429-1437},
abstract={A Bloom filter is a compact and randomized data structure popularly used for networking applications. A standard Bloom filter only answers yes/no questions about membership, but recent studies have improved it so that the value of a queried item can be returned, supporting multiple-set membership testing. In this paper, we design a new data structure for multiple-set membership testing, Bloom tree, which not only achieves space compactness, but also operates more efficiently than existing ones. For example, when existing work requires 107 bits per item and 11 memory accesses for a search operation, a Bloom tree requires only 47 bits and 8 memory accesses. The advantages come from a new data structure that consists of multiple Bloom filters in a tree structure. We study a theoretical analysis model to find optimal parameters for Bloom trees, and its effectiveness is verified through experiments.},
keywords={data structures;query processing;set theory;search tree;Bloom tree;Bloom filters;multiple set membership testing;data structure;networking applications;queried item;space compactness;search operation;memory access;optimal parameters;Testing;Vegetation;Encoding;Memory management;Arrays;Computers},
doi={10.1109/INFOCOM.2014.6848077},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848078,
author={J. T. Araújo and R. Landa and R. G. Clegg and G. Pavlou and K. Fukuda},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A longitudinal analysis of Internet rate limitations},
year={2014},
volume={},
number={},
pages={1438-1446},
abstract={TCP remains the dominant transport protocol for Internet traffic, but the preponderance of its congestion control mechanisms in determining flow throughput is often disputed. This paper analyzes the extent to which network, host and application settings define flow throughput over time and across autonomous systems. Drawing from a longitudinal study spanning five years of passive traces collected from a single transit link, our results show that continuing OS upgrades have reduced the influence of host limitations owing both to windowscale deployment, which by 2011 covered 80% of inbound traffic, and increased socket buffer sizes. On the other hand, we show that for this data set, approximately half of all inbound traffic remains throttled by constraints beyond network capacity, challenging the traditional model of congestion control in TCP traffic as governed primarily by loss and delay.},
keywords={Internet;telecommunication congestion control;telecommunication links;telecommunication traffic;transport protocols;Internet rate limitations;TCP;transport protocol;Internet traffic;congestion control mechanisms;single transit link;OS;windowscale deployment;inbound traffic;socket buffer size;network capacity;Throughput;Receivers;Internet;Bandwidth;Conferences;Computers;Sockets},
doi={10.1109/INFOCOM.2014.6848078},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848079,
author={L. Ma and T. He and K. K. Leung and A. Swami and D. Towsley},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Monitor placement for maximal identifiability in network tomography},
year={2014},
volume={},
number={},
pages={1447-1455},
abstract={We investigate the problem of placing a given number of monitors in a communication network to identify the maximum number of link metrics from end-to-end measurements between monitors, assuming that link metrics are additive, and measurement paths cannot contain cycles. Motivated by our previous result that complete identification of all link metrics can require a large number of monitors, we focus on partial identification using a limited number of monitors. The basis to our solution is an efficient algorithm for determining all identifiable links for a given monitor placement. Based on this algorithm, we develop a polynomial-time greedy algorithm to incrementally place monitors such that each newly placed monitor maximizes the number of additional identifiable links. We prove that the proposed algorithm is optimal for 2-vertex-connected networks, and demonstrate that it is near-optimal for several real ISP topologies that are not 2-vertex-connected. Our solution provides a quantifiable tradeoff between level of identifiability and available monitor resources.},
keywords={computational complexity;greedy algorithms;Internet;network theory (graphs);radio links;tomography;maximal identifiability;network tomography;monitor placement problem;communication network;link metrics identification;end-to-end measurement;partial identification;polynomial-time greedy algorithm;vertex connected network;ISP topology;monitor resources;Monitoring;Measurement;Routing;Conferences;Computers;Additives;Electronic mail},
doi={10.1109/INFOCOM.2014.6848079},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848080,
author={C. Yu and Y. Xu and B. Liu and Y. Liu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={“Can you SEE me now?” A measurement study of mobile video calls},
year={2014},
volume={},
number={},
pages={1456-1464},
abstract={Video telephony is increasingly being adopted by end consumers. It is extremely challenging to deliver video calls over wireless networks. In this paper, we conduct a measurement study on three popular mobile video call applications: Face-Time, Google Plus Hangout, and Skype, over both WiFi and Cellular links. We study the following questions: 1) how they encode/decode video in realtime under tight resource constraints on mobile devices? 2) how they transmit video smoothly in the face of various wireless network impairments? 3) what is their delivered video conferencing quality under different mobile network conditions? 4) how different system architectures and design choices contribute to their delivered quality? Through detailed analysis of measurement results, we obtain valuable insights regarding the unique challenges, advantages and disadvantages of existing design solutions, and possible directions to deliver high-quality video calls in wireless networks.},
keywords={cellular radio;mobile radio;teleconferencing;telephony;wireless LAN;mobile video calls;video telephony;Face-Time;Google Plus Hangout;Skype;WiFi;cellular links;video conferencing;Computers;Delays;IEEE 802.11 Standards;Packet loss;Smart phones;Mobile communication},
doi={10.1109/INFOCOM.2014.6848080},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848081,
author={Y. Li and H. Zhang and Z. Huang and M. Albert},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Optimal link scheduling for delay-constrained periodic traffic over unreliable wireless links},
year={2014},
volume={},
number={},
pages={1465-1473},
abstract={This paper investigates the problem of scheduling delay-constrained traffic in a single-hop wireless industrial network in which different source devices have different data rates. We aim to maximize the packet delivery reliability while meeting the deadline for each packet. The transmission scheduling problem is decomposed into two sub-problems: subperiod-based slot allocation and slot-based transmission scheduling. The former sub-problem is formulated as a nonlinear integer programming problem, and we present a solution with polynomial-time complexity by converting it to a linear integer programming problem. For the latter sub-problem, we demonstrate that the existence of a feasible optimal schedule depends on the order of the elements in the slot allocation vector produced by solving the former subproblem. An algorithm is designed to compute a feasible slot allocation that sustains a realizable schedule. Simulation results demonstrate that our scheme ensures each device has almost the same packet delivery rate in different report periods, which is important for maintaining the stability of control systems.},
keywords={computational complexity;integer programming;linear programming;nonlinear programming;scheduling;telecommunication traffic;wireless sensor networks;optimal link scheduling;delay-constrained periodic traffic;unreliable wireless links;single-hop wireless industrial network;transmission scheduling problem;subperiod-based slot allocation;slot-based transmission scheduling;nonlinear integer programming problem;linear integer programming problem;control system stability;wireless sensor networks;Optimal scheduling;Job shop scheduling;Resource management;Wireless sensor networks;Wireless communication;Sensors},
doi={10.1109/INFOCOM.2014.6848081},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848082,
author={X. Fang and H. Gao and J. Li and Y. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Approximate multiple count in Wireless Sensor Networks},
year={2014},
volume={},
number={},
pages={1474-1482},
abstract={COUNT is a typical aggregation operation in Wireless Sensor Networks (WSNs). In such an operation, the total number of the items which are of the same kind is obtained and only one numerical value is returned as the result. This paper identifies the multiple count problem which counts items belonging to multiple categories. For each category, the total number of the items belonging to this category is calculated. Therefore, the returned result is a set of values instead of a single value. The multiple count problem is more challenging than the traditional count problem as the former incurs more communication overhead. This paper proposes a distributed approximate multiple count algorithm which can derive an error bounded result under a specified communication cost constraint for each node. The error of the derived result is hN/L, where h is the depth of the routing tree, N is the total number of all the items belonging to all the categories, and L is a representation of the communication cost constraint for each node. Furthermore, the weighted multiple count problem is investigated where different kinds of items to be counted have different weights. The proposed algorithms are evaluated through TOSSIM, a widely used simulation tool for WSNs. The theoretical analysis and simulation results both demonstrate the correctness and effectiveness of the proposed algorithms.},
keywords={approximation theory;telecommunication network routing;trees (mathematics);wireless sensor networks;approximate multiple count;wireless sensor networks;distributed approximate multiple count algorithm;routing tree;communication cost constraint;TOSSIM;aggregation operation;Wireless sensor networks;Approximation algorithms;Carbon dioxide;Clustering algorithms;Monitoring;Aggregates;Approximation methods},
doi={10.1109/INFOCOM.2014.6848082},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848083,
author={G. Li and J. Teng and F. Yang and A. C. Champion and D. Xuan and H. Luan and Y. F. Zheng},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={EV-sounding: A visual assisted electronic channel sounding system},
year={2014},
volume={},
number={},
pages={1483-1491},
abstract={Electronic channel sounding plays a vital role in developing wireless communication systems. It is critical for transceivers' equalization and filtering operations. However, current pure electronic channel sounding techniques are not well-suited for emerging scenarios such as opportunistic spectrum access, channel impulse response (CIR) based wireless positioning, and wireless security applications, which demand rapid, high-resolution, and spectrum agile channel measurements on commercial off-the-shelf (COTS) devices. To address these critical issues, this paper proposes EV-Sounding, a novel methodology for visual assisted electronic channel sounding. Based on frequency domain channel sounding, EV-Sounding leverages cameras for visual estimation of sparsity locations to reduce the number of frequency samples, thus speeding up the sounding process. EV-Sounding achieves both high-resolution CIR measurements and spectrum agility. We prototype an EV-Sounding system on COTS devices. Our real-world experimental results and extensive simulations validate EV-Sounding's performance.},
keywords={frequency-domain analysis;transient response;wireless channels;spectrum agility;frequency samples;sparsity locations;EV-sounding leverages cameras;frequency domain channel sounding;COTS devices;commercial off-the-shelf devices;channel measurements;wireless security applications;CIR based wireless positioning;channel impulse response based wireless positioning;opportunistic spectrum access;electronic channel sounding techniques;filtering operations;transceivers equalization;wireless communication systems;Visualization;Wireless communication;Cameras;Time-frequency analysis;Communication system security;Image reconstruction;Channel sounding;wireless measurement},
doi={10.1109/INFOCOM.2014.6848083},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848084,
author={V. P. Munishwar and V. Kolar and N. B. Abu-Ghazaleh},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Coverage in visual sensor networks with Pan-Tilt-Zoom cameras: The MaxFoV problem},
year={2014},
volume={},
number={},
pages={1492-1500},
abstract={We consider the problem of target coverage in visual sensor networks with Pan-Tilt-Zoom (PTZ) cameras. The finely controllable movement in PTZ dimensions creates a large number of possible Field-of-View (FoV) settings, making it prohibitively expensive to consider them all in coverage algorithms. However, these FoVs are redundant as each group of targets is generally covered by many FoVs. Thus, an important problem is, how to identify FoVs that cover all maximal subsets of targets (MaxFoV) efficiently? We show that MaxFoV is an instance of generating all maximal cliques, which is NP-hard in general but polynomial if the number of cliques is polynomial. We construct an optimal algorithm to solve the problem with a worst case complexity of O(n<sup>3</sup>). Simulation and testbed experiments show that the algorithm drastically reduces the number of FoVs allowing multi-camera coverage to scale without sacrificing coverage quality.},
keywords={computational complexity;video cameras;visual communication;wireless sensor networks;visual sensor network;pan-tilt-zoom camera;MaxFoV problem;PTZ;field of view;maximal clique generation;NP-hard problem;polynomial;worst case complexity;multicamera coverage;Cameras;Polynomials;Complexity theory;Shape;Conferences;Computers;Visualization},
doi={10.1109/INFOCOM.2014.6848084},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848085,
author={Z. Lu and W. Wang and C. Wang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={How can botnets cause storms? Understanding the evolution and impact of mobile botnets},
year={2014},
volume={},
number={},
pages={1501-1509},
abstract={A botnet in mobile networks is a collection of compromised nodes due to mobile malware, which are able to perform coordinated attacks. Different from Internet botnets, mobile botnets do not need to propagate using centralized infrastructures, but can keep compromising vulnerable nodes in close proximity and evolving organically via data forwarding. Such a distributed mechanism relies heavily on node mobility as well as wireless links, therefore breaks down the underlying premise in existing epidemic modeling for Internet botnets. In this paper, we adopt a stochastic approach to study the evolution and impact of mobile botnets. We find that node mobility can be a trigger to botnet propagation storms: the average size (i.e., number of compromised nodes) of a botnet increases quadratically over time if the mobility range that each node can reach exceeds a threshold; otherwise, the botnet can only contaminate a limited number of nodes with average size always bounded above. This also reveals that mobile botnets can propagate at the fastest rate of quadratic growth in size, which is substantially slower than the exponential growth of Internet botnets. To measure the denial-of-service impact of a mobile botnet, we define a new metric, called last chipper time, which is the last time that service requests, even partially, can still be processed on time as the botnet keeps propagating and launching attacks. The last chipper time is identified to decrease at most on the order of 1/√B, where B is the network bandwidth. This result reveals that although increasing network bandwidth can help with mobile services; at the same time, it can indeed escalate the risk for services being disrupted by mobile botnets.},
keywords={computer network security;invasive software;mobile computing;mobile botnets;mobile networks;compromised nodes;mobile malware;coordinated attacks;Internet botnets;data forwarding;distributed mechanism;node mobility;wireless links;epidemic modeling;botnet propagation storms;mobility range;exponential growth;quadratic growth;denial-of-service impact;last chipper time;service requests;network bandwidth;mobile services;Malware;Mobile nodes;Peer-to-peer computing;Mobile computing;Internet},
doi={10.1109/INFOCOM.2014.6848085},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848086,
author={S. Qi and Y. Zheng and M. Li and L. Lu and Y. Liu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={COLLECTOR: A secure RFID-enabled batch recall protocol},
year={2014},
volume={},
number={},
pages={1510-1518},
abstract={Batch recall is a practically important problem for most industry manufacturers. The batches of products which contain flawed parts need to be recalled by manufacturers in time to prevent further economic and health loss. Accurate batch recall could be a challenging issue as flawed parts may have already been integrated into a large number of products and distributed to customers. The recent development of Radio Frequency Identification (RFID) provides us a promising opportunity to implement batch recall in an accurate and efficient way. RFID-enabled batch recall provides us the opportunity to further enhance the security of batch recall operation, allowing us to achieve recognition of problematic products, privacy preserving of production pattern, recall authentication and non-repudiation, etc. In this paper, we thoroughly study the security aspects and identify the unique requirements in RFID-enabled batch recall. We propose a practically secure protocol, COLLECTOR, to enable accurate, secure and efficient RFID batch recall.},
keywords={batch processing (industrial);batch production systems;radiofrequency identification;COLLECTOR;secure RFID-enabled batch recall protocol operation;industry manufacturers;radio frequency identification;privacy preserving;production pattern;recall authentication;Production;Protocols;Radiofrequency identification;Authentication;Indexes;Computers;RFID;batch recall;security},
doi={10.1109/INFOCOM.2014.6848086},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848087,
author={I. Murynets and M. Zabarankin and R. P. Jover and A. Panagia},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Analysis and detection of SIMbox fraud in mobility networks},
year={2014},
volume={},
number={},
pages={1519-1526},
abstract={Voice traffic termination fraud, often referred to as Subscriber Identity Module box (SIMbox) fraud, is a common illegal practice on mobile networks. As a result, cellular operators around the globe lose billions annually. Moreover, SIMboxes compromise the cellular network infrastructure by overloading local base stations serving these devices. This paper analyzes the fraudulent traffic from SIMboxes operating with a large number of SIM cards. It processes hundreds of millions of anonymized voice call detail records (CDRs) from one of the main cellular operators in the United States. In addition to overloading voice traffic, fraudulent SIMboxes are observed to have static physical locations and to generate disproportionately large volume of outgoing calls. Based on these observations, novel classifiers for fraudulent SIMbox detection in mobility networks are proposed. Their outputs are optimally fused to increase the detection rate. The operator's fraud department confirmed that the algorithm succeeds in detecting new fraudulent SIMboxes.},
keywords={cellular radio;fraud;mobile computing;pattern classification;security of data;telecommunication traffic;voice communication;SIMbox fraud analysis;SIMbox fraud detection;mobility networks;voice traffic termination fraud;subscriber identity module fraud;mobile networks;cellular network infrastructure;SIM cards;anonymized voice call detail records;United States;fraudulent SIMboxes;classifiers;Mobile communication;Mobile computing;Base stations;Conferences;Computers;Wireless communication;Receivers},
doi={10.1109/INFOCOM.2014.6848087},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848088,
author={M. Mirmohseni and P. Papadimitratos},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Scaling laws for secrecy capacity in cooperative wireless networks},
year={2014},
volume={},
number={},
pages={1527-1535},
abstract={We investigate large wireless networks subject to security constraints. In contrast to point-to-point, interference-limited communications considered in prior works, we propose active cooperative relaying based schemes. We consider a network with nllegitimate nodes and neeavesdroppers, and path loss exponent α ≥ 2. As long as ne2(log(ne))γ= o(nl) holds for some positive γ, we show one can obtain unbounded secure aggregate rate. This means zero-cost secure communication, given a fixed total power constraint for the entire network. We achieve this result with (i) the source using Wyner randomized encoder and a serial (multi-stage) block Markov scheme, to cooperate with the relays, and (ii) the relays acting as a virtual multi-antenna to apply beamforming against the eavesdroppers. Our simpler parallel (two-stage) relaying scheme can achieve the same unbounded secure aggregate rate when neα/2 + 1(log(ne))γ+δ(α/2+1)= o(nl) holds, for some positive γ, δ.},
keywords={array signal processing;cooperative communication;interference (signal);Markov processes;relay networks (telecommunication);telecommunication security;scaling laws;secrecy capacity;cooperative wireless networks;secure communication;interference limited communications;active cooperative relaying;path loss exponent;Wyner randomized encoder;serial block Markov scheme;beamforming;parallel relaying scheme;Relays;Aggregates;Encoding;Wireless networks;Array signal processing;Tin;Transmitters},
doi={10.1109/INFOCOM.2014.6848088},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848089,
author={F. Malandrino and C. Casetti and C. Chiasserini and Z. Limani},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Fast resource scheduling in HetNets with D2D support},
year={2014},
volume={},
number={},
pages={1536-1544},
abstract={Resource allocation in LTE networks is known to be an NP-hard problem. In this paper, we address an even more complex scenario: an LTE-based, 2-tier heterogeneous network where D2D mode is supported under the network control. All communications (macrocell, microcell and D2D-based) share the same frequency bands, hence they may interfere. We then determine (i) the network node that should serve each user and (ii) the radio resources to be scheduled for such communication. To this end, we develop an accurate model of the system and apply approximate dynamic programming to solve it. Our algorithms allow us to deal with realistic, large-scale scenarios. In such scenarios, we compare our approach to today's networks where eICIC techniques and proportional fairness scheduling are implemented. Results highlight that our solution increases the system throughput while greatly reducing energy consumption. We also show that D2D mode can effectively support content delivery without significantly harming macrocells or microcells traffic, leading to an increased system capacity. Interestingly, we find that D2D mode can be a low-cost alternative to microcells.},
keywords={computational complexity;dynamic programming;energy consumption;Long Term Evolution;microcellular radio;radiofrequency interference;resource allocation;scheduling;telecommunication control;telecommunication power management;telecommunication traffic;system capacity;microcell traffic;macrocell traffic;content delivery;energy consumption;system throughput;proportional fairness scheduling;eICIC techniques;dynamic programming;radio resources;network node;D2D-based communication;microcell communication;macrocell communication;network control;D2D mode;HetNets;two-tier heterogeneous network;LTE-based heterogeneous network;NP-hard problem;LTE networks;resource allocation;D2D support;device-to-device support;fast resource scheduling;Resource management;Dynamic programming;Interference;Microcell networks;Macrocell networks;Complexity theory;Conferences},
doi={10.1109/INFOCOM.2014.6848089},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848090,
author={T. Yu and Z. Zhou and D. Zhang and X. Wang and Y. Liu and S. Lu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={INDAPSON: An incentive data plan sharing system based on self-organizing network},
year={2014},
volume={},
number={},
pages={1545-1553},
abstract={The contradiction between dynamic user traffic and fixed data plans has drawn increasing attention in the field of mobile applications. In this paper we build a data plan sharing system named INDAPSON to consider a scenario where some smartphone users have surplus data traffic and are willing to help others download data. Virtual credits can be gained as reward, which can be used to ask for future help in downloading. To realize this model, we make the following contributions: 1) A dynamic self-organization strategy to adapt to mobile terminals; 2) An incentive mechanism named RAP to encourage participation; 3) Power-saving strategies to reduce power consumption. The main advantage of our system is that users gain improvement in download rate while being able to convert their surplus data traffic to virtual credits. The results of experiment and simulation show that users in our system can manage their surplus data plan more efficiently while a highspeed download rate can be achieved.},
keywords={incentive schemes;mobile computing;smart phones;telecommunication power management;telecommunication traffic;incentive data plan sharing system based on self-organizing network;high-speed download rate;power consumption reduction;power-saving strategies;RAP;incentive mechanism;mobile terminals;dynamic self-organization strategy;surplus data traffic;data traffic;smartphone users;mobile applications;dynamic user traffic;INDAPSON;Gold;Servers;IEEE 802.11 Standards;Internet;Bluetooth;Mobile communication;Wireless communication},
doi={10.1109/INFOCOM.2014.6848090},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848091,
author={M. Manjrekar and V. Ramaswamy and S. Shakkottai},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A mean field game approach to scheduling in cellular systems},
year={2014},
volume={},
number={},
pages={1554-1562},
abstract={We study auction-theoretic scheduling in cellular networks using the idea of mean field equilibrium (MFE). Here, agents model their opponents through a distribution over their action spaces and play the best response. The system is at an MFE if this action is itself a sample drawn from the assumed distribution. In our setting, the agents are smart phone apps that generate service requests, experience waiting costs, and bid for service from base stations. We show that if we conduct a second-price auction at each base station, there exists an MFE that would schedule the app with the longest queue at each time. The result suggests that auctions can attain the same desirable results as queue-length-based scheduling. We present results on the asymptotic convergence of a system with a finite number of agents to the mean field case, and conclude with simulation results illustrating the simplicity of computation of the MFE.},
keywords={cellular radio;game theory;scheduling;smart phones;mean field game;cellular systems;auction-theoretic scheduling;cellular networks;mean field equilibrium;smart phone;base stations;Games;Base stations;Bayes methods;Random variables;Markov processes;Computational modeling;Conferences},
doi={10.1109/INFOCOM.2014.6848091},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848092,
author={T. Bansal and K. Sundaresan and S. Rangarajan and P. Sinha},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={R2D2: Embracing device-to-device communication in next generation cellular networks},
year={2014},
volume={},
number={},
pages={1563-1571},
abstract={Device-to-device (D2D) communications is being pursued as an important feature in next generation cellular networks. D2D can improve resource utilization in two ways: Offloading cellular traffic to D2D, and Reuse of resources used by conventional cellular transmissions for D2D communication. In this paper, we show that in multi-cell environments that employ FFR (Fractional Frequency Reuse), the benefits from D2D toward reuse are limited. We then propose R2D2- a holistic approach to efficient offloading with D2D traffic. R2D2 leverages the flexible nature of D2D traffic (in using downlink/uplink resources) to cater effectively to the spatial and temporal asymmetry in traffic load both across and within cells. R2D2 incorporates a two time-scale solution: a coarse time-scale dynamic FFR scheme that leverages D2D traffic to determine the FFR patterns for downlink and uplink jointly among interfering sectors; and a fine time-scale scheduling solution that intelligently schedules cellular and D2D traffic jointly across DL (Downlink) and UL (Uplink) resources. We establish the hardness of the scheduling problem and present efficient and low complexity algorithms with approximation guarantees. Through extensive evaluations, we confirm that R2D2 delivers the offloading benefits of D2D, with its proposed algorithms performing very close to the optimal.},
keywords={cellular radio;frequency allocation;next generation networks;telecommunication congestion control;fine time-scale scheduling solution;time-scale dynamic FFR scheme;traffic load;temporal asymmetry;spatial asymmetry;D2D traffic;fractional frequency reuse;multicell environments;cellular transmissions;offloading cellular traffic;resource utilization;next generation cellular networks;D2D communications;device-to-device communication;R2D2;Resource management;Dynamic scheduling;Interference;Base stations;Uplink;Heuristic algorithms;Throughput},
doi={10.1109/INFOCOM.2014.6848092},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848093,
author={J. Guo and F. Liu and X. Huang and J. C. S. Lui and M. Hu and Q. Gao and H. Jin},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={On efficient bandwidth allocation for traffic variability in datacenters},
year={2014},
volume={},
number={},
pages={1572-1580},
abstract={Datacenter networks suffer unpredictable performance due to a lack of application level bandwidth guarantees. A lot of attentions have been drawn to solve this problem such as how to provide bandwidth guarantees for Virtualized Machines (VMs), proportional bandwidth share among tenants, and high network utilization under peak traffic. However, existing solutions fail to cope with highly dynamic traffic in datacenter networks. In this paper, we consider the effects of large numbers of short flows and massive bursty traffic in the datacenter, and design a novel distributed rate allocation algorithm based on the Logistic model under the control-theoretic framework. The theoretical analysis and experimental results using OpenFlow show that our algorithm not only guarantees the bandwidth for VMs, but also provides fast convergence to efficiency and fairness, and smooth response to bursty traffic.},
keywords={bandwidth allocation;computer centres;telecommunication traffic;virtual machines;bandwidth allocation;traffic variability;datacenter networks;application level bandwidth guarantees;virtualized machines;proportional bandwidth share;VM;massive bursty traffic;distributed rate allocation algorithm;logistic model;control-theoretic framework;OpenFlow;high network utilization;Bandwidth;Convergence;Heuristic algorithms;Algorithm design and analysis;Resource management;Channel allocation;Logistics},
doi={10.1109/INFOCOM.2014.6848093},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848094,
author={H. Xu and B. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={RepFlow: Minimizing flow completion times with replicated flows in data centers},
year={2014},
volume={},
number={},
pages={1581-1589},
abstract={Short TCP flows that are critical for many interactive applications in data centers are plagued by long flows and head-of-line blocking in switches. Hash-based load balancing schemes such as ECMP aggravate the matter and result in long-tailed flow completion times (FCT). Previous work on reducing FCT usually requires custom switch hardware and/or protocol changes. We propose RepFlow, a simple yet practically effective approach that replicates each short flow to reduce the completion times, without any change to switches or host kernels. With ECMP the original and replicated flows traverse distinct paths with different congestion levels, thereby reducing the probability of having long queueing delay. We develop a simple analytical model to demonstrate the potential improvement. Further, we conduct NS-3 simulations and Mininet implementation and show that RepFlow provides 50%-70% speedup in both mean and 99-th percentile FCT for all loads, and offers near-optimal FCT when used with DCTCP.},
keywords={computer centres;computer networks;interactive systems;queueing theory;resource allocation;transport protocols;flow completion times;FCT;replicated flows;RepFlow;data centers;TCP flows;interactive applications;head-of-line blocking;hash-based load balancing schemes;ECMP;custom switch hardware;protocol changes;congestion levels;queueing delay;analytical model;NS-3 simulations;Mininet implementation;DCTCP;Delays;Load modeling;Web search;Computers;Load management;Conferences;Transport protocols},
doi={10.1109/INFOCOM.2014.6848094},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848095,
author={M. Chiesa and G. Kindler and M. Schapira},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Traffic engineering with Equal-Cost-Multipath: An algorithmic perspective},
year={2014},
volume={},
number={},
pages={1590-1598},
abstract={To efficiently exploit network resources operators do traffic engineering (TE), i.e., adapt the routing of traffic to the prevailing demands. TE in large IP networks typically relies on configuring static link weights and splitting traffic between the resulting shortest-paths via the Equal-Cost-MultiPath (ECMP) mechanism. Yet, despite its vast popularity, crucial operational aspects of TE via ECMP are still little-understood from an algorithmic viewpoint. We embark upon a systematic algorithmic study of TE with ECMP. We consider the standard model of TE with ECMP and prove that, in general, even approximating the optimal link-weight configuration for ECMP within any constant ratio is an intractable feat, settling a long-standing open question. We establish, in contrast, that ECMP can provably achieve optimal traffic flow for the important category of Clos datacenter networks. We last consider a well-documented shortcoming of ECMP: suboptimal routing of large (“elephant”) flows. We present algorithms for scheduling “elephant” flows on top of ECMP (as in, e.g., Hedera [1]) with provable approximation guarantees. Our results complement and shed new light on past experimental and empirical studies of the performance of TE with ECMP.},
keywords={IP networks;telecommunication network routing;traffic engineering;algorithmic perspective;equal cost multipath;TE;traffic routing;IP networks;static link weights;splitting traffic;ECMP mechanism;optimal link weight configuration;datacenter networks;optimal traffic flow;Routing;Network topology;Approximation algorithms;Computers;Standards;Conferences;Educational institutions},
doi={10.1109/INFOCOM.2014.6848095},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848096,
author={L. Chen and Y. Feng and B. Li and B. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Towards performance-centric fairness in datacenter networks},
year={2014},
volume={},
number={},
pages={1599-1607},
abstract={Fair bandwidth allocation in datacenter networks has been a focus of research recently, yet this has not received adequate attention in the context of private cloud, where link bandwidth is often shared among applications running data parallel frameworks, such as MapReduce. In this paper, we introduce a rigorous definition of performance-centric fairness, with the guiding principle that the performance of data parallel applications should be proportional to their weights. We investigate the problem of maximizing application performance while maintaining strict performance-centric fairness and present the inherent tradeoff between resource utilization and fairness. We then formulate the link bandwidth allocation problem with the objective of maximizing social welfare across all applications, so that resource utilization can be manipulated and improved by allowing a tunable degree of relaxation on performance-centric fairness. Based on dual based decomposition, we present a distributed algorithm to solve this problem, and evaluate its performance with extensive simulations.},
keywords={bandwidth allocation;computer centres;computer network performance evaluation;distributed algorithms;telecommunication links;distributed algorithm;data parallel applications;performance-centric fairness;MapReduce;bandwidth link sharing;bandwidth allocation;datacenter networks;Bandwidth;Resource management;Servers;Channel allocation;Optimization;Conferences;Computers},
doi={10.1109/INFOCOM.2014.6848096},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848097,
author={H. Saito},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Geometric evaluation of survivability of disaster-affected network with probabilistic failure},
year={2014},
volume={},
number={},
pages={1608-1616},
abstract={This paper presents an algorithm for evaluating the probability that connectivity can be maintained between two given nodes in a physical network affected by a disaster. Nodes and links in a disaster area are probabilistically broken, and the disaster area is modeled using a half plane. This paper also proves that this probability of connectivity increases for a generic network topology when the perimeter length of the convex hull of a physical link route decreases, and that it becomes maximum when these physical link routes become straight line segments. In addition, this paper proposes an optimal server placement method by considering robustness against disaster and an optimal link/node replacement strategy determining which nodes or links should be replaced with those robust against disaster. Intuitive node (link) replacement strategies are also suggested based on the analysis of this paper.},
keywords={probability;telecommunication network planning;telecommunication network reliability;telecommunication network topology;geometric evaluation;network survivability;connectivity probability;probabilistic failure;network topology;physical link routes;optimal server placement;optimal node replacement;Approximation methods;Polynomials;Robustness;Measurement;Conferences;Computers;Analytical models},
doi={10.1109/INFOCOM.2014.6848097},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848098,
author={B. Yang and J. Liu and S. Shenker and J. Li and K. Zheng},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Keep Forwarding: Towards k-link failure resilient routing},
year={2014},
volume={},
number={},
pages={1617-1625},
abstract={Handling link failures is the fundamental task of routing schemes. Routing protocols based on link state (e.g., OSPF) require a global state advertisement and re-computation when link failure happens, and will cause inevitable delivery failures. To improve the routing resilience without introducing significant extra overhead, we propose a new routing approach, Keep Forwarding (KF) to achieve k-link failure resilience using inport-aware forwarding. KF is (i) flexible to handle multiple failures (or k-failure) with only small path stretch, (ii) efficient in recovery speed by instant and local lookup, (iii) bounded on memory requirement. Besides, the proposed approach is compatible with existing Internet protocols and routing infrastructures (e.g., requires no packet labeling or state recording), and the pre-computation has a linear temporal complexity. Experimental results on real ISP and datacenter networks reveal that KF guarantees near-optimal resilience (99.9%~100% for single failure and over 99.7% for multiple failures), with the average path stretch increment less than 5%.},
keywords={computational complexity;computer network reliability;failure analysis;Internet;routing protocols;k-link failure resilient routing;routing protocols;global state advertisement;keep forwarding;KF;inport-aware forwarding;Internet protocols;routing infrastructures;linear temporal complexity;ISP;data center networks;Routing;Resilience;Ports (Computers);IP networks;Computers;Topology;Conferences},
doi={10.1109/INFOCOM.2014.6848098},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848099,
author={R. Cohen and G. Nakibly},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Restorable logical topology in the face of no or partial traffic demand knowledge},
year={2014},
volume={},
number={},
pages={1626-1634},
abstract={The construction of a logical network on top of a physical (optical) infrastructure involves two intertwined tasks: logical link selection - deciding which pairs of routers will be connected by logical links (lightpaths), and logical link routing - deciding how to route each logical link across the optical network. The operator of such networks is often required to maximize the available throughput while guaranteeing its restorability. This paper is the first to combine these seemingly conflicting goals into one optimization criterion: maximizing the restorable throughput of the end-to-end paths. We address this problem in three cases: when the operator has no knowledge of the future bandwidth demands, when it has partial knowledge, and when it has full knowledge. We present efficient algorithms for each of these cases and use extensive simulations to compare their performance.},
keywords={optical links;optimisation;telecommunication network routing;telecommunication network topology;logical topology;traffic demand knowledge;logical link selection;logical link routing;optical network;restorable network throughput;optimization criterion;Approximation algorithms;Throughput;Routing;Approximation methods;Bandwidth;Optical fiber networks},
doi={10.1109/INFOCOM.2014.6848099},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848100,
author={Y. Kobayashi and K. Otsuki},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Max-flow min-cut theorem and faster algorithms in a circular disk failure model},
year={2014},
volume={},
number={},
pages={1635-1643},
abstract={Fault-tolerance is one of the most important factors in designing networks. Failures in networks are sometimes caused by an event occurring in specific geographical regions such as hurricanes, earthquakes, bomb attacks, and Electromagnetic Pulse (EMP) attacks. In INFOCOM 2012, Neumayer et al. introduced geographical variants of max-flow min-cut problems in a circular disk failure model, in which each failure is represented by a disk with a predetermined size. In this paper, we solve two open problems in this model: we give a first polynomial-time algorithm for the geographic max-flow problem, and prove a conjecture of Neumayer et al. on a relationship between the geographic max-flow and the geographic min-cut.},
keywords={communication complexity;fault tolerance;minimax techniques;radio networks;telecommunication network planning;telecommunication network reliability;geographic max-flow min-cut theorem;circular disk failure model;fault tolerance;network design;geographical region;geographical variant;polynomial-time algorithm;hurricanes;earthquakes;bomb attacks;electromagnetic pulse;Clocks;Face;Computers;Conferences;Polynomials;Joining processes;Computational modeling},
doi={10.1109/INFOCOM.2014.6848100},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848101,
author={P. M. van de Ven and B. Zhang and A. Schörgendorfer},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Distributed backup scheduling: Modeling and optimization},
year={2014},
volume={},
number={},
pages={1644-1652},
abstract={Recent years have seen rapid growth in data storage, magnifying the importance of ensuring data safety by performing regular backups. However, traffic created by such backups can be a significant burden on the underlying communication network. In the present paper we address the tradeoff between frequent backups (increased safety) and reducing the network peak load. We address the problem of shifting backup traffic from peak hours to off-peak hours within the constraints imposed by user connectivity. Backups are scheduled using a distributed protocol characterized by a set of probabilities that indicate the likelihood of a user initiating a backup during a given hour. Given these probabilities, we study the network capacity by investigating the rate at which users can generate data while retaining stable backlog processes. We then derive explicit expressions for the stationary behavior of the backup process, and discuss how to choose the backup probabilities that strike the right balance between a low peak load and data safety. Via simulation experiments we show that this approach is highly successful in reducing costs.},
keywords={distributed processing;scheduling;security of data;distributed backup scheduling;modeling;optimization;data storage;data safety;communication network;frequent backups;network peak load;backup traffic;user connectivity;distributed protocol;network capacity;stable backlog processes;Stability analysis;Servers;Safety;Conferences;Computers;Data models;Distributed databases},
doi={10.1109/INFOCOM.2014.6848101},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848102,
author={P. Wan and X. Jia and G. Dai and H. Du and O. Frieder},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Fast and simple approximation algorithms for maximum weighted independent set of links},
year={2014},
volume={},
number={},
pages={1653-1661},
abstract={Finding a maximum-weighted independent set of links is a fundamental problem in wireless networking and has broad applications in various wireless link scheduling problems. Under protocol interference model, it is NP-hard even when all nodes have uniform (and fixed) interference radii and the positions of all nodes are available. On one hand, it admits a polynomial-time approximation scheme (PTAS). In other words, for any fixed ε > 0, it has a polynomial-time (depending on ε) (1 + ε)-approximation algorithm. However, such PTAS is of theoretical interest only and is quite infeasible practically. On the other hand, only with the uniform interference radii is a simple (greedy) constant-approximation algorithm known. For the arbitrary interference radii, fast constant-approximation algorithms are still missing. In this paper, we present a number of fast and simple approximation algorithms under the general protocol interference model. When applied to the plane geometric variants of the protocol interference model, these algorithms produce constant-approximate solutions efficiently.},
keywords={approximation theory;optimisation;protocols;radio links;radio networks;radiofrequency interference;scheduling;maximum-weighted independent set of links;wireless networking;wireless link scheduling problems;protocol interference model;NP-hard;polynomial-time approximation scheme;PTAS;constant-approximation algorithm;Interference;Approximation algorithms;Approximation methods;Protocols;Personal digital assistants;Conferences},
doi={10.1109/INFOCOM.2014.6848102},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848103,
author={J. Kwak and C. Lee and D. Y. Eun},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A high-order Markov chain based scheduling algorithm for low delay in CSMA networks},
year={2014},
volume={},
number={},
pages={1662-1670},
abstract={Recently, several CSMA algorithms based on the Glauber dynamics model have been proposed for multihop wireless scheduling, as viable solutions to achieve the throughput optimality, yet simple to implement. However, their delay performance still remains unsatisfactory, mainly due to the nature of the underlying Markov chains that imposes a fundamental constraint on how the link state can evolve over time. In this paper, we propose a new approach toward better queueing delay performance, based on our observation that the algorithm needs not be Markovian, as long as it can be implemented in a distributed manner. Our approach hinges upon utilizing past state information observed by local link and then constructing a high-order Markov chain for the evolution of the feasible link schedules. We show in theory and simulation that our proposed algorithm, named delayed CSMA, achieves the throughput optimality, and also provides much better delay performance by effectively `de-correlating' the link state process (and thus resolves link starvation). Our extensive simulations demonstrate that the delay under our algorithm can be often reduced by a factor of 20 over a wide range of scenarios, compared to the standard Glauber-dynamics-based CSMA algorithm.},
keywords={carrier sense multiple access;Markov processes;queueing theory;high-order Markov chain based scheduling algorithm;CSMA networks;Glauber dynamics model;multihop wireless scheduling;queueing delay performance;link state process;Schedules;Multiaccess communication;Delays;Markov processes;Correlation;Throughput;Heuristic algorithms},
doi={10.1109/INFOCOM.2014.6848103},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848104,
author={Y. Sun and C. E. Koksal and K. Kim and N. B. Shroff},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Scheduling of multicast and unicast services under limited feedback by using rateless codes},
year={2014},
volume={},
number={},
pages={1671-1679},
abstract={Many opportunistic scheduling techniques are impractical because they require accurate channel state information (CSI) at the transmitter. In this paper, we investigate the scheduling of unicast and multicast services in a downlink network with a very limited amount of feedback information. Specifically, unicast users send imperfect (or no) CSI and infrequent acknowledgements (ACKs) to a base station, and multicast users only report infrequent ACKs to avoid feedback implosion. We consider the use of physical-layer rateless codes, which not only combats channel uncertainty, but also reduces the overhead of ACK feedback. A joint scheduling and power allocation scheme is developed to realize multiuser diversity gain for unicast service and multicast gain for multicast service. We prove that our scheme achieves a near-optimal throughput region. Our simulation results show that our scheme significantly improves the network throughput over schemes employing fixed-rate codes or using only unicast communications.},
keywords={encoding;scheduling;transmitters;wireless channels;unicast services;multicast services;limited feedback;rateless codes;opportunistic scheduling techniques;channel state information;CSI;transmitter;downlink network;feedback information;infrequent acknowledgements;ACK;base station;physical layer rateless codes;channel uncertainty;joint scheduling;power allocation scheme;unicast communications;Unicast;Throughput;Mutual information;Base stations;Resource management;Decoding;Downlink},
doi={10.1109/INFOCOM.2014.6848104},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848105,
author={W. Wei and X. Zhu and Q. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={LBSNSim: Analyzing and modeling location-based social networks},
year={2014},
volume={},
number={},
pages={1680-1688},
abstract={The soaring adoption of location-based social networks (LBSNs) makes it possible to analyze human socio-spatial behaviors based on large-scale realistic data, which is important to both the research community and the design of new location-based social applications. However, performing direct measurements on LBSNs is impractical, because of the security mechanisms of existing LBSNs, and high time and resource costs. The problem is exacerbated by the scarcity of available LBSN datasets, which is mainly due to the privacy concerns and the hardness of distributing large-volume data. As a result, only a very few number of LBSN datasets are publicly released. In this paper, we extract and study the universal statistical features of three LBSN datasets, and propose LBSNSim, a trace-driven model for generating synthetic LBSN datasets capturing the properties of the original datasets. Our evaluation shows that LBSNSim provides an accurate representation of target LBSNs.},
keywords={behavioural sciences computing;data privacy;mobile computing;security of data;social networking (online);statistical analysis;LBSNSim;location-based social networks;human socio-spatial behavior analysis;large-scale realistic data;security mechanisms;resource costs;large-volume data distribution;universal statistical features;trace-driven model;synthetic LBSN dataset generation;Conferences;Computers;Method of moments;Social network services;Data privacy;Feature extraction;Probability density function},
doi={10.1109/INFOCOM.2014.6848105},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848106,
author={T. Meng and F. Wu and G. Chen},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={On designing neighbor discovery protocols: A code-based approach},
year={2014},
volume={},
number={},
pages={1689-1697},
abstract={In mobile wireless networks, the emerging proximity-based applications have led to needs for highly effective and energy-efficient neighbor discovery protocols. However, existing works cannot realize the optimal worst-case latency in symmetric case, and their performances with asymmetric duty cycles can still be improved. In this work, we investigate asynchronous neighbor discovery through a code-based approach, including the symmetric and asymmetric cases. We derive the tight worst-case latency bound in the case of symmetric duty cycle. We design a novel class of symmetric patterns called Diff-Codes, which is optimal when the Diff-Code can be extended from a perfect difference set. We further consider the asymmetric case, and design ADiff-Codes. To evaluate (A)Diff-Codes, we conduct both simulations and testbed experiments. Both simulation and experiment results show that (A)Diff-Codes significantly outperform existing neighbor discovery protocols in both the median case and worst-case. Specifically, in symmetric case, the maximum worst-case improvement is up to 50%; in both symmetric and asymmetric cases, the median case gain is as high as 30%.},
keywords={mobile radio;protocols;neighbor discovery protocols;code-based approach;mobile wireless networks;proximity-based applications;energy-efficient neighbor discovery protocols;optimal worst-case latency;asymmetric duty cycles;symmetric duty cycle;Diff-Codes;ADiff-Codes;Protocols;Probabilistic logic;Mobile communication;Mobile computing;Conferences;Computers;Wireless networks},
doi={10.1109/INFOCOM.2014.6848106},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848107,
author={G. Liu and Q. Yang and H. Wang and X. Lin and M. P. Wittie},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Assessment of multi-hop interpersonal trust in social networks by Three-Valued Subjective Logic},
year={2014},
volume={},
number={},
pages={1698-1706},
abstract={Assessing multi-hop interpersonal trust in online social networks (OSNs) is critical for many social network applications such as online marketing but challenging due to the difficulties of handling complex OSN topology, in existing models such as subjective logic, and the lack of effective validation methods. To address these challenges, we for the first time properly define trust propagation and combination in arbitrary OSN topologies by proposing 3VSL (Three-Valued Subjective Logic). The 3VSL distinguishes the posteriori and priori uncertainties existing in trust, and the difference between distorting and original opinions, thus be able to compute multi-hop trusts in arbitrary graphs. We theoretically proved the capability based on the Dirichlet distribution. Furthermore, an online survey system is implemented to collect interpersonal trust data and validate the correctness and accuracy of 3VSL in real world. Both experimental and numerical results show that 3VSL is accurate in computing interpersonal trust in OSNs.},
keywords={social networking (online);telecommunication network topology;ternary logic;trusted computing;multihop interpersonal trust assessment;three-valued subjective logic;online social networks;complex OSN topology;trust propagation;trust combination;3VSL;Dirichlet distribution;online survey system;interpersonal trust data;Uncertainty;Topology;Social network services;Network topology;Vectors;Computational modeling;Computers;Interpersonal trust;trust establishment;online social networks;three-valued subjective logic;Dirichlet distribution},
doi={10.1109/INFOCOM.2014.6848107},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848108,
author={W. Jiang and J. Wu and G. Wang and H. Zheng},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={FluidRating: A time-evolving rating scheme in trust-based recommendation systems using fluid dynamics},
year={2014},
volume={},
number={},
pages={1707-1715},
abstract={The goal of a trust-based recommendation system is to predict unknown ratings based on the ratings expressed by trusted friends. However, most of the existing work only considers the ratings at the current time slot. In real life, a user receives the influence of different opinions sequentially; accordingly, his opinion evolves over time. We propose a novel rating prediction scheme, FluidRating, which uses fluid dynamics theory to reveal the time-evolving formulation process of human opinions. The recommendation is modeled as fluid with two dimensions: the temperature is taken as the “opinion/rating,” and its volume is deemed as the “persistency,” representing how much one insists on his opinion. When new opinions come, each user refines his opinion through a round of fluid exchange with his neighbors. Opinions from multiple rounds are aggregated to gain a final prediction; both uniform and non-uniform aggregation are tested. Moreover, Three sampling approaches are proposed and examined. The experimental evaluation of a real data set validates the feasibility of the proposed model, and also demonstrates its effectiveness.},
keywords={fluid dynamics;recommender systems;trusted computing;time-evolving rating scheme;trust-based recommendation systems;fluid dynamics;unknown ratings;rating prediction scheme;FluidRating;time-evolving formulation process;human opinions;fluid exchange;uniform aggregation;nonuniform aggregation;Containers;Computers;Temperature measurement;Time complexity;Conferences;Fluid dynamics;Computational modeling;Fluid dynamics theory;rating prediction;time-evolving;trust-based recommendation system},
doi={10.1109/INFOCOM.2014.6848108},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848109,
author={U. C. Kozat and G. Liang and K. Kökten},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={On diagnosis of forwarding plane via static forwarding rules in Software Defined Networks},
year={2014},
volume={},
number={},
pages={1716-1724},
abstract={Software Defined Networks (SDN) decouple the forwarding and control planes from each other. The control plane is assumed to have a global knowledge of the underlying physical and/or logical network topology so that it can monitor, abstract and control the forwarding plane. When parts of the control plane become unavailable or unreliable, individual controllers should still be able to monitor the forwarding plane and run diagnostics to take the correct routing actions. In our paper, we present solutions that install an optimal or near-optimal number of static forwarding rules on switches/routers for any controller to be able to verify the topology connectivity and detect/locate link failures at data plane speeds without relying on state updates from forwarding plane nodes and other controllers while requiring reachability to only one (arbitrary) forwarding node. Our upper bounds on performance indicate that sub-second link failure localization is possible even at data-center scale networks. For networks with hundreds or few thousand links, tens of milliseconds of latency is achievable.},
keywords={computer centres;computer network performance evaluation;telecommunication network routing;telecommunication network topology;telecommunication switching;forwarding plane diagnosis;static forwarding rules;software defined networks;SDN;control planes;physical network topology;logical network topology;routing actions;switches;topology connectivity;link failures;data plane speeds;forwarding node;subsecond link failure localization;data-center scale networks;routers;Topology;Switches;IP networks;Flyback transformers;Network topology;Bridges},
doi={10.1109/INFOCOM.2014.6848109},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848110,
author={L. Kekely and V. Puš and J. Kořenek},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Software Defined Monitoring of application protocols},
year={2014},
volume={},
number={},
pages={1725-1733},
abstract={Current high-speed network monitoring systems focus more and more on the data from the application layers. Flow data is usually enriched by the information from HTTP, DNS and other protocols. The increasing speed of the network links, together with the time consuming application protocol parsing, require a new way of hardware acceleration. Therefore we propose a new concept of hardware acceleration for flexible flow-based application level monitoring which we call Software Defined Monitoring (SDM). The concept relies on smart monitoring tasks implemented in the software in conjunction with a configurable hardware accelerator. The hardware accelerator is an application-specific processor tailored to stateful flow processing. The monitoring tasks reside in the software and can easily control the level of detail retained by the hardware for each flow. This way the measurement of bulk/uninteresting traffic is offloaded to the hardware while the advanced monitoring over the interesting traffic is performed in the software. The proposed concept allows one to create flexible monitoring systems capable of deep packet inspection at high throughput. Our pilot implementation in FPGA is able to perform a 100 Gb/s flow traffic measurement augmented by a selected application-level protocol parsing.},
keywords={field programmable gate arrays;protocols;telecommunication links;telecommunication traffic;software defined monitoring;application protocols;high-speed network monitoring systems;application layers;flow data;HTTP;DNS;network links;hardware acceleration;hardware accelerator;FPGA;flow traffic measurement;application-level protocol parsing;bit rate 100 Gbit/s;Software;Monitoring;Hardware;Protocols;Acceleration;Conferences;Computers},
doi={10.1109/INFOCOM.2014.6848110},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848111,
author={R. Cohen and L. Lewin-Eytan and J. S. Naor and D. Raz},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={On the effect of forwarding table size on SDN network utilization},
year={2014},
volume={},
number={},
pages={1734-1742},
abstract={Software Defined Networks (SDNs) are becoming the leading technology behind many traffic engineering solutions, both for backbone and data-center networks, since it allows a central controller to globally plan the path of the flows according to the operator's objective. Nevertheless, networking devices' forwarding table is a limited and expensive resource (e.g., TCAM-based switches) which should thus be considered upon configuring the network. In this paper, we concentrate on satisfying global network objectives, such as maximum flow, in environments where the size of the forwarding table in network devices is limited. We formulate this problem as an (NP-hard) optimization problem and present approximation algorithms for it. We show through extensive simulations that practical use of our algorithms (both in Data Center and backbone scenarios) result in a significant reduction (factor 3) in forwarding table size, while having a small effect on the global objective (maximum flow).},
keywords={computational complexity;computer centres;optimisation;software radio;telecommunication computing;approximation algorithms;NP-hard optimization problem;network devices;maximum flow;global network objectives;TCAM-based switches;operator objective;central controller;data-center networks;backbone networks;traffic engineering solutions;forwarding table size;software defined networks;SDN network utilization;Approximation methods;Approximation algorithms;Joining processes;Random variables;Conferences;Computers;Routing},
doi={10.1109/INFOCOM.2014.6848111},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848112,
author={D. Li and Y. Shang and C. Chen},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Software defined green data center network with exclusive routing},
year={2014},
volume={},
number={},
pages={1743-1751},
abstract={The explosive expansion of data center sizes aggravates the power consumption and carbon footprint, which has restricted the sustainable growth of cloud services and seriously troubled data center operators. In recent years, plenty of advanced data center network architectures have been proposed. They usually employ richly-connected topologies and multi-path routing to provide high network capacity. Unfortunately, they also undergo inefficient network energy usage during the traffic valley time. To address the problem, many energy-aware flow scheduling algorithms are proposed recently, primarily considering how to aggregate traffic by flexibly choosing the routing paths, with flows fairly sharing the link bandwidths. In this paper, we leverage software defined network (SDN) technique and explore a new solution to energy-aware flow scheduling, i.e., scheduling flows in the time dimension and using exclusive routing (EXR) for each flow, i.e., a flow always exclusively utilizes the links of its routing path. The key insight is that exclusive occupation of link resources usually results in higher link utilization in high-radix data center networks, since each flow does not need to compete for the link bandwidths with others. When scheduling the flows, EXR leaves flexibility to operators to define the priorities of flows, e.g., based on flow size, flow deadline, etc. Extensive simulations and testbed experiments both show that EXR can effectively save network energy compared with the regular fair-sharing routing (FSR), and significantly reduce the average flow completion time if assigning higher scheduling priorities to smaller flows.},
keywords={cloud computing;computer centres;computer networks;green computing;power aware computing;power consumption;scheduling;telecommunication network routing;software defined green data center network;exclusive routing;power consumption;carbon footprint;sustainable growth;cloud services;richly-connected topologies;energy-aware flow scheduling algorithms;SDN technique;EXR;fair-sharing routing;FSR;Routing;Ports (Computers);Servers;Bandwidth;Control systems;Network topology;Energy consumption},
doi={10.1109/INFOCOM.2014.6848112},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848113,
author={S. Hu and H. Liu and L. Su and H. Wang and T. F. Abdelzaher and P. Hui and W. Zheng and Z. Xie and J. A. Stankovic},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Towards automatic phone-to-phone communication for vehicular networking applications},
year={2014},
volume={},
number={},
pages={1752-1760},
abstract={This paper explores direct phone-to-phone communication (via WiFi interface) among vehicles to support mobile sensing applications. Direct communication among drivers' phones is important in improving data collection efficiency and sharing participatory sensing information in an inexpensive manner. We design a practical and optimized communication mechanism for direct phone-to-phone data transfer among drivers' phones that strategically enables phone-to-phone and/or phone-to-WiFiAP communications by optimally toggles the phone between the normal client and the hotspot modes. We take advantage of the WiFi hotspot functionality on smartphones, and hence require neither involvement of participants nor changes to existing wireless infrastructure and protocols. An analytical model is established to optimize toggling between client and hotspot modes for optimal system efficiency. We fully implement this system on off-the-shelf Google Galaxy Nexus and Nexus S phones. Through a 35-vehicle 2-month deployment study, as well as simulation experiments using the real-world T-drive 9,211-taxicab dataset, we show that our solution significantly reduces data transfer delay time and maintains over 80% efficiency under varying system parameters. We even achieve 90% for parameter settings of the latest smartphones.},
keywords={mobile radio;protocols;smart phones;wireless LAN;automatic phone-to-phone communication;vehicular networking application;direct phone-to-phone communication;WiFi interface;mobile sensing application;data collection efficiency;participatory sensing information;optimized communication mechanism;direct phone-to-phone data transfer;driver phones;phone-to-WiFiAP communication;hotspot mode;WiFi hotspot functionality;smartphones;wireless infrastructure;protocols;analytical model;toggling optimization;optimal system efficiency;off-the-shelf Google Galaxy Nexus S phones;real-world T-drive taxicab dataset;data transfer delay time reduction;IEEE 802.11 Standards;Sensors;Smart phones;Vehicles;Mobile communication;Switches;Servers},
doi={10.1109/INFOCOM.2014.6848113},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848114,
author={J. Qin and H. Zhu and Y. Zhu and L. Lu and G. Xue and M. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={POST: Exploiting dynamic sociality for mobile advertising in vehicular networks},
year={2014},
volume={},
number={},
pages={1761-1769},
abstract={Mobile advertising in vehicular networks is of great interest with which timely information can be fast spread into the network. Given a limited budget for hiring seed vehicles, how to achieve the maximum advertising coverage within a given period of time is NP-hard. In this paper, we propose an innovative scheme, POST, for mobile advertising in vehicular networks. The POST design is based on two key observations we have found by analyzing three large-scale vehicle traces. First, vehicles demonstrate dynamic sociality in the network; second, such vehicular sociality has strong temporal correlations. With the knowledge, POST uses Markov chains to infer future vehicular sociality and adopts one greedy heuristic to select the most “centric” vehicles as seeds for mobile advertising. Extensive trace-driven simulation results show that POST can greatly improve the coverage and the intensity of advertising.},
keywords={advertising;greedy algorithms;Markov processes;mobile computing;vehicular ad hoc networks;POST;mobile advertising;vehicular networks;dynamic sociality;seed vehicle hiring;maximum advertising coverage;innovative scheme;large-scale vehicle traces;vehicular sociality;Markov chains;greedy heuristic;centric vehicles;Vehicles;Advertising;Mobile communication;Mobile computing;Vehicle dynamics;Social network services;Measurement;vehicular networks;mobile advertising;dynamic sociality;social network analysis},
doi={10.1109/INFOCOM.2014.6848114},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848115,
author={H. A. Omar and W. Zhuang and L. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={On multihop communications for in-vehicle Internet access based on a TDMA MAC protocol},
year={2014},
volume={},
number={},
pages={1770-1778},
abstract={A vehicular ad hoc network (VANET) is an emerging technology which has a great potential in realizing a variety of new applications. This paper presents a new packet routing scheme which allows a vehicle to discover the existence of a gateway to the Internet and to send/receive packets to/from the gateway via multihop communications. The proposed routing scheme is based on a multichannel medium access control protocol, known as VeMAC [1], [2], using time division multiple access. The performance of this cross-layer design is evaluated for a multichannel VANET in terms of the end-to-end packet delay and the percentage of occupied time slots per frame in a highway scenario. Both packet queueing and service delays are considered in the end-to-end delay calculation by modeling each relay vehicle as a queueing system, in which the packets are served in batches of no more than a specified maximum batch-size.},
keywords={Internet;packet radio networks;queueing theory;routing protocols;time division multiple access;vehicular ad hoc networks;in-vehicle Internet access;TDMA MAC protocol;vehicular ad hoc network;packet routing scheme;gateway;multihop communications;multichannel medium access control protocol;VeMAC;time division multiple access;multichannel VANET;end-to-end packet delay;packet queueing;Vehicles;Logic gates;Economic indicators;Relays;Delays;Routing;Vehicular ad hoc networks},
doi={10.1109/INFOCOM.2014.6848115},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848116,
author={L. Zhang and B. Yu and J. Pan},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={GeoMob: A mobility-aware geocast scheme in metropolitans via taxicabs and buses},
year={2014},
volume={},
number={},
pages={1279-1787},
abstract={Geocast, delivering messages to a specific location, has become an important issue with the accelerated development of the location-based services in mobile networks. Geocast in the automotive domain is of particular interest, enabling many promising applications, such as geographic advertising, location-based traffic alerts, etc. Different from the conventional geocast algorithms focusing on the distance-based approaches, in this paper, we propose a mobility-aware geocast algorithm (GeoMob) for urban VANETs from the Delay-Tolerant Network (DTN) perspective to better deal with the high mobility and transient connectivity issues. Different levels and aspects of vehicle mobility information are employed, making GeoMob very simple, scalable and communication and compunction-effective. Practical issues are well considered by introducing real-world trace analysis, trace-driven simulation and efficient buffer management. Extensive performance comparisons with other protocols have been conducted to show the advantages of GeoMob.},
keywords={delay tolerant networks;mobility management (mobile radio);telecommunication network routing;vehicular ad hoc networks;mobility-aware geocast scheme;GeoMob;location based services;mobile networks;VANET;delay tolerant network;DTN;transient connectivity;vehicle mobility information;trace analysis;trace driven simulation;buffer management;Vehicles;Routing;Microscopy;Cities and towns;History;Entropy;Global Positioning System;Geocast;VANETs;mobility;DTNs},
doi={10.1109/INFOCOM.2014.6848116},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848117,
author={M. Haddad and H. Sidi and P. Wiecek and E. Altman},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Automated dynamic offset applied to cell association},
year={2014},
volume={},
number={},
pages={1788-1796},
abstract={In this paper, we develop a hierarchical Bayesian game framework for automated dynamic offset selection. Users compete to maximize their throughput by picking the best locally serving radio access network (RAN) with respect to their own measurement, their demand and a partial statistical channel state information (CSI) of other users. In particular, we investigate the properties of a Stackelberg game, in which the base station is a player on its own. We derive analytically the utilities related to the channel quality perceived by users to obtain the equilibria. We study the Price of Anarchy (PoA) of such system, where the PoA is the ratio of the social welfare attained when a network planner chooses policies to maximize social welfare versus the social welfare attained in Nash/Stackeleberg equilibrium when users choose their policies strategically. We show by means of a Stackelberg formulation, how the operator, by sending appropriate information about the state of the channel, can configure a dynamic offset that optimizes its global utility while users maximize their individual utilities. The proposed hierarchical decision approach for wireless networks can reach a good trade-off between the global network performance at the equilibrium and the requested amount of signaling. Typically, it is shown that when the network goal is orthogonal to user's goal, this can lead the users to a misleading association problem.},
keywords={Bayes methods;distributed decision making;game theory;wireless LAN;hierarchical Bayesian game framework;automated dynamic offset selection;radio access network;RAN;partial statistical channel state information;partial CSI;Stackelberg game;base station;channel quality;price of anarchy;PoA;social welfare;network planner;Nash-Stackeleberg equilibrium;global utility;hierarchical decision approach;wireless networks;global network performance;cell association;Games;Throughput;IEEE 802.11 Standards;Base stations;Wireless communication;Conferences;Computers;WLAN;3G;association problem;misleading information;channel state information;game theory;Bayes-Nash equilibrium;Bayes-Stackelberg equilibrium;Price of Anarchy},
doi={10.1109/INFOCOM.2014.6848117},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848118,
author={V. Sciancalepore and V. Mancuso and A. Banchs and S. Zaks and A. Capone},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Interference coordination strategies for content update dissemination in LTE-A},
year={2014},
volume={},
number={},
pages={1797-1805},
abstract={Opportunistic traffic offloading has been proposed to tackle overload problems in cellular networks. However, they only address the problem of deadline-based content propagation in the cellular system, given wireless environment characterization. In contrast, we cope with the traffic offloading issue from another perspective: the base station interference coordination problem. In particular, we aim at the minimization of the total transmission time spent by the base stations in order to inject contents into the network, and we leverage the recently proposed ABSF technique to keep under control intercell interference. We formulate an optimization problem, prove that it is NP-Complete, and propose a near-optimal heuristic. Our proposed algorithm substantially outperforms classical intercell interference approaches proposed in the literature, as we evaluate through the simulation of dense LTE-A network scenarios.},
keywords={cellular radio;Long Term Evolution;radiofrequency interference;telecommunication traffic;interference coordination strategies;content update dissemination;LTE-A;opportunistic traffic offloading;cellular networks;deadline-based content propagation;cellular system;base station interference coordination;ABSF technique;intercell interference;Base stations;Interference;Optimization;Schedules;Approximation methods;Approximation algorithms;Signal to noise ratio},
doi={10.1109/INFOCOM.2014.6848118},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848119,
author={D. Naboulsi and R. Stanica and M. Fiore},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Classifying call profiles in large-scale mobile traffic datasets},
year={2014},
volume={},
number={},
pages={1806-1814},
abstract={Cellular communications are undergoing significant evolutions in order to accommodate the load generated by increasingly pervasive smart mobile devices. Dynamic access network adaptation to customers' demands is one of the most promising paths taken by network operators. To that end, one must be able to process large amount of mobile traffic data and outline the network utilization in an automated manner. In this paper, we propose a framework to analyze broad sets of Call Detail Records (CDRs) so as to define categories of mobile call profiles and classify network usages accordingly. We evaluate our framework on a CDR dataset including more than 300 million calls recorded in an urban area over 5 months. We show how our approach allows to classify similar network usage profiles and to tell apart normal and outlying call behaviors.},
keywords={cellular radio;mobile computing;telecommunication traffic;large-scale mobile traffic dataset;call profiles classification;cellular communication;pervasive smart mobile devices;dynamic access network adaptation;call detail records;mobile access networks;Mobile communication;Base stations;Mobile computing;Clustering algorithms;Indexes;Antennas;Training},
doi={10.1109/INFOCOM.2014.6848119},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848120,
author={Y. Lim and Y. Chen and E. M. Nahum and D. Towsley and K. Lee},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Cross-layer path management in multi-path transport protocol for mobile devices},
year={2014},
volume={},
number={},
pages={1815-1823},
abstract={MPTCP is a new transport protocol that enables mobile devices to use several physical paths simultaneously through multiple network interfaces, such as WiFi and cellular. However, wireless path characteristics change frequently in mobile environments, causing challenges for MPTCP: For example, WiFi associated paths often become unavailable as devices move, since WiFi has intermittent connectivity caused by the short signal range and susceptibility to interference. In this work, we improve MPTCP to manage path usage based on the associated link status. This variant, called MPTCP-MA, uses MAC-Layer information to locally estimate path quality and connectivity. By suspending/releasing paths based on their quality, MPTCP-MA can more effectively utilize restored paths. We have implemented and deployed MPTCP-MA in Linux and Android. Our experimental results show that MPTCP-MA can efficiently utilize an intermittently available path, with Wifi throughput improvements of up to 72 percent.},
keywords={access protocols;Android (operating system);cellular radio;mobile computing;multipath channels;radiofrequency interference;telecommunication network management;transport protocols;wireless LAN;cross-layer path management;multipath transport protocol;mobile devices;MPTCP;physical paths;multiple network interfaces;WiFi;cellular;wireless path characteristics;intermittent connectivity estimation;short signal range;interference susceptibility;MPTCP-MA;path usage management;MAC-Layer information;path quality estimation;Linux;Android;Decision support systems;Handheld computers;Conferences;Estimation;IEEE 802.11 Standards;Mobile communication},
doi={10.1109/INFOCOM.2014.6848120},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848121,
author={B. Stephens and A. L. Cox and A. Singla and J. Carter and C. Dixon and W. Felter},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Practical DCB for improved data center networks},
year={2014},
volume={},
number={},
pages={1824-1832},
abstract={Storage area networking is driving commodity data center switches to support lossless Ethernet (DCB). Unfortunately, to enable DCB for all traffic on arbitrary network topologies, we must address several problems that can arise in lossless networks, e.g., large buffering delays, unfairness, head of line blocking, and deadlock. We propose TCP-Bolt, a TCP variant that not only addresses the first three problems but reduces flow completion times by as much as 70%. We also introduce a simple, practical deadlock-free routing scheme that eliminates deadlock while achieving aggregate network throughput within 15% of ECMP routing. This small compromise in potential routing capacity is well worth the gains in flow completion time. We note that our results on deadlock-free routing are also of independent interest to the storage area networking community. Further, as our hardware testbed illustrates, these gains are achievable today, without hardware changes to switches or NICs.},
keywords={computer centres;local area networks;routing protocols;switching networks;telecommunication network topology;telecommunication traffic;transport protocols;DCB;improved data center network;storage area networking;commodity data center switch;lossless Ethernet;arbitrary network topology traffic;buffering delay;line blocking head;TCP-bolt;deadlock-free routing scheme;ECMP routing;NIC;data center bridging;Routing;System recovery;Throughput;Vegetation;Topology;Hardware;Ports (Computers)},
doi={10.1109/INFOCOM.2014.6848121},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848122,
author={V. Aggarwal and C. Tian and V. A. Vaishampayan and Y. R. Chen},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Distributed data storage systems with opportunistic repair},
year={2014},
volume={},
number={},
pages={1833-1841},
abstract={The reliability of erasure-coded distributed storage systems, as measured by the mean time to data loss (MTTDL), depends on the repair bandwidth of the code. Repair-efficient codes provide reliability values several orders of magnitude better than conventional erasure codes. Current state of the art codes fix the number of helper nodes (nodes participating in repair) a priori. In practice, however, it is desirable to allow the number of helper nodes to be adaptively determined by the network traffic conditions. In this work, we propose an opportunistic repair framework to address this issue. It is shown that there exists a threshold on the storage overhead, below which such an opportunistic approach does not lose any efficiency from the optimal storage-repair-bandwidth tradeoff; i.e. it is possible to construct a code simultaneously optimal for different numbers of helper nodes. We further examine the benefits of such opportunistic codes, and derive the MTTDL improvement for two repair models: one with limited total repair bandwidth and the other with limited individual-node repair bandwidth. In both settings, we show orders of magnitude improvement in MTTDL. Finally, the proposed framework is examined in a network setting where a significant improvement in MTTDL is observed.},
keywords={storage area networks;storage management;telecommunication traffic;erasure-coded distributed data storage system reliability;mean time-to-data loss;MTTDL;repair-efficient codes;reliability values;helper nodes;network traffic conditions;opportunistic repair framework;storage overhead;optimal storage-repair-bandwidth tradeoff;opportunistic codes;repair models;limited total repair bandwidth;limited individual-node repair bandwidth;Maintenance engineering;Bandwidth;Peer-to-peer computing;Reliability;Conferences;Computers;Loss measurement},
doi={10.1109/INFOCOM.2014.6848122},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848123,
author={X. Li and J. Wu and S. Tang and S. Lu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Let's stay together: Towards traffic aware virtual machine placement in data centers},
year={2014},
volume={},
number={},
pages={1842-1850},
abstract={As tenants take networked virtual machines (VMs) as their requirements, effective placement of VMs is needed to reduce the network cost in cloud data centers. The cost is one of the major concerns for the cloud providers. In addition to the cost caused by network traffics (N-cost), the cost caused by the utilization of physical machines (PM-cost) is also non-negligible. In this paper, we focus on the optimized placement of VMs to minimize the cost, the combination of N-cost and PM-cost. We define N-cost by various functions, according to different communication models. We formulate the placement problem, and prove it to be NP-hard. We investigate the problem from two aspects. Firstly, we put a special emphasis on minimizing the N-cost with fixed PM-cost. For the case that tenants request the same amount of VMs, we present optimal algorithms under various definitions of N-cost. For the case that tenants require different numbers of VMs, we propose an approximation algorithm. Also, a greedy algorithm is implemented as the baseline to evaluate the performance. Secondly, we study the general case of the VM placement problem, in which both N-cost and PM-cost are taken into account. We present an effective binary-search-based algorithm to determine how many PMs should be used, which makes a tradeoff between PM-cost and N-cost. For all of the algorithms, we conduct theoretical analysis and extensive simulations to evaluate their performance and efficiency.},
keywords={approximation theory;cloud computing;computational complexity;computer centres;cost reduction;greedy algorithms;search problems;virtual machines;traffic aware virtual machine placement;networked virtual machines;VM;network cost reduction;cloud data centers;network traffic;N-cost;physical machines;PM-cost;different communication models;NP-hard problem;optimal algorithms;approximation algorithm;greedy algorithm;binary-search-based algorithm;Cost function;Approximation algorithms;Algorithm design and analysis;Optimized production technology;Computers;Conferences;Virtual machining;Clouds;cost optimization;data center;subset-sum problem;vector bin packing;virtual machine placement},
doi={10.1109/INFOCOM.2014.6848123},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848124,
author={D. Li and J. Wu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={On the design and analysis of Data Center Network architectures for interconnecting dual-port servers},
year={2014},
volume={},
number={},
pages={1851-1859},
abstract={We consider the design and analysis of Data Center Network (DCN) architectures for interconnecting dual-port servers. Unlike existing works, we propose the concept of Normalized Switch Delay (NSD) to distinguish a server-to-server-direct hop and a server-to-server-via-switch hop, to unify the design of DCN architectures. We then consider a fundamental problem: maximizing the number of dual-port servers, given network diameter and switch port number; and give an upper bound on this maximum number. Two novel architectures are proposed: SWCube and SWKautz, based on the generalized hypercube and Kautz graph, respectively, which in most cases accommodate more servers than BCN [1], which was claimed to be the largest known architecture. Compared with three existing architectures, SWCube and SWKautz demonstrate various advantages. Analysis and simulations also show that SWCube and SWKautz have nice properties for DCNs, such as low diameter, good fault-tolerance, and capability of efficiently handling network congestion.},
keywords={computer centres;hypercube networks;network servers;network theory (graphs);data center network architecture;dual port server interconnection;normalized switch delay;NSD;server-to-server-direct hop;server-to-server-via-switch hop;DCN architecture design;network diameter;switch port number;SWCube;SWKautz;generalized hypercube;Kautz graph;Servers;Switches;Ports (Computers);Hypercubes;Upper bound;Delays;Data center networks (DCNs);dual-port servers;generalized hypercubes;Kautz graphs},
doi={10.1109/INFOCOM.2014.6848124},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848125,
author={J. Tapolcai and L. Rónyai and È. Hosszu and P. Ho and S. Subramaniam},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Signaling free localization of node failures in all-optical networks},
year={2014},
volume={},
number={},
pages={1860-1868},
abstract={Network-wide local unambiguous failure localization (NL-UFL) [1] has been demonstrated as an interesting scenario of monitoring trails (m-trails). It attempts to enable every node to autonomously localize any failure event in the network in a distributed and all-optical manner by inspecting a set of m-trails traversing through the node. This paper investigates the m-trail allocation problem under the NL-UFL scenario by taking each link and node failure event into consideration. Bound analysis is performed using combinatorial group testing (CGT) theory and this is followed by the introduction of a novel heuristic on general topologies. Extensive simulation is conducted to examine the proposed heuristic in terms of the required cover length and the number of m-trails to achieve NL-UFL.},
keywords={combinatorial mathematics;computer network reliability;fault location;optical fibre networks;signaling free node localization;node failure;all optical networks;network wide local unambiguous failure localization;trail monitoring;m-trail allocation problem;combinatorial group testing theory;Observatories;Computers;Monitoring;Resource management;Topology;Conferences;Testing},
doi={10.1109/INFOCOM.2014.6848125},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848126,
author={Z. Wang and H. Chen and Q. Cao and H. Qi and Z. Wang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Fault tolerant barrier coverage for wireless sensor networks},
year={2014},
volume={},
number={},
pages={1869-1877},
abstract={Barrier coverage is a critical issue in wireless sensor networks for security applications (e.g., border protection), the performance of which is highly related with locations of sensor nodes. Existing work on barrier coverage mainly assume that sensor nodes have accurate location information, however, little work explores the effects of location errors on barrier coverage. In this paper, we study the barrier coverage problem when sensor nodes have location errors and deploy mobile sensor nodes to improve barrier coverage if the network is not barrier covered after initial deployment. We analyze the relationship between the true distance and the measured distance of two stationary sensor nodes and derive the minimum number of mobile sensor nodes needed to connect them with a guarantee when nodes location errors. Furthermore, we propose a fault tolerant weighted barrier graph, based on which we prove that the minimum number of mobile sensor nodes needed to form barrier coverage with a guarantee is the length of the shortest path on the graph. Simulation results validate the correctness of our analysis.},
keywords={fault tolerance;graph theory;wireless sensor networks;wireless sensor networks;fault tolerant barrier coverage problem;mobile sensor nodes;location errors;stationary sensor nodes;fault tolerant weighted barrier graph;shortest path;Mobile nodes;Robot sensing systems;Fault tolerance;Fault tolerant systems;Measurement uncertainty},
doi={10.1109/INFOCOM.2014.6848126},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848127,
author={Y. Wang and D. Wei and X. Yin and X. Wang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Heterogeneity-aware data regeneration in distributed storage systems},
year={2014},
volume={},
number={},
pages={1878-1886},
abstract={Distributed storage systems provide large-scale reliable data storage services by spreading redundancy across a large group of storage nodes. In such big systems, node failures take place on a regular basis. When a node fails or leaves the system, to maintain the same level of redundancy, it is expected to regenerate the redundant data at a replacement node as soon as possible. Previous studies aim to minimize the network traffic in the regeneration process, but in practical networks, where link capacities vary in a wide range, minimizing network traffic does not always mean minimizing regeneration time. Considering the heterogeneous link capacities, Li et al. proposed a tree-structured regeneration scheme, called RCTREE, to bypass the low-capacitated link encountered in direct transmissions. However, we find that RCTREE may rapidly lose data integrity after several regenerations. In this paper, we reconsider the problem of minimizing regeneration time in networks with heterogeneous link capacities. We derive the minimum amount of data to be transmitted through each link to preserve data integrity. We prove that building an optimal regeneration tree is NP-complete and propose a heuristic algorithm for a near-optimal solution. We further introduce a flexible regeneration scheme, which allows providers to generate different amount of coded data. Simulation results show that the flexible tree-structured regeneration scheme can reduce the regeneration time significantly.},
keywords={computer network reliability;data integrity;distributed processing;redundancy;storage area networks;storage management;telecommunication links;telecommunication traffic;trees (mathematics);heterogeneity-aware data regeneration;distributed storage systems;large-scale reliable data storage services;storage nodes;node failures;redundant data regeneration;network traffic;regeneration process;regeneration time minimization;RCTREE;low-capacitated link;data integrity;heterogeneous link capacities;NP-complete optimal regeneration tree;heuristic algorithm;flexible tree-structured regeneration scheme;Bandwidth;Peer-to-peer computing;Maintenance engineering;Topology;Overlay networks;Distributed databases;Conferences},
doi={10.1109/INFOCOM.2014.6848127},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848128,
author={P. Chen and Y. Qi and P. Zheng and D. Hou},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={CauseInfer: Automatic and distributed performance diagnosis with hierarchical causality graph in large distributed systems},
year={2014},
volume={},
number={},
pages={1887-1895},
abstract={Modern applications especially cloud-based or cloud-centric applications always have many components running in the large distributed environment with complex interactions. They are vulnerable to suffer from performance or availability problems due to the highly dynamic runtime environment such as resource hogs, configuration changes and software bugs. In order to make efficient software maintenance and provide some hints to software bugs, we build a system named CauseInfer, a low cost and blackbox cause inference system without instrumenting the application source code. CauseInfer can automatically construct a two layered hierarchical causality graph and infer the causes of performance problems along the causal paths in the graph with a series of statistical methods. According to the experimental evaluation in the controlled environment, we find out CauseInfer can achieve an average 80% precision and 85% recall in a list of top two causes to identify the root causes, higher than several state-of-the-art methods and a good scalability to scale up in the distributed systems.},
keywords={cloud computing;graph theory;program debugging;software maintenance;software performance evaluation;statistical analysis;CauseInfer;automatic performance diagnosis;distributed performance diagnosis;hierarchical causality graph;distributed systems;cloud-based applications;cloud-centric applications;availability problems;dynamic runtime environment;software maintenance;software bugs;blackbox cause inference system;statistical methods;Measurement;Ports (Computers);Computer bugs;Buildings;Software;Protocols;Correlation},
doi={10.1109/INFOCOM.2014.6848128},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848129,
author={F. Ciucu and F. Poloczek and J. Schmitt},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Sharp per-flow delay bounds for bursty arrivals: The case of FIFO, SP, and EDF scheduling},
year={2014},
volume={},
number={},
pages={1896-1904},
abstract={The practicality of the stochastic network calculus (SNC) is often questioned on grounds of potential looseness of its performance bounds. In this paper, it is uncovered that for bursty arrival processes (specifically Markov-Modulated On-Off (MMOO)), whose amenability to per-flow analysis is typically proclaimed as a highlight of SNC, the bounds can unfortunately be very loose (e.g., by several orders of magnitude off). In response to this uncovered weakness of SNC, the (Standard) per-flow bounds are herein improved by deriving a general sample-path bound, using martingale based techniques, which accommodates FIFO, SP, and EDF scheduling. The obtained (Martingale) bounds capture an extra exponential decay factor of O (e-αn) in the number of flows n. Moreover, numerical comparisons against simulations show that the Martingale bounds are not only remarkably accurate, but they also improve the Standard SNC bounds by factors as large as 100 or even 1000.},
keywords={calculus;scheduling;stochastic processes;sharp perflow delay bounds;bursty arrivals;FIFO;SP;EDF scheduling;stochastic network calculus;SNC;Markov-Modulated On-Off;MMOO;perflow analysis;general sample path bound;numerical comparisons;Delays;Queueing analysis;Bandwidth;Servers;Numerical models;Standards;Markov processes},
doi={10.1109/INFOCOM.2014.6848129},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848130,
author={C. Singh and A. Nedić and R. Srikant},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={LP-relaxation based distributed algorithms for scheduling in wireless networks},
year={2014},
volume={},
number={},
pages={1905-1913},
abstract={LP relaxations of Maximum Weighted Independent Set (MWIS) problems have been widely studied. A key motivation for this prior work comes from the central role that MWIS plays in designing throughput-optimal algorithms for wireless networks. However, to the best of our knowledge, the actual packet delay performance of these algorithms has not been studied in the context of wireless networks. In this paper, we first present an algorithm for solving the LP relaxation of MWIS which exhibits faster convergence to an optimal solution. Further, we show that one does not have to wait for infinite time for convergence to occur, but a simple rounding technique can be used to identify the ON/OFF states of the wireless links in finite time. As in prior work, such an approach only identifies the optimal MWIS states of some of the links in the network. Therefore, we present a scheme to combine this solution with Q-CSMA. Simulations indicate that the proposed scheme significantly improves the performance of Q-CSMA. Further, the proposed algorithm is shown to perform much better than previously suggested LP relaxation schemes due to its superior convergence properties.},
keywords={carrier sense multiple access;distributed algorithms;graph theory;linear programming;radio networks;LP-relaxation based distributed algorithms;wireless networks;maximal scheduling;maximum weighted independent set problems;MWIS;throughput-optimal algorithms;rounding technique;on-off state identification;wireless links;Q-CSMA;carrier sense multiple access protocols;conÀict graph;Heuristic algorithms;Multiaccess communication;Schedules;Nickel;Throughput;Delays;Scheduling algorithms},
doi={10.1109/INFOCOM.2014.6848130},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848131,
author={W. Wang and B. Liang and B. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Low complexity multi-resource fair queueing with bounded delay},
year={2014},
volume={},
number={},
pages={1914-1922},
abstract={Middleboxes are ubiquitous in today's networks. They perform deep packet processing such as content-based filtering and transformation, which requires multiple categories of resources (e.g., CPU, memory bandwidth, and link bandwidth). Depending on the processing requirement of traffic, packet processing for different flows may consume vastly different amounts of resources. Multi-resource fair queueing allows flows to obtain a fair share of these resources, providing service isolation across flows. However, previous solutions for multi-resource fair queueing are either expensive to implement at high speeds, or incurring high scheduling delay for flows with uneven weights. In this paper, we present a new fair queueing algorithm, called Group Multi-Resource Round Robin (GMR<sup>3</sup>), that schedules packets in O(1) time, while achieving near-perfect fairness with a low scheduling delay bounded by a small constant. To our knowledge, it is the first provably fair, highly efficient multi-resource fair queueing algorithm with bounded delay.},
keywords={queueing theory;low complexity multiresource fair queueing;packet processing;service isolation;high scheduling delay;group multiresource round robin;near-perfect fairness;middleboxes;Delays;Scheduling;Schedules;Complexity theory;Scheduling algorithms;Bandwidth;Algorithm design and analysis},
doi={10.1109/INFOCOM.2014.6848131},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848132,
author={S. Zhao and X. Lin},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Rate-control and multi-channel scheduling for wireless live streaming with stringent deadlines},
year={2014},
volume={},
number={},
pages={1923-1931},
abstract={SVC-based live video-streaming in multi-channel wireless networks leads to a challenging joint rate-control and scheduling problem with stringent deadline constraints. Traditional utility-based approaches often did not explicitly account for deadlines. In this paper, we explicitly account for deadlines and study the problem of optimizing the total reward from packets meeting their deadlines in a modern 4G OFDM system. Motivated by a heuristic utility-based approach, we propose a class of threshold-based rate-control and wireless scheduling policies that can respect the deadline constraints and approach the optimal system reward asymptotically as the system size increases. We also propose a distributed realization of our threshold-based policies that can be easily implemented in practical scenarios. We substantiate the result via both analysis and simulation.},
keywords={4G mobile communication;OFDM modulation;video coding;video streaming;multichannel scheduling;multichannel wireless networks;wireless live streaming;stringent deadlines;joint rate-control and scheduling problem;modern 4G OFDM system;heuristic utility-based approach;threshold-based rate-control;SVC-based live video-streaming;Base stations;OFDM;Streaming media;Joints;Wireless communication;Static VAr compensators;Upper bound},
doi={10.1109/INFOCOM.2014.6848132},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848133,
author={Z. Lu and Y. Wen and G. Cao},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Information diffusion in mobile social networks: The speed perspective},
year={2014},
volume={},
number={},
pages={1932-1940},
abstract={The emerging of mobile social networks opens opportunities for viral marketing. However, before fully utilizing mobile social networks as a platform for viral marketing, many challenges have to be addressed. In this paper, we address the problem of identifying a small number of individuals through whom the information can be diffused to the network as soon as possible, referred to as the diffusion minimization problem. Diffusion minimization under the probabilistic diffusion model can be formulated as an asymmetric k-center problem which is NP-hard, and the best known approximation algorithm for the asymmetric k-center problem has approximation ratio of log<sup>*</sup> n and time complexity O(n<sup>5</sup>). Clearly, the performance and the time complexity of the approximation algorithm are not satisfiable in large-scale mobile social networks. To deal with this problem, we propose a community based algorithm and a distributed set-cover algorithm. The performance of the proposed algorithms is evaluated by extensive experiments on both synthetic networks and a real trace. The results show that the community based algorithm has the best performance in both synthetic networks and the real trace, and the distributed setcover algorithm outperforms the approximation algorithm in the real trace in terms of diffusion time.},
keywords={approximation theory;computational complexity;marketing;mobile computing;social networking (online);information diffusion;mobile social networks;viral marketing;probabilistic diffusion model;asymmetric k-center problem;NP-hard problem;time complexity;approximation algorithm;Communities;Approximation algorithms;Social network services;Mobile computing;Mobile communication;Approximation methods;Algorithm design and analysis},
doi={10.1109/INFOCOM.2014.6848133},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848134,
author={Z. Li and C. Wang and S. Yang and C. Jiang and I. Stojmenovic},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Improving data forwarding in Mobile Social Networks with infrastructure support: A space-crossing community approach},
year={2014},
volume={},
number={},
pages={1941-1949},
abstract={In this paper, we study two tightly coupled issues: space-crossing community detection and its influence on data forwarding in Mobile Social Networks (MSNs) by taking the hybrid underlying networks with infrastructure support into consideration. The hybrid underlying network is composed of large numbers of mobile users and a small portion of Access Points (APs). Because APs can facilitate the communication among long-distance nodes, the concept of physical proximity community can be extended to be one across the geographical space. In this work, we first investigate a space-crossing community detection method for MSNs. Based on the detection results, we design a novel data forwarding algorithm SAAS (Social Attraction and AP Spreading), and show how to exploit the space-crossing communities to improve the data forwarding efficiency. We evaluate our SAAS algorithm on real-life data from MIT Reality Mining and University of Illinois Movement (UIM). Results show that space-crossing community plays a positive role in data forwarding in MSNs in terms of delivery ratio and delay. Based on this new type of community, SAAS achieves a better performance than existing social community-based data forwarding algorithms in practice, including Bubble Rap and Nguyen's Routing algorithms.},
keywords={human factors;mobile computing;social networking (online);mobile social networks;infrastructure support;space-crossing community detection approach;MSN;hybrid underlying networks;mobile users;access points;long-distance node communication;physical proximity community;social attraction;space-crossing communities;SAAS algorithm;MIT reality mining;University of Illinois Movement;social community-based data forwarding algorithms;Nguyens routing algorithms;Bubble Rap algorithms;AP spreading;Communities;Mobile communication;Local activities;Mobile computing;Social network services;Educational institutions;Vectors},
doi={10.1109/INFOCOM.2014.6848134},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848135,
author={N. Do and Y. Zhao and S. Wang and C. Hsu and N. Venkatasubramanian},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Optimizing offline access to social network content on mobile devices},
year={2014},
volume={},
number={},
pages={1950-1958},
abstract={In this paper, we explore the problem of supporting efficient access to social media contents on social network sites for mobile devices without requiring mobile users to be online all the time. We propose and implement a broker/proxy based architecture that stages data at a broker/proxy, and selectively downloads to the mobile device only those contents that have a high likelihood of being viewed. The system determines the relevance of social media updates that continuously arrive (e.g., Facebook friend updates) for each user. Using knowledge of this relevance and current network/system conditions, we develop scheduling algorithms that determine which social contents are sent to the devices. We develop an Android app providing offline access to Facebook. Our experimental results indicate that our system is energy efficient, which saves energy by 6.9 times for WiFi and 9.1 times for cellular connections. We also use data traces gathered from our app to further drive extensive simulation based evaluations which show that our proposed algorithms provide efficient facilities for tuning the system's performance.},
keywords={Android (operating system);mobile computing;optimisation;scheduling;social networking (online);wireless LAN;offline access optimization;social network content;mobile devices;social network sites;broker-proxy based architecture;social media updates;network-system conditions;scheduling algorithms;Android app;Facebook;WiFi;cellular connections;data traces;Mobile handsets;Mobile communication;Social network services;Media;Prefetching;Mobile computing;Scheduling algorithms;Social networks;mobile devices;optimization;energy consumption;offline access},
doi={10.1109/INFOCOM.2014.6848135},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848136,
author={X. Chen and X. Gong and L. Yang and J. Zhang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A social group utility maximization framework with applications in database assisted spectrum access},
year={2014},
volume={},
number={},
pages={1959-1967},
abstract={In this paper, we develop a social group utility maximization (SGUM) framework for cooperative networking that takes into account both social relationships and physical coupling among users. Specifically, instead of maximizing its individual utility or the overall network utility, each user aims to maximize its social group utility that hinges heavily on its social ties with other users. We show that this framework provides rich modeling flexibility and spans the continuum space between non-cooperative game and network utility maximization (NUM) - two traditionally disjoint paradigms for network optimization. Based on this framework, we study an important application in database assisted spectrum access. We formulate the distributed spectrum access problem among white-space users with social ties as a SGUM game. We show that the game is a potential game and always admits a social-aware Nash equilibrium. We also design a distributed spectrum access algorithm that can achieve the social-aware Nash equilibrium of the game and quantify its performance gap. We evaluate the performance of the SGUM solution using real social data traces. Numerical results demonstrate that the performance gap between the SGUM solution and the NUM (social welfare optimal) solution is at most 15%.},
keywords={computer networks;distributed algorithms;game theory;information services;social networking (online);social group utility maximization framework;database assisted spectrum access;SGUM framework;cooperative networking;social relationships;networking coupling;rich modeling flexibility;continuum space;noncooperative game;network utility maximization;NUM;network optimization;white-space users;social ties;social-aware Nash equilibrium;distributed spectrum access algorithm;performance gap;real social data traces;social welfare optimal;Games;Couplings;Interference;Databases;Nash equilibrium;Algorithm design and analysis;Manganese},
doi={10.1109/INFOCOM.2014.6848136},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848137,
author={M. Forster and R. Frank and M. Gerla and T. Engel},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A Cooperative Advanced Driver Assistance System to mitigate vehicular traffic shock waves},
year={2014},
volume={},
number={},
pages={1968-1976},
abstract={We address the problem of shock wave formation in uncoordinated highway traffic. First, we identify the combination of heavy traffic and small traffic perturbations or unexpected driver actions as the main causes of highway traffic jams. Then we introduce a novel distributed communication protocol that enables us to eliminate upstream shock wave formation even with low system penetration rates. Based on traffic information ahead, we propose a Cooperative Advanced Driver Assistance System (CADAS) that recommends non-intuitive velocity reductions in order to redistribute traffic more uniformly thereby eliminating traffic peaks. Simulation results show that CADAS significantly increases the average velocity and therewith reduces the overall travel time and avoids unnecessary slowdowns.},
keywords={cooperative systems;driver information systems;perturbation techniques;protocols;road traffic;road vehicles;shock waves;cooperative advanced driver assistance system;vehicular traffic shock waves;uncoordinated highway traffic;heavy traffic perturbation;small traffic perturbation;highway traffic jam;distributed communication protocol;upstream shock wave formation;system penetration rate;traffic information;CADAS;nonintuitive velocity reduction;traffic peak;Vehicles;Mathematical model;Shock waves;Equations;Roads;Protocols;Traffic Modeling;Vehicular Networks;Congested Flow;Shock Waves},
doi={10.1109/INFOCOM.2014.6848137},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848138,
author={Y. Wang and L. Huang and T. Gu and H. Wei and K. Xing and J. Zhang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Data-driven traffic flow analysis for vehicular communications},
year={2014},
volume={},
number={},
pages={1977-1985},
abstract={Due to high mobility and frequent disconnections in a vehicular network, reliable and efficient vehicular communication is very challenging. Previous studies focus on predicting the trajectories of single vehicles. Due to many random factors, however, there is little regularity in the movements of a single vehicle in an urban area, and this motivates us to take a holistic network perspective. With this insight, we model the time varying regularities of road traffic flows in road segments and intersections by mining statistic trajectories of all vehicles in the network. Based on these regularities and local real-time traffic information, we propose a new method to calculate the expected transfer delay from a current position to a given destination. We also propose a method to collect updated destination information. By combining the above two methods, we design a routing algorithm for vehicle-to-vehicle data transmission in vehicular networks, and then prove that it is a linear-time algorithm. Finally, we evaluate our algorithm by using information of real taxi vehicles. The results show that the performance of our algorithm is significantly better than other solutions in terms of packet delay.},
keywords={mobile radio;road traffic;telecommunication network reliability;telecommunication network routing;data-driven traffic flow analysis;vehicular communication reliability;vehicular communication efficiency;vehicular network mobility;vehicular network disconnection;vehicle trajectory prediction;urban area;holistic network perspective;time-varying regularity;road traffic flows;road segments;road intersections;statistic trajectory mining;local real-time traffic information;transfer delay;updated destination information;routing algorithm;vehicle-to-vehicle data transmission;linear-time algorithm;real taxi vehicle information;packet delay;Vehicles;Roads;Delays;Trajectory;Algorithm design and analysis;Prediction algorithms;Equations},
doi={10.1109/INFOCOM.2014.6848138},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848139,
author={Y. Zhao and Y. Zhang and T. Yu and T. Liu and X. Wang and X. Tian and X. Liu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={CityDrive: A map-generating and speed-optimizing driving system},
year={2014},
volume={},
number={},
pages={1986-1994},
abstract={There have been many traffic light control systems around the globe, but the high cost of infrastructure and maintenance hinders their wide deployment. However, speed-advisory systems enabled by on-vehicle devices are much cheaper and easier to deploy. The first challenge of such systems is to get the traffic signal schedule in complex intersections. The second challenge is to get map information and calculate the distance. Facing these challenges we devise and implement a speed-advisory driving system called CityDrive, which harnesses the sensor and GPS data from a wide participation of smartphones to suggest proper speed for drivers so that they arrive at intersections in green phase. CityDrive first generates a road map and then infers traffic signal schedules, using only smartphones and a server. CityDrive does not eliminate stops at intersections, but it tries to maximize the probability that vehicles cruise through intersections in green phase. Both simulation and real test show that this continuous speed advisory service effectively smoothes traffic flow and significantly reduces energy consumption.},
keywords={driver information systems;Global Positioning System;probability;road traffic control;smart phones;CityDrive;map-generating driving system;speed-optimizing driving system;traffic light control systems;speed-advisory systems;on-vehicle devices;GPS data;smartphones;probability;Acceleration;Vehicles;Global Positioning System;Smart phones;Roads;Silicon;Servers},
doi={10.1109/INFOCOM.2014.6848139},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848140,
author={D. Li and Z. Lu and T. Bansal and E. Schilling and P. Sinha},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={ForeSight: Mapping vehicles in visual domain and electronic domain},
year={2014},
volume={},
number={},
pages={1995-2003},
abstract={Using broadcast in vehicular applications such as autonomous cruise control and collaborative driving can disturb unrelated drivers and fail to convey the message due to unspecified receiver, resulting in increased risk of accidents. For supporting the unicast communication primitive, it is important to know the electronic identities (EIDs), e.g., the IP addresses and the relative positions of the nearby vehicles. We show that the estimated GPS coordinates alone are not accurate enough to uniquely identify the intended vehicle. On the other hand, there is an increasing array of devices, such as on-board camera, RADAR, and DSRC radio that are becoming available in newer vehicles. These heterogeneously deployed devices provide information sources that have varying levels of accuracy and potentially different coverage regions, making it challenging to accurately identify the vehicle. As a first step, we design ForeSight, a system that dynamically integrates the information observed in the visual domain (e.g., from camera) and the electronic domain (e.g., WiFi radio) to match the vehicles observed in these two domains with high accuracy. The experiment and simulation results show that ForeSight is able to significantly improve the vehicle identification accuracy compared to using GPS or other algorithms. In our case study, ForeSight reduces disturbing messages by 14 × as compared to the number of a GPS-based communication method.},
keywords={mobile radio;road accidents;vehicle mapping;visual domain;electronic domain;ForeSight;autonomous cruise control;collaborative driving;unicast communication primitive;electronic identities;EIDs;IP addresses;estimated GPS coordinates;on-board camera;radar;DSRC radio;information sources;vehicle identification accuracy;GPS-based communication method;Vehicles;Cameras;Global Positioning System;Visualization;Image color analysis;Accuracy;Clustering algorithms},
doi={10.1109/INFOCOM.2014.6848140},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848141,
author={Y. Le and J. Liu and F. Ergün and D. Wang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Online load balancing for MapReduce with skewed data input},
year={2014},
volume={},
number={},
pages={2004-2012},
abstract={MapReduce has emerged as a powerful tool for distributed and scalable processing of voluminous data. In this paper, we, for the first time, examine the problem of accommodating data skew in MapReduce with online operations. Different from earlier heuristics in the very late reduce stage or after seeing all the data, we address the skew from the beginning of data input, and make no assumption about a priori knowledge of the data distribution nor require synchronized operations. We examine the input in a continuous fashion and adaptively assign tasks with a load-balanced strategy. We show that the optimal strategy is a constrained version of online minimum makespan and, in the MapReduce context where pairs with identical keys must be scheduled to the same machine, there is an online algorithm with a provable 2-competitive ratio. We further suggest a sample-based enhancement, which, probabilistically, achieves a 3/2-competitive ratio with a bounded error.},
keywords={distributed processing;resource allocation;online load balancing;MapReduce;skewed data input;voluminous data;online operations;data distribution;load-balanced strategy;online minimum makespan;provable 2-competitive ratio;sample-based enhancement;bounded error;Conferences;Computers;Educational institutions;Distributed databases;Frequency estimation;Computational modeling;Load management},
doi={10.1109/INFOCOM.2014.6848141},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848142,
author={K. Yang and X. Jia and K. Ren and R. Xie and L. Huang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Enabling efficient access control with dynamic policy updating for big data in the cloud},
year={2014},
volume={},
number={},
pages={2013-2021},
abstract={Due to the high volume and velocity of big data, it is an effective option to store big data in the cloud, because the cloud has capabilities of storing big data and processing high volume of user access requests. Attribute-Based Encryption (ABE) is a promising technique to ensure the end-to-end security of big data in the cloud. However, the policy updating has always been a challenging issue when ABE is used to construct access control schemes. A trivial implementation is to let data owners retrieve the data and re-encrypt it under the new access policy, and then send it back to the cloud. This method incurs a high communication overhead and heavy computation burden on data owners. In this paper, we propose a novel scheme that enabling efficient access control with dynamic policy updating for big data in the cloud. We focus on developing an outsourced policy updating method for ABE systems. Our method can avoid the transmission of encrypted data and minimize the computation work of data owners, by making use of the previously encrypted data with old access policies. Moreover, we also design policy updating algorithms for different types of access policies. The analysis show that our scheme is correct, complete, secure and efficient.},
keywords={authorisation;Big Data;cloud computing;cryptography;access control;dynamic policy updating;Big Data;cloud;attribute-based encryption;ABE;end-to-end security;access policy;outsourced policy updating method;Servers;Encryption;Big data;Public key;Access control;Access Control;Policy Updating;ABE;Big Data;Cloud},
doi={10.1109/INFOCOM.2014.6848142},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848143,
author={L. Zhang and Z. Li and C. Wu and M. Chen},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Online algorithms for uploading deferrable big data to the cloud},
year={2014},
volume={},
number={},
pages={2022-2030},
abstract={This work studies how to minimize the bandwidth cost for uploading deferral big data to a cloud computing platform, for processing by a MapReduce framework, assuming the Internet service provider (ISP) adopts the MAX contract pricing scheme. We first analyze the single ISP case and then generalize to the MapReduce framework over a cloud platform. In the former, we design a Heuristic Smoothing algorithm whose worst-case competitive ratio is proved to fall between 2-1/(D+1) and 2(1 - 1/e), where D is the maximum tolerable delay. In the latter, we employ the Heuristic Smoothing algorithm as a building block, and design an efficient distributed randomized online algorithm, achieving a constant expected competitive ratio. The Heuristic Smoothing algorithm is shown to outperform the best known algorithm in the literature through both theoretical analysis and empirical studies. The efficacy of the randomized online algorithm is also verified through simulation studies.},
keywords={Big Data;cloud computing;distributed algorithms;distributed programming;Internet;deferrable Big Data uploading;cloud computing platform;MapReduce framework;Internet service provider;ISP;MAX contract pricing scheme;heuristic smoothing algorithm;worst-case competitive ratio;maximum tolerable delay;distributed randomized online algorithm;constant expected competitive ratio;Smoothing methods;Algorithm design and analysis;Delays;Heuristic algorithms;Cloud computing;Minimization;Data models},
doi={10.1109/INFOCOM.2014.6848143},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848144,
author={D. Williams and S. Zheng and X. Zhang and H. Jamjoom},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={TideWatch: Fingerprinting the cyclicality of big data workloads},
year={2014},
volume={},
number={},
pages={2031-2039},
abstract={Intrinsic to “big data” processing workloads (e.g., iterative MapReduce, Pregel, etc.) are cyclical resource utilization patterns that are highly synchronized across different resource types as well as the workers in a cluster. In Infrastructure as a Service settings, cloud providers do not exploit this characteristic to better manage VMs because they view VMs as “black boxes.” We present TideWatch, a system that automatically identifies cyclicality and similarity in running VMs. TideWatch predicts period lengths of most VMs in Hadoop workloads within 9% of actual iteration boundaries and successfully classifies up to 95% of running VMs as participating in the appropriate Hadoop cluster. Furthermore, we show how TideWatch can be used to improve the timing of VM migrations, reducing both migration time and network impact by over 50% when compared to a random approach.},
keywords={cloud computing;data handling;iterative methods;resource allocation;virtual machines;TideWatch;big data workload cyclicality;big data processing workloads;cyclical resource utilization;cyclical resource utilization patterns;infrastructure as a service settings;cloud providers;VM;black boxes;iteration boundaries;Hadoop cluster;Resource management;Smoothing methods;Time series analysis;Conferences;Computers;Synchronization;Noise},
doi={10.1109/INFOCOM.2014.6848144},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848145,
author={V. Martina and M. Garetto and E. Leonardi},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A unified approach to the performance analysis of caching systems},
year={2014},
volume={},
number={},
pages={2040-2048},
abstract={We propose a unified methodology to analyse the performance of caches (both isolated and interconnected), by extending and generalizing a decoupling technique originally known as Che's approximation, which provides very accurate results at low computational cost. We consider several caching policies, taking into account the effects of temporal locality. In the case of interconnected caches, our approach allows us to do better than the Poisson approximation commonly adopted in prior work. Our results, validated against simulations and trace-driven experiments, provide interesting insights into the performance of caching systems.},
keywords={cache storage;unified approach;performance analysis;caching systems;isolated cache;interconnected cache;decoupling technique;Che approximation;caching policies;temporal locality;Poisson approximation;Approximation methods;Computational modeling;Standards;Computers;Conferences;Computational efficiency;Analytical models},
doi={10.1109/INFOCOM.2014.6848145},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848146,
author={H. Yuan and P. Crowley},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Scalable Pending Interest Table design: From principles to practice},
year={2014},
volume={},
number={},
pages={2049-2057},
abstract={A Pending Interest Table (PIT) is a core component in Named Data Networking. Scalable PIT design is challenging because it requires per-packet updates, and the names stored in the PIT are long, requiring more memory. As the line speed keeps increasing, e.g., 100 Gbps, traditional hash-table based methods cannot meet these requirements. In this paper, we propose a novel Pending Interest Table design that guarantees packet delivery with a compact and approximate storage representation. To achieve this, the PIT stores fixed-length fingerprints instead of name strings. To overcome the classical fingerprint collision problem, the Interest aggregation feature in the core routers is relaxed. The memory requirement and network traffic overhead are analyzed, and the performance of a software implementation of the proposed design is measured. Our results show that 37 MiB to 245 MiB are required at 100 Gbps, so that the PIT can fit into SRAM or RLDRAM chips.},
keywords={computer networks;hash table;pending interest table design;packet delivery;approximate storage representation;fixed length fingerprints;classical fingerprint collision problem;memory requirement;software implementation;computer networks;scalable PIT design;named data networking;scalable pending interest table design;Face;Memory management;Random access memory;Conferences;Computers;Fingerprint recognition},
doi={10.1109/INFOCOM.2014.6848146},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848147,
author={S. Qian and J. Cao and Y. Zhu and M. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={REIN: A fast event matching approach for content-based publish/subscribe systems},
year={2014},
volume={},
number={},
pages={2058-2066},
abstract={Event matching is the process of checking high volumes of events against large numbers of subscriptions and is a fundamental issue for the overall performance of a large-scale distributed publish/subscribe system. Most existing algorithms are based on counting satisfied component constraints in each subscription. As the scale of a system grows, these algorithms inevitably suffer from performance degradation. We present REIN (REctangle INtersection), a fast event matching approach for large-scale content-based publish/subscribe systems. The idea behind REIN is to quickly filter out unlikely matched subscriptions. In REIN, the event matching problem is first transformed into the rectangle intersection problem. Then, an efficient index structure is designed to address the problem by using bit operations. Experimental results show that REIN has a better matching performance than its counterparts. In particular, the event matching speed is faster by an order of magnitude when the selectivity of subscriptions is high and the number of subscriptions is large.},
keywords={data structures;message passing;middleware;REIN approach;fast event matching approach;content-based publish-subscribe systems;distributed publish-subscribe system;rectangle intersection algorithm;index structure;bit operations;rectangle intersection problem;Subscriptions;Indexes;Routing;Conferences;Computers;Educational institutions;Merging},
doi={10.1109/INFOCOM.2014.6848147},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848148,
author={C. Tsilopoulos and G. Xylomenos and Y. Thomas},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Reducing forwarding state in content-centric networks with semi-stateless forwarding},
year={2014},
volume={},
number={},
pages={2067-2075},
abstract={Routers in the Content-Centric Networking (CCN) architecture maintain state for all pending content requests, so as to be able to later return the corresponding content. By employing stateful forwarding, CCN supports native multicast, enhances security and enables adaptive forwarding, at the cost of excessive forwarding state that raises scalability concerns. We propose a semi-stateless forwarding scheme in which, instead of tracking each request at every on-path router, requests are tracked at every d hops. At intermediate hops, requests gather reverse path information, which is later used to deliver responses between routers using Bloom filter-based stateless forwarding. Our approach effectively reduces forwarding state, while preserving the advantages of CCN forwarding. Evaluation results over realistic ISP topologies show that our approach reduces forwarding state by 54%-70% in unicast delivery, without any bandwidth penalties, while in multicast delivery it reduces forwarding state by 34%-55% at the expense of 6%-13% in bandwidth overhead.},
keywords={computer networks;data structures;topology;unicast delivery;ISP topologies;Bloom filter-based stateless forwarding;adaptive forwarding;CCN architecture;semi-stateless forwarding scheme;content-centric networking architecture;Ports (Computers);Unicast;Bandwidth;Probabilistic logic;Computer architecture;Conferences;Computers},
doi={10.1109/INFOCOM.2014.6848148},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848149,
author={Y. -. P. Hong and C. W. Tan and L. Zheng and C. Hsieh and C. Lee},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A unified framework for wireless max-min utility optimization with general monotonic constraints},
year={2014},
volume={},
number={},
pages={2076-2084},
abstract={This paper presents a unifying and systematic framework to solve wireless max-min utility fairness optimization problems in multiuser wireless networks with generalized monotonic constraints. These problems are often challenging to solve due to their nonlinearity and non-convexity. Our framework leverages a general result in nonlinear Perron-Frobenius theory to characterize the global optimal solution of these problems analytically, and to design scalable and fast-convergent algorithms for the computation of the optimal solution. This work advances the state-of-the-art in handling wireless utility optimization problems with nonlinear monotonic constraints, which existing methodologies cannot handle, and also unifies previous works in this area. Several representative applications are considered to illustrate the effectiveness of the proposed framework, including max-min quality of service subject to robust interference temperature constraints in cognitive radio networks, min-max outage subject to outage constraints in heterogeneous networks, and min-max weighted MSE subject to SINR constraints in multiuser downlink system.},
keywords={cognitive radio;minimax techniques;multi-access systems;unified framework;wireless max-min utility optimization;general monotonic constraints;utility fairness optimization problem;multiuser wireless networks;generalized monotonic constraint;nonlinear Perron-Frobenius theory;wireless utility optimization problems;nonlinear monotonic constraint;cognitive radio;min-max outage;Interference;Vectors;Receivers;Wireless networks;Signal to noise ratio;Standards},
doi={10.1109/INFOCOM.2014.6848149},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848150,
author={X. Liu and J. Xie},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A practical self-adaptive rendezvous protocol in cognitive radio ad hoc networks},
year={2014},
volume={},
number={},
pages={2085-2093},
abstract={In cognitive radio ad-hoc networks, two users rendezvous on a common available channel to realize communications. Most existing rendezvous papers focus on success-guaranteed channel-hopping sequence design. However, the theoretical rendezvous successful rate may suffer from the available channel status changing, collisions on channels, congestion at users, and target users unavailability in practical scenarios. Thus, a practical rendezvous framework that can address these issues is highly demanded. In this paper, we develop analytical models for each possible factor which may influence the performance of rendezvous. Then, based on the analysis of each factor, we propose corresponding schemes and integrate them into a self-adaptive protocol which can adjust its reaction and optimize system parameters to adapt to the dynamic network. Simulation results demonstrate that our proposed protocol gains better performance in terms of true rendezvous successful rate, short rendezvous delay, and low congestion. To the best of our knowledge, this is the first rendezvous protocol that addresses practical issues in realistic communication scenarios in cognitive radio networks.},
keywords={ad hoc networks;cognitive radio;protocols;practical self-adaptive rendezvous protocol;cognitive radio ad hoc networks;success-guaranteed channel-hopping sequence design;dynamic network;short rendezvous delay;true rendezvous successful rate;Receivers;Protocols;Sensors;Delays;Computers;Cognitive radio;Data communication},
doi={10.1109/INFOCOM.2014.6848150},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848151,
author={J. Zhao and W. Gao and Y. Wang and G. Cao},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Delay-constrained caching in cognitive radio networks},
year={2014},
volume={},
number={},
pages={2094-2102},
abstract={In cognitive radio networks, unlicensed users can use under-utilized licensed spectrum to achieve substantial performance improvement. To avoid interference with licensed users, unlicensed users must vacate the spectrum when it is accessed by licensed (primary) users. Since it takes some time for unlicensed users to switch to other available channels, the ongoing data transmissions may have to be interrupted and the transmission delay can be significantly increased. This makes it hard for cognitive radio networks to meet the delay constraints of many applications. To the best of our knowledge, we are the first to use caching techniques to address this problem. We formulate the cache placement problem in cognitive radio networks as an optimization problem, where the goal is to minimize the total cost, subject to some delay constraint, i.e., the data access delay can be statistically bounded. To solve this problem, we propose three approaches: cost-based, delay-based, and hybrid. Simulation results show that our approaches outperform existing caching solutions in terms of total cost and delay constraint, and the hybrid approach performs the best among the approaches satisfying the delay constraint.},
keywords={cache storage;cognitive radio;data communication;delays;multi-access systems;optimisation;radio spectrum management;delay-constrained caching;cognitive radio networks;unlicensed users;under-utilized licensed spectrum;primary users;data transmissions;transmission delay;caching techniques;cache placement problem;optimization problem;data access delay;cost-based approach;delay-based approach;hybrid approach;Delays;Cognitive radio;Markov processes;Generators;Switches;Data communication;Conferences},
doi={10.1109/INFOCOM.2014.6848151},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848152,
author={J. Misic and V. B. Misic},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Probability distribution of spectral hole duration in cognitive networks},
year={2014},
volume={},
number={},
pages={2103-2111},
abstract={Operation of cognitive secondary networks is critically dependent on the activity patterns of primary users. In this paper, we investigate the probability distribution of spectral holes, assuming that active and idle periods of primary users are independent random variables (which need not be identically distributed). We consider black, white, and gray holes, which correspond to time intervals when all channels are busy, all channels are idle, and some channels are busy while others are idle, respectively. We show that the duration of black and white holes may be described using an exponential approximation which holds regardless of the actual probability distribution of channel active and idle times, as long as the number of channels is not too small. The time interval between successive black hole occurrences is shown to be exponentially distributed as well. We also analyze the behavior of gray holes and quantify their impact using a simple proxy measure.},
keywords={approximation theory;cognitive radio;probability;probability distribution;spectral hole duration;cognitive secondary networks;exponential approximation;black hole occurrence;Approximation methods;Exponential distribution;Conferences;Computers;Probability density function;Random variables},
doi={10.1109/INFOCOM.2014.6848152},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848153,
author={B. Wang and S. Yu and W. Lou and Y. T. Hou},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Privacy-preserving multi-keyword fuzzy search over encrypted data in the cloud},
year={2014},
volume={},
number={},
pages={2112-2120},
abstract={Enabling keyword search directly over encrypted data is a desirable technique for effective utilization of encrypted data outsourced to the cloud. Existing solutions provide multi-keyword exact search that does not tolerate keyword spelling error, or single keyword fuzzy search that tolerates typos to certain extent. The current fuzzy search schemes rely on building an expanded index that covers possible keyword misspelling, which lead to significantly larger index file size and higher search complexity. In this paper, we propose a novel multi-keyword fuzzy search scheme by exploiting the locality-sensitive hashing technique. Our proposed scheme achieves fuzzy matching through algorithmic design rather than expanding the index file. It also eliminates the need of a predefined dictionary and effectively supports multiple keyword fuzzy search without increasing the index or search complexity. Extensive analysis and experiments on real-world data show that our proposed scheme is secure, efficient and accurate. To the best of our knowledge, this is the first work that achieves multi-keyword fuzzy search over encrypted cloud data.},
keywords={cloud computing;computational complexity;cryptography;data privacy;database indexing;file organisation;fuzzy set theory;search problems;privacy-preserving multikeyword fuzzy search;index file size;search complexity;locality-sensitive hashing technique;fuzzy matching;algorithmic design;predefined dictionary;encrypted cloud data;Indexes;Cryptography;Vectors;Servers;Keyword search;Privacy},
doi={10.1109/INFOCOM.2014.6848153},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848154,
author={J. Yuan and S. Yu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Efficient public integrity checking for cloud data sharing with multi-user modification},
year={2014},
volume={},
number={},
pages={2121-2129},
abstract={In past years a body of data integrity checking techniques have been proposed for securing cloud data services. Most of these works assume that only the data owner can modify cloud-stored data. Recently a few attempts started considering more realistic scenarios by allowing multiple cloud users to modify data with integrity assurance. However, these attempts are still far from practical due to the tremendous computational cost on cloud users. Moreover, collusion between misbehaving cloud servers and revoked users is not considered. This paper proposes a novel data integrity checking scheme characterized by multi-user modification, collusion resistance and a constant computational cost of integrity checking for cloud users, thanks to our novel design of polynomial-based authentication tags and proxy tag update techniques. Our scheme also supports public checking and efficient user revocation and is provably secure. Numerical analysis and extensive experimental results show the efficiency and scalability of our proposed scheme.},
keywords={cloud computing;data integrity;groupware;polynomials;security of data;public integrity checking;cloud data sharing;multiuser modification;data integrity checking;cloud data services security;cloud-stored data;computational cost;collusion resistance;numerical analysis;polynomial-based authentication tags;proxy tag update techniques;Servers;Authentication;Polynomials;Radio frequency;Computational efficiency;Algorithm design and analysis},
doi={10.1109/INFOCOM.2014.6848154},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848155,
author={C. Wang and B. Zhang and K. Ren and J. M. Roveda and C. W. Chen and Z. Xu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A privacy-aware cloud-assisted healthcare monitoring system via compressive sensing},
year={2014},
volume={},
number={},
pages={2130-2138},
abstract={Wireless sensors are being increasingly used to monitor/collect information in healthcare medical systems. For resource-efficient data acquisition, one major trend today is to utilize compressive sensing, for it unifies traditional data sampling and compression. Despite the increasing popularity, how to effectively process the ever-growing healthcare data and simultaneously protect data privacy, while maintaining low overhead at sensors, remains challenging. To address the problem, we propose a privacy-aware cloud-assisted healthcare monitoring system via compressive sensing, which integrates different domain techniques with following benefits. By design, acquired sensitive data samples never leave sensors in unprotected form. Protected samples are later sent to cloud, for storage, processing, and disseminating reconstructed data to receivers. The system is privacy-assured where cloud sees neither the original samples nor underlying data. It handles well sparse and general data, and data tampered with noise. Theoretical and empirical evaluations demonstrate the system achieves privacy-assurance, efficiency, effectiveness, and resource-savings simultaneously.},
keywords={cloud computing;compressed sensing;data acquisition;data compression;data protection;health care;information dissemination;medical information systems;patient monitoring;wireless sensor networks;privacy-aware cloud-assisted healthcare monitoring system;compressive sensing;wireless sensors;healthcare medical systems;resource-efficient data acquisition;data sampling;data compression;data privacy;data reconstruction;privacy-assurance;data protection;information dissemination;Medical services;Receivers;Security;Monitoring;Sensor systems;Compressed sensing},
doi={10.1109/INFOCOM.2014.6848155},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848156,
author={J. Zhang and X. Liao and S. Li and Y. Hua and X. Liu and B. Lin},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Aggrecode: Constructing route intersection for data reconstruction in erasure coded storage},
year={2014},
volume={},
number={},
pages={2139-2147},
abstract={Node failures often occur in large-scale data centers today. Erasure coded storage system provides high data reliability via data reconstruction. Existing work can improve reconstruction performance, while considering the transmission of recovery data as the main source of reconstruction overheads. Transmission costs are highly related with network topology, which is unfortunately overlooked. An ideal connected topology assumes that two nodes in a data center has a direct link. The unmatching design between the network model and the practical topology may lead to an underestimated transmission costs. In this paper, we propose an erasure coded storage system for data reconstruction, which uses the practical network topology to minimize the reconstruction transmission costs. First, we identify the aggregation feature of erasure coding reconstruction and propose Aggregation Decoding, which splits the decoding process into several sub-decoding operations during reconstruction routing to reduce overall recovery data to be transmitted. We further improve Aggrecode to construct efficient route basing on the location of participating nodes to exploit the aggregation feature of Aggregation Decoding. We formulate this routing problem as a relaxed Steiner Tree problem. We design two heuristic routing algorithms based on ant-colony optimization specialized for two failure recovery cases, e.g., node recovery and degraded read. Our analytical results demonstrate the important properties of Aggrecode. These properties are evaluated by extensive experiments deployed on popular data center topologies, such as Torus, Fat-tree, DCell and BCube. The results show that Aggrecode can reduce data transmission costs by at least 37.12% for all settings.},
keywords={ant colony optimisation;computer centres;encoding;telecommunication network reliability;telecommunication network routing;telecommunication network topology;aggrecode;route intersection construction;data reconstruction;node failures;erasure coded storage system;data reliability;transmission costs;network topology;connected topology;data center;direct link;unmatching design;underestimated transmission costs;aggregation decoding;erasure coding reconstruction;Steiner tree problem;ant colony optimization;Network topology;Decoding;Routing;Topology;Encoding;Heuristic algorithms;Vegetation},
doi={10.1109/INFOCOM.2014.6848156},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848157,
author={T. Elhourani and A. Gopalan and S. Ramasubramanian},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={IP fast rerouting for multi-link failures},
year={2014},
volume={},
number={},
pages={2148-2156},
abstract={The introduction of coherent optics and wavelength division multiplexing (WDM) in telecommunication networks has led to unprecedented gains in backbone capacity. The increase in optical layer capacity inadvertently exacerbates the problem of traffic loss due to optical component failures. IP networks are designed over optical backbone networks, where each IP link traverses a multihop optical lightpath. Therefore, the failure of optical components often lead to multiple link failures in the IP network. In this paper, we develop an IP fast reroute mechanism using rooted arc-disjoint spanning trees that guarantees recovery from (k-1) link failures in a k-edge-connected network. As arc-disjoint spanning trees may be constructed in sub-quadratic time in the size of the network, our approach offers excellent scalability. Through experimental results, we show that employing arc-disjoint spanning trees to recover from multiple failures reduces path stretch in comparison with previously known techniques.},
keywords={computer network reliability;IP networks;light coherence;optical links;telecommunication network routing;telecommunication traffic;trees (mathematics);wavelength division multiplexing;multilink failure;coherent optics;wavelength division multiplexing;telecommunication network;optical layer capacity;traffic loss;optical component failure;IP network link;optical backbone networks;multihop optical lightpath;IP fast reroute mechanism;rooted arc disjoint spanning tree;(k-1) link failure;k-edge connected network;subquadratic time;IP networks;Vectors;Optical fiber networks;Computers;Routing;Conferences},
doi={10.1109/INFOCOM.2014.6848157},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848158,
author={A. Zand and G. Vigna and R. Kemmerer and C. Kruegel},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Rippler: Delay injection for service dependency detection},
year={2014},
volume={},
number={},
pages={2157-2165},
abstract={Detecting dependencies among network services has been well-studied in previous research. These attempts at service dependency detection fall into two classes: active and passive approaches. While passive approaches suffer from high false positives, active approaches suffer from applicability problems. In this paper, we design a new application-independent active approach for detecting dependencies among services. We present a traffic watermarking approach with arbitrarily low false positives and easy applicability. We provide statistical tests for detecting watermarked flows, and we compute the false positive and false negative rates of these tests both analytically and experimentally. Furthermore, we implemented the proposed watermarking system (Rippler) in a small university lab network. We ran our system for four months and detected 38 dependencies among 54 services. Finally, we compared the efficiency of our approach against three previous systems by testing them on this real-world network data.},
keywords={delays;electronic mail;statistical testing;telecommunication security;telecommunication traffic;watermarking;Rippler;delay injection;service dependency detection;traffic watermarking approach;statistical tests;electronic mail;Delays;Servers;Watermarking;Noise;Electronic mail;Jitter;Computers},
doi={10.1109/INFOCOM.2014.6848158},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848159,
author={Y. Zhu and Y. Jiang and W. Wu and L. Ding and A. Teredesai and D. Li and W. Lee},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Minimizing makespan and total completion time in MapReduce-like systems},
year={2014},
volume={},
number={},
pages={2166-2174},
abstract={Effectiveness of MapReduce as a big data processing framework depends on efficiencies of scale for both map and reduce phases. While most map tasks are preemptive and parallelizable, the reduce tasks typically are not easily decomposed and often become a bottleneck due to constraints of data locality and task complexity. By assuming that reduce tasks are non-parallelizable, we study offline scheduling of minimizing makespan and minimizing total completion time, respectively. Both preemptive and non-preemptive reduce tasks are considered. On makespan minimization, for preemptive version we design an algorithm and prove its optimality, for non-preemptive version we design an approximation algorithm with the worst ratio of 3/2-1/2h where h is the number of machines. On total complete time minimization, for non-preemptive version we devise an approximation algorithm with worst case ratio of 2-1/h, and for preemptive version we devise a heuristic. We confirm that our algorithms outperform state-of-art schedulers through experiments.},
keywords={approximation theory;Big Data;data mining;makespan minimization;total completion time;MapReduce-like system;big data processing;offline scheduling;approximation algorithm;Schedules;Optimal scheduling;Minimization;Approximation algorithms;Algorithm design and analysis;Educational institutions;Approximation methods},
doi={10.1109/INFOCOM.2014.6848159},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848160,
author={Y. Yuan and D. Wang and J. Liu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Joint scheduling of MapReduce jobs with servers: Performance bounds and experiments},
year={2014},
volume={},
number={},
pages={2175-2183},
abstract={MapReduce has achieved tremendous success for large-scale data processing in data centers. A key feature distinguishing MapReduce from previous parallel models is that it interleaves parallel and sequential computation. Past schemes, and especially their theoretical bounds, on general parallel models are therefore, unlikely to be applied to MapReduce directly. There are many recent studies on MapReduce job and task scheduling. These studies assume that the servers are assigned in advance. In current data centers, multiple MapReduce jobs of different importance levels run together. In this paper, we investigate a schedule problem for MapReduce taking server assignment into consideration as well. We formulate a MapReduce server-job organizer problem (MSJO) and show that it is NP-complete. We develop a 3-approximation algorithm and a fast heuristic. We evaluate our algorithms through both simulations and experiments on Amazon EC2 with an implementation in Hadoop. The results confirm the advantage of our algorithms.},
keywords={approximation theory;computational complexity;parallel programming;scheduling;joint scheduling;large-scale data processing;data centers;parallel models;parallel computation;sequential computation;task scheduling;job scheduling;server assignment;NP-complete problem;MapReduce server-job organizer problem;MSJO;3-approximation algorithm;fast heuristic;Amazon EC2;Hadoop;Schedules;Servers;Scheduling;Mars;Processor scheduling;Complexity theory;Delays},
doi={10.1109/INFOCOM.2014.6848160},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848161,
author={X. Wang and Y. Chen and L. Cai and J. Pan},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Scheduling in a secure wireless network},
year={2014},
volume={},
number={},
pages={2184-2192},
abstract={We consider a scheduling problem in a wireless network which consists of one base station, N legitimate users and one (or more) eavesdropper(s). The scheduling problem jointly considers the reliability, security and stability of the system, and is to allocate wireless resources to the legitimate users, stabilize the system and maximize the secure transmission rate. Based on the stochastic network optimization framework, the scheduling problem is decomposed to an online optimization problem. A scheduling algorithm and a low computational complexity algorithm that both do not consider power adaptation are proposed, along with a power adaptive one. Extensive simulations are conducted to show the impact of the information arrival rate and the eavesdropper's channel condition on the system performance. These observations provide important insights and guidelines for the design and resource management of future wireless networks using secure communication technologies.},
keywords={computational complexity;optimisation;radio networks;resource allocation;scheduling;telecommunication security;wireless channels;secure wireless network;base station;legitimate user;wireless resource allocation;system stability;system reliability;transmission rate;stochastic network optimization framework;online optimization;scheduling algorithm;computational complexity algorithm;information arrival rate;eavesdropper channel;resource management;Security;Receivers;Wireless networks;Computational complexity;Reliability;Fading},
doi={10.1109/INFOCOM.2014.6848161},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848162,
author={K. S. Kim and C. Li and E. Modiano},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Scheduling multicast traffic with deadlines in wireless networks},
year={2014},
volume={},
number={},
pages={2193-2201},
abstract={We consider the problem of transmitting multicast flows with hard deadlines over unreliable wireless channels. Every user in the network subscribes to several multicast flows, and requires a minimum throughput for each subscribed flow to meet the QoS constraints. The network controller schedules the transmissions of multicast traffic based on the instant feedback from the users. We characterize the multicast throughput region by analyzing its boundary points, each of which is the solution to a finite-horizon dynamic programming problem over an exponentially large state space. Using backward induction and interchange arguments, we show that the dynamic programming problems are solved by greedy policies that maximize the immediate weighted sum throughput in every slot. Furthermore, we develop a dynamic throughput-optimal policy that achieves any feasible throughput vector by tracking the running performance received by the users.},
keywords={dynamic programming;greedy algorithms;multicast communication;quality of service;radio networks;scheduling;wireless channels;multicast traffic transmission scheduling;deadlines;wireless networks;multicast flow transmission;unreliable wireless channel;network subscribe;QoS constraint;network controller;user feedback;multicast throughput region;finite horizon dynamic programming problem;backward induction;greedy policy;weighted sum throughput maximization;dynamic throughput optimal policy;running performance tracking;Throughput;Vectors;Base stations;Unicast;Dynamic programming;Wireless networks},
doi={10.1109/INFOCOM.2014.6848162},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848163,
author={S. Banerjee and A. Chatterjee and S. Shakkottai},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Epidemic thresholds with external agents},
year={2014},
volume={},
number={},
pages={2202-2210},
abstract={We study the effect of external infection sources on phase transitions in epidemic processes. In particular, we consider an epidemic spreading on a network via the SIS/SIR dynamics, which in addition is aided by external agents - sources unconstrained by the graph, but possessing a limited infection rate or virulence. Such a model captures many existing models of externally aided epidemics, and finds use in many settings - epidemiology, marketing and advertising, network robustness, etc. We provide a detailed characterization of the impact of external agents on epidemic thresholds. In particular, for the SIS model, we show that any external infection strategy with constant virulence either fails to significantly affect the lifetime of an epidemic, or at best, sustains the epidemic for a lifetime which is polynomial in the number of nodes. On the other hand, a random external-infection strategy, with rate increasing linearly in the number of infected nodes, succeeds under some conditions to sustain an exponential epidemic lifetime. We obtain similar sharp thresholds for the SIR model, and discuss the relevance of our results in a variety of settings.},
keywords={epidemics;graph theory;epidemic thresholds;external infection sources;phase transitions;epidemic processes;epidemic spreading;SIS-SIR dynamics;external agents;graph;limited infection rate;virulence;externally aided epidemics;epidemiology;network robustness;advertising;marketing;random external-infection strategy;Polynomials;Advertising;Computers;Markov processes;Conferences;Computational modeling;Analytical models},
doi={10.1109/INFOCOM.2014.6848163},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848164,
author={K. Zhu and L. Ying},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A robust information source estimator with sparse observations},
year={2014},
volume={},
number={},
pages={2211-2219},
abstract={In this paper, we consider the problem of locating the information source with sparse observations. We assume that a piece of information spreads in a network following a heterogeneous susceptible-infected-recovered (SIR) model, where a node is said to be <i>infected</i> when it receives the information and <i>recovered</i> when it removes or hides the information. We further assume that a small subset of infected nodes are reported, from which we need to find the source of the information. We adopt the sample path based estimator developed in [1], and prove that on infinite trees, the sample path based estimator is a Jordan infection center with respect to the set of observed infected nodes. In other words, the sample path based estimator minimizes the maximum distance to observed infected nodes. We further prove that the distance between the estimator and the actual source is upper bounded by a constant independent of the number of infected nodes with a high probability on infinite trees. Our simulations on tree networks and real world networks show that the sample path based estimator is closer to the actual source than several other algorithms.},
keywords={computer viruses;social networking (online);tree networks;infinite trees;Jordan infection center;sample path;infected nodes;SIR model;susceptible infected recovered model;sparse observations;robust information source estimator;Computational modeling;Computers;Robustness;Conferences;Network topology;Maximum likelihood estimation;Heuristic algorithms},
doi={10.1109/INFOCOM.2014.6848164},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848165,
author={M. Jang and K. Schwan and K. Bhardwaj and A. Gavrilovska and A. Avasthi},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Personal clouds: Sharing and integrating networked resources to enhance end user experiences},
year={2014},
volume={},
number={},
pages={2220-2228},
abstract={End user experiences on mobile devices with their rich sets of sensors are constrained by limited device battery lives and restricted form factors, as well as by the `scope' of the data available locally. The `Personal Cloud' distributed software abstractions address these issues by enhancing the capabilities of a mobile device via seamless use of both nearby and remote cloud resources. In contrast to vendor-specific, middleware-based cloud solutions, Personal Cloud instances are created at hypervisor-level, to create for each end user the federation of networked resources best suited for the current environment and use. Specifically, the Cirrostratus extensions of the Xen hypervisor can federate a user's networked resources to establish a personal execution environment, governed by policies that go beyond evaluating network connectivity to also consider device ownership and access rights, the latter managed in a secure fashion via standard Social Network Services. Experimental evaluations with both Linux- and Android-based devices, and using Facebook as the SNS, show the approach capable of substantially augmenting a device's innate capabilities, improving application performance and the effective functionality seen by end users.},
keywords={cloud computing;mobile computing;smart phones;social networking (online);virtualisation;end user experiences;personal cloud;mobile devices;remote cloud resources;middleware-based cloud solutions;cirrostratus extensions;Xen hypervisor-level;personal execution environment;network connectivity;social network services;Linux-based devices;Android-based devices;facebook;SNS;cloud computing;distributed software abstractions;user networked resource sharing;Runtime;Face recognition;Mobile handsets;Resource management;Facebook;Authentication;Clouds;mobile computing;workload offloading;capability sharing;cloud computing;management of distributed resources;Android;virtualization;social network services},
doi={10.1109/INFOCOM.2014.6848165},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848166,
author={B. Proulx and J. Zhang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Modeling social network relationships via t-cherry junction trees},
year={2014},
volume={},
number={},
pages={2229-2237},
abstract={The massive scale of online social networks makes it very challenging to characterize the underlying structure therein. In this paper, we employ the t-cherry junction tree, a very recent advancement in probabilistic graphical models, to develop a compact representation and good approximation of an otherwise intractable model for users' relationships in a social network. There are a number of advantages in this approach: (1) the best approximation possible via junction trees belongs to the class of t-cherry junction trees; (2) constructing a t-cherry junction tree can be largely parallelized; and (3) inference can be performed using distributed computation. To improve the quality of approximation, we also devise an algorithm to build a higher order tree gracefully from an existing one, without constructing it from scratch. We apply this approach to Twitter data containing 100,000 nodes and study the problem of recommending connections to new users.},
keywords={inference mechanisms;probability;social networking (online);trees (mathematics);social network relationship modeling;t-cherry junction trees;online social networks;probabilistic graphical models;user relationships;higher-order tree;Twitter data;inference;approximation quality improvement;user connection recommendation problem;Junctions;Approximation methods;Particle separators;Clustering algorithms;Social network services;Random variables;Approximation algorithms},
doi={10.1109/INFOCOM.2014.6848166},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848167,
author={J. Zhao and G. Cao},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Spectrum-aware data replication in intermittently connected cognitive radio networks},
year={2014},
volume={},
number={},
pages={2238-2246},
abstract={The opening of under-utilized spectrum creates an opportunity for unlicensed users to achieve substantial performance improvement through cognitive radio techniques. In cognitive radio ad-hoc networks, with node mobility and low node density, the network topology is highly dynamic and end-to-end connection is hard to maintain. We propose data replication techniques to address these problems and improve data access performance in such intermittently connected cognitive radio network. Although data replication has been extensively studied in traditional disruption tolerant networks, existing techniques cannot be directly applied here since they do not consider the effects of primary user appearance on data replication. In this paper, we formulate spectrum-aware data replication as an optimization problem which tries to maximize the average data retrieval probability, subject to storage and time constraints. Since the problem is hard to solve based on mixed integer programming, we further design a distributed replication scheme based on the metric of replication benefit. Extensive simulations based on synthetic and realistic traces show that our scheme outperforms existing schemes in terms of data retrieval probability in various scenarios.},
keywords={ad hoc networks;cognitive radio;integer programming;telecommunication network reliability;spectrum aware data replication;intermittently connected cognitive radio networks;underutilized spectrum;ad hoc network;node mobility;network topology;data access performance;disruption tolerant network;optimization problem;average data retrieval probability;mixed integer programming;Time factors;Peer-to-peer computing;Cognitive radio;Probability;Availability;Ad hoc networks;Data communication},
doi={10.1109/INFOCOM.2014.6848167},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848168,
author={S. Wu and C. Wu and W. Hon and K. G. Shin},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Rendezvous for heterogeneous spectrum-agile devices},
year={2014},
volume={},
number={},
pages={2247-2255},
abstract={Cognitive radio (CR) is intended to meet the exponentially growing demand for spectrum by allowing for opportunistic utilization of idle legacy channels. Rendezvous, where two radios complete handshaking in an idle channel, is a key step for “stranger” (unknown to each other) CRs to start communication. However, none of existing algorithms guarantee rendezvous for heterogeneous or stranger CRs with different spectrum-sensing capabilities, in spite of the fact that (i) a wide variety of mobile devices are equipped with heterogeneous radios and (ii) there are numerous applications requiring efficient rendezvous for heterogeneous radios/CRs. In this paper, we propose a new channel hopping algorithm, called Heterogeneous Hopping (HH), that guarantees rendezvous without assuming existence of a universal channel set that can be sensed by all radios. HH is realized with a two-layer design that harmonizes the fixed-short-cycle and parity-alignment techniques we propose here, in order to guide CRs to rendezvous in two complementary situations resulting from the different capabilities of mobile wireless devices. To best of our knowledge, HH is the first channel-hopping scheme that guarantees rendezvous between heterogeneous radios. Our in-depth evaluation has shown HH to be significantly faster than simple extensions of existing schemes. Moreover, the latter cannot guarantee successful rendezvous, either.},
keywords={cognitive radio;mobile handsets;radio spectrum management;signal detection;heterogeneous spectrum-agile devices;cognitive radio;idle legacy channel opportunistic utilization;heterogeneous CR;stranger CR;spectrum-sensing capability;heterogeneous radio;rendezvous efficiency;channel hopping algorithm;heterogeneous hopping;HH;universal channel set;two-layer design;fixed-short-cycle technique;parity-alignment technique;mobile wireless devices;Wireless communication;Silicon;Wireless sensor networks;Ad hoc networks;Sensors;Availability;Nickel},
doi={10.1109/INFOCOM.2014.6848168},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848169,
author={H. Hassanieh and L. Shi and O. Abari and E. Hamed and D. Katabi},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={GHz-wide sensing and decoding using the sparse Fourier transform},
year={2014},
volume={},
number={},
pages={2256-2264},
abstract={We present BigBand, a technology that can capture GHz of spectrum in realtime without sampling the signal at GS/s - i.e., without high speed ADCs. Further, it is simple and can be implemented on commodity low-power radios. Our approach builds on recent advances in the area of sparse Fourier transforms, which show that it is possible to reconstruct a sparse signal without sampling it at the Nyquist rate. To demonstrate our design, we implement it using 3 software radios, each sampling the spectrum at 50 MS/s, producing a device that captures 0.9 GHz - i.e., 6× larger digital bandwidth than the three software radios combined. Finally, an extension of BigBand can perform GHz spectrum sensing even in scenarios where the spectrum is not sparse.},
keywords={compressed sensing;decoding;Fourier transforms;radio spectrum management;signal reconstruction;signal sampling;software radio;gigahertz-wide spectrum sensing;gigahertz-wide decoding;sparse Fourier transform;BigBand;commodity low-power radios;sparse signal reconstruction;Nyquist rate;software radios;digital bandwidth;wireless communication;frequency 0.9 GHz;Frequency estimation;Sensors;Bandwidth;Time-frequency analysis;Fourier transforms;Hardware;Wireless communication},
doi={10.1109/INFOCOM.2014.6848169},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848170,
author={P. Lin and X. Feng and Q. Zhang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Flexauc: Serving dynamic demands in spectrum trading markets with flexible auction},
year={2014},
volume={},
number={},
pages={2265-2273},
abstract={In spectrum trading markets, auctions are organized by spectrum holders (SHs) to distribute channels. As buyers, wireless service providers (WSPs) acquire channels to deploy services to end users. To optimize the profits, it is essential for the WSPs to determine their bidding strategies, which are affected by two key aspects: the service position to the end users and the auction schemes enforced by the SH. In this paper, we jointly study the strategy of the SH in the auction design and the WSPs' strategies in the service provisions and biddings. The WSP's optimal strategy in the auction can be flexible in term of demands and valuations. To optimize social welfare and enable the WSPs to reveal truthful flexible demands, we design Flexauc, a novel auction mechanism for the SH. We prove theoretically that Flexauc not only maximizes the social welfare but also preserves other nice properties: truthfulness and computational tractability.},
keywords={radio spectrum management;tendering;wireless channels;Flexauc;spectrum trading markets;flexible auction;spectrum holders;SHs;wireless service providers;bidding strategy;auction schemes;WSP optimal strategy;social welfare;Pricing;Nickel;Cost accounting;Bandwidth;Algorithm design and analysis;Conferences;Computers},
doi={10.1109/INFOCOM.2014.6848170},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848171,
author={L. Xue and D. Kim and Y. Zhu and D. Li and W. Wang and A. O. Tokuta},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Multiple heterogeneous data ferry trajectory planning in wireless sensor networks},
year={2014},
volume={},
number={},
pages={2274-2282},
abstract={This paper investigates two new groups of trajectory optimization problems which stem from networked multi-robotic systems. In particular, we study how to efficiently collect data from stationary sensor nodes using multiple robotic vehicles such as data ferries under different circumstance. The first group includes two new problems which aim to find the tours and the paths, respectively, of k robot vehicles with different mobilization conditions to collect data from ground sensor nodes with minimum latency. The second group consists of one new problem whose goal is to determine the quality tours of k robot vehicles with different speeds, where each of which follows its corresponding tour to repeatedly collect data from stationary sensors. We prove the three problems are NP-hard and propose constant factor approximation strategies for them. Through a simulation, an analytical study is conducted to evaluate the average performance of our core contribution.},
keywords={approximation theory;computational complexity;graph theory;mobile robots;multi-robot systems;path planning;trajectory control;travelling salesman problems;wireless sensor networks;unmanned aerial vehicles;mobile robots;trajectory control;traveling salesperson problem;constant factor approximation strategies;NP-hard problem;quality tours;minimum latency;mobilization conditions;multiple robotic vehicles;stationary sensor nodes;data collection;networked multirobotic systems;trajectory optimization problems;wireless sensor networks;multiple heterogeneous data ferry trajectory planning;Approximation algorithms;Approximation methods;Trajectory;Robot sensing systems;Algorithm design and analysis;Mobile nodes;Approximation algorithm;graph theory;wireless sensor network;mobile elements;traveling salesperson problem;trajectory control},
doi={10.1109/INFOCOM.2014.6848171},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848172,
author={K. Chen and H. Shen},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={DSearching: Distributed searching of mobile nodes in DTNs with floating mobility information},
year={2014},
volume={},
number={},
pages={2283-2291},
abstract={In delay tolerant networks (DTNs), enabling a node to search and find an interested mobile node is an important function in many applications. However, the movement of nodes in DTNs makes the problem formidable. Current methods in disconnected networks mainly rely on fixed stations and infrastructure-based communication to collect node position information, which is difficult to implement in DTNs. In this paper, we present DSearching, a distributed mobile node searching scheme for DTNs that requires no infrastructure except the GPS on mobile nodes. In DSearching, the entire DTN area is split into sub-areas, and each node summarizes its mobility information as both transient sub-area visiting record and long-term movement pattern among sub-areas. Each node distributes its transient visiting record for a newly entered sub-area to nodes that are likely to stay in the sub-area that it just moves out, so that the information flows in the network for the locator to trace it along its actual movement path. Each node also stores different parts of its long-term mobility pattern to long-staying nodes in different sub-areas for the locator to trace it when visiting records are absent. Considering that nodes in DTNs usually have limited resources, DSearching constrains the communication and storage cost in the information distribution process. Extensive trace-driven experiments with real traces demonstrate the high efficiency and high effectiveness of DSearching.},
keywords={delay tolerant networks;Global Positioning System;mobile communication;mobility management (mobile radio);floating mobility information distribution;delay tolerant networks;disconnected networks;infrastructure based communication;node position information;DSearching;distributed mobile node searching scheme;GPS;DTN area;transient subarea visiting record;long term movement pattern;storage cost;trace driven experiment;Nickel;Mobile nodes;Computers;Transient analysis;Animals;Conferences},
doi={10.1109/INFOCOM.2014.6848172},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848173,
author={J. Kwak and O. Choi and S. Chong and P. Mohapatra},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Dynamic speed scaling for energy minimization in delay-tolerant smartphone applications},
year={2014},
volume={},
number={},
pages={2292-2300},
abstract={Energy-delay tradeoffs in smartphone applications have been studied independently in dynamic voltage and frequency scaling (DVFS) problem and network interface selection problem. We optimize the two problems jointly to quantify how much energy can be saved further and propose a scheme called SpeedControl which jointly manages application scheduling, CPU speed control and wireless interface selection. The scheme is shown to be near-optimal in that it tends to minimize energy consumption for given delay constraints. This paper is the first to reveal energy-delay tradeoffs in a holistic view considering multiple wireless interfaces, DVFS and multitasking in smartphone. We perform real measurements on WiFi/3G coverage and throughput, power consumption of CPU and WiFi/3G interfaces, and CPU workloads. Trace-driven simulations based on the measurements demonstrate that SpeedControl can save over 30% of battery by trading 10 min delay as compared to existing schemes when WiFi temporal coverage is 65%, moreover, the saving tendency increases as WiFi coverage increases.},
keywords={energy consumption;mobile computing;power aware computing;smart phones;wireless LAN;CPU;trace-driven simulation;WiFi-3G interfaces;DVFS;energy-delay tradeoffs;wireless interface selection;SpeedControl scheme;dynamic voltage and frequency scaling problem;dynamic speed scaling;energy minimization;delay-tolerant smartphone;Artificial neural networks;IEEE 802.11 Standards;Power demand;Network interfaces;Delays;Energy consumption;Algorithm design and analysis},
doi={10.1109/INFOCOM.2014.6848173},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848174,
author={W. Gao and Q. Li and G. Cao},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Forwarding redundancy in opportunistic mobile networks: Investigation and elimination},
year={2014},
volume={},
number={},
pages={2301-2309},
abstract={Opportunistic mobile networks consist of mobile devices which are intermittently connected via short-range radios. Forwarding in such networks relies on selecting relays to carry and deliver data to destinations upon opportunistic contacts. Due to the intermittent network connectivity, relays in current forwarding schemes are selected separately in a distributed manner. The contact capabilities of relays hence may overlap when they contact the same nodes and cause forwarding redundancy. This redundancy reduces the efficiency of resource utilization in the network, and may impair the forwarding performance if being ignored. In this paper, based on experimental investigations on the characteristics of forwarding redundancy in realistic mobile networks, we propose methods to eliminate unnecessary forwarding redundancy and ensure efficient utilization of network resources. We first develop techniques to eliminate forwarding redundancy with global network information, and then improve these techniques to be operable in a fully distributed manner with limited network information.},
keywords={delay tolerant networks;mobile radio;relay networks (telecommunication);forwarding redundancy elimination;opportunistic mobile networks;mobile devices;short-range radios;opportunistic contacts;intermittent network connectivity;relays;resource utilization;realistic mobile networks;network resources;global network information;delay-disruption tolerant networks;Redundancy;Relays;Mobile communication;Mobile computing;Computers;Peer-to-peer computing;Correlation},
doi={10.1109/INFOCOM.2014.6848174},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848175,
author={X. Chen and X. Wu and X. Li and Y. He and Y. Liu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Privacy-preserving high-quality map generation with participatory sensing},
year={2014},
volume={},
number={},
pages={2310-2318},
abstract={Accurate maps are increasingly important with the growth of smart phones and the development of location-based services. Several crowdsourcing based map generation protocols have been proposed that rely on volunteers to provide their traces. Being creative, however, those methods pose a significant threat to user privacy as the traces can easily imply user behavior patterns. On the flip side, crowdsourcing-based map generation method does need individual locations. To address the issue, we present a systematic participatory-sensing-based high-quality map generation scheme, PMG, that meets the privacy demand of individual users. In this approach, individual users merely need to upload unorganized sparse location points so as to reduce the risk of exposing privacy, while the server generates accurate maps with unorganized points, instead of user traces. Experiments show that our solution is able to generate high-quality maps for a real environment that is robust to noisy data. The difference between the ground-truth map and the produced map is &lt;; 10m, even when the collected locations are about 32m apart after clustering for the purpose of removing noise.},
keywords={data privacy;mobile computing;smart phones;privacy-preserving high-quality map generation;participatory sensing;smart phones;location-based services;crowdsourcing based map generation;Privacy;Servers;Global Positioning System;Roads;Greedy algorithms;Conferences;Computers;privacy-preserving;map generation;crust},
doi={10.1109/INFOCOM.2014.6848175},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848176,
author={T. Shu and Y. Chen and J. Yang and A. Williams},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Multi-lateral privacy-preserving localization in pervasive environments},
year={2014},
volume={},
number={},
pages={2319-2327},
abstract={Location based services (LBSs) have raised serious privacy concerns in the society, due to the possibility of leaking a mobile user's location information in enabling location-dependent services. While existing location-privacy studies are mainly focused on preventing the leakage of user's location in accessing the LBS server, the possible privacy leakage during the localization process has been largely ignored. Such a privacy leakage stems from the fact that a localization algorithm typically takes the location of anchors (i.e., reference points for localization) as input, and generates the target's location as output. As such, the location of anchors, and consequently the target's location, could be leaked to others. An adversary could further utilize the leakage of anchor's locations to attack the localization infrastructure and undermine the accurate estimation of the target's location. To address this issue, in this paper, we study the multi-lateral privacy preserving localization problem, whereby the location of a target is calculated without the need of revealing anchors' location, and the knowledge of the localization outcome is strictly limited to the target itself. To fully protect user's privacy, our study protects not only the user's exact location information (the geo-coordinates), but also any side information that may lead to a coarse estimate of the location. Three privacy-preserving localization solutions are developed by leveraging combinations of information hiding and homomorphic encryption. These solutions provide different levels of protection for location side information and resilience to node collusion, and have the advantage of being able to trade user's privacy requirements for better computation/communication efficiency.},
keywords={cryptography;data encapsulation;data privacy;mobile computing;multilateral privacy-preserving localization;pervasive environments;location based services;LBS;mobile user location information;location-dependent services;location-privacy studies;user location leakage prevention;privacy leakage;anchor location;localization infrastructure;information hiding;homomorphic encryption;location side information;node collusion;user privacy requirements;computation-communication efficiency;Privacy;Protocols;Distance measurement;Mobile communication;Servers;Estimation;Encryption},
doi={10.1109/INFOCOM.2014.6848176},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848177,
author={W. Wang and Q. Zhang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A stochastic game for privacy preserving context sensing on mobile phone},
year={2014},
volume={},
number={},
pages={2328-2336},
abstract={The proliferation of sensor-equipped smartphones has enabled an increasing number of context-aware applications that provide personalized services based on users' contexts. However, most of these applications aggressively collect users sensing data without providing clear statements on the usage and disclosure strategies of such sensitive information, which raises severe privacy concerns and leads to some initial investigation on privacy preservation mechanisms design. While most prior studies have assumed static adversary models, we investigate the context dynamics and call attention to the existence of intelligent adversaries. In this paper, we first identify the context privacy problem with consideration of the context dynamics and malicious adversaries with capabilities of adjusting their attacking strategies, and then formulate the interactive competition between users and adversaries as a zero-sum stochastic game. In addition, we propose an efficient minimax learning algorithm to obtain the optimal defense strategy. Our evaluations on real smartphone context traces of 94 users validate the proposed algorithm.},
keywords={data privacy;learning (artificial intelligence);minimax techniques;smart phones;stochastic games;ubiquitous computing;privacy preserving context sensing;mobile phone;sensor-equipped smartphones;context-aware application;personalized services;user context;user sensing data;disclosure strategy;privacy preservation mechanisms design;static adversary model;intelligent adversary;context privacy problem;context dynamics;attacking strategy;interactive competition;zero-sum stochastic game;minimax learning algorithm;optimal defense strategy;Context;Privacy;Sensors;Games;Smart phones;Stochastic processes;Context-aware services},
doi={10.1109/INFOCOM.2014.6848177},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848178,
author={H. Li and L. Sun and H. Zhu and X. Lu and X. Cheng},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Achieving privacy preservation in WiFi fingerprint-based localization},
year={2014},
volume={},
number={},
pages={2337-2345},
abstract={WiFi fingerprint-based localization is regarded as one of the most promising techniques for indoor localization. The location of a to-be-localized client is estimated by mapping the measured fingerprint (WiFi signal strengths) against a database owned by the localization service provider. A common concern of this approach that has never been addressed in literature is that it may leak the client's location information or disclose the service provider's data privacy. In this paper, we first analyze the privacy issues of WiFi fingerprint-based localization and then propose a Privacy-Preserving WiFi Fingerprint Localization scheme (PriWFL) that can protect both the client's location privacy and the service provider's data privacy. To reduce the computational overhead at the client side, we also present a performance enhancement algorithm by exploiting the indoor mobility prediction. Theoretical performance analysis and experimental study are carried out to validate the effectiveness of PriWFL. Our implementation of PriWFL in a typical Android smartphone and experimental results demonstrate the practicality and efficiency of PriWFL in real-world environments.},
keywords={computer network security;data privacy;mobile computing;smart phones;wireless LAN;indoor localization;signal strengths;localization service provider;data privacy;privacy-preserving WiFi fingerprint localization scheme;PriWFL;computational overhead reduction;performance enhancement algorithm;indoor mobility prediction;Android smartphone;real-world environments;IEEE 802.11 Standards;Servers;Privacy;Databases;Data privacy;Cryptography;Accuracy;WiFi fingerprint-based localization;location privacy;data privacy;homomorphic encryption},
doi={10.1109/INFOCOM.2014.6848178},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848179,
author={X. Wang and M. Chen and Z. Han and D. O. Wu and T. T. Kwon},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={TOSS: Traffic offloading by social network service-based opportunistic sharing in mobile social networks},
year={2014},
volume={},
number={},
pages={2346-2354},
abstract={The ever increasing traffic demand becomes a serious concern of mobile network operators. To solve this traffic explosion problem, there have been many efforts to offload the traffic from cellular links to direct communications among users. In this paper, we propose the framework of Traffic Offloading assisted by Social network services (SNS) via opportunistic Sharing in mobile social networks, TOSS, to offload SNS-based cellular traffic by user-to-user sharing. First we select a subset of users who are to receive the same content as initial seeds depending on their content spreading impacts in online SNSs and their mobility patterns in offline mobile social networks (MSNs). Then users share the content via opportunistic local connectivity (e.g., Bluetooth, Wi-Fi Direct, Device-to-device in LTE) with each other. The observation of SNS user activities reveals that individual users have distinct access patterns, which allows TOSS to exploit the user-dependent access delay between the content generation time and each user's access time for traffic offloading purposes. We model and analyze the traffic offloading and content spreading among users by taking into account various options in linking SNS and MSN trace data. The trace-driven evaluation demonstrates that TOSS can reduce up to 86.5% of the cellular traffic while satisfying the access delay requirements of all users.},
keywords={Bluetooth;cellular radio;Long Term Evolution;mobile computing;mobility management (mobile radio);social networking (online);telecommunication traffic;wireless LAN;TOSS;SNS-based cellular traffic offloading;social network service-based opportunistic sharing;offline mobile social networks;traffic demand;mobile network operators;traffic explosion problem;cellular links;direct communications;user-to-user sharing;user subset selection;content spreading impacts;online SNS;mobility patterns;opportunistic local connectivity;Bluetooth;Wi-Fi Direct;device-to-device;LTE;access patterns;user-dependent access delay;content generation time;trace-driven evaluation;Delays;Mobile communication;Mobile computing;Social network services;Silicon;Computers;Educational institutions;Traffic Offloading;Mobile Social Networks;Social Network Service;Opportunistic Networks},
doi={10.1109/INFOCOM.2014.6848179},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848180,
author={M. V. Barbera and S. Kosta and A. Mei and V. C. Perta and J. Stefa},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Mobile offloading in the wild: Findings and lessons learned through a real-life experiment with a new cloud-aware system},
year={2014},
volume={},
number={},
pages={2355-2363},
abstract={Mobile-cloud offloading mechanisms delegate heavy mobile computation to the cloud. In real life use, the energy tradeoff of computing the task locally or sending the input data and the code of the task to the cloud is often negative, especially with popular communication intensive jobs like social-networking, gaming, and emailing. We design and build a working implementation of CDroid, a system that tightly couples the device OS to its cloud counterpart. The cloud-side handles data traffic through the device efficiently and, at the same time, caches code and data optimally for possible future offloading. In our system, when offloading decision takes place, input and code are likely to be already on the cloud. CDroid makes mobile cloud offloading more practical enabling offloading of lightweight jobs and communication intensive apps. Our experiments with real users in everyday life show excellent results in terms of energy savings and user experience.},
keywords={cloud computing;mobile computing;user interfaces;mobile offloading decision;cloud-aware system;mobile cloud offloading mechanisms;mobile computation;communication intensive jobs;social networking;gaming;emailing;CDroid;data traffic;caches code;communication intensive apps;user experience;Mobile communication;Smart phones;Servers;Batteries;Synchronization;Security;Computers},
doi={10.1109/INFOCOM.2014.6848180},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848181,
author={F. Mehmeti and T. Spyropoulos},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Is it worth to be patient? Analysis and optimization of delayed mobile data offloading},
year={2014},
volume={},
number={},
pages={2364-2372},
abstract={Operators have recently resorted to WiFi offloading to deal with increasing data demand and induced congestion. Researchers have further suggested the use of “delayed offloading”: if no WiFi connection is available, (some) traffic can be delayed up to a given deadline, or until WiFi becomes available. Nevertheless, there is no clear consensus as to the benefits of delayed offloading, with a couple of recent experimental studies largely diverging in their conclusions. Nor is it clear how these benefits depend on network characteristics (e.g. WiFi availability), user traffic load, etc. In this paper, we propose a queueing analytic model for delayed offloading, and derive the mean delay, offloading efficiency, and other metrics of interest, as a function of the user's “patience”, and key network parameters. We validate the accuracy of our results using a range of realistic scenarios, and use these expressions to show how to optimally choose deadlines.},
keywords={mobile radio;queueing theory;wireless LAN;delayed mobile data offloading;WiFi offloading;data demand;WiFi connection;network characteristics;WiFi availability;user traffic load;queueing analytic model;mean delay;offloading efficiency;IEEE 802.11 Standards;Delays;Mobile communication;Availability;Approximation methods;Markov processes;Equations;Mobile data offloading;Deadlines;Queueing;2D Markov chain;Optimization},
doi={10.1109/INFOCOM.2014.6848181},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848182,
author={L. Xiang and S. Ye and Y. Feng and B. Li and B. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Ready, Set, Go: Coalesced offloading from mobile devices to the cloud},
year={2014},
volume={},
number={},
pages={2373-2381},
abstract={With an abundance of computing resources, cloud computing systems have been widely used to elastically offload the execution of computation-intensive applications on mobile devices, leading to performance gains and better power efficiency. However, existing works have so far focused on one application only, and multiple applications are not coordinated when sending their offloading requests to the cloud. In this paper, we propose the new technique of coalesced offloading, which exploits the potential for multiple applications to coordinate their offloading requests with the objective of saving additional energy on mobile devices. The intuition is that, by sending these requests in “bundles,” the period of time that the network interface stays in the high-power state can be reduced. We present two online algorithms, collectively referred to as Ready, Set, Go (RSG), that make near-optimal decisions on how offloading requests from multiple applications are to be best coalesced. We show, both analytically and experimentally using actual smartphones, that RSG is able to achieve additional energy savings while maintaining satisfactory performance.},
keywords={cloud computing;decision making;power aware computing;smart phones;telecommunication power management;coalesced offloading;mobile devices;computing resources;cloud computing systems;computation-intensive applications;power efficiency;online algorithms;Ready-Set-Go;RSG;near-optimal decision making;smartphones;energy savings;Algorithm design and analysis;Mobile handsets;Network interfaces;Optimization;Heuristic algorithms;Delays;Mobile communication},
doi={10.1109/INFOCOM.2014.6848182},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848183,
author={M. A. Beck and S. A. Henningsen and S. B. Birnbach and J. B. Schmitt},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Towards a statistical network calculus — Dealing with uncertainty in arrivals},
year={2014},
volume={},
number={},
pages={2382-2390},
abstract={The stochastic network calculus (SNC) has become an attractive methodology to derive probabilistic performance bounds. So far the SNC is based on (tacitly assumed) exact probabilistic assumptions about the arrival processes. Yet, in practice, these are only true approximately-at best. In many situations it is hard, if possible at all, to make such assumptions a priori. A more practical approach would be to base the SNC operations on measurements of the arrival processes (preferably even on-line). In this paper, we develop this idea and incorporate measurements into the framework of SNC taking the further uncertainty resulting from estimation errors into account. This is a crucial step towards a statistical network calculus (StatNC) eventually lending itself to a self-modelling operation of networks with a minimum of a priori assumptions. In numerical experiments, we are able to substantiate the novel opportunities by StatNC.},
keywords={calculus;probability;statistical analysis;stochastic processes;statistical network calculus;SNC;stochastic network calculus;probabilistic performance bounds;exact probabilistic assumptions;arrival process measurement;estimation errors;StatNC;self-modelling operation;Calculus;Probabilistic logic;Uncertainty;Computers;Stochastic processes;Conferences;Random variables},
doi={10.1109/INFOCOM.2014.6848183},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848184,
author={F. Ciucu and R. Khalili and Y. Jiang and L. Yang and Y. Cui},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Towards a system theoretic approach to wireless network capacity in finite time and space},
year={2014},
volume={},
number={},
pages={2391-2399},
abstract={In asymptotic regimes, both in time and space (network size), the derivation of network capacity results is grossly simplified by brushing aside queueing behavior in nonJackson networks. This simplifying double-limit model, however, lends itself to conservative numerical results in finite regimes. To properly account for queueing behavior beyond a simple calculus based on average rates, we advocate a system theoretic methodology for the capacity problem in finite time and space regimes. This methodology also accounts for spatial correlations arising in networks with CSMA/CA scheduling and it delivers rigorous closed-form capacity results in terms of probability distributions. Unlike numerous existing asymptotic results, subject to anecdotal practical concerns, our transient results can be used in practical settings, e.g., to compute the time scales at which multi-hop routing is more advantageous than single-hop routing.},
keywords={carrier sense multiple access;probability;scheduling;telecommunication congestion control;telecommunication network routing;system theoretic approach;wireless network capacity;finite time;asymptotic regimes;queueing behavior;nonJackson networks;finite regimes;space regimes;spatial correlations;CSMA/CA scheduling;closed-form capacity;probability distributions;multihop routing;single-hop routing;Multiaccess communication;Routing;Media Access Protocol;Calculus;Upper bound;Relays;Markov processes},
doi={10.1109/INFOCOM.2014.6848184},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848185,
author={R. Singh and I. Hou and P. R. Kumar},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Fluctuation analysis of debt based policies for wireless networks with hard delay constraints},
year={2014},
volume={},
number={},
pages={2400-2408},
abstract={Hou et al. have analyzed wireless networks where clients served by an access point require a timely-throughput of packets to be delivered by hard per-packet deadlines and also proved the timely-throughput optimality of certain debt-based policies. However, this is a weak notion of optimality; there might be long time intervals in which a client does not receive any packets, undesirable for real-time applications. Motivated by this, the authors, in an earlier work, introduced a pathwise cost function based on the law of the iterated logarithm, studied in fluctuation theory, which captures the deviation from a steady stream of packet deliveries and showed that a debt-based policy is optimal if the frame length is one. This work extends the analysis of debt-based policies to general frame lengths greater than one, as is important for general applications.},
keywords={iterative methods;radio networks;debt fluctuation analysis;wireless network;hard delay constraint;access point;debt-based policy;pathwise cost function;iterated logarithm;steady packet delivery stream;Vectors;Throughput;Markov processes;Computers;Limiting;Conferences;Delays},
doi={10.1109/INFOCOM.2014.6848185},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848186,
author={E. Monsef and T. Anjali and S. Kapoor},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Price of Anarchy in network routing with class based capacity guarantees},
year={2014},
volume={},
number={},
pages={2409-2417},
abstract={In this paper, we consider the inefficiency of distributed routing in a network of parallel links with class-based traffic. Network link behavior is modeled by the M/M/1-GPS queue (i.e. when links use General Processor Sharing(GPS) scheduling scheme to serve packets). Each traffic type is guaranteed a minimum capacity rate on each link using GPS scheduling. We show under specific demand conditions that, among multiple equilibria the worst-case Nash equilibrium occurs when each class dispatcher utilizes all the links to fulfill its demand. Using this fact, we give an upper bound on the Price of Anarchy (PoA). Our results also indicate that, while the price of selfish behavior can be unbounded in a specific demand setting, there exist demand regimes where the bound on PoA is reasonable. These results also apply to the resource allocation or load balancing applications in the processor sharing systems.},
keywords={game theory;queueing theory;routing protocols;telecommunication traffic;price-of-anarchy;network routing protocols;class based capacity guarantees;distributed routing inefficiency;parallel links;class-based traffic;network link behavior;M/M/1-GPS queue;GPS scheduling scheme;general processor sharing;minimum capacity rate;worst-case Nash equilibrium;PoA;resource allocation;load balancing;quality-of-service;QoS;Nash equilibrium;Global Positioning System;Routing;Quality of service;Delays;Cost function;Games},
doi={10.1109/INFOCOM.2014.6848186},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848187,
author={M. J. Neely},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Distributed stochastic optimization via correlated scheduling},
year={2014},
volume={},
number={},
pages={2418-2426},
abstract={This paper considers a problem where multiple users make repeated decisions based on their own observed events. The events and decisions at each time step determine the values of a utility function and a collection of penalty functions. The goal is to make distributed decisions over time to maximize time average utility subject to time average constraints on the penalties. An example is a collection of power constrained sensors that repeatedly report their own observations to a fusion center. Maximum time average utility is fundamentally reduced because users do not know the events observed by others. Optimality is characterized for this distributed context. It is shown that optimality is achieved by correlating user decisions through a commonly known pseudorandom sequence. An optimal algorithm is developed that chooses pure strategies at each time step based on a set of time-varying weights.},
keywords={distributed decision making;random sequences;scheduling;set theory;stochastic programming;utility theory;distributed stochastic optimization;correlated scheduling;utility function;penalty functions;distributed decision making;time average utility;power constrained sensors;maximum time average utility;pseudorandom sequence;optimal algorithm;time-varying weights;Vectors;Sensor fusion;Distributed algorithms;Optimization;Conferences;Computers},
doi={10.1109/INFOCOM.2014.6848187},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848188,
author={R. Cohen and G. Grebla},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Multi-dimensional OFDMA scheduling in a wireless network with relay nodes},
year={2014},
volume={},
number={},
pages={2427-2435},
abstract={LTE-advanced and other 4G cellular standards allow relay nodes (RNs) to be deployed as a substitute for base stations (BSs). Unlike a BS, an RN is not directly connected to the backbone. Rather, each RN is associated with a donor BS, to which it is connected through the OFDMA wireless link. A very important task in the operation of a wireless network is packet scheduling. In a network with RNs, such scheduling decisions must be made in each cell not only for the BS, but also for the RNs. Because the scheduler in a network with RNs must take into account the transmission resources of the BS and the RNs, it needs to find a feasible schedule that does not exceed the resources of a multi-dimensional resource pool. This makes the scheduling problem computationally harder than in a network without RNs. In this paper we define and study for the first time the packet-level scheduling problem for a network with RNs. This problem is shown to be not only NP-hard, but also very hard to approximate. To solve it, we propose an approximation with a performance guarantee, and a simple water-filling heuristic. Using simulations, we evaluate our new algorithms and show that they perform very well.},
keywords={approximation theory;computational complexity;frequency division multiple access;OFDM modulation;scheduling;multidimensional OFDMA scheduling;wireless network;relay nodes;OFDMA wireless link;packet scheduling;multidimensional resource pool;packet-level scheduling problem;approximation theory;water-filling heuristic;NP-hard problem;Scheduling;Interference;Relays;Schedules;Signal to noise ratio;Processor scheduling;Bandwidth},
doi={10.1109/INFOCOM.2014.6848188},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848189,
author={H. Wu and X. Lin and X. Liu and Y. Zhang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Application-level scheduling with deadline constraints},
year={2014},
volume={},
number={},
pages={2436-2444},
abstract={Opportunistic scheduling of delay-tolerant traffic has been shown to substantially improve spectrum efficiency. To encourage users to adopt delay-tolerant scheduling for capacity-improvement, it is critical to provide guarantees in terms of completion time. In this paper, we study application-level scheduling with deadline constraints, where the deadline is pre-specified by users/applications and is associated with a deadline violation probability. To address the exponentially-high complexity due to temporally-varying channel conditions and deadline constraints, we develop a novel asymptotic approach that exploits the largeness of the network to our advantage. Specifically, we identify a lower bound on the deadline violation probability, and propose simple policies that achieve the lower bound in the large-system regime. The results in this paper thus provide a rigorous analytical framework to develop and analyze policies for application-level scheduling under very general settings of channel models and deadline requirements. Further, based on the asymptotic approach, we propose the notion of Application-Level Effective Capacity region, i.e., the throughput region that can be supported subject to deadline constraints, which allows us to quantify the potential gain of application-level scheduling.},
keywords={delay tolerant networks;Internet;mobile computing;probability;radio spectrum management;scheduling;telecommunication channels;telecommunication traffic;application-level scheduling;deadline constraints;opportunistic scheduling;delay-tolerant traffic;spectrum efficiency;delay-tolerant scheduling;completion time;deadline violation probability;temporally-varying channel conditions;asymptotic approach;channel models;deadline requirements;application-level effective capacity region;throughput region;Scheduling;Optimal scheduling;Computers;Delays;Conferences;Processor scheduling;Educational institutions},
doi={10.1109/INFOCOM.2014.6848189},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848190,
author={Z. Cao and M. Kodialam and T. V. Lakshman},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Joint static and dynamic traffic scheduling in data center networks},
year={2014},
volume={},
number={},
pages={2445-2453},
abstract={The advent and continued growth of large data centers has led to much interest in switch architectures that can economically meet the high capacities needed for interconnecting the thousands of servers in these data centers. Various multilayer architectures employing thousands of switches have been proposed in the literature. We make use of the observation that the traffic in a data center is a mixture of relatively static and rapidly fluctuating components, and develop a combined scheduler for both these components using a generalization of the load-balanced scheduler. The presence of the known static component introduces asymmetries in the ingress-egress capacities, which preclude the use of a load-balanced scheduler as is. We generalize the load-balanced scheduler and also incorporate an opportunistic scheduler which sends traffic on a direct path when feasible to enhance the overall switch throughput. Our evaluations show that this scheduler works very well despite avoiding the use of a central scheduler for making packet-by-packet scheduling decisions.},
keywords={computer centres;local area networks;multiprocessor interconnection networks;processor scheduling;resource allocation;telecommunication switching;telecommunication traffic;joint static-and-dynamic traffic scheduling;data center networks;multi- layer architectures;load-balanced scheduler;static component;ingress-egress capacities;opportunistic scheduler;overall switch throughput enhancement;packet-by-packet scheduling decisions;top-of-rack Ethernet switch;Optical switches;Dynamic scheduling;Matrix decomposition;Computer architecture;Throughput;Schedules},
doi={10.1109/INFOCOM.2014.6848190},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848191,
author={G. Wang and K. Wu and Q. Zhang and L. M. Ni},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={SimCast: Efficient video delivery in MU-MIMO WLANs},
year={2014},
volume={},
number={},
pages={2454-2462},
abstract={Wireless video stream delivery is choppy. This problem becomes much severer in MU-MIMO's simultaneous video transmission within the same band. Conventional schemes achieve graceful video delivery by harnessing from high data redundancy. However, with concurrent transmission in the same band, the leverage of high data redundancy leads to high probability of collisions and packets loss, which limits the performance. In concurrent video transmission, achieving efficiency over varied link condition is the main issue. To address this issue, this paper presents SimCast (Simultaneous), a cross-layer design for achieving efficient concurrent video uploading/downloading in MU-MIMO WLANs. The key idea of SimCast is to harness frequency diversity of the channel and spatial similarity of users. We implemented SimCast on USRP2 and conducted extensive simulations. Result shows that SimCast achieves higher throughput than traditional schemes by 1.2× on average. Video quality of SimCast outperforms competitive schemes, which is up to 5 dB in PSNR.},
keywords={MIMO communication;multi-access systems;video coding;video streaming;wireless LAN;SimCast;video delivery;MU-MIMO WLAN;wireless video stream delivery;multiuser multiple input multiple output system;simultaneous video transmission;concurrent video transmission;video uploading;video downloading;Decoding;Streaming media;Redundancy;Encoding;Entropy;Wireless communication;Frequency diversity},
doi={10.1109/INFOCOM.2014.6848191},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848192,
author={R. Anwar and K. Nishat and M. Ali and Z. Akhtar and H. Niaz and I. A. Qazi},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Loss differentiation: Moving onto high-speed wireless LANs},
year={2014},
volume={},
number={},
pages={2463-2471},
abstract={A fundamental problem in 802.11 wireless networks is to accurately determine the cause of packet losses. This becomes increasingly important as wireless data rates scale to Gbps, where lack of loss differentiation leads to higher loss in throughput. Recent and upcoming high-speed WLAN standards, such as 802.11n and 802.11ac, use frame aggregation and block acknowledgements for achieving efficient communication. This paper presents BLMon, a framework for loss differentiation, that uses loss patterns within aggregate frames and aggregate frame retries to achieve accurate and low overhead loss differentiation. Towards this end, we carry out a detailed measurement study on a real testbed to ascertain the differences in loss patterns due to noise, collisions, and hidden nodes. We then devise metrics to quantitatively capture these differences. Finally, we design BLMon, which collectively uses these metrics to infer the cause of loss without requiring any out-of-band communication, protocol changes, or customized hardware support. BLMon can be readily deployed on commodity devices using only driver-level changes at the sender-side. We implement BLMon in the ath9k driver and using real testbed experiments, show that it can provide up to 5× improvement in throughput.},
keywords={telecommunication congestion control;telecommunication standards;wireless LAN;testbed experiments;ath9k driver;out-of-band communication;loss patterns;BLMon;burst loss monitor;block acknowledgements;frame aggregation;802.11ac;802.11n;loss differentiation;wireless data rates;packet losses;802.11 wireless networks;high-speed wireless LAN;Bit rate;Receivers;Loss measurement;IEEE 802.11n Standard;Noise;Throughput},
doi={10.1109/INFOCOM.2014.6848192},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848193,
author={J. Ou and Y. Zheng and M. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={MISC: Merging incorrect symbols using constellation diversity for 802.11 retransmission},
year={2014},
volume={},
number={},
pages={2472-2480},
abstract={802.11 WLANs suffer from high packet losses due to interference and noise. Packet retransmission is a fundamental way to recover a lost packet. To extract useful information from incorrect symbols and improve retransmission efficiency, we present MISC, a packet retransmission scheme that merges incorrect symbols from multiple transmissions to produce correct ones. MISC proactively creates constellation diversity by rearranging the constellation maps in retransmissions. MISC addresses practical implementation issues and makes minimum amendments to integrate into current 802.11 WLAN framework. We implement MISC in an 802.11-based GNURadio/USRP platform and conduct extensive experiments to evaluate its efficacy. Experiment results demonstrate that MISC can substantially improve the throughput.},
keywords={diversity reception;encoding;wireless LAN;MISC;merging incorrect symbol;constellation diversity;IEEE 802.11 retransmission;WLAN;extract useful information;packet retransmission scheme;GNURadio platform;USRP platform;IEEE 802.11 Standards;Receivers;Wireless LAN;Decoding;Forward error correction;Transmitters;Noise},
doi={10.1109/INFOCOM.2014.6848193},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848194,
author={J. Huang and G. Xing and G. Zhou},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Unleashing exposed terminals in enterprise WLANs: A rate adaptation approach},
year={2014},
volume={},
number={},
pages={2481-2489},
abstract={The increasing availability of inexpensive off-the-shelf 802.11 hardware has made it possible to deploy access points (APs) densely to ensure the coverage of complex enterprise environments such as business and college campuses. However, dense AP deployment often leads to increased level of wireless contention, resulting in low system throughput. A promising approach to address this issue is to enable the transmission concurrency of exposed terminals in which two senders lie in the range of one another but do not interfere each other's receiver. However, existing solutions ignore the rate diversity of 802.11 and hence cannot fully exploit concurrent transmission opportunities in a WLAN. In this paper, we present <i>TRACK</i> - <i>T</i>ransmission <i>R</i>ate <i>A</i>daptation for <i>C</i>olliding lin<i>K</i>s, a novel protocol for harnessing exposed terminals with a rate adaptation approach in enterprise WLANs. Using measurement-based channel models, TRACK can optimize the bit rates of concurrent links to improve system throughput while maintaining link fairness. Our extensive experiments on a testbed of 17 nodes show that TRACK improves system throughput by up to 67% and 35% over 802.11 CSMA and conventional approaches of harnessing exposed terminals.},
keywords={business communication;computer network reliability;protocols;wireless LAN;unleashing exposed terminals;enterprise WLANs;rate adaptation approach;inexpensive off-the-shelf 802.11 hardware availability;access points;complex enterprise environments;AP deployment;wireless contention level;low system throughput;transmission concurrency;transmission rate adaptation for colliding links;TRACK;protocol;measurement-based channel models;bit rate optimization;link fairness;802.11 CSMA;Bit rate;Interference;Throughput;IEEE 802.11 Standards;Downlink;Aggregates;Benchmark testing},
doi={10.1109/INFOCOM.2014.6848194},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848195,
author={T. Zhang and Z. Li and R. Safavi-Naini},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Incentivize cooperative sensing in distributed cognitive radio networks with reputation-based pricing},
year={2014},
volume={},
number={},
pages={2490-2498},
abstract={In a cognitive radio network, selfish secondary users may not voluntarily contribute to desired cooperative sensing. We design the first fully distributed scheme to incentivize participation of nodes in cooperative sensing, by connecting sensing and spectrum allocation, and offering incentive from latter to the former. Secondary users that are more active and report more accurate sensing values will be given higher reputation values, which results in lower prices in the spectrum allocation phase. Theoretical analysis and simulation results indicate that the proposed method effectively incentivizes sensing participation, and rewards truthful and accurate reporting. Our proposed system is fully distributed and does not rely on a central authority, and so is more applicable in dynamic cognitive radio networks in practice. We also show how to improve the robustness of reputation when malicious nodes report spurious reputation.},
keywords={cognitive radio;cooperative communication;pricing;radio spectrum management;incentivize cooperative sensing;distributed cognitive radio networks;reputation-based pricing;selfish secondary users;spectrum allocation phase;dynamic cognitive radio networks;spurious reputation;Sensors;Resource management;Pricing;Accuracy;Games;Peer-to-peer computing;Cognitive radio;Cognitive Radio Networks;Cooperative Sensing;Reputation;Spectrum Allocation;Incentive;Pricing},
doi={10.1109/INFOCOM.2014.6848195},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848196,
author={X. Ji and J. Wang and M. Liu and Y. Yan and P. Yang and Y. Liu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Hitchhike: Riding control on preambles},
year={2014},
volume={},
number={},
pages={2499-2507},
abstract={Recently, carrying control signals on passing data packets has emerged as a promising direction for efficient control information transmission. With control messages carried on data payload, the extra air time needed for control packets like RTS/CTS is eliminated and thus channel utilization is improved. However, carrying control signals on the data payload of a packet requires the data packet to have a sufficiently large SNR, otherwise both the data packet and the control messages are lost. In this paper, we propose <i>Hitchhike</i>, a technique that utilizes the preamble field to carry control messages. Hitchhike completely decouples the control messages from the payload and therefore the superposition of (multiple) control messages has little adverse effect on the operation of the payload decoding. We implement and evaluate Hitchhike in the USRP2 platform with 5 nodes. Evaluation results demonstrate the feasibility and effectiveness of Hitchhike. Compared with the state-of-the-art, e.g., Side-channel in 802.15.4, Hitchhike improves the detection accuracy of control messages by 40% and reduces the data loss caused by control messages by 15%.},
keywords={data communication;message switching;packet radio networks;riding control;data packet transmission;control information transmission;control messages;data payload;control packet;control signal;Hitchhike;control message decoupling;adverse effect;payload decoding;USRP2;Payloads;Correlation;Signal to noise ratio;Decoding;Receivers;IEEE 802.15 Standards;Accuracy},
doi={10.1109/INFOCOM.2014.6848196},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848197,
author={S. Brahma and M. Duarte and A. Sengupta and I. Wang and C. Fragouli and S. Diggavi},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={QUILT: A Decode/Quantize-Interleave-Transmit approach to cooperative relaying},
year={2014},
volume={},
number={},
pages={2508-2516},
abstract={Physical layer cooperation of a source with a relay can significantly boost the performance of a wireless connection. However, the best practical relaying scheme can vary depending on the relative strengths of the channels that connect the source, relay and destination. This paper proposes and evaluates QUILT, a system for physical-layer relaying that seamlessly adapts to the underlying network configuration to achieve competitive or better performance as compared to the best current approaches. QUILT combines on-demand, opportunistic use of Decode-Forward (DF) or Quantize-Map-Forward (QMF) followed by interleaving at the relay, with hybrid decoding at the destination that extracts information from received frames even if these are not decodable. We theoretically quantify how our design choices for QUILT affect the system performance. We also deploy QUILT on the WarpLab software radio platform, and show through over-the-air experiments up to 5 times FER improvement over the next best cooperative protocol.},
keywords={cooperative communication;decode and forward communication;decoding;relay networks (telecommunication);QUILT;decode-quantize-interleave-transmit approach;cooperative relaying scheme;physical layer cooperation;wireless connection;physical-layer relaying;network configuration;decode-forward approach;quantize-map-forward approach;QMF;hybrid decoding;WarpLab software radio platform;Relays;Decoding;OFDM;Quantization (signal);Vectors;Physical layer;IEEE 802.11 Standards},
doi={10.1109/INFOCOM.2014.6848197},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848198,
author={T. Bansal and B. Chen and P. Sinha},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={FastProbe: Malicious user detection in Cognitive Radio Networks through active transmissions},
year={2014},
volume={},
number={},
pages={2517-2525},
abstract={Sensing white space channels to detect whether a particular channel is free or not is very crucial to the operation of Cognitive Radio Networks (CRNs). Cooperative sensing has been shown to improve the performance of channel sensing. However, cooperative sensing is susceptible to malicious users that may not faithfully follow sensing instructions to save energy and/or time, or to launch denial of service attacks against the network. In this paper, we propose a novel active transmissions based algorithm, FastProbe for detecting malicious users. FastProbe proactively detects malicious users before the CRN causes any interference to the Primary Users. Further, using active transmissions, FastProbe achieves higher detection accuracy while maintaining lower overheads when compared with existing algorithms. Simulations and experiments show that in the presence of malicious nodes operating under 2 different attack models, FastProbe reduces the throughput loss due to sensing by as much as 65% compared to existing algorithms.},
keywords={cognitive radio;cooperative communication;signal detection;malicious user detection;cognitive radio networks;active transmissions;white space channel sensing;CRN;novel active transmissions based algorithm;FastProbe;attack models;cooperative sensing;denial of service attacks;interference;Sensors;Scattering;Propagation losses;Noise;Testing;Throughput;Schedules},
doi={10.1109/INFOCOM.2014.6848198},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848199,
author={H. Li and N. Vaidya},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Optimal CSMA-based wireless communication with worst-case delay and non-uniform sizes},
year={2014},
volume={},
number={},
pages={2526-2534},
abstract={Carrier Sense Multiple Access (CSMA) protocols have been shown to reach the full capacity region for data communication in wireless networks, with polynomial complexity. However, current literature achieves the throughput optimality with an exponential delay scaling with the network size, even in a simplified scenario for transmission jobs with uniform sizes. Although CSMA protocols with order-optimal average delay have been proposed for specific topologies, no existing work can provide worst-case delay guarantee for each job in general network settings, not to mention the case when the jobs have non-uniform lengths while the throughput optimality is still targeted. In this paper, we tackle on this issue by proposing a two-timescale CSMA-based data communication protocol with dynamic decisions on rate control, link scheduling, job transmission and dropping in polynomial complexity. Through rigorous analysis, we demonstrate that the proposed protocol can achieve a throughput utility arbitrarily close to its offline optima for jobs with non-uniform sizes and worst-case delay guarantees, with a tradeoff of longer maximum allowable delay.},
keywords={carrier sense multiple access;computational complexity;optimal CSMA-based wireless communication;worst-case delay;nonuniform sizes;carrier sense multiple access protocols;data communication;wireless networks;polynomial complexity;order-optimal average delay;CSMA-based data communication protocol;rate control;link scheduling;job transmission;Delays;Throughput;Heuristic algorithms;Protocols;Multiaccess communication;Complexity theory;Network topology},
doi={10.1109/INFOCOM.2014.6848199},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848200,
author={D. Lee and D. Yun and J. Shin and Y. Yi and S. Yun},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Provable per-link delay-optimal CSMA for general wireless network topology},
year={2014},
volume={},
number={},
pages={2535-2543},
abstract={In the past few years, an exciting progress has been made on CSMA (Carrier Sense Multiple Access) algorithms that achieve throughput and utility optimality for wireless networks. However, most of these algorithms are known to exhibit poor delay performance making them impractical for implementation. Recently, several papers have addressed the delay issue of CSMA and yet, most of them are limited, in the sense that they focus merely on specific network scenarios with certain conditions rather than general network topology, achieve low delay at the cost of throughput reduction, or lack rigorous provable guarantees. In this paper, we focus on the recent idea of exploiting multiple channels (actually or virtually) for delay reduction in CSMA, and prove that it is <i>per-link delay orderoptimal</i>, i.e., <i>O</i>(1)-asymptotic-delay per link, if the number of virtual channels is logarithmic with respect to mixing time of the underlying CSMA Markov chain. The logarithmic number is typically small, i.e., at most linear with respect to the network size. In other words, our contribution provides not only a provable framework for the multiple-channel based CSMA, but also the required explicit number of virtual-multi-channels, which is of great importance for actual implementation. The key step of our analytic framework lies in using quadratic Lyapunov functions in conjunction with (recursively applying) Lindley equation and Azuma's inequality for obtaining an exponential decaying property in certain queueing dynamics. We believe that our technique is of broad interest in analyzing the delay performances of other general queueing systems.},
keywords={carrier sense multiple access;Markov processes;radio networks;telecommunication network topology;provable per-link delay-optimal CSMA;general wireless network topology;carrier sense multiple access algorithms;throughput reduction cost;delay reduction;CSMA Markov chain;logarithmic number;multiple-channel based CSMA;virtual-multichannels;quadratic Lyapunov functions;Azuma inequality;Lindley equation;exponential decaying property;queueing dynamics;general queueing systems;Multiaccess communication;Delays;Markov processes;Schedules;Throughput;Scheduling algorithms;Network topology},
doi={10.1109/INFOCOM.2014.6848200},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848201,
author={F. Baccelli and B. Błaszczyszyn and C. Singh},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Analysis of a proportionally fair and locally adaptive Spatial Aloha in Poisson Networks},
year={2014},
volume={},
number={},
pages={2544-2552},
abstract={The proportionally fair sharing of the capacity of a Poisson network using Spatial-Aloha leads to closed-form performance expressions in two extreme cases: (1) the case without topology information, where the analysis boils down to a parametric optimization problem leveraging stochastic geometry; (2) the case with full network topology information, which was recently solved using shot-noise techniques. We show that there exists a continuum of adaptive controls between these two extremes, based on local stopping sets, which can also be analyzed in closed form. We also show that these control schemes are implementable, in contrast to the full information case which is not. As local information increases, the performance levels of these schemes are shown to get arbitrarily close to those of the full information scheme. The analytical results are combined with discrete event simulation to provide a detailed evaluation of the performance of this class of medium access controls.},
keywords={access protocols;optimisation;stochastic processes;telecommunication network topology;adaptive spatial Aloha;closed-form performance expressions;parametric optimization problem;stochastic geometry;network topology information;shot-noise techniques;adaptive control;medium access controls;Poisson networks;Receivers;Transmitters;Optimization;Geometry;Interference;Throughput;Network topology},
doi={10.1109/INFOCOM.2014.6848201},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848202,
author={A. Vahid and M. A. Maddah-Ali and A. S. Avestimehr},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Communication through collisions: Opportunistic utilization of past receptions},
year={2014},
volume={},
number={},
pages={2553-2561},
abstract={When several wireless users are sharing the spectrum, packet collision is a simple, yet widely used model for interference. Under this model, when transmitters cause interference at any of the receivers, their collided packets are discarded and need to be retransmitted. However, in reality, that receiver can still store its analog received signal and utilize it for decoding the packets in the future (for example, by successive interference cancellation techniques). In this work, we propose a physical layer model for wireless packet networks that allows for such flexibility at the receivers. We assume that the transmitters will be aware of the state of the channel (i.e. when and where collisions occur, or an unintended receiver overhears the signal) with some delay, and propose several coding opportunities that can be utilized by the transmitters to exploit the available signal at the receivers for interference management (as opposed to discarding them). We analyze the achievable throughput of our strategy in a canonical interference channel with two transmitter-receiver pairs, and demonstrate the gain over conventional schemes. By deriving an outer-bound, we also prove the optimality of our scheme for the corresponding model.},
keywords={interference suppression;packet radio networks;radio receivers;radio transmitters;radiofrequency interference;communication through collisions;past receptions opportunistic utilization;wireless users;packet collision;transmitters;receivers;interference cancellation;physical layer model;wireless packet networks;interference management;interference channel;transmitter-receiver pairs;Receivers;Interference;Transmitters;Encoding;Throughput;Signal to noise ratio;Physical layer;Packet collision;wireless networks;interference;delayed channel state knowledge;physical layer model},
doi={10.1109/INFOCOM.2014.6848202},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848203,
author={A. Lodhi and A. Dhamdhere and C. Dovrolis},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Open peering by Internet transit providers: Peer preference or peer pressure?},
year={2014},
volume={},
number={},
pages={2562-2570},
abstract={Peering agreements between Autonomous Systems affect not only the flow of interdomain traffic but also the economics of the entire Internet ecosystem. The conventional wisdom is that transit providers are selective in choosing their settlement-free peers because they prefer to offer revenue-generating transit service to others. Surprisingly, however, a large percentage of transit providers use an Open peering strategy. What causes this large-scale adoption of Open peering, especially among transit providers? More importantly, what is the impact of this peering trend on the economic performance of the population of transit providers? We approach these questions through game-theoretic modeling and agent-based simulations, capturing the dynamics of peering strategy adoption, inter-network formation and interdomain traffic flow. We explain why transit providers gravitate towards Open peering even though that move may be detrimental to their economic fitness. Finally, we examine the impact of an Open peering variant that requires some coordination among providers.},
keywords={game theory;Internet;Internet transit providers;peer preference;peer pressure;autonomous systems;interdomain traffic;Internet ecosystem;open peering strategy;economic performance;game theoretic modeling;agent based simulations;peering strategy adoption;internetwork formation;interdomain traffic flow;Peer-to-peer computing;Internet;Economics;Games;Computational modeling;Switches;Analytical models;Internet;Autonomous System interconnections;settlement-free peering;peering strategies;economic utility},
doi={10.1109/INFOCOM.2014.6848203},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848204,
author={C. Westphal},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A stable fountain code mechanism for peer-to-peer content distribution},
year={2014},
volume={},
number={},
pages={2571-2579},
abstract={Most peer-to-peer content distribution systems require the peers to privilege the welfare of the overall system over greedily maximizing their own utility. When downloading a file broken up into multiple pieces, peers are often asked to pass on some possible download opportunities of common pieces in order to favor rare pieces. This is to avoid the missing piece syndrome, which throttles the download rate of the peer-to-peer system to that of downloading the file straight from the server. In other situations, peers are asked to stay in the system even though they have collected all the file's pieces and have an incentive to leave right away. We propose a mechanism which allows peers to act greedily and yet stabilizes the peer-to-peer content sharing system. Our mechanism combines a fountain code at the server to generate innovative new pieces, and a prioritization for the server to deliver pieces only to new peers. While by itself, neither the fountain code nor the prioritization of new peers alone stabilizes the system, we demonstrate that their combination does, through both analytical and numerical evaluation.},
keywords={network coding;peer-to-peer computing;fountain code mechanism;peer-to-peer content distribution;file downloading;missing piece syndrome;peer-to-peer content sharing system;innovative piece generation;peer prioritization;numerical evaluation;Peer-to-peer computing;Servers;Stability analysis;Markov processes;Computers;Conferences;Bandwidth},
doi={10.1109/INFOCOM.2014.6848204},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848205,
author={V. Setty and G. Kreitz and G. Urdaneta and R. Vitenberg and M. van Steen},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Maximizing the number of satisfied subscribers in pub/sub systems under capacity constraints},
year={2014},
volume={},
number={},
pages={2580-2588},
abstract={Publish/subscribe (pub/sub) is a popular communication paradigm in the design of large-scale distributed systems. A provider of a pub/sub service (whether centralized, peer-assisted, or based on a federated organization of cooperatively managed servers) commonly faces a fundamental challenge: given limited resources, how to maximize the satisfaction of subscribers? We provide, to the best of our knowledge, the first formal treatment of this problem by introducing two metrics that capture subscriber satisfaction in the presence of limited resources. This allows us to formulate matters as two new flavors of maximum coverage optimization problems. Unfortunately, both variants of the problem prove to be NP-hard. By subsequently providing formal approximation bounds and heuristics, we show, however, that efficient approximations can be attained. We validate our approach using real-world traces from Spotify and show that our solutions can be executed periodically in real-time in order to adapt to workload variations.},
keywords={distributed processing;message passing;middleware;optimisation;pub-sub systems;capacity constraints;subscriber satisfaction;publish-subscribe systems;communication paradigm;large-scale distributed systems;cooperatively managed servers;formal treatment;maximum coverage optimization problems;formal approximation bounds;Servers;Measurement;Approximation methods;TV;Linear programming;Computers;Computer architecture},
doi={10.1109/INFOCOM.2014.6848205},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848206,
author={S. Yao and X. Wang and X. Tian and Q. Zhang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Delay-throughput tradeoff with correlated mobility of ad-hoc networks},
year={2014},
volume={},
number={},
pages={2589-2597},
abstract={We analyze the scaling law in wireless ad hoc networks with the correlated mobility model. The former work about correlated mobility has shown the maximum throughput and the corresponding delay of several sub-cases, but the optimal throughput performances under various delay tolerant condition (the optimal delay-throughput tradeoff) remains open. We study the properties of correlated mobility model and establish the upper bound of delay-throughput tradeoff for several sub-cases. Then we find out an achievable lower bound by studying the optimal scheduling parameters and their constrains. We exploit the node correlation to achieve the delay-throughput tradeoff and give a picture that how node correlation impacts the packet delay, asymptotic throughput, and their tradeoff.},
keywords={ad hoc networks;delays;scheduling;delay-throughput tradeoff;wireless ad hoc networks;correlated mobility model;delay tolerant condition;upper bound;lower bound;optimal scheduling parameters;node correlation;packet delay;asymptotic throughput;scaling law;Delays;Relays;Throughput;Correlation;Upper bound;Unicast;Conferences},
doi={10.1109/INFOCOM.2014.6848206},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848207,
author={K. Zheng and X. Wang and L. Li and X. Wang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Joint power optimization of data center network and servers with correlation analysis},
year={2014},
volume={},
number={},
pages={2598-2606},
abstract={Data center power optimization has recently received a great deal of research attention. For example, server consolidation has been demonstrated as one of the most effective energy saving methodologies. Likewise, traffic consolidation has also been recently proposed to save energy for data center networks (DCNs). However, current research on data center power optimization focuses on servers and DCN separately. As a result, the optimization results are often inferior, because server consolidation without considering the DCN may cause traffic congestion and thus degraded network performance. On the other hand, server consolidation may change the DCN topology, allowing new opportunities for energy savings. In this paper, we propose PowerNetS, a power optimization strategy that leverages workload correlation analysis to jointly minimize the total power consumption of servers and the DCN. The design of PowerNetS is based on the key observations that the workloads of different servers and DCN traffic flows do not peak at exactly the same time. Thus, more energy savings can be achieved if the workload correlations are considered in server and traffic consolidations. In addition, PowerNetS considers the DCN topology during server consolidation, which leads to less inter-server traffic and thus more energy savings and shorter network delays. We implement PowerNetS on a hardware testbed composed of 10 virtual switches configured with a production 48-port OpenFlow switch and 6 servers. Our empirical results with Wikipedia, Yahoo!, and IBM traces demonstrate that PowerNetS can save up to 51.6% of energy for a data center. PowerNetS also outperforms two state-of-the-art baselines by 44.3% and 15.8% on energy savings, respectively. Our simulation results with 72 switches and 122 servers also show the superior energy efficiency of PowerNetS over the baselines.},
keywords={computer centres;computer network management;correlation methods;network servers;telecommunication network topology;telecommunication power management;telecommunication traffic;data center network;data center servers;correlation analysis;data center power optimization;server consolidation;energy saving methodologies;traffic consolidation;traffic congestion;network performance;DCN topology;PowerNetS;power optimization strategy;workload correlation analysis;DCN traffic;interserver traffic;network delays;48-port OpenFlow switch;energy efficiency;Servers;Correlation;Optimization;Power demand;Topology;Joints;Network topology},
doi={10.1109/INFOCOM.2014.6848207},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848208,
author={V. Shah and G. de Veciana},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Performance evaluation and asymptotics for Content Delivery Networks},
year={2014},
volume={},
number={},
pages={2607-2615},
abstract={Large scale Content Delivery Networks (CDNs) are one of the key components of today's information infrastructure. This paper proposes and analyzes a simple stochastic model for a file-server system wherein servers can work together, as a pooled resource, to meet individual user requests. In such systems basic questions include: How and where to replicate files? What is the impact of dynamic service allocation across request types, and whether it can provide substantial gains over simpler load balancing policies? What are tradeoffs amongst performance, reliability and recovery costs, and energy? The paper provides both explicit and asymptotic approximations for large systems towards addressing these basic questions.},
keywords={computer network performance evaluation;file servers;performance evaluation;large scale content delivery networks;CDN;information infrastructure;stochastic model;file-server system;dynamic service allocation;load balancing policies;Servers;Resource management;Delays;Load modeling;Approximation methods;Conferences;Computers},
doi={10.1109/INFOCOM.2014.6848208},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848209,
author={F. Ciucu and J. Schmitt},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={On the catalyzing effect of randomness on the per-flow throughput in wireless networks},
year={2014},
volume={},
number={},
pages={2616-2624},
abstract={This paper investigates the throughput capacity of a flow crossing a multi-hop wireless network, whose geometry is characterized by general randomness laws including Uniform, Poisson, Heavy-Tailed distributions for both the nodes' densities and the number of hops. The key contribution is to demonstrate how the per-flow throughput depends on the distribution of 1) the number of nodes Njinside hops' interference sets, 2) the number of hops K, and 3) the degree of spatial correlations. The randomness in both Nj's and K is advantageous, i.e., it can yield larger scalings (as large as Θ(n)) than in non-random settings. An interesting consequence is that the per-flow capacity can exhibit the opposite behavior to the network capacity, which was shown to suffer from a logarithmic decrease in the presence of randomness. In turn, spatial correlations along the end-to-end path are detrimental by a logarithmic term.},
keywords={correlation methods;geometry;Poisson distribution;radio networks;telecommunication network reliability;per-flow throughput;multihop wireless network;catalyzing effect;throughput capacity;geometry;general randomness laws;heavy-tailed distributions;uniform distributions;Poisson distributions;interference sets;spatial correlations;nonrandom settings;network capacity;logarithmic term;Correlation;Geometry;Relays;Throughput;Interference;Spread spectrum communication;Capacity planning},
doi={10.1109/INFOCOM.2014.6848209},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848210,
author={M. Wei and W. Wang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Greenbench: A benchmark for observing power grid vulnerability under data-centric threats},
year={2014},
volume={},
number={},
pages={2625-2633},
abstract={Smart grid is a cyber-physical system which integrates communication networks into traditional power grid. This integration, however, makes the power grid susceptible to cyber attacks. One of the most distinguished challenges in studying the aftermath of cyber attacks in smart grid lies in <i>data-centric</i> threats. Even though such attacks are critical to the information network, they will result in much more Domino-like impact than they behave in cyber world. This is because for an information-centric network, distorted or delayed information undermines services and applications. But in power grid, these data-centric attacks may result in instable power systems, and further detrimental impact of power supplies. In this paper, we present Greenbench, a benchmark that is designed to evaluate real-time power grid dynamics in response to data-centric attacks. The simulation results provide several counter-intuitive suggestions to both smart grid security research and deployment.},
keywords={power system security;smart power grids;telecommunication networks;telecommunication security;greenbench;power grid vulnerability;data-centric threats;cyber-physical system;communication networks;cyber attacks;Domino-like impact;cyber world;information-centric network;power systems;power supplies;real-time power grid dynamic evaluation;smart grid security research;Green products;Smart grids;PSCAD;Synchronization;Smart meters},
doi={10.1109/INFOCOM.2014.6848210},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848211,
author={A. Bernstein and D. Bienstock and D. Hay and M. Uzunoglu and G. Zussman},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Power grid vulnerability to geographically correlated failures — Analysis and control implications},
year={2014},
volume={},
number={},
pages={2634-2642},
abstract={We consider line outages in the transmission network of the power grid, and specifically those caused by natural disasters or large-scale physical attacks. In such networks, an outage of a line may lead to overload on other lines, thereby leading to their outage. Such a cascade may have devastating effects not only on the power grid but also on the interconnected communication networks. We study a model of such failures and show that it differs from other models used to analyze cascades (e.g., epidemic/percolation-based models). Inspired by methods developed for network-survivability analysis, we show how to identify the most vulnerable locations in the network. We also perform extensive numerical experiments with real grid data to estimate the effects of geographically correlated outages and briefly discuss mitigation methods. The developed techniques can indicate potential locations for grid monitoring, and hence, will have impact on the deployment of the smart-grid networking infrastructure.},
keywords={failure analysis;power transmission lines;power transmission reliability;smart power grids;power grid vulnerability;geographically correlated failures;transmission network;natural disasters;large-scale physical attacks;interconnected communication networks;network-survivability analysis;geographically correlated outage effects;mitigation methods;grid monitoring;smart-grid networking infrastructure;line outages;Power grids;Power system faults;Power system protection;Computational modeling;Numerical models;Computers;Analytical models;Survivability;Geographically-Correlated Failures;Cascading Failures;Power Grid;Network Science},
doi={10.1109/INFOCOM.2014.6848211},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848212,
author={Y. Zhang and M. van der Schaar},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Structure-aware stochastic load management in smart grids},
year={2014},
volume={},
number={},
pages={2643-2651},
abstract={Load management based on dynamic pricing has been advocated as a key approach for demand-side management in smart grids. By appropriately pricing energy, economic incentives are given to consumers to shift their usage away from peak hours, thereby limiting the amount of energy that needs to be produced. However, traditional pricing-based load management methods usually rely on the assumption that the statistics of the system dynamics (e.g. the time-varying electricity price, the arrival distribution of consumers' load demands) are known a priori, which is not true in practice. In this paper, we propose a novel price-dependent load scheduling algorithm which, unlike previous works, can operate optimally in systems where such statistical knowledge is unknown. We consider a power grid system where each consumer is equipped with an energy storage device that has the capability of storing electrical energy during peak hours. Specifically, we allow each consumer to proactively determine the amount of energy to purchase from the utility companies (or energy producers) while taking into consideration that its load demand and the electricity price dynamically vary over time in an a priori unknown manner. We first assume that all the dynamics are known and formulate the real-time load scheduling as a Markov decision process and systematically unravel the structural properties exhibited by the resulting optimal load scheduling policy. By utilizing these structural properties, we then prove that our proposed load scheduling algorithm can learn the system dynamics in an online manner and converge to the optimal solution. A distinctive feature of our algorithm is that it actively exploits partial information about the system dynamics so that less information needs to be learned than when using conventional reinforcement learning methods, which significantly improves the adaptation speed and the runtime performance. Our simulation results demonstrate that the proposed load scheduling algorithm achieves efficiency by more than 30% compared to existing state-of-the-art online learning algorithms.},
keywords={demand side management;energy storage;learning (artificial intelligence);Markov processes;power engineering computing;power system economics;pricing;smart power grids;structure-aware stochastic load management;smart grids;dynamic pricing;demand-side management;energy pricing;economic incentives;price-dependent load scheduling algorithm;power grid system;energy storage device;utility companies;Markov decision process;optimal load scheduling policy;system dynamics;reinforcement learning methods;adaptation speed improvement;runtime performance improvement;Electricity;Heuristic algorithms;Dynamic scheduling;Pricing;Smart grids;Power system dynamics;Energy storage},
doi={10.1109/INFOCOM.2014.6848212},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848213,
author={H. Liang and A. Abdrabou and W. Zhuang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Stochastic information management for voltage regulation in smart distribution systems},
year={2014},
volume={},
number={},
pages={2652-2660},
abstract={In this paper, we study distributed generation (DG) integration in smart grid, with a focus on the voltage regulation in smart distribution systems. To ensure the operation of a smart distribution system at an acceptable voltage level, voltage regulators are deployed at some strategic locations for voltage control. The two-way communication functionality of the smart distribution system is leveraged such that the voltage regulators are coordinated by a distribution substation. Based on the measurement reports from remote terminal units (RTUs) deployed at DG unit and load connection points, stochastic information management is performed by the distribution substation to address the randomness in renewable power generation and load demand. In this paper, we formulate a voltage regulation problem in the smart distribution system based on power flow analysis, while taking into account communication delays. We show that the problem can be represented as a partially observed Markov decision process (POMDP). Since voltage regulation is performed at a relatively low frequency to avoid excessive wear and tear on the voltage regulators, a large amount of measurements should be reported by the RTUs and processed by the distribution substation for optimal voltage regulation. In order to reduce the communication and computational overhead, we further investigate the voltage regulation problem and mathematically prove that a relatively small amount of information is sufficient for the distribution substation to make an optimal decision. The theoretical results are evaluated based on a case study of IEEE 13-bus test system with real DG power generation and demand data.},
keywords={distributed power generation;load flow;Markov processes;power distribution control;voltage control;stochastic information management;voltage regulation;smart distribution systems;distributed generation integration;voltage regulator;remote terminal unit;ditributed generator unit;load connection point;power flow analysis;communication delays;partially observed Markov decision process;Voltage control;Regulators;Delays;Power generation;Substations;Voltage measurement;Markov processes},
doi={10.1109/INFOCOM.2014.6848213},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848214,
author={B. Zhang and K. Ren and G. Xing and X. Fu and C. Wang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={SBVLC: Secure barcode-based visible light communication for smartphones},
year={2014},
volume={},
number={},
pages={2661-2669},
abstract={As an alternative to NFC technology, 2D barcodes have been increasingly used for security-sensitive applications including payments and personal identification. However, the security of barcode-based communication in mobile applications has not been systematically studied. Due to the visual nature, 2D barcodes are subject to eavesdropping when they are displayed on the screen of a smartphone. On the other hand, the fundamental design principles of 2D barcodes make it difficult to add security features. In this paper, we propose SBVLC - a secure system for barcode-based visible light communication (VLC) between smartphones. We formally analyze the security of SBVLC based on geometric models and propose physical security enhancement mechanisms for barcode communication by manipulating screen view angles and leveraging user-induced motions. We then develop two secure data exchange schemes. These schemes are useful in many security-sensitive mobile applications including private information sharing, secure device pairing, and mobile payment. SBVLC is evaluated through extensive experiments on both Android and iOS smartphones.},
keywords={Android (operating system);bar codes;electronic data interchange;mobile commerce;near-field communication;radiofrequency identification;smart phones;telecommunication security;Android smartphones;iOS smartphones;mobile payment;secure device pairing;private information sharing;security sensitive mobile application;secure data exchange scheme;user induced motion;screen view angle manipulation;physical security enhancement mechanism;geometric model;SBVLC;eavesdropping;personal identification;payments identification;security sensitive application;2D barcodes;NFC technology;secure barcode-based visible light communication;Receivers;Smart phones;Cameras;Security;Solid modeling;Three-dimensional displays;Decoding},
doi={10.1109/INFOCOM.2014.6848214},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848215,
author={L. Yang and Y. Qi and J. Fang and X. Ding and T. Liu and M. Li},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Frogeye: Perception of the slightest tag motion},
year={2014},
volume={},
number={},
pages={2670-2678},
abstract={Existing methods in RFID systems often employ presence or absence fashion to detect the tags' motions, so they cannot meet motion detection requirement in many applications. Our recent observations suggest that the signal strength backscattered from the tag is hypersensitive to its position, inspiring us to perceive the tag motion through its radio signal strength changes. Motion perception is not trivial and challenged by weak stability of strength in that any other interference or noise may incur significant changes as well, resulting in high false positives. To tackle this issue, we propose to model the strength via the Mixture of Gaussian Model (MoG). The problem is thus converted to foreground segment in computer vision with the help of Strength Image, where the technique of MoG based background subtraction is employed. We then implement a prototype using commercial off-the-shelf products. The evaluation results show that the slightest tag motion (~ 10cm) can be precisely perceived, and the accuracy is up to 92.34% while the false positive is suppressed under 0.5%.},
keywords={Gaussian processes;mixture models;radiofrequency identification;RFID system;Frogeye;signal strength backscattering;radio signal strength;motion perception;mixture of Gaussian model;MoG based background subtraction;commercial off-the-shelf products;Conferences;Computers;RFID;Motion detection;Background substruction;Mixture of Gaussian Model},
doi={10.1109/INFOCOM.2014.6848215},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848216,
author={G. E. Santagati and T. Melodia},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Sonar inside your body: Prototyping ultrasonic intra-body sensor networks},
year={2014},
volume={},
number={},
pages={2679-2687},
abstract={Biomedical systems of implanted miniaturized sensors and actuators interconnected into an intra-body area network could enable revolutionary healthcare and clinical applications. Given the well-understood limitations of radio frequency (RF) propagation in the human body, in our previous work we investigated the use of ultrasonic waves as an alternative physical carrier of information [1], and proposed Ultrasonic WideBand (UsWB), an ultrasonic multipath-resilient integrated physical and medium access control (MAC) layer protocol [2]. In this paper, we discuss the design and implementation of a software-defined testbed architecture for ultrasonic intra-body area networks, and propose the first experimental demonstration of the feasibility of ultrasonic communications in biological tissues. We first discuss in detail our FPGA-based prototype implementation of UsWB. We then demonstrate how the prototype can flexibly trade performance off for power consumption, and achieve, for bit error rates (BER) no higher than 10-6, either (i) high-data rate transmissions up to 700 kbit/s at a transmit power of -14 dBm (≈ 40 μW), or (ii) low-data rate and lower-power transmissions down to -21dBm (≈ 8μW) at 70kbit/s. Finally, we show how the UsWB MAC protocol allows multiple transmitter-receiver pairs to coexist and dynamically adapt the transmission rate according to channel and interference conditions to maximize throughput while satisfying predefined reliability constraints.},
keywords={access protocols;biological tissues;biomedical telemetry;biomedical ultrasonics;body sensor networks;broadband networks;data communication;error statistics;multipath channels;power consumption;prosthetics;prototypes;radiowave propagation;telemedicine;ultrasonic propagation;sonar;ultrasonic intrabody sensor network prototyping;biomedical systems;miniaturized sensor implantaion;actuator interconnection;ultrasonic intrabody area network;healthcare;clinical applications;radiofrequency propagation limitations;RF propagation limitations;ultrasonic waves;physical carrier;information carrier;ultrasonic wideband;ultrasonic multipath-resilient integrated physical layer protocol;medium access control layer protocol;MAC layer protocol;design;software-defined testbed architecture;biological tissue ultrasonic communications;FPGA-based prototype implementation;UsWB implementation;prototype performance;prototype power consumption;bit error rates;BER;high-data rate transmissions;low-data rate transmissions;lower-power transmissions;UsWB MAC protocol;multiple transmitter-receiver pairs;transmission rate;channel conditions;interference conditions;throughput maximization;reliability constraints;Acoustics;Field programmable gate arrays;Computer architecture;Phantoms;Radio frequency;Media Access Protocol;Transducers},
doi={10.1109/INFOCOM.2014.6848216},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848217,
author={Z. Zhou and Z. Yang and C. Wu and W. Sun and Y. Liu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={LiFi: Line-Of-Sight identification with WiFi},
year={2014},
volume={},
number={},
pages={2688-2696},
abstract={Wireless LANs, especially WiFi, have been pervasively deployed and have fostered myriad wireless communication services and ubiquitous computing applications. A primary concern in designing each scenario-tailored application is to combat harsh indoor propagation environments, particularly Non-Line-Of-Sight (NLOS) propagation. The ability to distinguish Line-Of-Sight (LOS) path from NLOS paths acts as a key enabler for adaptive communication, cognitive radios, robust localization, etc. Enabling such capability on commodity WiFi infrastructure, however, is prohibitive due to the coarse multipath resolution with mere MAC layer RSSI. In this work, we dive into the PHY layer and strive to eliminate irrelevant noise and NLOS paths with long delays from the multipath channel responses. To further break away from the intrinsic bandwidth limit of WiFi, we extend to the spatial domain and harness natural mobility to magnify the randomness of NLOS paths while retaining the deterministic nature of the LOS component. We prototype LiFi, a statistical LOS identification scheme for commodity WiFi infrastructure and evaluate it in typical indoor environments covering an area of 1500 m<sup>2</sup>. Experimental results demonstrate an overall LOS identification rate of 90.4% with a false alarm rate of 9.3%.},
keywords={radiofrequency interference;wireless LAN;LiFi;line-of-sight identification;Wi-Fi;wireless LAN;harsh indoor propagation environment;non-line-of-sight propagation;physical layer;noise elimination;harness natural mobility;IEEE 802.11 Standards;Delays;Feature extraction;Rician channels;Noise;Wireless communication;Bandwidth},
doi={10.1109/INFOCOM.2014.6848217},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848218,
author={Q. Yan and H. Zeng and T. Jiang and M. Li and W. Lou and Y. T. Hou},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={MIMO-based jamming resilient communication in wireless networks},
year={2014},
volume={},
number={},
pages={2697-2706},
abstract={Reactive jamming is considered the most powerful jamming attack as the attack efficiency is maximized while the risk of being detected is minimized. Currently, there are no effective anti-jamming solutions to secure OFDM wireless communications under reactive jamming attack. On the other hand, MIMO has emerged as a technology of great research interest in recent years mostly due to its capacity gain. In this paper, we explore the use of MIMO technology for jamming resilient OFDM communication, especially its capability to communicate against the powerful reactive jammer. We first investigate the jamming strategies and their impacts on the OFDM-MIMO receivers. We then present a MIMO-based anti-jamming scheme that exploits interference cancellation and transmit precoding capabilities of MIMO technology to turn a jammed non-connectivity scenario into an operational network. Our testbed evaluation shows the destructive power of reactive jamming attack, and also validates the efficacy and efficiency of our defense mechanisms.},
keywords={jamming;MIMO communication;OFDM modulation;precoding;radio receivers;telecommunication security;MIMO-based jamming resilient communication;wireless networks;OFDM wireless communications;reactive jamming attack;OFDM-MIMO receivers;antijamming scheme;interference cancellation;precoding;Jamming;Channel estimation;OFDM;MIMO;Integrated circuits;Receivers;Decoding},
doi={10.1109/INFOCOM.2014.6848218},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848219,
author={T. Wu and W. Liao and C. Chang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={CACH: Cycle-Adjustable Channel hopping for control channel establishment in cognitive radio networks},
year={2014},
volume={},
number={},
pages={2706-2714},
abstract={Establishing control channels in a cognitive radio network (CRN) is an important and challenging problem. To cope with the problem of control channel saturation and the problem of channel blocking by primary users, channel hopping (CH) schemes are commonly used in the literature for control channel establishment in CRNs. There are three metrics that are widely used for evaluating the performance of CH schemes: (i) degree of overlapping (the number of distinct rendezvous channels), (ii) worst case time-to-rendezvous (TTR), and (iii) system load. In this paper, we focus on the symmetric and synchronous setting and propose a novel Cycle-Adjustable Channel Hopping (CACH) scheme that outperforms several existing CH schemes, including SSCH and QCH, in terms of the three metrics. The key idea of CACH is to create an additional layer of logical channels on the top of physical channels so that the cycle of channel hopping sequences can be adjusted to optimize system performance. The mathematic tools for our scheme are based on the operations in Galois fields that are more general than the prime number modular arithmetic used in SSCH. We show that CACH is much more general than SSCH and it can achieve the maximum degree of overlapping while allowing the worst case TTR to be adjustable. It is also much better than QCH in terms of reducing system load while keeping the same degree of overlapping and the same worst case TTR. Our simulation results show that CACH outperforms several existing schemes in many other aspects, including throughput, and robustness to the disturbance of PUs.},
keywords={cognitive radio;mathematical analysis;telecommunication control;wireless channels;channel blocking;primary users;CH;time-to-rendezvous;TTR;CACH scheme;logical channels;physical channels;channel hopping sequences;mathematic tools;control channel saturation;CRN;cognitive radio networks;control channel establishment;cycle adjustable channel hopping;Barium;Conferences;Computers;Hafnium;Measurement;Galois fields;Cognitive radio;Cognitive radio;multiple rendezvous;dynamic channel hopping;Galois field},
doi={10.1109/INFOCOM.2014.6848219},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848220,
author={B. Gao and Y. Yang and J. J. Park},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A credit-token-based spectrum etiquette framework for coexistence of heterogeneous cognitive radio networks},
year={2014},
volume={},
number={},
pages={2715-2723},
abstract={The coexistence of cognitive radio (CR) networks in the same swath of spectrum has become an increasingly important problem, which is especially challenging when coexisting networks are heterogeneous (i.e., use different air interface standards), such as the case in TV white spaces. In this paper, we propose a credit-token-based spectrum etiquette framework that enables spectrum sharing among distributed heterogeneous CR networks with equal priority. Specifically, we propose a game-auction coexistence framework. Each network acts as either an offerer or a requester, and coexists with other networks via a non-cooperative game and a truthful multi-winner auction. The framework addresses the trade-offs among social welfare and offerer's revenue in the auction and requester's utility in the game. We prove that the framework guarantees system stability. Our simulation results show that the proposed coexistence framework always converges to a near-optimal distributed solution and improves coexistence fairness and spectrum utilization.},
keywords={cognitive radio;game theory;spectrum utilization;near-optimal distributed solution;noncooperative game;auction coexistence;distributed heterogeneous CR networks;TV white spaces;air interface standards;spectrum swath;heterogeneous cognitive radio networks;credit-token-based spectrum etiquette framework;Games;Cost accounting;Stability analysis;Pricing;Computers;White spaces;Switches},
doi={10.1109/INFOCOM.2014.6848220},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848221,
author={Y. Han and E. Ekici and H. Kremo and O. Altintas},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Throughput-efficient channel allocation in multi-channel cognitive vehicular networks},
year={2014},
volume={},
number={},
pages={2724-2732},
abstract={Recent studies show that the Dedicated Short Range Communication (DSRC) band allocated to vehicular networks is insufficient to carry the wireless traffic load generated by emerging applications for vehicular systems. A promising bandwidth expansion possibility presents itself through the release of large TV band spectra by FCC for cognitive access. One of the primary challenges of the so-called TV White Space (TVWS) access in vehicular networks is the design of efficient channel allocation mechanisms in face of high vehicular mobility and spatial-temporal variations of TVWS. In this paper, we address the channel allocation problem for multi-channel cognitive vehicular networks with the objective of system-wide throughput maximization. We show that the problem is a NP-hard combinatorial optimization problem, to which we present two solution approaches. We first propose a probabilistic polynomial-time (1 - 1/e)-approximation algorithm based on linear programming. Next, we prove that our objective function can be written as a submodular set function, based on which we develop a deterministic polynomial-time constant-factor approximation algorithm with a more favorable time complexity. Finally, we show the efficacy of our algorithms through numerical examples.},
keywords={channel allocation;cognitive radio;computational complexity;mobile radio;optimisation;polynomial approximation;road vehicles;telecommunication traffic;throughput-efficient channel allocation;multichannel cognitive vehicular network;dedicated short range communication;DSRC;wireless traffic load;vehicular system application;bandwidth expansion possibility;TV band spectra;FCC;cognitive access;TV white space;TVWS access;channel allocation mechanism;high vehicular mobility;spatial-temporal variation;system-wide throughput maximization;NP-hard combinatorial optimization problem;probabilistic polynomial time approximation algorithm;linear programming;submodular set function;deterministic polynomial-time constant-factor approximation;time complexity;Vehicles;Polynomials;Throughput;Channel allocation;Ellipsoids;Linear programming},
doi={10.1109/INFOCOM.2014.6848221},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848222,
author={D. M. Blough and G. Resta and P. Santi},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Interference-aware proportional fairness for multi-rate wireless networks},
year={2014},
volume={},
number={},
pages={2733-2741},
abstract={In this paper, we consider how proportional fairness in wireless networks is impacted by spatial reuse and the interference it produces. We observe that, in scenarios where spatial reuse is possible (e.g., in high-density WLAN environments), the classic notion of time-based proportional fairness can be severely impacted: some users might experience very large interference penalties while other users might get larger bandwidth proportions than what they would have received with time-based proportional fairness and no spatial reuse. To account for this, we introduce the concept of interference-aware STDMA time-based proportional fairness (i-STPF), and compare it to ordinary STDMA time-based proportional fairness (STPF). We present an εi-STPF scheduling algorithm, and prove that it approximates the time-based fair bandwidth allocation (up to a small positive constant ε), while providing an aggregate throughput that is within a constant factor from optimal. We also present a heuristic i-STPF scheduling algorithm and compare it through simulation to a similar heuristic STPF scheduler, and to an interference-aware, rate-based scheduler. The results show that the i-STPF scheduler: i) achieves excellent aggregate throughput about 35% higher than rate-based throughput; and ii) maintains a close approximation to time-based fairness without interference.},
keywords={radiofrequency interference;scheduling;time division multiple access;wireless LAN;interference-aware proportional fairness;multirate wireless networks;spatial reuse;high-density WLAN environment;interference penalties;bandwidth proportion;interference-aware STDMA time-based proportional fairness;εi-STPF scheduling algorithm;time-based fair bandwidth allocation;heuristic i-STPF scheduling algorithm;interference-aware rate-based scheduler;aggregate throughput;rate-based throughput;Interference;Throughput;Bandwidth;Signal to noise ratio;Schedules;Aggregates;Wireless networks},
doi={10.1109/INFOCOM.2014.6848222},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848223,
author={Y. Zhang and L. Lazos and K. Chen and B. Hu and S. Shivaramaiah},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={FD-MMAC: Combating multi-channel hidden and exposed terminals using a single transceiver},
year={2014},
volume={},
number={},
pages={2742-2750},
abstract={We address the problem of improving the throughput and delay efficiency of distributed multi-channel MAC (MMAC) protocols. We design an MMAC protocol called FD-MMAC that exploits recent advances in full-duplex (FD) communications to coordinate channel access in a distributed manner. Compared with prior MMAC designs, the FD-MMAC protocol eliminates the use of in-band or out-of-band control channels for combating the multi-channel hidden terminal problem, discovering the resident channel of destinations, and performing load balancing. Furthermore, FD-MMAC improves the spectral efficiency by enabling the operation of multi-channel exposed terminals. To achieve its goals, FD-MMAC integrates an advanced suite of PHY-layer techniques, including self interference suppression, error vector magnitude and received power measurements, and signal correlation techniques. We validate the proposed PHY-layer techniques on NI USRP devices. Further, we show via simulations that FD-MMAC achieves significantly higher throughput and lower delay compared with prior art.},
keywords={access protocols;delays;interference suppression;radio networks;radio transceivers;resource allocation;single transceiver;delay efficiency;distributed multichannel MAC protocols;full-duplex communication;channel access;FD-MMAC protocol;out-of-band control channels;in-band control channels;multichannel hidden terminal problem;multichannel exposed terminals;resident channel;load balancing;spectral efficiency;PHY-layer techniques;self interference suppression;error vector magnitude;received power measurements;signal correlation techniques;NI USRP devices;low-power wireless environments;Protocols;Correlation;Sensors;Delays;Computers;Load management;Conferences},
doi={10.1109/INFOCOM.2014.6848223},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848224,
author={A. Ogierman and A. Richa and C. Scheideler and S. Schmid and J. Zhang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Competitive MAC under adversarial SINR},
year={2014},
volume={},
number={},
pages={2751-2759},
abstract={This paper considers the problem of how to efficiently share a wireless medium which is subject to harsh external interference or even jamming. While this problem has already been studied intensively for simplistic single-hop or unit disk graph models, we make a leap forward and study MAC protocols for the SINR interference model (a.k.a. the physical model). We make two contributions. First, we introduce a new adversarial SINR model which captures a wide range of interference phenomena. Concretely, we consider a powerful, adaptive adversary which can jam nodes at arbitrary times and which is only limited by some energy budget. The second contribution of this paper is a distributed MAC protocol which provably achieves a constant competitive throughput in this environment: we show that, with high probability, the protocol ensures that a constant fraction of the non-blocked time periods is used for successful transmissions. Our results also highlight an inherent difference between the SINR model and unit disk graph models.},
keywords={access protocols;jamming;radiocommunication;competitive MAC;adversarial SINR;wireless media;harsh external interference;jamming;MAC protocols;SINR interference model;adaptive adversary;unit disk graph model;Interference;Media Access Protocol;TV;Aggregates;Signal to noise ratio},
doi={10.1109/INFOCOM.2014.6848224},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848225,
author={R. Combes and A. Proutiere and D. Yun and J. Ok and Y. Yi},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Optimal Rate Sampling in 802.11 systems},
year={2014},
volume={},
number={},
pages={2760-2767},
abstract={Rate Adaptation (RA) is a fundamental mechanism in 802.11 systems. It allows transmitters to adapt the coding and modulation scheme as well as the MIMO transmission mode to the radio channel conditions, and in turn, to learn and track the (mode, rate) pair providing the highest throughput. So far, the design of RA mechanisms has been mainly driven by heuristics. In contrast, in this paper, we rigorously formulate such design as an online stochastic optimisation problem. We solve this problem and present ORS (Optimal Rate Sampling), a family of (mode, rate) pair adaptation algorithms that provably learn as fast as it is possible the best pair for transmission. We study the performance of ORS algorithms in stationary radio environments where the successful packet transmission probabilities at the various (mode, rate) pairs do not vary over time, and in non-stationary environments where these probabilities evolve. We show that under ORS algorithms, the throughput loss due to the need to explore sub-optimal (mode, rate) pairs does not depend on the number of available pairs. This is a crucial advantage as evolving 802.11 standards offer an increasingly large number of (mode, rate) pairs. We illustrate the efficiency of ORS algorithms (compared to the state-of-the-art algorithms) using simulations and traces extracted from 802.11 test-beds.},
keywords={MIMO communication;optimisation;stochastic processes;transmitters;wireless channels;wireless LAN;optimal rate sampling;802.11 systems;rate adaptation;RA;transmitters;modulation scheme;coding scheme;MIMO transmission mode;radio channel conditions;online stochastic optimisation problem;ORS;stationary radio environments;packet transmission probabilities;Algorithm design and analysis;Throughput;MIMO;Radio transmitters;Optimization;IEEE 802.11n Standard},
doi={10.1109/INFOCOM.2014.6848225},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848226,
author={J. Liu and H. Nishiyama and N. Kato and J. Ma and X. Jiang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Throughput-delay tradeoff in mobile ad hoc networks with correlated mobility},
year={2014},
volume={},
number={},
pages={2768-2776},
abstract={Reference Point Group Mobility (RPGM) has been a practical mobility model used to efficiently capture the potential correlation among mobile nodes in many important applications. In this paper, we explore the throughput-delay tradeoff in a mobile ad hoc network (MANET) operating under the RPGM model and also a general setting of node moving speed. In particular, we consider a MANET with unit area and n nodes being divided evenly into Θ(nα) groups, α ∈ [0,1], where the center of each group moves according to a random direction model with speed no more than v ∈ [0,1]. We determine the regions of per node throughput, average delay and their tradeoffs that can be achieved (in order sense) in such a network. For the regime of v =0, we first prove that the per node throughput capacity is Θ(n-α/2), and then develop a routing scheme to achieve this capacity, resulting an average delay of Θ(max1/2, n1-α) for any α ∈ [0,1]. Regarding the regime of v > 0, we prove that the per node throughput capacity there can be improved to Θ(1), which is achievable by adopting a new routing scheme with an average delay of Θ(max{n1-α, na/2/v}) for v = o(1) and Θ(n) for v = Θ(1). The results in this paper help us to have a deep understanding on the fundamental performance scaling laws and also enable an efficient throughput-delay tradeoff to be achieved in MANETs with correlated mobility.},
keywords={mobile ad hoc networks;mobility management (mobile radio);telecommunication network routing;throughput-delay tradeoff;mobile ad hoc networks;correlated mobility;reference point group;RPGM model;mobile nodes;MANET;throughput capacity;routing scheme;Throughput;Delays;Relays;Mobile ad hoc networks;Routing;Computers;Upper bound},
doi={10.1109/INFOCOM.2014.6848226},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848227,
author={Y. Zhu and W. Wu and J. Willson and L. Ding and L. Wu and D. Li and W. Lee},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={An approximation algorithm for client assignment in client/server systems},
year={2014},
volume={},
number={},
pages={2777-2785},
abstract={One type of distributed systems is the client/server system consist of clients and servers. In order to improve the performance of such a system, client assignment strategy plays an important role. There are two criteria to evaluate the load on the servers - total load and load balance. The total load increases when the load balance decreases, vice versa. It has been proved that finding the best client assignment is NP-hard. In this paper, we propose a new model for the client assignment problem and design an algorithm based on Semidefinite programming (SDP). Our method has a (relaxed) performance ratio 0.87 when only 2 servers exist. In general case, our method becomes a heuristic, and the ratio of each iteration is 0.87. We are the first one to give these bounds. Our simulation results are compared with the state-of-art client assignment method, and our strategy outperforms it in terms of running time while keeps the load in similar level.},
keywords={client-server systems;mathematical programming;resource allocation;approximation algorithm;client-server systems;distributed systems;client assignment strategy;server total load;load balancing;NP-hard problem;semidefinite programming;Servers;Vectors;Educational institutions;Computers;Approximation algorithms;Computer architecture;Clustering algorithms},
doi={10.1109/INFOCOM.2014.6848227},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848228,
author={A. Izagirre and U. Ayesta and I. M. Verloop},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Sojourn time approximations in a multi-class time-sharing server},
year={2014},
volume={},
number={},
pages={2786-2794},
abstract={We study a multi-class time-sharing discipline with relative priorities known as Discriminatory Processor Sharing (DPS), which provides a natural framework to model service differentiation in systems. The analysis of DPS is extremely challenging and analytical results are scarce. We develop closed-form approximations for the mean conditional and unconditional sojourn times. The main benefits of the approximations lie in its simplicity, the fact that it applies for general service requirements with finite second moments, and that it provides insights into the dependency of the performance on the system parameters. We show that the approximation for the mean (un)conditional sojourn time of a customer is decreasing as its relative priority increases. We also show that the approximation is exact in various scenarios, and that it is uniformly bounded in the second moments of the service requirements. Finally we numerically illustrate that the approximation is accurate across a broad range of parameters.},
keywords={approximation theory;queueing theory;sojourn time approximations;multiclass time-sharing server;discriminatory processor sharing;DPS;service differentiation model;closed-form approximation;mean conditional sojourn time;unconditional sojourn time;service requirements;finite second moments;relative priority;queueing model;Equations;Interpolation;Mathematical model;Numerical models;Conferences;Computers},
doi={10.1109/INFOCOM.2014.6848228},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848229,
author={X. Xu and C. Lee and D. Y. Eun},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={A general framework of hybrid graph sampling for complex network analysis},
year={2014},
volume={},
number={},
pages={2795-2803},
abstract={Being able to capture the properties of massive real graphs and also greatly reduce data scale and processing complexity, graph sampling techniques provide an efficient tool for complex network analysis. Random walk-based sampling has become popular to obtain asymptotically uniform samples in the recent literature. However, it produces highly correlated samples and often leads to poor estimation accuracy in sampling large networks. Another widely-used approach is to launch random jump by querying randomly generated user/node ID, but also has the drawback of unexpected cost when the ID space is sparsely populated. In this paper, we develop a hybrid graph sampling framework that inherits the benefit of returning immediate samples from random walk-based crawling, while incorporating the advantage of reducing the correlation in the obtained samples from random jump. We aim to strike the right balance between random jump and crawling by analyzing the resulting asymptotic variance of an estimator of any graph nodal property, in order to give guidelines on the design of better graph sampling methods. We also provide simulation results on real network (graph) to confirm our theoretical findings.},
keywords={data reduction;graph theory;network theory (graphs);random processes;sampling methods;social networking (online);complex network analysis;massive real graphs;data scale reduction;data processing complexity;ID space;hybrid graph sampling framework;random walk-based crawling;random jump;asymptotic variance analysis;graph nodal property;hybrid graph sampling;Markov processes;Estimation;Sampling methods;Eigenvalues and eigenfunctions;Computers;Complex networks;Correlation},
doi={10.1109/INFOCOM.2014.6848229},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848230,
author={J. Won and C. Y. T. Ma and D. K. Y. Yau and N. S. V. Rao},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Proactive fault-tolerant aggregation protocol for privacy-assured smart metering},
year={2014},
volume={},
number={},
pages={2804-2812},
abstract={Smart meters are integral to demand response in emerging smart grids, by reporting the electricity consumption of users to serve application needs. But reporting real-time usage information for individual households raises privacy concerns. Existing techniques to guarantee differential privacy (DP) of smart meter users either are not fault tolerant or achieve (possibly partial) fault tolerance at high communication overheads. In this paper, we propose a fault-tolerant protocol for smart metering that can handle general communication failures while ensuring DP with significantly improved efficiency and lower errors compared with the state of the art. Our protocol handles fail-stop faults proactively by using a novel design of future ciphertexts, and distributes trust among the smart meters by sharing secret keys among them. We prove the DP properties of our protocol and analyze its advantages in fault tolerance, accuracy, and communication efficiency relative to competing techniques. We illustrate our analysis by simulations driven by real-world traces of electricity consumption.},
keywords={fault tolerance;smart meters;proactive fault-tolerant aggregation protocol;privacy-assured smart metering;fail-stop faults;ciphertexts;secret key sharing;communication efficiency;electricity consumption;Protocols;Smart meters;Privacy;Fault tolerance;Fault tolerant systems;Noise;Bandwidth},
doi={10.1109/INFOCOM.2014.6848230},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848231,
author={F. Kong and C. Dong and X. Liu and H. Zeng},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Blowing hard is not all we want: Quantity vs quality of wind power in the smart grid},
year={2014},
volume={},
number={},
pages={2813-2821},
abstract={The growing awareness about global climate change has boosted the need to mitigate greenhouse gas emissions from existing power systems and spurred efforts to accelerate the integration of renewable energy sources (e.g. wind and solar power) into the electrical grid. A fundamental difficulty here is that renewable energy sources are usually of high variability. The electrical grid must absorb this variability through employing many additional operations (e.g., operating reserves, energy storage), which will largely raise the cost of electricity from renewable energy sources. To make it affordable, numerous advancements in technologies and methods for the smart grid are required. In this paper, we will confine ourselves to one of them: how to plan the construction of wind farms with high capacity and low variability locally and distributedly. We first study the characteristics of both wind resources and wind turbines and present a more accurate wind power evaluation method based on Gaussian Regression. Then, we analyze a trade-off between wind power's quantity and quality and propose an approach to optimally combine different types of wind turbines to balance the trade-off for a specific site. Finally, we explore geographical diversity among different sites and develop an extended approach that jointly optimizes the combination of sites and turbine types. Extensive experiments using the realistic historical wind resource data are conducted for either of the local and distributed case. Encouraging results are shown for the proposed approaches and some interesting insights are also provided.},
keywords={Gaussian processes;power generation planning;power supply quality;regression analysis;smart power grids;wind power plants;wind turbines;wind power;global climate change;greenhouse gas emission mitigation;power system;renewable energy source;solar power;electrical smart grid;operating reserve;energy storage;wind farm construction plan;wind turbine;wind power evaluation method;Gaussian regression analysis;geographical diversity;historical wind resource data;power quality;Wind turbines;Wind speed;Wind power generation;Renewable energy sources;Equations},
doi={10.1109/INFOCOM.2014.6848231},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848232,
author={D. Shin and S. He and J. Zhang},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Robust and cost-effective architecture design for smart grid communications: A multi-stage middleware deployment approach},
year={2014},
volume={},
number={},
pages={2822-2830},
abstract={Wide-area monitoring, protection and control (WAMPAC) plays a critical role in smart grid, for protection against possible contingencies, by using the Supervisory Control and Data Acquisition (SCADA) system. However, a general consensus is that such a hierarchical system can be highly vulnerable to component (i.e., nodes and links) failures, calling for arobustandcost-effectivecommunication system for smart grid. To this end, we consider amiddlewareapproach to leverage the existing commercial communication infrastructure with abundant connectivity. In this approach, a natural question is how to use the middleware to cohesively “glue” the power grid and the commercial communication infrastructure together, in order to enhance robustness and cost-effectiveness. We tackle this problem while taking into consideration themulti-stage deploymentof power devices and their redundant connections. We show that this problem can be cast as aminimum-cost middleware design under incremental deployment-an “online” problem where the input is provided gradually due to the incremental deployment. We design a randomized “online” algorithm, and show that it achieves theorder-optimalaverage competitive ratio. Simulation results demonstrate the performance of our proposed algorithm, compared to the optimal offline solution.},
keywords={middleware;power system control;power system measurement;power system protection;SCADA systems;smart power grids;architecture design;smart grid communications;multi-stage middleware;supervisory control and data acquisition system;SCADA;power devices;Middleware;Algorithm design and analysis;Smart grids;Sensors;Logic gates;Robustness;Redundancy},
doi={10.1109/INFOCOM.2014.6848232},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848233,
author={K. Zhou and J. Pan and L. Cai},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Optimal Combined Heat and Power system scheduling in smart grid},
year={2014},
volume={},
number={},
pages={2831-2839},
abstract={Combined Heat and Power (CHP) systems are well known for their high efficiency and relatively low emissions. Existing CHP economic dispatch schemes do not use the energy buffer to minimize the average cost in the long term. Motivated by the queueing analysis and buffer management solutions in data communication systems, in this paper, we investigate how to use a battery pack and a water tank to optimize the average cost for the CHP systems by jointly considering the real-time electricity price, renewable energy generation, energy buffer states, etc. We first formulate the queueing models for the CHP systems, and then propose an algorithm based on the Lyapunov optimization technique which does not need any statistical information about the system dynamics. The optimal control actions are obtained by solving a non-convex optimization problem. We then discuss when it can be converted into a convex optimization problem. Since the battery pack queue and water tank queue are correlated by the CHP, the capacity relationship between them is further explored. Through the theoretical performance analysis, we also show the tradeoff between the cost saving and the energy buffer capacity. Finally, the effectiveness of the proposed algorithms is evaluated with practical data.},
keywords={cogeneration;power system economics;scheduling;smart power grids;combined heat and power system scheduling;smart grid;economic dispatch schemes;queueing analysis;buffer management solutions;data communication systems;real-time electricity price;renewable energy generation;energy buffer states;Lyapunov optimization technique;statistical information;nonconvex optimization problem;battery pack queue;water tank queue;Electricity;Cogeneration;Batteries;Storage tanks;Optimization;Natural gas;Resistance heating;Combined Heat and Power;Lyapunov Optimization;Smart Grid},
doi={10.1109/INFOCOM.2014.6848233},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848234,
author={Y. Guo and L. Yang and B. Li and T. Liu and Y. Liu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={RollCaller: User-friendly indoor navigation system using human-item spatial relation},
year={2014},
volume={},
number={},
pages={2840-2848},
abstract={Indoor navigation has received much attention in academics and industry in recent years. Previous methods often attempt to locate users with various localization algorithms in combination with an indoor map, so they need expensive infrastructures deployed in advance. In this study, we propose to utilize existing indoor objects attached RFID tags and the reader to navigate the user to the destination, without need of any extra hardware. The key insight is that the personal movement takes an impact on the Doppler frequency shift values collected from the indoor objects when getting close to the tag. Such local human-item spatial relation is leveraged to infer the users position and further navigate user to destination step by step. We implement a prototype navigation system, called RollCaller, and conduct comprehensive experiments to examine its performance.},
keywords={radiofrequency identification;RollCaller;user friendly indoor navigation system;human item spatial relation;localization algorithms;indoor map;RFID tags;extra hardware;Doppler frequency shift;users position;Antennas;Doppler effect;Navigation;Antenna measurements;Radiofrequency identification;Noise;Frequency measurement;RFID;Doppler Frequency Shift;Human-Item Spatial Relation;Indoor Navigation},
doi={10.1109/INFOCOM.2014.6848234},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848235,
author={J. Huang and W. Albazrqaoe and G. Xing},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={BlueID: A practical system for Bluetooth device identification},
year={2014},
volume={},
number={},
pages={2849-2857},
abstract={Despite the widespread use of Bluetooth technology, identity management of Bluetooth devices remains a significant challenge because the MAC address and name of Bluetooth device are easy to forge. In this paper, we present BlueID - a practical system that identifies Bluetooth devices by fingerprinting their clocks. Previous approaches to clock fingerprinting exclusively rely on the timestamps carried by packet headers, which can be easily spoofed by hacking the user-space device driver. In comparison, BlueID performs clock fingerprinting based on the temporal feature of Bluetooth frequency hopping, which is impossible to forge without a customized baseband. Due to the proprietary nature of chipset firmware that implements baseband on commodity Bluetooth devices, BlueID will significantly raise the bar of identity spoofing. Moreover, BlueID employs simple yet efficient techniques to detect and differentiate low power Bluetooth transmissions from a distance, making it suitable for mobile applications like energy efficient localization and tracking. BlueID is implemented on a low cost wireless development platform and extensively evaluated based on 56 commodity devices. We show that BlueID can detect Bluetooth radios from 100m away, and identify different devices with high accuracy, short delay, and low computational overhead. Although this paper focuses on Bluetooth, the design of BlueID is general and can be applied to other frequency hopping based wireless systems.},
keywords={access protocols;Bluetooth;computer crime;computer network management;computer network security;firmware;frequency hop communication;bluetooth device identification;Bluetooth device management;MAC address;BlueID;clock fingerprinting;timestamp;packet header;user space device driver hacking;chipset firmware;commodity Bluetooth device;identity spoofing;low power Bluetooth transmission;mobile applications;Bluetooth radio detection;frequency hopping based wireless system;Bluetooth;Clocks;Noise;Wireless communication;Accuracy;Baseband},
doi={10.1109/INFOCOM.2014.6848235},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6848236,
author={P. Guo and J. Cao and K. Zhang and X. Liu},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Enhancing ZigBee throughput under WiFi interference using real-time adaptive coding},
year={2014},
volume={},
number={},
pages={2858-2866},
abstract={Co-existing in the unlicensed ISM band, ZigBee transmissions can be significantly interfered by WiFi. Although several approaches recently are proposed to enable ZigBee transmission under WiFi interference, the ZigBee throughput still decreases to zero when WiFi throughput (generated by D-ITG) is over 8Mbps. In this paper, we propose a real-time (&lt;; 5ms) adaptive transmission (RAT) scheme to efficiently adapt forward error-correction coding (FEC) on ZigBee devices in dynamic WiFi environment. We find that sizes of WiFi frames well follow the power law distribution model. With the model, corruption in ZigBee packets can be estimated to some extent, thus facilitating ZigBee device to choose a suitable FEC coding to maximize the throughput. Extensive experimental results show that, compared with existing works, RAT achieves significant performance improvement of ZigBee transmissions in WiFi environment with different traffic load. Particularly, the ZigBee throughput of RAT can be about 10kbps when the WiFi throughput is 8Mbps.},
keywords={adaptive codes;error correction codes;forward error correction;radiofrequency interference;wireless LAN;Zigbee;ZigBee throughput enhancement;Wi-Fi interference;real-time adaptive coding;unlicensed ISM band;real-time adaptive transmission scheme;RAT scheme;forward error-correction coding;FEC;ZigBee devices;dynamic WiFi environment;Wi-Fi frame size;power law distribution model;ZigBee packets;traffic load;IEEE 802.11 Standards;Zigbee;Encoding;Interference;Real-time systems;Forward error correction;Throughput},
doi={10.1109/INFOCOM.2014.6848236},
ISSN={0743-166X},
month={April},}

