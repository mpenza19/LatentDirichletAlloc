@INPROCEEDINGS{6566716,
author={X. N. Nguyen and D. Saucez and T. Turletti},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Efficient caching in content-centric networks using OpenFlow},
year={2013},
volume={},
number={},
pages={1-2},
abstract={Content-Centric Networking (CCN) is designed for efficient content dissemination and supports caching contents on the path from content providers to content consumers to improve user experience and reduce costs. However, this strategy is not optimal inside a domain. In this paper, we propose a solution to improve caching in CCN using a Software-Defined Networking approach.},
keywords={cache storage;cost reduction;protocols;software-defined networking approach;cost reduction;content dissemination;content-centric networking;OpenFlow;caching efficiency;Optimization;Computer architecture;IP networks;Protocols;Internet;Bandwidth;Routing;Content-Centric Networking;Software-Defined Networking;OpenFlow},
doi={10.1109/INFCOM.2013.6566716},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566717,
author={M. Michel and B. Quoitin},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Impact of a sleep schedule on the AODV convergence time in WSNs},
year={2013},
volume={},
number={},
pages={3-4},
abstract={As part of a search for a better understanding of MAC/routing interactions in Wireless Sensor Networks, this paper presents an analysis of the behaviour of the AODV routing protocol when combined with a duty-cycle MAC protocol. In particular, the paper focuses on the time it takes for a route request to be answered in presence of increased latency due to the MAC protocol sleep schedule. The paper quantifies how the response time varies with the duty-cycle ratio. Moreover, recommendations are provided to enhance this response time.},
keywords={Routing protocols;Time factors;Media Access Protocol;Delays;Wireless sensor networks;Schedules;Routing},
doi={10.1109/INFCOM.2013.6566717},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566718,
author={J. M. C. Silva and P. Carvalho and S. R. Lima},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Enhancing traffic sampling scope and efficiency},
year={2013},
volume={},
number={},
pages={5-6},
abstract={Traffic Sampling is a crucial step towards scalable network measurements, enclosing manifold challenges. The wide variety of foreseeable sampling scenarios demands for a modular view of sampling components and features, grounded on a consistent architecture. Articulating the measurement scope, the required information model and the adequate sampling strategy is a major design issue for achieving an encompassing and efficient sampling solution. This is the main focus of the present work, where a layered architecture, a taxonomy of existing sampling techniques distinguishing their inner characteristics and a flexible framework able to combine these characteristics are introduced. In addition, a new multiadaptive technique proposal, based on linear prediction, allows to reduce the measurement overhead significantly, while assuring that traffic samples reflect the statistical behavior of the global traffic under analysis.},
keywords={Internet;telecommunication traffic;global traffic;linear prediction;multiadaptive technique proposal;layered architecture;information model;scalable network measurements;traffic sampling scope;Taxonomy;Computer architecture;Loss measurement;Current measurement;Accuracy;Process control},
doi={10.1109/INFCOM.2013.6566718},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566719,
author={A. Villani and D. Riboni and D. Vitali and C. Bettini and L. V. Mancini},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Obsidian: A scalable and efficient framework for NetFlow obfuscation},
year={2013},
volume={},
number={},
pages={7-8},
abstract={Through this software the authors aim to promote the sharing of network logs within the research community. The (k, j)obfuscation technique opens sundry interesting future directions. In fact, many networking and security tasks can be re-thought based on obfuscated datasets, for instance, quality of service (QoS), traffic classification, anomaly detection and more.},
keywords={quality of service;telecommunication computing;telecommunication security;telecommunication traffic;anomaly detection;traffic classification;QoS;quality of service;obfuscated datasets;security tasks;(k, j)obfuscation;research community;network log sharing;NetFlow obfuscation;efficient framework;scalable framework;Obsidian;IP networks;Security;Electronic mail;Knowledge engineering;Standards;Random access memory;Indexes},
doi={10.1109/INFCOM.2013.6566719},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566720,
author={A. Cammarano and D. Spenza and C. Petrioli},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Energy-harvesting WSNs for structural health monitoring of underground train tunnels},
year={2013},
volume={},
number={},
pages={9-10},
abstract={Structural health monitoring is a vital tool to help engineers improving the safety of critical structures, avoiding the risks of catastrophic failures. Wireless sensor networks (WSNs) are a very promising technology for structural health monitoring, as they can provide a quality of monitoring similar to conventional (wired) SHM systems with lower cost. In addiction, WSNs are both non-intrusive and non-disruptive and can be employed from the very early stages of construction.The main goal of this work is to investigate the feasibility of a WSN with energy-harvesting capabilities for structural health monitoring, specifically targeting underground tunnels.},
keywords={condition monitoring;energy harvesting;failure (mechanical);geotechnical engineering;railways;risk analysis;structural engineering;tunnels;wireless sensor networks;wireless sensor networks;catastrophic failures;risk analysis;critical structure safety;underground train tunnels;structural health monitoring;energy harvesting WSN;Wireless sensor networks;Monitoring;Wires;Wireless communication;Strain;Ad hoc networks;Energy harvesting},
doi={10.1109/INFCOM.2013.6566720},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566721,
author={A. M. Sheikh and A. Fiandrotti and E. Magli},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Distributed scheduling for scalable P2P video streaming with network coding},
year={2013},
volume={},
number={},
pages={11-12},
abstract={Previous research has shown the benefits of random-push Network Coding (NC) for P2P video streaming. On the other hand, scalable video coding provides graceful quality adaptation to heterogeneous network conditions. Nevertheless, packet scheduling for scalable media streaming with P2P NC is still a largely unexplored problem. Our ongoing research aims at designing a packet scheduling scheme that maximizes the quality of the video with minimal coordination among peers. In this work, we provide a preliminary description of our scheduling scheme and preliminary performance measurements.},
keywords={network coding;peer-to-peer computing;scheduling;video coding;video streaming;packet scheduling scheme;scalable media streaming;video coding;random-push network coding;P2P video streaming;distributed scheduling;Streaming media;Peer-to-peer computing;Bandwidth;Decoding;Network coding;Video coding;Scheduling algorithms},
doi={10.1109/INFCOM.2013.6566721},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566722,
author={P. Di Marco and C. Fischione and G. Athanasiou and P. Mekikis},
booktitle={2013 Proceedings IEEE INFOCOM},
title={MAC-aware routing metrics for low power and lossy networks},
year={2013},
volume={},
number={},
pages={13-14},
abstract={In this paper, routing metrics for low power and lossy networks are designed and evaluated. The cross-layer interactions between routing and medium access control (MAC) are explored, by considering the specifications of IETF RPL over the IEEE 802.15.4 MAC. In particular, the experimental study of a reliability metric that extends the expected transmission count (ETX) to include the effects of the level of contention and the parameters at MAC layer is presented. Moreover, a novel metric that guarantees load balancing and increased network lifetime by fulfilling reliability constraints is introduced. The aforementioned metrics are compared to a routing approach based on backpressure mechanism.},
keywords={radio networks;telecommunication network reliability;telecommunication network routing;MAC-aware routing metrics;low power networks;lossy networks;cross-layer interactions;medium access control;IETF RPL;IEEE 802.15.4 MAC;reliability metric;expected transmission count;contention level;MAC layer;load balancing;network lifetime;reliability constraints;backpressure mechanism;Measurement;Routing;Reliability;Power demand;IEEE 802.15 Standards;Media Access Protocol},
doi={10.1109/INFCOM.2013.6566722},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566723,
author={M. S. Ilyas and Z. A. Uzmi},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Electricity cost efficient workload mapping},
year={2013},
volume={},
number={},
pages={15-16},
abstract={In this thesis, we formulate a generalized optimization problem to minimize the electricity cost of network operation while using techniques applicable to operational networks. We applied our optimization formulation to two different networks: geo-diverse data centers and cellular networks. In case of cellular networks, using traffic traces from an large operational network, we observed that an operator can save up to 22% in electricity costs by using our proposed scheme. While the percentage amount seems modest, for an operator with 7000 cellular sites in an urban setting, this translates to savings of up to 35.36 MWh annually.},
keywords={cellular radio;computer centres;cost reduction;optimisation;telecommunication power supplies;cellular sites;traffic traces;cellular networks;geo-diverse data centers;network operation;electricity cost minimization;generalized optimization problem;electricity cost efficient workload mapping;Electricity;Power demand;Optimization;Computational modeling;Educational institutions;Transceivers;Base stations},
doi={10.1109/INFCOM.2013.6566723},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566724,
author={A. Nguyen and P. Senac and M. Diaz},
booktitle={2013 Proceedings IEEE INFOCOM},
title={On the impact of disorder on dynamic network navigation},
year={2013},
volume={},
number={},
pages={17-18},
abstract={Dynamic networks like Online Social Networks or Disruption Tolerant Networks (DTNs), when considering their spatial, temporal and size complexity, even if partly wired, are exposed to nodes and links churns and failures which can be modeled with dynamic graphs with time varying edges and vertices. Recently, it has been shown that dynamic networks exhibit some regularity in their temporal contact patterns. The impact of this regularity on network performances has not been well studied and analyzed. One of the most interesting problem in research on dynamic networks is the issue of efficient navigation techniques in such networks. For dynamic networks, because there is still no widely developed theoretical background to understand deeply the problems, research traditionally tends to propose heuristic solutions. In the context of DTNs, these solutions tend to answer to some specific questions about navigating in a dynamic network (e.g., how to reduce energy consumption of routing, how to maximize the delivery probability) while usually ignoring and not leveraging on the profound structural properties of the dynamic network. In this work, we aim to contribute to understanding the impact of this dynamic structure on information routing and show how to exploit this structure for efficient navigation in such networks.},
keywords={graph theory;telecommunication network routing;information routing;dynamic structure;structural properties;delivery probability;energy consumption;temporal contact patterns;time varying vertices;time varying edges;dynamic graphs;size complexity;temporal complexity;spatial complexity;disruption tolerant networks;online social networks;dynamic network navigation;Routing;Delays;Navigation;Peer-to-peer computing;Heuristic algorithms;Social network services},
doi={10.1109/INFCOM.2013.6566724},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566725,
author={A. Lertsinsrubtavee and N. Malouch and S. Fdida},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Efficient dynamic spectrum sharing through rate compensation and spectrum handoff},
year={2013},
volume={},
number={},
pages={19-20},
abstract={In this work, we propose a heuristic for dynamic spectrum sharing in cognitive radio networks. The concept of rate compensation is introduced so that cognitive radio users are able to achieve their rate requirement by performing adequately spectrum handoffs. Indeed, performing spectrum handoff can increase the achieved rate obtained by moving from unavailable channels to available ones. However, handoffs should also be reduced to decrease handoff delays and access contention in the network which can in turn impact the achieved rate.},
keywords={cognitive radio;radio spectrum management;wireless channels;dynamic spectrum sharing;rate compensation;spectrum handoff;cognitive radio networks;cognitive radio users;rate requirement;unavailable channels;handoff delays;access contention;Resource management;Cognitive radio;Availability;Bandwidth;Delays;Dynamic scheduling},
doi={10.1109/INFCOM.2013.6566725},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566726,
author={S. Ali and G. Rizzo and B. Rengarajan and M. A. Marsan},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A simple approximate analysis of floating content for context-aware applications},
year={2013},
volume={},
number={},
pages={21-22},
abstract={Context-awareness is a peculiar characteristic of an ever expanding set of applications that make use of a combination of restricted spatio-temporal locality and mobile communications, to deliver a variety of services to the end user. It is expected that by 2014 more than 1.5 billion people would be using applications based on local search (search restricted on the basis of spatio-temporal locality), and that mobile location based services will drive revenues of more than $15 billion worldwide. A common feature of such context-aware applications is the fact that their communication requirements significantly differ from ordinary applications. For most of them, the scope of generated content itself is local. This locally relevant content may be of little concern to the rest of the world, therefore moving this content from the user device to store it in a well-accessible centralized location and/or making this information available beyond its scope represents a clear waste of resources (connectivity, storage). Due to these specific requirements, opportunistic communication can play a special role when coupled with context-awareness. The benefit of opportunistic communications is that it naturally incorporates context as spatial proximity is closely associated with connectivity.},
keywords={mobile computing;spatial proximity;user device;communication requirements;mobile location based services;local search;mobile communications;restricted spatio-temporal locality;context-aware applications;floating content;Analytical models;Computational modeling;Adaptation models;Open area test sites;Context-aware services;Predictive models;Probability density function},
doi={10.1109/INFCOM.2013.6566726},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566727,
author={K. Garg},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Data dissemination bounds in people-centric systems},
year={2013},
volume={},
number={},
pages={23-24},
abstract={This paper presents an overview of our research proposal. We present the key challenge in deploying people-centric systems that rely on opportunistic data dissemination. We propose to address this challenge by developing a model to predict the minimum number of nodes required to disseminate information to all nodes in a given network. We conduct some simulations in OMNET++ and present our preliminary results for an opportunistic network with static data. We further plan to develop and validate our mathematical model using real world experiments.},
keywords={communication complexity;graph theory;mathematical analysis;mobile radio;dynamic communication graph;NP hard problem;mobile network;mathematical model;static data;opportunistic network;OMNET++;network node;information dissemination;opportunistic data dissemination;people-centric system;data dissemination bounds;Mathematical model;Mobile communication;Wireless sensor networks;Data models;Mobile computing;Wireless communication;Sensors},
doi={10.1109/INFCOM.2013.6566727},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566728,
author={H. Li and C. Wu and Z. Li and F. C. M. Lau},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Profit-maximizing virtual machine trading in a federation of selfish clouds},
year={2013},
volume={},
number={},
pages={25-29},
abstract={The emerging federated cloud paradigm advocates sharing of resources among cloud providers, to exploit temporal availability of resources and diversity of operational costs for job serving. While extensive studies exist on enabling interoperability across different cloud platforms, a fundamental question on cloud economics remains unanswered: When and how should a cloud trade VMs with others, such that its net profit is maximized over the long run? In order to answer this question by the federation, a number of important, correlated decisions, including job scheduling, server provisioning and resource pricing, need to be dynamically made, with long-term profit optimality being a goal. In this work, we design efficient algorithms for inter-cloud resource trading and scheduling in a federation of geo-distributed clouds. For VM trading among clouds, we apply a double auction-based mechanism that is strategy proof, individual rational, and ex-post budget balanced. Coupling with the auction mechanism is an efficient, dynamic resource trading and scheduling algorithm, which carefully decides the true valuations of VMs in the auction, optimally schedules stochastic job arrivals with different SLAs onto the VMs, and judiciously turns on and off servers based on the current electricity prices. Through rigorous analysis, we show that each individual cloud, by carrying out our dynamic algorithm, can achieve a time-averaged profit arbitrarily close to the offline optimum.},
keywords={cloud computing;scheduling;virtual machines;profit-maximizing virtual machine trading;selfish clouds;federated cloud paradigm;resource sharing;cloud providers;temporal availability;operational costs;job serving;interoperability;cloud platforms;cloud economics;job scheduling;server provisioning;resource pricing;long-term profit optimality;inter-cloud resource trading;geo-distributed clouds;VM trading;auction-based mechanism;strategy proof;budget balance;auction mechanism;dynamic resource trading;scheduling algorithm;stochastic job arrivals;SLA;Servers;Heuristic algorithms;Electricity;Dynamic scheduling;Cost accounting;Schedules;Delays},
doi={10.1109/INFCOM.2013.6566728},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566729,
author={O. Sukwong and H. S. Kim},
booktitle={2013 Proceedings IEEE INFOCOM},
title={DPack: Disk scheduler for highly consolidated cloud},
year={2013},
volume={},
number={},
pages={30-34},
abstract={Virtualization allows us to consolidate multiple servers onto a single physical machine, saving infrastructure cost. Yet, consolidation can lead to performance degradation, jeopardizing Service Level Agreement (SLA). In this paper, we analyze and identify the factors to the performance degradation due to consolidation - that is the wait time and the ready time. The wait time is the queuing time caused by other virtual machines (VMs). The ready time is the time the resource takes to be ready to service, such as the seek time incurred in traditional storage. The ready time can substantially deteriorate the request response time. Unfortunately, existing schedulers can only manage the wait time, but not the ready time. To control both quantities, we propose an adaptive disk scheduler called DPack. DPack schedules the VMs based on the likelihood of the VM failing the SLAs. DPack then adjusts the exclusive access time based on the VM resource access prediction. DPack considers the workload changes and request arrival to enhance robustness. We develop DPack based on the default disk scheduler in KVM and evaluate it against several existing disk schedulers available in KVM and Xen. The results show that DPack can improve the 99th percentile response time up to 76%. In the highly consolidated environment, DPack can also satisfy all the SLAs, while the other schedulers cannot meet the SLAs for at least 50% of the VMs.},
keywords={cloud computing;contracts;resource allocation;scheduling;virtual machines;virtualisation;DPack;adaptive disk scheduler;highly consolidated cloud;virtualization;multiple server consolidation;performance degradation;service level agreement;SLA;wait time;ready time;queuing time;virtual machines;VM resource access prediction;request response time;exclusive access time;KVM;Xen;Time factors;Throughput;Dynamic scheduling;Optimization;Servers;Degradation;Virtual machining;Adaptive scheduling;virtual machine;quality of service;web services;disk scheduler;hypervisor},
doi={10.1109/INFCOM.2013.6566729},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566730,
author={Y. Zhao and J. Wu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Dache: A data aware caching for big-data applications using the MapReduce framework},
year={2013},
volume={},
number={},
pages={35-39},
abstract={The buzz-word big-data (application) refers to the large-scale distributed applications that work on unprecedentedly large data sets. Google's MapReduce framework and Apache's Hadoop, its open-source implementation, are the defacto software system for big-data applications. An observation regarding these applications is that they generate a large amount of intermediate data, and these abundant information is thrown away after the processing finish. Motivated by this observation, we propose a data-aware cache framework for big-data applications, which is called Dache. In Dache, tasks submit their intermediate results to the cache manager. A task, before initiating its execution, queries the cache manager for potential matched processing results, which could accelerate its execution or even completely saves the execution. A novel cache description scheme and a cache request and reply protocol are designed. We implement Dache by extending the relevant components of the Hadoop project. Testbed experiment results demonstrate that Dache significantly improves the completion time of MapReduce jobs and saves a significant chunk of CPU execution time.},
keywords={cache storage;parallel programming;public domain software;query processing;Dache;buzz-word big-data application;large-scale distributed applications;Google MapReduce framework;Apache Hadoop project;defacto software system;data-aware cache framework;cache manager;potential matched processing;cache description scheme;cache request-reply protocol design;CPU execution time;query processing;Sorting;Protocols;Indexes;Distributed databases;Acceleration;Context;Pricing;Big-data;MapReduce;Hadoop;distributed file system;cache management},
doi={10.1109/INFCOM.2013.6566730},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566731,
author={C. Zhang and X. Liu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={HBaseMQ: A distributed message queuing system on clouds with HBase},
year={2013},
volume={},
number={},
pages={40-44},
abstract={Message queuing systems can be used to support a plethora of fundamental services in distributed systems. This paper presents HBaseMQ, the first distributed message queuing system based on bare-bones HBase. HBaseMQ directly inherits HBase's properties such as scalability and fault tolerance, enabling HBase users to rapidly instantiate a message queuing system with no extra program deployment or modification to HBase. As a result, HBaseMQ effectively enhances the data processing capability of an existing HBase installation. HBaseMQ supports reliable and total order message delivery with “at least once” or “at most once” message delivery guarantees with no limit on message size. Furthermore, HBaseMQ provides atomicity, consistency, isolation and durability on any operation over messages.},
keywords={cloud computing;fault tolerant computing;message passing;queueing theory;HBaseMQ;distributed message queuing system;bare-bones HBase;fault tolerance;scalability;HBase properties;data processing capability;HBase installation;total order message delivery;at least once message delivery guarantee;at most once message delivery guarantee;Protocols;Scalability;Libraries;Fault tolerance;Fault tolerant systems;Receivers;Distributed message queuing system;HBase;Clouds},
doi={10.1109/INFCOM.2013.6566731},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566732,
author={X. Cheng and H. Li and J. Liu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Video sharing propagation in social networks: Measurement, modeling, and analysis},
year={2013},
volume={},
number={},
pages={45-49},
abstract={The social networking services (SNS) have drastically changed the information distribution landscape and people's daily life. With the development in broadband accesses, video has become one of the most important types of objects spreading among social networking service users, yet presents more significant challenges than other types of objects, not only to the SNS management, but also to the network traffic engineering. In this paper, we take an important step towards understanding the characteristics of video sharing propagation in SNS, based on the real viewing event traces from a popular SNS in China. We further extend the epidemic models to accommodate the diversity of the propagation, and our model effectively captures the propagation process of video sharing in SNS.},
keywords={social networking (online);video sharing propagation;information distribution landscape;social networking service users;SNS management;network traffic engineering;real viewing event;China;Streaming media;Watches;Vegetation;Market research;Peer-to-peer computing;Twitter},
doi={10.1109/INFCOM.2013.6566732},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566733,
author={H. Li and H. Wang and J. Liu and K. Xu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Video requests from Online Social Networks: Characterization, analysis and generation},
year={2013},
volume={},
number={},
pages={50-54},
abstract={The deep penetration of Online Social Networks (OSNs) have made them major portals for video content sharing. It is known that a significant portion of the accesses to video sharing sites are now coming from OSN users. Yet the unique features of video sharing over OSNs and their impact remain largely unknown. In this paper, we present a measurement study towards understanding the video requests from OSNs. We closely collaborated with a large-scale Facebook-like OSN to analyze its user access logs spanning over four months. Our measurement reveals a number of distinctive features on the popularity distribution of videos shared over the OSN. In particular, we observe that the OSN amplifies the skewness of video popularity so largely that about 2% most popular videos account for 90% of total views; the video requests distribution also exhibits perfect powerlaw feature; video popularity evolution shows more dynamics. All these noticeably differ from that of conventional videos, such as YouTube videos. To further understand the characteristics, we model the video viewing and sharing behaviors in OSNs, leading to the development of a practical emulator. It reveals the gap between the sharing rate and the viewing rate, and generates user requests that well capture the video popularity distribution and dynamics as observed in our empirical data.},
keywords={content management;Internet;social networking (online);video retrieval;video signal processing;online social networks;video content sharing;video sharing sites;OSN users;Facebook-like OSN;user access logs;video popularity;video request distribution;power-law feature;video popularity evolution;video sharing rate;video popularity distribution;video popularity dynamics;YouTube;Streaming media;Silicon;Facebook;Correlation coefficient;Atmospheric measurements},
doi={10.1109/INFCOM.2013.6566733},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566734,
author={T. N. Dinh and N. P. Nguyen and M. T. Thai},
booktitle={2013 Proceedings IEEE INFOCOM},
title={An adaptive approximation algorithm for community detection in dynamic scale-free networks},
year={2013},
volume={},
number={},
pages={55-59},
abstract={We introduce A3CS, an adaptive framework with approximation guarantees for quickly identifying community structure in dynamic networks via maximizing Modularity Q. Our framework explores the advantages of power-law distribution property, is scalable for very large networks, and more excitingly, possesses approximation factors to ensure the quality of its detected community structure. To the best of our knowledge, this is the first framework that achieves approximation guarantees for the NP-hard modularity maximization problem, especially on dynamic networks. To certify our approach, we conduct extensive experiments in comparison with other adaptive methods on both synthesized networks with known community structures and real-world traces including ArXiv e-print citation and Facebook social networks. Excellent empirical results not only confirm our theoretical results but also promise the practical applicability of A3CS in a wide range of dynamic networks.},
keywords={approximation theory;complex networks;computational complexity;optimisation;social networking (online);dynamic scale-free networks;adaptive approximation algorithm;community detection;A3CS;adaptive framework;community structure;modularity Q;power-law distribution property;NP-hard modularity maximization problem;synthesized networks;ArXiv e-print citation;Facebook social networks;Communities;Heuristic algorithms;Approximation methods;Adaptive algorithms;Approximation algorithms;Social network services;Time complexity;Adaptive approximation algorithm;Community structure;Modularity;Social networks},
doi={10.1109/INFCOM.2013.6566734},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566735,
author={J. Chandra and B. Mitra and N. Ganguly},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Effect of constraints on superpeer topologies},
year={2013},
volume={},
number={},
pages={60-64},
abstract={In superpeer based networks, the superpeers are discovered through the process of bootstrapping, whereby resourceful peers get upgraded to superpeers. However, bootstrapping is influenced by several factors like limitation on the maximum number of connections a peer can have due to bandwidth constraints, limitation on the availability of information of existing peers due to cache size constraints and also by the attachment policy of the newly arriving peers to the resourceful peers. In this paper, we derive closed form equations that model the effect of these factors on superpeer related topological properties of the networks. Based on the model, we observe that the cache parameters and the preferentiality parameters must be suitably tuned so as to increase the fraction of superpeers in the network. Finally, we perform an empirical analysis of social networks like Twitter and Facebook using our model to obtain and derive insights for suitably bootstrapping superpeer topology.},
keywords={cache storage;computer bootstrapping;peer-to-peer computing;social networking (online);telecommunication network topology;superpeer based networks;bootstrapping process;bandwidth constraints;information availability;cache size constraints;attachment policy;resourceful peers;superpeer related topological properties;cache parameters;empirical analysis;social networks;Facebook;Twitter;bootstrapping superpeer topology;Peer-to-peer computing;Mathematical model;Facebook;Equations;Twitter;Protocols;Topology;Superpeer networks;Bootstrapping protocols;Webcache;Rate equation},
doi={10.1109/INFCOM.2013.6566735},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566736,
author={H. Li and C. Hu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={ROOM: Rule Organized Optimal Matching for fine-grained traffic identification},
year={2013},
volume={},
number={},
pages={65-69},
abstract={Fine-grained traffic identification (FGTI) reveals the context/purpose of each packet that flows through the network nodes/links. Instead of only indicating the application/protocol that a packet is related to, FGTI further maps the packet to a meaningful user behavior or application context. In this paper, we propose a Rule Organized Optimal Matching (ROOM) for fast and memory efficient fine-grained traffic identification. ROOM splits the identification rules into several fields and elaborately organizes the matching order of the fields. We formulate and model the optimal rule organization problem of ROOM mathematically, which is demonstrated to be NP-hard, and then we propose an approximate algorithm to solve the problem with the time complexity of O(N2) (N is the number of fields in a rule). In order to perform evaluations, we implement ROOM and related work as real prototype systems. Also, real traces collected in wired Internet and mobile Internet are used as the experiment input. The evaluations show very promising results: 1.6X to 104.7X throughput improvement is achieved by ROOM in the real system with acceptable small memory cost.},
keywords={Internet;mobile computing;optimisation;telecommunication traffic;rule organized optimal matching;ROOM;fine-grained traffic identification;FGTI;network nodes;identification rules;NP-hard problem;wired Internet;mobile Internet;Throughput;Memory management;Complexity theory;Internet;Protocols;Organizations;Mobile communication},
doi={10.1109/INFCOM.2013.6566736},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566737,
author={A. di Pietro and F. Huici and N. Bonelli and B. Trammell and P. Kastovsky and T. Groleat and S. Vaton and M. Dusi},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Toward composable network traffic measurement},
year={2013},
volume={},
number={},
pages={70-74},
abstract={As the growth of Internet traffic volume and diversity continues, passive monitoring and data analysis, crucial to the correct operation of networks and the systems that rely on them, has become an increasingly difficult task. We present the design and implementation of Blockmon, a flexible, high performance system for network monitoring and analysis. We present experimental results demonstrating Blockmon's performance, running simple analyses at 10Gb/s line rate on commodity hardware; and compare its performance with that of existing programmable measurement systems, showing significant improvement (as much as twice as fast) especially for small packet sizes. We further demonstrate Blockmon's applicability to measurement and data analysis by implementing and evaluating three sample applications: a flow meter, a TCP SYN flood detector, and a VoIP anomaly-detection system.},
keywords={computer network performance evaluation;computer network reliability;Internet;telecommunication traffic;transport protocols;network traffic measurement;Internet;traffic volume;traffic diversity;passive monitoring;data analysis;flexible high performance system;network monitoring;network analysis;Blockmon performance;line rate;commodity hardware;programmable measurement system;packet size;flow meter;TCP SYN flood detector;VoIP anomaly-detection system;bit rate 10 Gbit/s;Logic gates;Hardware;Message systems;Radiation detectors;Resource management;Monitoring;Optimization},
doi={10.1109/INFCOM.2013.6566737},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566738,
author={B. Eriksson and M. Crovella},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Understanding geolocation accuracy using network geometry},
year={2013},
volume={},
number={},
pages={75-79},
abstract={The ability to estimate the geographic position of a network host has a vast array of uses, and many measurement-based geolocation methods have been proposed. Unfortunately, comparing results across multiple studies is difficult. A key contributor to that difficulty is network geometry - the spatial arrangement of hosts and links. In this paper, we study the relationship between network geometry and geolocation accuracy. We define the notion of scaling dimension to characterize the geometry of a wide array of different networks. We show that the scaling dimension correlates with a number of aspects of geolocation accuracy. In networks with low scaling dimension, geolocation accuracy improves more rapidly with the addition of landmarks. Further, we show that the scaling dimension of operator networks varies considerably across different regions of the world. Our results point to the complexity of, and suggest standards for, the meaningful evaluation of geolocation algorithms.},
keywords={geometry;Global Positioning System;Internet;network theory (graphs);network host geographic position estimation;network geometry;spatial host arrangement;spatial link arrangement;measurement-based geolocation method;geolocation accuracy;scaling dimension notion;operator network;Geology;Accuracy;Extraterrestrial measurements;Geometry;Delays;Network topology;Internet},
doi={10.1109/INFCOM.2013.6566738},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566739,
author={K. Fukuda and S. Sato and T. Mitamura},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A technique for counting DNSSEC validators},
year={2013},
volume={},
number={},
pages={80-84},
abstract={The DNS security extensions (DNSSEC) is a new feature of DNS that provides an authentication mechanism that is now being deployed worldwide. However, we do not have enough knowledge about the deployment status of DNSSEC in the wild due to the difficulty of identifying DNSSEC validators (caching validating resolvers). In this paper, a simple and robust method is proposed that estimates DNSSEC validators from DNS query data passively measured at the server side. The key idea of the estimation method relies on the query patterns of the original query and the DNSSEC queries triggered by the original query, which is the ratio of the number of DS queries to the number of total queries per host (DSR: DS ratio). To show the effectiveness of the proposed method, we analyze passive traffic traces measured for all the “.jp” servers and actively send DNSSEC validation requests to caching resolvers that appear in the traces to obtain the ground truth data of DNSSEC validators. Our results of the active measurement reveal that less than 50% of the potential DNSSEC validators were validating caching resolvers in the wild; the remainder was related to stub validators (e.g., browser plugins) behind non-validating caching resolvers. Thus, simple IP address-based counts overestimated the number of DNSSEC validators in an investigation of the deployment of DNSSEC at the organization level (e.g., ISPs). Then, we demonstrate the effectiveness of the DSR by using the active and passive traffic traces. In summary, the ratio of validating caching resolvers in our dataset was estimated to be approximately 70% of the potential DNSSEC validators, and also 15-20% of the ASes sending DNSSEC queries were overestimated as ones with validating caching resolvers. In particular, our results show that some ASes providing public DNS service had few validating caching resolvers though they had a large number of hosts sending DNSSEC queries.},
keywords={Internet;query processing;security of data;DNSSEC validators;DNS security extensions;authentication mechanism;DNS query data;query patterns;DS queries;nonvalidating caching resolvers;simple IP address-based counts;Servers;Software;Internet;IP networks;Security;Robustness;Organizations},
doi={10.1109/INFCOM.2013.6566739},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566740,
author={E. J. Rosensweig and J. Kurose},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A network calculus for cache networks},
year={2013},
volume={},
number={},
pages={85-89},
abstract={Over the past few years Content-Centric Networking, a networking architecture in which host-to-content communication protocols are introduced, has been gaining much attention. A central component of such an architecture is a large-scale interconnected caching system. To date, the way these Cache Networks operate and perform is still poorly understood. Following the work of Cruz on queueing networks, in this paper we develop a network calculus for bounding flows in LRU cache networks of arbitrary topology. We analyze the tightness of these bounds as a function of several system parameters. Also, we derive from it several analytical results regarding these systems: the uniformizing impact of LRU on the request stream, and the significance of cache and routing diversity on performance.},
keywords={protocols;queueing theory;telecommunication network routing;telecommunication network topology;network calculus;LRU cache networks;content centric networking;host-to-content communication protocols;queueing networks;bounding flows;request stream;routing diversity;Calculus;Network topology;Topology;Computer architecture;Delays;Writing;Computational modeling},
doi={10.1109/INFCOM.2013.6566740},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566741,
author={R. Ahmed and M. F. Bari and S. R. Chowdhury and M. G. Rabbani and R. Boutaba and B. Mathieu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={αRoute: A name based routing scheme for Information Centric Networks},
year={2013},
volume={},
number={},
pages={90-94},
abstract={One of the crucial building blocks for Information Centric Networking (ICN) is a name based routing scheme that can route directly on content names instead of IP addresses. However, moving the address space from IP addresses to content names brings scalability issues to a whole new level, due to two reasons. First, name aggregation is not as trivial a task as the IP address aggregation in BGP routing. Second, the number of addressable contents in the Internet is several orders of magnitude higher than the number of IP addresses. With the current size of the Internet, name based, anycast routing is very challenging specially when routing efficiency is of prime importance. We propose a novel name-based routing scheme (αRoute) for ICN that offers efficient bandwidth usage, guaranteed content lookup and scalable routing table size.},
keywords={Internet;telecommunication network routing;αRoute;name based routing scheme;information centric networks;content names;address space;name aggregation;IP address aggregation;BGP routing;addressable contents;Internet;anycast routing;routing efficiency;bandwidth usage;guaranteed content lookup;scalable routing table size;Routing;Internet;Vegetation;Routing protocols;Indexing;Silicon},
doi={10.1109/INFCOM.2013.6566741},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566742,
author={Y. Wang and T. Pan and Z. Mi and H. Dai and X. Guo and T. Zhang and B. Liu and Q. Dong},
booktitle={2013 Proceedings IEEE INFOCOM},
title={NameFilter: Achieving fast name lookup with low memory cost via applying two-stage Bloom filters},
year={2013},
volume={},
number={},
pages={95-99},
abstract={In this paper we design, implement and evaluate NameFilter, a two-stage Bloom filter-based scheme for Named Data Networking name lookup, in which the first stage determines the length of a name prefix, and the second stage looks up the prefix in a narrowed group of Bloom filters based on the results from the first stage. Moreover, we optimize the hash value calculation of name strings, as well as the data structure to store multiple Bloom filters, which significantly reduces the memory access times compared with that of non-optimized Bloom filters. We conduct extensive experiments on a commodity server to test NameFilter's throughput, memory occupation, name update as well as scalability. Evaluation results on a name prefix table with 10M entries show that our proposed scheme achieves lookup throughput of 37 million searches per second at low memory cost of only 234.27 MB, which means 12 times speedup and 77% memory savings compared to the traditional character trie structure. The results also demonstrate that NameFilter can achieve 3M per second incremental updates and exhibit good scalability to large-scale prefix tables.},
keywords={data structures;information retrieval;NameFilter;fast name lookup;memory cost;two-stage Bloom filter-based scheme;named data networking name lookup;data structure;nonoptimized Bloom filters;memory occupation;name update;name prefix table;lookup throughput;character trie structure;large-scale prefix tables;IP networks;Memory management;Ports (Computers);Throughput;Instruction sets;Scalability;Internet},
doi={10.1109/INFCOM.2013.6566742},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566743,
author={S. Saha and A. Lukyanenko and A. Ylä-Jääski},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Cooperative caching through routing control in information-centric networks},
year={2013},
volume={},
number={},
pages={100-104},
abstract={Information-centric network (ICN), which is one of the prominent Internet re-design architectures, relies on in-network caching for its fundamental operation. However, previous works argue that the performance of in-network caching is highly degraded with the current cache-along-default-path design, which makes popular objects to be cached redundantly in many places. Thus, it would be beneficial to have a distributed and uncoordinated design. Although cooperative caches could be an answer to this, previous research showed that they are generally unfeasible due to excessive signaling burden, protocol complexity, and a need for fault tolerance. In this work we illustrate the ICN caching problem, and propose a novel architecture to overcome the problem of uncooperative caches. Our design possesses the cooperation property intrinsically. We utilize controlled off-path caching to achieve almost 9-fold increase in cache efficiency, and around 20% increase in server load reduction when compared to the classic on-path caching used in ICN proposals.},
keywords={cache storage;computer network reliability;Internet;telecommunication network routing;cooperative caching;routing control;information-centric networks;Internet redesign architectures;in-network caching;fundamental operation;cache-along-default-path design;distributed design;uncoordinated design;protocol complexity;fault tolerance;ICN caching problem;uncooperative cache;cooperation property;controlled off-path caching;cache efficiency;server load reduction;classic on-path caching;Servers;Routing;Indexes;Internet;Sociology;Statistics;Network topology},
doi={10.1109/INFCOM.2013.6566743},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566744,
author={F. Malandrino and C. Casetti and C. F. Chiasserini and M. Fiore and R. S. Yokoyama and C. Borgiattino},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A-VIP: Anonymous verification and inference of positions in vehicular networks},
year={2013},
volume={},
number={},
pages={105-109},
abstract={Knowledge of the location of vehicles and tracking of the routes they follow are a requirement for a number of applications. However, public disclosure of the identity and position of drivers jeopardizes user privacy, and securing the tracking through asymmetric cryptography may have an exceedingly high computational cost. In this paper, we address all of the issues above by introducing A-VIP, a lightweight privacy-preserving framework for tracking of vehicles. A-VIP leverages anonymous position beacons from vehicles, and the cooperation of nearby cars collecting and reporting the beacons they hear. Such information allows an authority to verify the locations announced by vehicles, or to infer the actual ones if needed. We assess the effectiveness of A-VIP through testbed implementation results.},
keywords={cryptography;mobile radio;A-VIP;anonymous verification;positions inference;vehicular networks;vehicles location;routes tracking;public disclosure;asymmetric cryptography;computational cost;lightweight privacy-preserving framework;cars;Vehicles;Cryptography;Tiles;Privacy;Protocols;Radiation detectors;Phantoms},
doi={10.1109/INFCOM.2013.6566744},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566745,
author={C. Sommer and S. Joerer and M. Segata and O. Tonguz and R. L. Cigno and F. Dressler},
booktitle={2013 Proceedings IEEE INFOCOM},
title={How shadowing hurts vehicular communications and how dynamic beaconing can help},
year={2013},
volume={},
number={},
pages={110-114},
abstract={We study the effect of radio signal shadowing dynamics, caused by vehicles and by buildings, on the performance of beaconing protocols in Inter-Vehicular Communication (IVC). Recent research indicates that beaconing, i.e., one hop message broadcast, shows excellent characteristics and can outperform other communication approaches for both safety and efficiency applications, which require low latency and wide area information dissemination, respectively. We show how shadowing dynamics of moving obstacles hurt IVC, reducing the performance of beaconing protocols. At the same time, shadowing also limits the risk of overloading the wireless channel. To the best of our knowledge, this is the first study identifying the problems and resulting possibilities of such dynamic radio shadowing. We demonstrate how these challenges and opportunities can be taken into account and outline a novel approach to dynamic beaconing. It provides low-latency communication (i.e., very short beaconing intervals), while ensuring not to overload the wireless channel. The presented simulation results substantiate our theoretical considerations.},
keywords={mobile communication;protocols;wireless channels;dynamic beaconing;radio signal shadowing dynamics;beaconing protocols;inter-vehicular communication;one hop message broadcast;wide area information dissemination;moving obstacles;wireless channel;dynamic radio shadowing;low-latency communication;beaconing intervals;Shadow mapping;Vehicles;Vehicle dynamics;Buildings;Telecommunication standards;Traffic control;Protocols},
doi={10.1109/INFCOM.2013.6566745},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566746,
author={K. Xing and T. Gu and Z. Zhao and L. Shi and Y. Liu and P. Hu and Y. Wang and Y. Liang and S. Zhang and Y. Wang and L. Huang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Approaching reliable realtime communications? A novel system design and implementation for roadway safety oriented vehicular communications},
year={2013},
volume={},
number={},
pages={115-119},
abstract={Though there exist ready-made DSRC/WiFi/3G/4G cellular systems for roadway communications, there are common defects in these systems for roadway safety oriented applications and the corresponding challenges remain unsolved for years, i.e., WiFi cannot work well in vehicular networks due to the high probability of packet loss caused by burst communications, which is a common phenomenon in roadway networks; 3G/4G cannot well support real-time communications due to the nature of their designs; DSRC lacks the support to roadway safety oriented applications with hard realtime and reliability requirements [1]. To solve the conflict between the capability limitations of existing systems and the ever-growing demands of roadway safety oriented communication applications, we propose a novel system design and implementation for realtime reliable roadway communications, aiming at providing safety messages to users in a realtime and reliable manner. In our extensive experimental study, the latency is well controlled within the hard realtime requirement (100ms) for roadway safety applications given by NHTSA [2], and the reliability is proved to be improved by two orders of magnitude compared with existing experimental results [1]. Our experiments show that the proposed system for roadway safety communications can provide guaranteed highly reliable packet delivery ratio (PDR) of 99% within the hard realtime requirement 100ms under various scenarios, e.g., highways, city areas, rural areas, tunnels, bridges. Our design can be widely applied for roadway communications and facilitate the current research in both hardware and software design and further provide an opportunity to consolidate the existing work on a practical and easy-configurable low-cost roadway communication platform.},
keywords={3G mobile communication;4G mobile communication;cellular radio;mobile communication;road safety;wireless LAN;reliable realtime communications;system design;roadway safety oriented vehicular communications;DSRC/WiFi/3G/4G cellular systems;roadway communications;vehicular networks;packet loss;burst communications;roadway networks;NHTSA;packet delivery ratio;highways;city areas;rural areas;tunnels;bridges;hardware design;software design;low-cost roadway communication;Safety;Reliability;Wireless communication;Standards;Ad hoc networks;Road transportation},
doi={10.1109/INFCOM.2013.6566746},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566747,
author={E. Yanmaz and R. Kuschnig and C. Bettstetter},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Achieving air-ground communications in 802.11 networks with three-dimensional aerial mobility},
year={2013},
volume={},
number={},
pages={120-124},
abstract={Increasing availability of autonomous small-size aerial vehicles leads to a variety of applications for aerial exploration and surveillance, transport, and other domains. Many of these applications rely on networks between aerial nodes, that will have high mobility dynamics with vehicles moving in all directions in 3D space and positioning in different orientations, leading to restrictions on network connectivity. In this paper, we propose a simple antenna extension to 802.11 devices to be used on aerial nodes. Path loss and small-scale fading characteristics of air-to-ground links are analyzed using signal strength samples obtained via real-world measurements at 5 GHz. Finally, network performance in terms of throughput and number of retransmissions are presented. Results show that a throughput of 12Mbps can be achieved at distances in the order of 300m.},
keywords={antenna radiation patterns;Nakagami channels;radio networks;remotely operated vehicles;network performance;real-world measurements;signal strength samples;air-to-ground links;small-scale fading characteristics;path loss;802.11 devices;antenna extension;network connectivity;3D positioning;3D space;high mobility dynamics;aerial nodes;surveillance;aerial exploration;autonomous small-size aerial vehicles;three-dimensional aerial mobility;802.11 networks;air-ground communications;frequency 5 GHz;Throughput;Fading;Dipole antennas;Ad hoc networks;Antenna measurements;Wireless communication;3D networks;802.11;quadrotors;UAVs;vehicular communications;link modeling;Nakagami fading},
doi={10.1109/INFCOM.2013.6566747},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566748,
author={S. Yun and L. Qiu and A. Bhartia},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Multi-point to multi-point MIMO in wireless LANs},
year={2013},
volume={},
number={},
pages={125-129},
abstract={Distributed multiple-input multiple-output (MIMO) promises a dramatic capacity increase. While significant theoretical work has been done on distributed MIMO at the physical layer, how to translate the physical layer innovation into tangible benefits to real networks remains open. In particular, realizing multi-point to multi-point MIMO involves the following challenges: (i) how to accurately synchronize multiple APs in phase and time in order to successfully deliver precoded signals to the clients, and (ii) how to develop a MAC protocol to effectively support multi-point to multi-point MIMO. In this paper, we develop a practical approach to address the above challenges. We implement multi-point to multi-point MIMO for both uplink and downlink to enable multiple APs to simultaneously communicate with multiple clients. We examine a number of important MAC design issues, such as how to access the medium, perform rate adaptation, support acknowledgments in unicast traffic, deal with losses/collisions, and schedule transmissions. We demonstrate its feasibility and effectiveness through a prototype implementation on USRP and SORA, two of the most well-known software defined radio platforms.},
keywords={access protocols;MIMO communication;precoding;software radio;wireless LAN;multipoint to multipoint MIMO;wireless LAN;distributed MIMO;physical layer innovation;precoded signals;MAC deliver;MAC design issues;unicast traffic;USRP;SORA;software defined radio platforms;Synchronization;Downlink;Uplink;MIMO;Multiplexing;Wireless communication;Throughput},
doi={10.1109/INFCOM.2013.6566748},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566749,
author={B. Mumey and J. Tang and I. Judson and R. S. Wolff},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Beam scheduling and relay assignment in wireless relay networks with smart antennas},
year={2013},
volume={},
number={},
pages={130-134},
abstract={Relay Stations (RSs) can be deployed in a wireless network to extend its coverage and improve its capacity. Smart (directional) antennas can enhance the functionalities of RSs by forming the beam only towards intended receiving Subscriber Stations (SSs). In this paper, we study a joint problem of selecting a beam width and direction for the smart antenna at each RS and determining the RS assignment for SSs in each scheduling period. The objective is to maximize a utility function that can lead to a stable and high-throughput system. We define this as the Beam Scheduling and Relay Assignment Problem (BS-RAP). We show that BS-RAP is NP-hard, present a Mixed Integer Linear Programming (MILP) formulation to provide optimal solutions and present two polynomial-time greedy algorithms, one of which is shown to have a constant factor approximation ratio.},
keywords={adaptive antenna arrays;computational complexity;directive antennas;greedy algorithms;optimisation;polynomial approximation;relay networks (telecommunication);scheduling;wireless relay networks;smart antennas;beam scheduling;relay stations;directional antennas;receiving subscriber stations;scheduling period;throughput system;relay assignment problem;BS-RAP;NP-hard;mixed integer linear programming;MILP formulation;polynomial-time greedy algorithms;Relays;Wireless communication;Approximation algorithms;Scheduling;Directional antennas;Directive antennas;Wireless relay networks;smart antenna;beam scheduling;relay assignment;approximation algorithm},
doi={10.1109/INFCOM.2013.6566749},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566750,
author={P. Wan and L. Wang and C. Ma and Z. Wang and B. Xu and M. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Maximizing wireless network capacity with linear power: Breaking the logarithmic barrier},
year={2013},
volume={},
number={},
pages={135-139},
abstract={Maximizing the wireless network capacity under physical interference model is notoriously hard due to the nonlocality and the additive nature of the wireless interference under the physical interference model. This problem has been extensively studied recently with the achievable approximation bounds progressively improved from the linear factor to logarithmic factor. It has been a major open problem whether there exists a constant-approximation approximation algorithm for maximizing the wireless network capacity under the physical interference model. In this paper, we improve the status quo for the case of linear transmission power assignment, which is widely adopted due to its advantage of energy conservation. By exploring and exploiting the rich nature of the wireless interference with the linear power assignment, we develop constant-approximation algorithms for maximizing the wireless network capacity with linear transmission power assignment under the physical interference model, in both the unidirectional mode and the bidirectional mode.},
keywords={approximation theory;radio networks;radiofrequency interference;wireless network capacity maximization;linear power;logarithmic barrier;physical interference model;wireless interference;achievable approximation bound;linear factor;logarithmic factor;constant-approximation approximation algorithm;linear transmission power assignment;energy conservation;unidirectional mode;bidirectional mode;Interference;Approximation algorithms;Approximation methods;Polynomials;Vectors;Wireless networks;Link scheduling;physical interference;approximation algorithms},
doi={10.1109/INFCOM.2013.6566750},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566751,
author={T. Lin and H. T. Kung},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Concurrent channel access and estimation for scalable multiuser MIMO networking},
year={2013},
volume={},
number={},
pages={140-144},
abstract={This paper presents MIMO/CON, a PHY/MAC cross-layer design for multiuser MIMO wireless networks that delivers throughput scalable to many users. MIMO/CON supports concurrent channel access from uncoordinated and loosely synchronized users. This new capability allows a multi-antenna MIMO access point (AP) to fully realize its MIMO capacity gain. MIMO/CON draws insight from compressive sensing to carry out concurrent channel estimation. In the MAC layer, MIMO/CON boosts channel utilization by exploiting normal MAC layer retransmissions to recover otherwise undecodable packets in a collision. MIMO/CON has been implemented and validated on a 4×4 MIMO testbed with software-defined radios. In software simulations, MIMO/CON achieves a 210% improvement in MAC throughput over existing staggered access protocols in a 5-antenna AP scenario.},
keywords={access protocols;antenna arrays;channel estimation;MIMO communication;multiuser channels;radio networks;software radio;concurrent channel access;concurrent channel estimation;scalable multiuser MIMO networking;PHY-MAC cross-layer design;multiantenna MIMO access point;synchronized users;MIMO capacity gain;compressive sensing;MIMO-CON boosts channel utilization;MAC layer retransmission;software-defined radio;5-antenna AP scenario;multiuser MIMO wireless networks;MIMO;Channel estimation;Decoding;Antennas;Delays;Vectors;Throughput},
doi={10.1109/INFCOM.2013.6566751},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566752,
author={J. Wang and D. Fang and X. Chen and Z. Yang and T. Xing and L. Cai},
booktitle={2013 Proceedings IEEE INFOCOM},
title={LCS: Compressive sensing based device-free localization for multiple targets in sensor networks},
year={2013},
volume={},
number={},
pages={145-149},
abstract={Without relying on devices carried by the target, device-free localization (DFL) is attractive for many applications, such as wildlife monitoring. There still exist many challenges for DFL for multiple targets without dense deployment of sensor nodes. To fit the gap, in this paper, we propose a multi-target localization method based on compressive sensing, named LCS. The key observation is that given a pair of nodes, the received signal strength (RSS) will be different when a target locates at different locations. Taking advantage of compressive sensing in sparse recovery to handle the sparse property of the localization problem, (i.e., the vector which contains the number and location information of k targets is an ideal k-sparse signal), we presented a scalable compressive sensing based multiple target counting and localization method i.e., LCS, and rigorously justify the validity of the problem formulation. The results from our realistic deployment in a 12m×12m open space are promising. For 12 people with 24 nodes, the worst localization error ratio and counting error ratio of our LCS is no more than 8.3% and 33.3% respectively.},
keywords={compressed sensing;signal sampling;target tracking;device free localization;DFL;wildlife monitoring;sensor nodes;multitarget localization method;compressive sensing;LCS;received signal strength;RSS;sparse recovery;sparse property;sensor networks;Monitoring;Sensors;Vectors;Compressed sensing;Wildlife;Accuracy;Gaussian distribution},
doi={10.1109/INFCOM.2013.6566752},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566753,
author={P. Nintanavongsa and M. Y. Naderi and K. R. Chowdhury},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Medium access control protocol design for sensors powered by wireless energy transfer},
year={2013},
volume={},
number={},
pages={150-154},
abstract={Wireless transfer of energy will help realize perennially operating sensors, where dedicated transmitters replenish the residual node battery level through directed radio frequency (RF) waves. However, as this radiative transfer is in-band, it directly impacts data communication in the network, requiring a fresh perspective on medium access control (MAC) protocol design for appropriately sharing the channel for these two critical functions. Through an experimental study, we first demonstrate how the placement, the chosen frequency, and number of the RF energy transmitters affect the sensor charging time. These studies are then used to design a MAC protocol called RFMAC that optimizes energy delivery to desirous sensor nodes on request. To the best of our knowledge, this is the first distributed MAC protocol for RF energy harvesting sensors, and through a combination of experimentation and simulation studies, we demonstrate 112% average network throughput improvement over the modified unslotted CSMA MAC protocol.},
keywords={access protocols;carrier sense multiple access;inductive power transmission;optimisation;radio transmitters;medium access control protocol design;wireless energy transfer;perennially operating sensors;dedicated transmitters;residual node battery level;directed radiofrequency waves;directed RF waves;radiative transfer;data communication;RF energy transmitters;sensor charging time;RF-MAC;energy delivery;sensor nodes;RF energy harvesting sensors;average network throughput improvement;modified unslotted CSMA MAC protocol;Sensors;Energy exchange;Radio frequency;Media Access Protocol;Wireless sensor networks;Multiaccess communication;RF harvesting;Optimization;Medium Access Protocol;Sensor;Wireless power transfer;915 MHz},
doi={10.1109/INFCOM.2013.6566753},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566754,
author={S. Tang and J. Yuan},
booktitle={2013 Proceedings IEEE INFOCOM},
title={DAMson: On distributed sensing scheduling to achieve high Quality of Monitoring},
year={2013},
volume={},
number={},
pages={155-159},
abstract={Wireless Sensor Networks (WSN) are widely adopted to monitor and collect data, such as temperature, humidity etc., from the physical environment. Those sensor readings often exhibit strong spacial-temporal correlations, e.g., sensor readings from nearby sensors tend to be similar, and sensor readings from consecutive time slots are also highly correlated. As in our previous works, we first introduce the concept of Quality of Monitoring (QoM), and further define an utility function to quantify the QoM under different sensing schedules. In particular, the utility function is non-decreasing submodular function which is able to capture the spacial-temporal correlations among sensor readings. The objective of this work is to develop a set of distributed sensing schedules in order to achieve the highest QoM subject to energy constraint (e.g., under fixed working duty cycle). Extensive experiments validate our theoretical results. Notice that most existing works on this topic put their focus on centralized sensing schedule, which is shown to be extremely difficult to implement in large scale networked sensor system.},
keywords={scheduling;wireless sensor networks;DAMson;distributed sensing scheduling;wireless sensor networks;spacial-temporal correlations;quality of monitoring;QoM;Schedules;Sensors;Games;Monitoring;Correlation;Wireless sensor networks;Algorithm design and analysis;sensing schedule;duty cycling;Quality of Monitoring;submodular},
doi={10.1109/INFCOM.2013.6566754},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566755,
author={A. Kamthe and M. A. Carreira-Perpinan and A. E. Cerpa},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Quick construction of data-driven models of the short-term behavior of wireless links},
year={2013},
volume={},
number={},
pages={160-164},
abstract={High-quality wireless link models can enable better simulations and reduce the development time for new algorithms and protocols. However, the models underlying current simulators are either based on too simple assumptions, so they are unrealistic, or are based on sophisticated machine learning techniques that require extensive training data from the target link, so they are more realistic but impractical. We consider the practical scenario where data collection time is limited (e.g. a few minutes) and cannot afford to deploy a testbed infrastructure with cabling, power and storage. We propose techniques that can construct an accurate machine learning model of the short-term behavior of a target wireless link given only limited training data for the latter, by adapting a reference model that was trained with abundant data. The parameters of the target model are a constrained transformation of the parameters of the reference model, thus the actual number of free parameters is much smaller, and can be reliably estimated with much less data. While estimating the target model from scratch requires 1 to 5 hours of target link data, we show our adaptation technique only requires under 3 minutes of data, for all packet reception rate regimes. We also show that we can construct adapted models for target links in different environments, packet sizes, interference conditions and radio technology (802.15.4 or 802.11b).},
keywords={learning (artificial intelligence);radio links;radiofrequency interference;telecommunication computing;data-driven model;short-term behavior;high-quality wireless link model;development time reduction;protocols;machine learning technique;training data;data collection time;testbed infrastructure;cabling;machine learning model;target wireless link;reference model;target model parameter;target link data;adaptation technique;packet reception rate;packet size;interference condition;radio technology;802.15.4;802.11b;Adaptation models;Data models;Computational modeling;Wireless communication;Vectors;Wireless sensor networks;Data collection},
doi={10.1109/INFCOM.2013.6566755},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566756,
author={H. Li and X. Cheng and K. Li and X. Xing and T. Jing},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Utility-based cooperative spectrum sensing scheduling in cognitive radio networks},
year={2013},
volume={},
number={},
pages={165-169},
abstract={In this paper, we consider the problem of cooperative spectrum sensing scheduling (C3S) in a cognitive radio network when there exist multiple primary channels. Deviated from the existing research our work focuses on a scenario in which each secondary user has the freedom to decide whether or not to participate in cooperative spectrum sensing; if not, the SU becomes a free rider who can eavesdrop the decision about the channel status made by others. Such a mechanism can conserve the energy for spectrum sensing at a risk of scarifying the spectrum sensing performance. To overcome this problem, we address the following two questions: “which action (contributing to spectrum sensing or not) to take?” and “which channel to sense?” To answer the first question, we model our framework as an evolutionary game in which each SU makes its decision based on its utility history, and takes an action more frequently if it brings a relatively higher utility. We also develop an entropy based coalition formation algorithm to answer the second question, where each SU always chooses the coalition (channel) that brings the most information regarding the status of the corresponding channel. All the SUs selecting the same channel to sense form a coalition. Our simulation study indicates that the proposed scheme can guarantee the detection probability at a low false alarm rate.},
keywords={cognitive radio;cooperative communication;evolutionary computation;game theory;probability;radio networks;radio spectrum management;wireless channels;utility-based cooperative spectrum sensing scheduling;cognitive radio networks;C3S;multiple primary channel;SU;energy conservation;evolutionary game theory;entropy based coalition formation algorithm;detection probability;false alarm rate;M primary channel;N secondary user;energy consumption;Sensors;Entropy;Games;Cognitive radio;Uncertainty;Energy consumption;History;Cognitive radio networks;cooperative spectrum sensing;free rider;evolutionary game;coalition formation},
doi={10.1109/INFCOM.2013.6566756},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566757,
author={C. Kam and S. Kompella and G. D. Nguyen and J. E. Wieselthier and A. Ephremides},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Multicast throughput stability analysis for cognitive cooperative random access},
year={2013},
volume={},
number={},
pages={170-174},
abstract={In this work, we investigate the queue stability of a two-user cognitive radio system with multicast traffic. We study the impact of network-level cooperation, in which one of the nodes can relay the packets of the other user that are not received at the destinations. Under this approach, if a packet transmitted by the primary user is not successfully received by the destination set but is captured by the secondary source, then the secondary user assumes responsibility for completing the transmission of the packet; therefore, the primary releases it from its queue, enabling it to process the next packet. We demonstrate that the stability region of this cooperative approach is larger than that of the noncooperative approach, which translates into a benefit for both users of this multicast system. Our system model allows for the possibility of multipacket reception, and the optimal transmission strategies for different levels of multipacket reception capability are observed in our numerical results.},
keywords={cognitive radio;cooperative communication;multicast communication;queueing theory;telecommunication traffic;multicast throughput stability analysis;cognitive cooperative random access;two-user cognitive radio system;queue stability;multicast traffic;network-level cooperation;primary user;secondary source;noncooperative approach;multipacket reception;optimal transmission strategies;Stability analysis;Numerical stability;Throughput;Relays;Receivers;Markov processes;Unicast},
doi={10.1109/INFCOM.2013.6566757},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566758,
author={T. Jing and S. Zhu and H. Li and X. Cheng and Y. Huo},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Cooperative relay selection in cognitive radio networks},
year={2013},
volume={},
number={},
pages={175-179},
abstract={The benefits of cognitive radio networks have been well recognized with the dramatic development of the wireless applications in recent years. While many existing works assume that the secondary transmissions are negative interference to the primary users (PUs), in this paper, we take secondary users (SUs) as positive potential cooperators for the primary users. In particular, we consider the problem of cooperative relay selection, in which the PUs actively select appropriate SUs as relay nodes to enhance their transmission performance. The most critical challenge for such a problem of cooperative relay selection is how to select a relay efficiently. But due to the potentially large number of secondary users, it is infeasible for a PU transmitter to first scan all the SUs and then pick the best one. Basically, the PU transmitter intends to observe the SUs sequentially. After observing a SU, the PU needs to make a decision on whether to terminate its observation and use the current SU as its relay or to skip it and observe the next SU. We address this problem by using the optimal stopping theory, and derive the optimal stopping rule. To evaluate the performance of our proposed scheme, we conduct an extensive simulation study. The results reveal the impact of different parameters on the system performance, which can be adjusted to satisfy specific system requirements.},
keywords={cognitive radio;cooperative communication;radio transmitters;radiofrequency interference;cooperative relay selection;cognitive radio networks;wireless applications;secondary transmissions;primary users;secondary users;transmission performance;cooperative relay selection problem;PU transmitter;optimal stopping theory;Relays;Cognitive radio;Radio transmitters;Signal to noise ratio;Receivers;System performance;Cognitive radio networks;cooperative relay selection;optimal stopping theory},
doi={10.1109/INFCOM.2013.6566758},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566759,
author={Y. Chen and J. Zhang and K. Wu and Q. Zhang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={TAMES: A Truthful Auction Mechanism for heterogeneous spectrum allocation},
year={2013},
volume={},
number={},
pages={180-184},
abstract={Spectrums are heterogeneous, especially from the aspect of their central frequency. According to signal propagation properties, low-frequency spectrum generally has lower path loss, thus longer transmission range, compared with high-frequency spectrum. Cellular operators with different targeted cell size will have different preferences for spectrums with different frequencies. Furthermore, the transmission range also affects the interference relationships among transmitters. Transmitters who can reuse the same high-frequency spectrum may interfere with each other when reusing the low-frequency spectrum, so it is difficult to decide how to construct the interference graph to exploit spectrum reusability among transmitters. Auction is considered as an efficient way for spectrum allocation. However, most of the previous works only considered homogenous spectrum auction, failing to address the problem of spectrum heterogeneity. In this paper, we propose TAMES, a Truthful Auction Mechanism for hEterogeneous Spectrum allocation, which allows buyers to freely express their different preferences towards different spectrums. Frequency-specific interference graphs are constructed to determine buyer groups. The proposed heterogeneous spectrum auction is theoretically proved to be truthful and individual rational. The simulation results verifies that the proposed auction mechanism outperforms other auction mechanisms with homogenous bid or homogenous interference graph. The proposed auction mechanism is able to yield higher buyers' satisfaction, seller's revenue and spectrum utilization.},
keywords={cellular radio;graph theory;radio transmitters;radiofrequency interference;TAMES;central frequency;signal propagation properties;low-frequency spectrum;path loss;high-frequency spectrum;cellular operators;interference relationships;transmitters;homogenous spectrum auction;truthful auction mechanism for heterogeneous spectrum allocation;frequency-specific interference graphs;buyer groups;homogenous interference graph;Interference;Cost accounting;Resource management;Transmitters;Economics;Educational institutions;Simulation},
doi={10.1109/INFCOM.2013.6566759},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566760,
author={S. Ren and M. van der Schaar},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Joint design of Dynamic Scheduling and Pricing in wireless cloud computing},
year={2013},
volume={},
number={},
pages={185-189},
abstract={In this paper, we consider a wireless cloud computing system in which a profit-maximizing wireless service provider provides cloud computing services to its subscribers. In particular, we focus on batch services, which, due to their non-urgent nature, allow more scheduling flexibility than their interactive counterparts. Unlike the existing research that studied separately demand-side management and energy cost saving techniques (both of which are critical to profit maximization), we propose a provably-efficient Dynamic Scheduling and Pricing (Dyn-SP) algorithm which proactively adapts the service demand to workload scheduling in the data center and opportunistically utilizes low electricity prices to process batch jobs for energy cost saving. Without the necessity of predicting future information as assumed by some prior works, Dyn-SP can be applied to an arbitrarily random environment in which the electricity price, available renewable energy supply, and wireless network capacities may evolve over time as arbitrary stochastic processes. It is proved that, compared to the optimal offline algorithm with future information, Dyn-SP can produce a close-to-optimal longterm profit while bounding the job queue length in the data center. We also show both analytically and numerically that a desired tradeoff between the profit and queueing delay can be obtained by appropriately tuning the control parameter. Finally, we perform a simulation study to demonstrate the effectiveness of Dyn-SP.},
keywords={cloud computing;computer centres;pricing;processor scheduling;profitability;queueing theory;radio networks;stochastic processes;telecommunication power management;profit-maximizing wireless service provider;wireless cloud computing system;batch services;provably-efficient dynamic scheduling and pricing algorithm;Dyn-SP algorithm;service demand;workload scheduling;data center;electricity prices;batch job processing;energy cost saving;arbitrarily random environment;renewable energy supply;wireless network capacities;arbitrary stochastic process;close-to-optimal long-term profit;job queue length;queueing delay;Pricing;Electricity;Servers;Dynamic scheduling;Delays;Heuristic algorithms;Cooling},
doi={10.1109/INFCOM.2013.6566760},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566761,
author={W. Zhang and Y. Wen and D. O. Wu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Energy-efficient scheduling policy for collaborative execution in mobile cloud computing},
year={2013},
volume={},
number={},
pages={190-194},
abstract={In this paper, we investigate the scheduling policy for collaborative execution in mobile cloud computing. A mobile application is represented by a sequence of fine-grained tasks formulating a linear topology, and each of them is executed either on the mobile device or offloaded onto the cloud side for execution. The design objective is to minimize the energy consumed by the mobile device, while meeting a time deadline. We formulate this minimum-energy task scheduling problem as a constrained shortest path problem on a directed acyclic graph, and adapt the canonical “LARAC” algorithm to solving this problem approximately. Numerical simulation suggests that a one-climb offloading policy is energy efficient for the Markovian stochastic channel, in which at most one migration from mobile device to the cloud is taken place for the collaborative task execution. Moreover, compared to standalone mobile execution and cloud execution, the optimal collaborative execution strategy can significantly save the energy consumed on the mobile device.},
keywords={cloud computing;directed graphs;energy conservation;energy consumption;groupware;Markov processes;mobile computing;scheduling;energy-efficient scheduling policy;collaborative task execution;mobile cloud computing;mobile application;linear topology;mobile device;energy consumption minimization;minimum-energy task scheduling problem;constrained shortest path problem;directed acyclic graph;LARAC algorithm;one-climb offloading policy;Markovian stochastic channel;mobile execution;cloud execution;optimal collaborative execution strategy;Mobile handsets;Mobile communication;Collaboration;Energy consumption;Cloud computing;Stochastic processes;Topology;collaborative execution;mobile cloud computing;scheduling policy},
doi={10.1109/INFCOM.2013.6566761},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566762,
author={P. Shu and F. Liu and H. Jin and M. Chen and F. Wen and Y. Qu and B. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={eTime: Energy-efficient transmission between cloud and mobile devices},
year={2013},
volume={},
number={},
pages={195-199},
abstract={Mobile cloud computing, promising to extend the capabilities of resource-constrained mobile devices, is emerging as a new computing paradigm which has fostered a wide range of exciting applications. In this new paradigm, efficient data transmission between the cloud and mobile devices becomes essential. This, however, is highly unreliable and unpredictable due to several uncontrollable factors, particularly the instability and intermittency of wireless connections, fluctuation of communication bandwidth, and user mobility. Consequently, this puts a heavy burden on the energy consumption of mobile devices. Confirmed by our experiments, significantly more energy is consumed during “bad” connectivity. Inspired by the feasibility to schedule data transmissions for prefetching-friendly or delay-tolerant applications, in this paper, we present eTime, a novel Energy-efficient data Transmission strategy between cloud and Mobile dEvices, based on Lyapunov optimization. It aggressively and adaptively seizes the timing of good connectivity to prefetch frequently used data while deferring delay-tolerant data in bad connectivity. To cope with the randomness and unpredictability of wireless connectivity, eTime only relies on the current status information to make a global energy-delay tradeoff decision. Our evaluations from both trace-driven simulation and realworld implementation show that eTime can be applied to various popular applications while achieving 20%-35% energy saving.},
keywords={cloud computing;delay tolerant networks;energy conservation;mobile computing;optimisation;radio networks;eTime;energy-efficient transmission;mobile cloud computing;resource-constrained mobile devices;computing paradigm;wireless connections;communication bandwidth;user mobility;energy consumption;bad connectivity;data transmissions;delay-tolerant applications;energy-efficient data transmission strategy;Lyapunov optimization;frequently used data;delay-tolerant data;wireless connectivity;status information;global energy-delay tradeoff decision;trace-driven simulation;realworld implementation;Bandwidth;Energy consumption;IEEE 802.11 Standards;Smart phones;Data communication;Mobile communication},
doi={10.1109/INFCOM.2013.6566762},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566763,
author={X. He and H. Dai and W. Shen and P. Ning},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Is link signature dependable for wireless security?},
year={2013},
volume={},
number={},
pages={200-204},
abstract={A fundamental assumption of link signature based security mechanisms is that the wireless signals received at two locations separated by more than half a wavelength are essentially uncorrelated. However, it has been observed that in certain circumstances (e.g., with poor scattering and/or a strong line-of-sight (LOS) component), this assumption is invalid. In this paper, a Correlation ATtack (CAT) is proposed to demonstrate the potential vulnerability of the link signature based security mechanisms in such circumstances. Based on statistical inference, CAT explicitly exploits the spatial correlations to reconstruct the legitimate link signature from the observations of multiple adversary receivers deployed in vicinity. Our findings are verified through theoretical analysis, well-known channel correlation models, and experiments on USRP platforms and GNURadio.},
keywords={correlation methods;radio links;radio receivers;telecommunication security;wireless channels;link signature;wireless security;wireless signals;poor scattering;line-of-sight component;LOS component;correlation attack;CAT;vulnerability;statistical inference;spatial correlations;multiple adversary receivers;channel correlation;USRP platforms;GNURadio;Receivers;Correlation;Channel estimation;Wireless communication;Communication system security;Security;Wireless sensor networks},
doi={10.1109/INFCOM.2013.6566763},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566764,
author={R. Di Pietro and S. Guarino},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Data confidentiality and availability via secret sharing and node mobility in UWSN},
year={2013},
volume={},
number={},
pages={205-209},
abstract={In Mobile Unattended Wireless Sensor Networks (MUWSNs), nodes sense the environment and store the acquired data until the arrival of a trusted data sink. In this paper, we address the fundamental issue of quantifying to which extent secret sharing schemes, combined with nodes mobility, can help in assuring data availability and confidentiality. We provide accurate analytical results binding the fraction of the network accessed by the sink and the adversary to the amount of information they can successfully recover. Extensive simulations support our findings.},
keywords={mobility management (mobile radio);telecommunication security;wireless sensor networks;data confidentiality;data availability;secret sharing scheme;node mobility;MUWSN;mobile unattended wireless sensor network;trusted data sink;resource-constrained autonomous sensor node;Wireless sensor networks;Availability;Cryptography;Mobile communication;Mobile computing;Data models;UWSN security and privacy;metrics;mobility models},
doi={10.1109/INFCOM.2013.6566764},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566765,
author={H. Park and S. Song and B. Choi and C. Huang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={PASSAGES: Preserving Anonymity of Sources and Sinks against Global Eavesdroppers},
year={2013},
volume={},
number={},
pages={210-214},
abstract={While many security schemes protect the content of messages in the Distributed Sensing Systems (DSS), the contextual information, such as communication patterns, is left vulnerable and can be utilized by attackers to identify critical information such as the locations of event sources and message sinks. Existing solutions for location anonymity are mostly designed to protect source or sink location anonymity individually against limited eavesdroppers on a small region at a time. However, they can be easily defeated by highly motivated global eavesdroppers that can monitor entire communication events on the DSS. To grapple with these challenges, we propose a mechanism for Preserving Anonymity of Sources and Sinks against Global Eavesdroppers (PASSAGES). PASSAGES uses a small number of stealthy permeability tunnels such as wormholes and message ferries to scatter and hide the communication patterns. Unlike prior schemes, PASSAGES effectively achieves a high anonymity level for both source and sink locations, without incurring extra communication overheads. We quantify the location anonymity level and evaluate the effectiveness of PASSAGES via analysis as well as extensive simulations. We also perform evaluations on the synergistic effect when PASSAGES is combined with other traditional solutions.},
keywords={distributed sensors;telecommunication security;telecommunication traffic;telecommunication transmission lines;PASSAGES;preserving anonymity;global eavesdroppers;security schemes;distributed sensing systems;DSS;contextual information;communication patterns;critical information;event sources;message sinks;location anonymity;stealthy permeability tunnels;wormholes;message ferries;source location;sink location;communication overheads;Uncertainty;Mobile communication;Wireless sensor networks;Sensors;Wireless communication;Privacy;Permeability},
doi={10.1109/INFCOM.2013.6566765},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566766,
author={S. Chen and A. Pande and K. Zeng and P. Mohapatra},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Video source identification in lossy wireless networks},
year={2013},
volume={},
number={},
pages={215-219},
abstract={Video source identification is very important in validating video evidence, tracking down video piracy crimes and regulating individual video sources. With the prevalence of wireless communication, wireless video cameras continue to replace their wired counterparts in security/surveillance systems and tactical networks. However, wirelessly streamed videos usually suffer from blocking and blurring due to inevitable packet loss in wireless transmissions. The existing source identification methods experience significant performance degradation or even fail to work when identifying videos with blocking and blurring. In this paper, we propose a method which is effective and efficient in identifying such wirelessly streamed videos. In addition, we also propose to incorporate wireless channel signatures and selective frame processing into source identification, which significantly improve the identification speed.},
keywords={cameras;radio networks;telecommunication security;video surveillance;wireless channels;video source identification;lossy wireless networks;video evidence;video piracy crimes;individual video sources;wireless communication;wireless video cameras;security-surveillance systems;tactical networks;wirelessly streamed videos;inevitable packet loss;wireless transmissions;wireless channel signatures;selective frame processing;source identification;Noise;Cameras;Streaming media;Wireless communication;Wireless sensor networks;Communication system security;Packet loss},
doi={10.1109/INFCOM.2013.6566766},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566767,
author={B. Zhang and G. Kreitz and M. Isaksson and J. Ubillos and G. Urdaneta and J. A. Pouwelse and D. Epema},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Understanding user behavior in Spotify},
year={2013},
volume={},
number={},
pages={220-224},
abstract={Spotify is a peer-assisted music streaming service that has gained worldwide popularity in the past few years. Until now, little has been published about user behavior in such services. In this paper, we study the user behavior in Spotify by analyzing a massive dataset collected between 2010 and 2011. Firstly, we investigate the system dynamics including session arrival patterns, playback arrival patterns, and daily variation of session length. Secondly, we analyze individual user behavior on both multiple and single devices. Our analysis reveals the favorite times of day for Spotify users. We also show the correlations between both the length and the downtime of successive user sessions on single devices. In particular, we conduct the first analysis of the device-switching behavior of a massive user base.},
keywords={behavioural sciences computing;media streaming;music;peer-to-peer computing;Spotify;peer assisted music streaming service;user behavior;collected dataset analysis;session arrival pattern;playback arrival pattern;successive user session;device switching behavior;massive user base;Mobile communication;Correlation;Switches;Music;Streaming media;Mobile handsets;Mobile computing},
doi={10.1109/INFCOM.2013.6566767},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566768,
author={J. M. Agosta and J. Chandrashekar and M. Crovella and N. Taft and D. Ting},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Mixture models of endhost network traffic},
year={2013},
volume={},
number={},
pages={225-229},
abstract={We model a little studied type of traffic, namely the network traffic generated from endhosts. We introduce a parsimonious model of the marginal distribution for connection arrivals consisting of mixture models with both heavy and light-tailed component distributions. Our methodology assumes that the underlying user data can be fitted to one of several models, and we apply Bayesian model selection criterion to choose the preferred combination of components. Our experiments show that a simple Pareto-exponential mixture model is preferred over more complex alternatives, for a wide range of users. This model has the desirable property of modeling the entire distribution, effectively clustering the traffic into the heavy-tailed as well as the non-heavy-tailed components. Also this method quantifies the wide diversity in the observed endhost traffic.},
keywords={Bayes methods;Pareto distribution;telecommunication networks;telecommunication traffic;endhost network traffic;parsimonious model;marginal distribution;connection arrivals;heavy-tailed component distributions;light-tailed component distributions;Bayesian model selection criterion;simple Pareto-exponential mixture;traffic clustering;wide diversity;Data models;Computational modeling;Maximum likelihood estimation;Approximation methods;Mathematical model;Educational institutions;Bayes methods},
doi={10.1109/INFCOM.2013.6566768},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566769,
author={I. Bermudez and S. Traverso and M. Mellia and M. Munafò},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Exploring the cloud from passive measurements: The Amazon AWS case},
year={2013},
volume={},
number={},
pages={230-234},
abstract={This paper presents a characterization of Amazon's Web Services (AWS), the most prominent cloud provider that offers computing, storage, and content delivery platforms. Leveraging passive measurements, we explore the EC2, S3 and CloudFront AWS services to unveil their infrastructure, the pervasiveness of content they host, and their traffic allocation policies. Measurements reveal that most of the content residing on EC2 and S3 is served by one Amazon datacenter, located in Virginia, which appears to be the worst performing one for Italian users. This causes traffic to take long and expensive paths in the network. Since no automatic migration and load-balancing policies are offered by AWS among different locations, content is exposed to the risks of outages. The CloudFront CDN, on the contrary, shows much better performance thanks to the effective cache selection policy that serves 98% of the traffic from the nearest available cache. CloudFront exhibits also dynamic load-balancing policies, in contrast to the static allocation of instances on EC2 and S3. Information presented in this paper will be useful for developers aiming at entrusting AWS to deploy their contents, and for researchers willing to improve cloud design.},
keywords={cache storage;cloud computing;Web services;cloud-based services;passive measurement;Amazon AWS case;Web services;EC2;S3;CloudFront AWS services;traffic allocation policy;automatic migration;load-balancing policy;CloudFront CDN;cache selection policy;static allocation;Servers;Time factors;IP networks;Monitoring;Availability;Measurement;Web services},
doi={10.1109/INFCOM.2013.6566769},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566770,
author={D. Joumblatt and J. Chandrashekar and B. Kveton and N. Taft and R. Teixeira},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Predicting user dissatisfaction with Internet application performance at end-hosts},
year={2013},
volume={},
number={},
pages={235-239},
abstract={We design predictors of user dissatisfaction with the performance of applications that use networking. Our approach combines user-level feedback with low level machine and networking metrics. The main challenges of predicting user dissatisfaction, that arises when networking conditions adversely affect applications, comes from the scarcity of user feedback and the fact that poor performance episodes are rare. We develop a methodology to handle these challenges. Our method processes low level data via quantization and feature selection steps. We combine this with user labels and employ supervised learning techniques to build predictors. Using data from 19 personal machines, we show how to build training sets and demonstrate that non-linear SVMs achieve higher true positive rates (around 0.9) than predictors based on linear models. Finally we quantify the benefits of building per-application predictors as compared to general predictors that use data from multiple applications simultaneously to anticipate user dissatisfaction.},
keywords={computer network performance evaluation;ergonomics;internetworking;learning (artificial intelligence);support vector machines;user dissatisfaction prediction;Internet application performance;end-hosts;user-level feedback;low-level machine metrics;low-level networking metrics;networking conditions;low-level data processing;quantization;feature selection;data labelling;supervised learning techniques;personal machines;training sets;nonlinear SVM;true-positive rates;Measurement;Feature extraction;Vectors;Postal services;YouTube;Training;Quality of service},
doi={10.1109/INFCOM.2013.6566770},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566771,
author={H. Yuan and P. Crowley},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Experimental evaluation of content distribution with NDN and HTTP},
year={2013},
volume={},
number={},
pages={240-244},
abstract={Content distribution is a primary activity on the Internet. Name-centric network architectures support content distribution intrinsically. Named Data Networking (NDN), one recent such scheme, names packets rather than end-hosts, thereby enabling packets to be cached and redistributed by routers. Among alternative name-based systems, HTTP is the most significant by any measure. A majority of today's content distribution services leverage the widely deployed HTTP infrastructure, such as web servers and caching proxies. As a result, HTTP can be viewed as a practical, name-based content distribution solution. Of course, NDN and HTTP do not overlap entirely in their capabilities and design goals, but both support name-based content distribution. This paper presents an experimental performance evaluation of NDN-based and HTTP-based content distribution solutions. Our findings verify popular intuition, but also surprise in some ways. In wired networks with local-area transmission latencies, the HTTP-based solution dramatically outperforms NDN, with roughly 10× greater sustained throughput. In networks with lossy access links, such as wireless links with 10% drop rates, or with non-local transmission delays, due to faster link retransmission brought by architectural advantages of NDN, the situation reverses and NDN outperforms HTTP, with sustained throughput increased by roughly 4× over a range of experimental scenarios.},
keywords={content management;hypermedia;Internet;multimedia communication;radio links;telecommunication network routing;transport protocols;Internet;name-centric network architecture;named data networking;names packet;end-host;router;name-based system;content distribution service;HTTP infrastructure;Web server;caching proxies;name-based content distribution solution;NDN;HTTP-based content distribution solution;wired network;local-area transmission latencies;network throughput;lossy access link;wireless link;drop rate;nonlocal transmission delay;link retransmission;Delays;Throughput;Computer architecture;Web servers},
doi={10.1109/INFCOM.2013.6566771},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566772,
author={J. Llorca and A. M. Tulino and K. Guan and J. Esteban and M. Varvello and N. Choi and D. C. Kilper},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Dynamic in-network caching for energy efficient content delivery},
year={2013},
volume={},
number={},
pages={245-249},
abstract={Consider a network of prosumers of media content in which users dynamically create and request content objects. The request process is governed by the objects' popularity and varies across network regions and over time. In order to meet user requests, content objects can be stored and transported over the network, characterized by the capacity and energy efficiency of the storage and transport resources. The energy efficient dynamic in-network caching problem aims at finding the evolution of the network configuration, in terms of the content objects being cached and transported over each network element at any given time, that meets user requests, satisfies network resource capacities and minimizes overall energy use. We provide 1) an information-centric optimization framework for the energy efficient dynamic in-network caching problem, 2) an offline solution, EE-OFD, based on an integer linear program (ILP) that obtains the maximum efficiency gains that can be achieved with global knowledge of user requests and network resources, and 3) an efficient fully distributed online solution, EEOND, that allows network nodes to make local caching decisions based on their current estimate of the global energy benefit. Our solutions take into account the network heterogeneity, in terms of capacity, energy efficiency and content popularity, and adapt to changing network conditions minimizing overall energy use.},
keywords={cache storage;computer network management;integer programming;linear programming;dynamic in-network caching;energy efficient content delivery;media content prosumers;dynamical content object creation;content object request;object popularity;user request;content object storage;content object transport;energy efficiency;transport resource;network configuration;network resource capacity;energy use minimization;information-centric optimization framework;EE-OFD;integer linear program;ILP;maximum efficiency gain;fully distributed online solution;EEOND;network nodes;local caching decision;network heterogeneity;network conditions;Energy consumption;Routing;Dynamic scheduling;Vegetation;Optimization;Internet;Content distribution networks;Energy efficiency;content delivery network;content centric networking;in-network caching;integer linear programming;distributed optimization},
doi={10.1109/INFCOM.2013.6566772},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566773,
author={Y. Liu and F. Li and L. Guo and B. Shen and S. Chen},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Effectively minimizing redundant Internet streaming traffic to iOS devices},
year={2013},
volume={},
number={},
pages={250-254},
abstract={The Internet has witnessed rapidly increasing streaming traffic to various mobile devices. In this paper, we find that for the popular iOS based mobile devices, accessing popular Internet streaming services typically involves about 10% - 70% unnecessary redundant traffic. Such a practice not only overutilizes and wastes resources on the server side and the network (cellular or Internet), but also consumes additional battery power on users' mobile devices and leads to possible monetary cost. To alleviate such a situation without changing the server side or the iOS, we design and implement a CStreamer prototype that can transparently work between existing iOS devices and media servers. We also build a CStreamer iOS App to enable end users to access Internet streaming services via CStreamer. Experiments conducted based on this prototype running on Amazon EC2 show that CStreamer can completely eliminate the redundant traffic without degrading user's QoS.},
keywords={Internet;mobile handsets;mobile radio;telecommunication services;video streaming;redundant Internet streaming traffic;iOS based mobile device;wastes resource;server side;user mobile device;CStreamer prototype;media server;Amazon EC2;user QoS;cellular network;CStreamer iOS app;Internet streaming services;video sharing Website;Streaming media;Servers;Mobile communication;Media;Internet;Mobile handsets;YouTube},
doi={10.1109/INFCOM.2013.6566773},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566774,
author={M. Sardari and A. Beirami and J. Zou and F. Fekri},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Content-aware network data compression using joint memorization and clustering},
year={2013},
volume={},
number={},
pages={255-259},
abstract={Recent studies have shown the existence of considerable amount of packet-level redundancy in the network flows. Since application-layer solutions cannot capture the packet-level redundancy, development of new content-aware approaches capable of redundancy elimination at the packet and sub-packet levels is necessary. These requirements motivate the redundancy elimination of packets from an information-theoretic point of view. For efficient compression of packets, a new framework called memory-assisted universal compression has been proposed. This framework is based on learning the statistics of the source generating the packets at some intermediate nodes and then leveraging these statistics to effectively compress a new packet. This paper investigates both theoretically and experimentally the memory-assisted compression of network packets. Clearly, a simple source cannot model the data traffic. Hence, we consider traffic from a complex source that is consisted of a mixture of simple information sources for our analytic study. We develop a practical code for memory-assisted compression and combine it with a proposed hierarchical clustering to better utilize the memory. Finally, we validate our results via simulation on real traffic traces. Memory-assisted compression combined with hierarchical clustering method results in compression of packets close to the fundamental limit. As a result, we report a factor of two improvement over traditional end-to-end compression.},
keywords={computer networks;data compression;encoding;pattern clustering;content-aware network data compression;packet-level redundancy;network flows;application-layer solutions;redundancy elimination;subpacket levels;memory-assisted universal compression;source statistics;intermediate nodes;network packet memory-assisted compression;hierarchical clustering method;universal coding techniques;Redundancy;Compounds;Clustering algorithms;Classification algorithms;Joints;Servers;Entropy},
doi={10.1109/INFCOM.2013.6566774},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566775,
author={A. O. F. Atya and I. Broustis and S. Singh and D. Syrivelis and S. V. Krishnamurthy and T. F. L. Porta},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Wireless network coding: Deciding when to flip the switch},
year={2013},
volume={},
number={},
pages={260-264},
abstract={Network coding has been shown to offer significant throughput benefits over store-and-forward routing in certain wireless network topologies. However, the application of network coding may not always improve the network performance. In this paper<sup>1</sup>, we provide a comprehensive analytical study, which helps in assessing when network coding is preferable to a traditional store-and-forward approach. Interestingly, our study reveals that in many topological scenarios, network coding can in fact hurt the throughput performance; in such scenarios, applying the store-and-forward approach leads to higher network throughput. We validate our analytical findings via extensive testbed experiments, and we extract guidelines on when network coding should be applied instead of store-and-forward.},
keywords={network coding;telecommunication network topology;telecommunication switching;wireless network topologies;network coding;store and forward approach;network throughput;testbed experiments;Throughput;Relays;Topology;Encoding;Lifting equipment;Network coding;Bit rate;Wireless Network Coding;Rate Adaptation;Network Policy;Simulation;Testbed;Measurements},
doi={10.1109/INFCOM.2013.6566775},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566776,
author={W. Huang and T. Ho and H. Yao and S. Jaggi},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Rateless resilient network coding against byzantine adversaries},
year={2013},
volume={},
number={},
pages={265-269},
abstract={This paper studies rateless network error correction codes for reliable multicast in the presence of adversarial errors. We present rateless coding schemes for two adversarial models, where the source sends more redundancy over time, until decoding succeeds. The first model assumes there is a secret channel between the source and the destination that the adversaries cannot overhear. The rate of the channel is negligible compared to the main network. In the second model the source and destination share random secrets independent of the input information. The amount of secret information required is negligible compared to the amount of information sent. Both schemes are capacity optimal, distributed, polynomial-time and end-to-end in that other than the source and destination nodes, other intermediate nodes carry out classical random linear network coding.},
keywords={decoding;error correction codes;multicast communication;network coding;telecommunication network reliability;capacity optimal;decoding succeeds;reliable multicast;error correction codes;byzantine adversaries;rateless resilient network coding;Redundancy;Network coding;Decoding;Encoding;Vectors;Equations;Error correction codes},
doi={10.1109/INFCOM.2013.6566776},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566777,
author={J. Wang and K. Lu and J. Wang and C. Qiao},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Untraceability of mobile devices in wireless mesh networks using linear network coding},
year={2013},
volume={},
number={},
pages={270-274},
abstract={To protect user privacy in wireless mesh networks (WMNs), it is important to address two major challenges, namely: flow untraceability and movement untraceability, which prevent malicious attackers from deducing the flow paths and the movement tracks of mobile devices. For these two privacy requirements, most existing approaches rely on encrypting the whole packet, appending random padding, and applying random delay for each message at every intermediate node, resulting in significant computational and communication overheads. Recently, linear network coding (LNC) has been introduced as an alternative but the global encoding vectors (GEVs) of coded messages have to be encrypted so as to conceal the relationships between the incoming and outgoing messages. In this paper, we aim to explore the potential of LNC to ensure the flow untraceability and movement untraceability. Specifically, we first determine the necessary and sufficient condition, with which the two privacy requirements can be achieved without encrypting either GEVs or message contents. We then design a deterministic untraceable LNC (ULNC) scheme to provide flow untraceability and movement untraceability when the sufficient and necessary condition is satisfied. Finally, we discuss the effectiveness of the proposed ULNC scheme against traffic analysis attacks in WMNs.},
keywords={linear codes;mobile handsets;network coding;telecommunication security;telecommunication traffic;wireless mesh networks;mobile device untraceability;wireless mesh networks;linear network coding;user privacy protection;WMN;flow untraceability;movement untraceability;malicious attackers;mobile device movement tracks;flow paths;privacy requirements;random padding;random delay;intermediate node;computational overheads;communication overheads;GEV;coded messages;incoming messages;outgoing messages;message contents;traffic analysis attacks;untraceable LNC scheme;ULNC scheme;Correlation;Mobile handsets;Network coding;Privacy;Vectors;Cryptography;Wireless communication},
doi={10.1109/INFCOM.2013.6566777},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566778,
author={S. Wang and G. Tan and Y. Liu and H. Jiang and T. He},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Coding Opportunity Aware Backbone metrics for broadcast in wireless networks},
year={2013},
volume={},
number={},
pages={275-279},
abstract={Reducing transmission redundancy is key to the efficiency of wireless network broadcast. A standard technique to achieve this is to create a network backbone consisting of a subset of nodes that are responsible for data forwarding, while other nodes act as passive receivers. On top of this, network coding (NC) is often used to further reduce unnecessary transmissions. The main problem with this backbone+NC approach is that the backbone construction process is blind of what is needed by NC, thus may produce a structure with little benefit to the NC algorithms. To address this problem, we propose a Coding Opportunity Aware Backbone (COAB) construction scheme, which seeks to maximally exploit coding opportunities when selecting backbone forwarders. We show that the better informed backbone construction process leads to significantly increased coding frequency, at minimal cost of localized information exchange. The highlight of our work is COAB's broad applicability and effectiveness. We integrate COAB with ten state-of-the-art broadcast algorithms, specified in eight publications [1]-[8], and evaluate it with prototype implementations with 30 MICAz nodes. The experimental results show that our design outperforms the existing schemes substantially.},
keywords={network coding;radio broadcasting;radio networks;coding opportunity aware backbone metrics;transmission redundancy reduction;wireless network broadcast;network backbone;data forwarding;passive receiver;network coding;NC approach;coding opportunity aware backbone construction scheme;COAB construction scheme;localized information exchange;COAB broad applicability;Encoding;Network coding;Algorithm design and analysis;Measurement;Wireless networks;Receivers;Clustering algorithms},
doi={10.1109/INFCOM.2013.6566778},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566779,
author={S. Wiethölter and A. Ruttor and U. Bergemann and M. Opper and A. Wolisz},
booktitle={2013 Proceedings IEEE INFOCOM},
title={DARA: Estimating the behavior of data rate adaptation algorithms in WLAN hotspots},
year={2013},
volume={},
number={},
pages={280-284},
abstract={Data rate adaptation (RA) schemes are the key means by which WLAN adapters adjust their operation to the variable quality of wireless channels. The IEEE 802.11 standard does not specify any RA preferences allowing for a competition in performance among vendors, thus numerous proprietary solutions coexist. While the RA schemes implemented in individual user terminals are unknown to the AP of a hotspot, it is well known that the way how individual stations adapt their rates strongly influences the performance of the whole WLAN cell. Therefore, the knowledge of the scheme applied by each station may be useful for the radio resource management in complex networks (e.g., HetNets or dense WLAN deployments in enterprise networks). In this paper, we present a novel approach to estimate the features of the RA schemes implemented in individual stations and demonstrate its efficiency using both simulated WLAN configurations as well as measurements.},
keywords={data communication;telecommunication network management;wireless channels;wireless LAN;DARA;data rate adaptation algorithms behavior;WLAN hotspots;data rate adaptation schemes;WLAN adapters;wireless channels;IEEE 802.11 standard;proprietary solutions;WLAN cell;radio resource management;HetNet;WLAN deployments;WLAN configurations;Wireless LAN;Estimation;Training;Adaptation models;IEEE 802.11 Standards;Data models;Interference},
doi={10.1109/INFCOM.2013.6566779},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566780,
author={Y. Xiao and J. Huang and C. Yuen and L. A. DaSilva},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Fairness and efficiency tradeoffs for user cooperation in distributed wireless networks},
year={2013},
volume={},
number={},
pages={285-289},
abstract={We propose a general framework to analyze incentives for user cooperation, and characterize the tradeoff between fairness and efficiency for cooperative networks. More specifically, we define the incentive region as a set of action profiles that provides cooperation benefits to all users and focus on the optimization of efficiency and fairness within this region. We introduce a linear resource allocation (LRA) scheme and show that most existing fairness measures can be converted to LRA with different linear coefficient vectors. We then propose the concept of strong price of fairness (SPoF) to study the network efficiency of the strong equilibrium. We show that both the SPoF and fairness measures are connected to the linear coefficient vector of LRA, which makes it possible to study the fairness and efficiency relationship. We then use the random access (RA) system as an example to show how to use the proposed framework to study a specific wireless network.},
keywords={cooperative communication;optimisation;resource allocation;wireless channels;distributed wireless networks;fairness tradeoffs;efficiency tradeoffs;user cooperation;cooperative networks;incentive region;optimization;linear resource allocation;LRA;linear coefficient vectors;strong price of fairness;SPoF;random access system;RA system;Resource management;Wireless networks;Games;Vectors;Educational institutions;Computers;Optimization},
doi={10.1109/INFCOM.2013.6566780},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566781,
author={H. Kowshik and P. Dutta and M. Chetlur and S. Kalyanaraman},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A quantitative framework for guaranteeing QoE of video delivery over wireless},
year={2013},
volume={},
number={},
pages={290-294},
abstract={In this paper, we study the problem of efficient video delivery over the cellular downlink. The key objective is to maximize the Quality of Experience (QoE) of the user, as measured by application level metrics such as the buffering ratio and low bit rate ratio. We present a two-tiered solution with a standard base-station scheduler that works on a per-packet basis and a Video Management System (VMS) that works at the granularity of thousands of video frames. The video management system uses knowledge of the video playout curves and future channel states to develop a scheduling policy that is feasibility optimal. The algorithms are simple and leverage recent results on real-time scheduling in wireless networks. We evaluate the performance of our algorithms using real video traces and a standard channel model. The VMS ensures that the per-user QoE guarantees are maintained, as compared with a standard PF scheduler that is oblivious to application level QoE requirements.},
keywords={cellular radio;quality of experience;scheduling;video communication;wireless channels;quantitative framework;video delivery;cellular downlink;quality of experience;application level metrics;bit rate ratio;two-tiered solution;standard base-station scheduler;video management system;VMS;video playout curves;scheduling policy;wireless networks;standard channel model;PF scheduler;QoE requirements;Streaming media;Bit rate;Wireless communication;Standards;Mobile communication;Lyapunov methods},
doi={10.1109/INFCOM.2013.6566781},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566782,
author={Q. Xiao and B. Xiao and S. Chen},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Differential estimation in dynamic RFID systems},
year={2013},
volume={},
number={},
pages={295-299},
abstract={Efficient estimation of tag population in RFID systems has many important applications. In this paper, we present a new problem called differential cardinality estimation, which tracks the population changes in a dynamic RFID system where tags are frequently moved in and out. In particular, we want to provide quick estimation on (1) the number of new tags that are moved in and (2) the number of old tags that are moved out, between any two consecutive scans of the system. We show that the traditional cardinality estimators cannot be applied here, and the tag identification protocols are too expensive if the estimation needs to be performed frequently in order to support real-time monitoring. This paper presents the first efficient solution for the problem of differential cardinality estimation. The solution is based on a novel differential estimation framework, and is named zero differential estimator. We show that this estimator can be configured to meet any pre-set accuracy requirement, with a probabilistic error bound that can be made arbitrarily small.},
keywords={error statistics;protocols;radiofrequency identification;dynamic RFID systems;differential cardinality estimation;consecutive scans;tag identification protocols;real-time monitoring;zero differential estimator;preset accuracy requirement;probabilistic error bound;Radiofrequency identification;Sociology;Statistics;Estimation error;Accuracy;Protocols},
doi={10.1109/INFCOM.2013.6566782},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566783,
author={Z. Zhang and H. Wang and X. Lin and H. Fang and D. Xuan},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Effective epidemic control and source tracing through mobile social sensing over WBANs},
year={2013},
volume={},
number={},
pages={300-304},
abstract={Accurate and real-time tracing of epidemic sources is critical for epidemic origin analyses and control when outbreaks of epidemic diseases occur. Such tracing requires the simultaneous availability of information about social interactions among people as well as their body vital signs. Existing epidemic control methods are limited due to their inability to collect the above two types of information at the same time. In this paper, for the first time, we propose integrating wireless body area networks (WBANs) for body vital signs collection with mobile phones for social interaction sensing to achieve the desired epidemic source tracing. In particular, we design a mobile phone capability driven hierarchical social interaction detection framework integrated with WBANs. With this framework, we further propose a set of epidemic source tracing and control algorithms including genetic algorithm based search and dominating set identification algorithms to effectively identify epidemic sources and inhibit epidemic spread. We have also conducted extensive simulations, analyses, and case studies based on real data sets, which demonstrate the accuracy and effectiveness of our proposed solutions.},
keywords={body area networks;diseases;epidemics;genetic algorithms;mobile handsets;network theory (graphs);search problems;telemedicine;effective epidemic control;source tracing;mobile social sensing;WBAN;real-time tracing;epidemic origin analysis;epidemic disease;wireless body area networks;body vital sign collection;social interaction sensing;mobile phone capability driven hierarchical social interaction detection framework;genetic algorithm;real data set;Social network services;Diseases;Mobile handsets;Sensors;Educational institutions;Real-time systems;Accuracy},
doi={10.1109/INFCOM.2013.6566783},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566784,
author={S. Xia and N. Ding and M. Jin and H. Wu and Y. Yang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Medial axis construction and applications in 3D wireless sensor networks},
year={2013},
volume={},
number={},
pages={305-309},
abstract={The medial axis of a shape provides a compact abstraction of its global topology and a proximity of its geometry. The construction of medial axis in two-dimensional (2D) sensor networks has been discussed in the literature, in support of several applications including routing and navigation. In this work, we first reveal the challenges of constructing medial axis in a three-dimensional (3D) sensor network. With more complicated geometric features and complex topology shapes, previous methods proposed for 2D settings cannot be extended easily to 3D networks. Then we propose a distributed algorithm with linear time complexity and communication cost to build a well-structured medial axis of a 3D sensor network without knowing its global shape or global position information. Furthermore we apply the computed medial axis for safe navigation and distributed information storage and retrieval in 3D sensor networks. Simulations are carried out to demonstrate the efficiency of the proposed medial axis-based applications in various 3D sensor networks.},
keywords={computational complexity;distributed algorithms;radionavigation;telecommunication network routing;telecommunication network topology;wireless sensor networks;medial axis construction;3D wireless sensor networks;compact abstraction;global topology;geometry proximity;two-dimensional sensor networks;2D sensor networks;routing;navigation;three-dimensional sensor network;geometric feature;complex topology shape;distributed algorithm;linear time complexity;communication cost;well-structured medial axis;global position information;global shape;Shape;Navigation;Routing;Noise;Wireless sensor networks;Surface treatment;Computational modeling},
doi={10.1109/INFCOM.2013.6566784},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566785,
author={L. Xu and X. Qi and Y. Wang and T. Moscibroda},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Efficient data gathering using Compressed Sparse Functions},
year={2013},
volume={},
number={},
pages={310-314},
abstract={Data gathering is one of the core algorithmic and theoretic problems in wireless sensor networks. In this paper, we propose a novel approach - Compressed Sparse Functions - to efficiently gather data through the use of highly sophisticated Compressive Sensing techniques. The idea of CSF is to gather a compressed version of a satisfying function (containing all the data) under a suitable function base, and to finally recover the original data. We show through theoretical analysis that our scheme significantly outperforms state-of-the-art methods in terms of efficiency, while matching them in terms of accuracy. For example, in a binary tree-structured network of n nodes, our solution reduces the number of packets from the best-known O(kn log n) to O(k log<sup>2</sup> n), where k is a parameter depending on the correlation of the underlying sensor data. Finally, we provide simulations showing that our solution can save up to 80% of communication overhead in a 100-node network. Extensive simulations further show that our solution is robust, high-capacity and low-delay.},
keywords={compressed sensing;wireless sensor networks;efficient data gathering;compressed sparse functions;core algorithmic;wireless sensor networks;highly sophisticated compressive sensing techniques;satisfying function;binary tree-structured network;sensor data;communication overhead;Topology;Network topology;Discrete cosine transforms;Accuracy;Mathematical model;Power demand;Wireless sensor networks},
doi={10.1109/INFCOM.2013.6566785},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566786,
author={W. Dong and B. Mo and C. Huang and Y. Liu and C. Chen},
booktitle={2013 Proceedings IEEE INFOCOM},
title={R3: Optimizing relocatable code for efficient reprogramming in networked embedded systems},
year={2013},
volume={},
number={},
pages={315-319},
abstract={We present a holistic reprogramming system called R3. R3 has two salient features. First, the binary differencing algorithm within R3 (R3diff) ensures an optimal result in terms of the delta size under a configurable cost measure. Second, the similarity preserving method within R3 (R3sim) optimizes the binary code format for achieving a large similarity with a small metadata overhead. Overall, R3 achieves the smallest delta size compared to other incremental approaches such as Rsync [11], RMTD [9], Zephyr/Hermes [17], [18], and R2 [2], e.g., 50%-99% reduction compared to Stream and about 20%-40% reduction compared to R2. R3's implementation on TelosB/TinyOS is lightweight and efficient. We release our code at http://code.google.com/p/r3-dongw.},
keywords={binary codes;embedded systems;meta data;optimising compilers;relocatable code optimization;networked embedded systems reprogramming;holistic reprogramming system;binary differencing algorithm;R3diff;binary code format;metadata overhead;R3sim;Rsync;RMTD;Zephyr-Hermes;R2;Stream;TelosB;TinyOS;Wireless sensor networks;Binary codes;Wireless communication;Indexes;Ash;Embedded systems},
doi={10.1109/INFCOM.2013.6566786},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566787,
author={E. Hyytiä and J. Ott},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Criticality of large delay tolerant networks via directed continuum percolation in space-time},
year={2013},
volume={},
number={},
pages={320-324},
abstract={We study delay tolerant networking (DTN) and in particular, its capacity to store, carry and forward messages to their final destination(s). We approach this broad question in the framework of percolation theory. To this end, we assume an elementary mobility model, where nodes arrive to an infinite plane according to a Poisson point process, move a certain distance ℓ, and then depart. In this setting, we characterize the mean density of nodes required to support DTN style networking. Under the given assumptions, we show that DTN communication is feasible when the mean node degree ν is greater than 4 · ηc(γ), where parameter γ= ℓ/d is the ratio of the distance ℓ to the transmission range d, and ηc(γ) is the critical reduced number density of tilted cylinders in a directed continuum percolation model. By means of Monte Carlo simulations, we give numerical values for ηc(γ). The asymptotic behavior of ηc(γ) when γ tends to ∞ is also derived from a fluid flow analysis.},
keywords={delay tolerant networks;mobility management (mobile radio);Monte Carlo methods;numerical analysis;large delay tolerant networks;directed continuum percolation model;space-time;DTN style networking;percolation theory;elementary mobility model;infinite plane;Poisson point process;DTN communication;Monte Carlo simulation;fluid flow analysis;numerical value;tilted cylinders;Mobile nodes;Ad hoc networks;Numerical models;Monte Carlo methods;Delays;Educational institutions;DTN;capacity;percolation;criticality;mobility},
doi={10.1109/INFCOM.2013.6566787},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566788,
author={W. Bao and B. Liang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={On the insensitivity of user distribution in multicell networks under general mobility and session patterns},
year={2013},
volume={},
number={},
pages={325-329},
abstract={The location of active users is an important factor in the performance analysis of mobile multicell networks, but it is difficult to quantify due to the wide variety of user mobility and session patterns. In particular, the channel holding times in each cell may be arbitrarily distributed and dependent on those in other cells. In this work, we study the stationary distribution of users by modeling the system as a multi-route queueing network with Poisson inputs. We consider arbitrary routing and arbitrary joint probability distributions for the channel holding times in each route. Using a decomposition-composition approach, we show that the user distribution (1) is insensitive to the user movement patterns, (2) is insensitive to general and dependent distributed channel holding times, (3) depends only on the average arrival rate and average channel holding time at each cell, and (4) is completely characterized by an open network with M/M/∞ queues. This result is validated by experiments with the Dartmouth user mobility traces.},
keywords={mobility management (mobile radio);Poisson equation;queueing theory;telecommunication network routing;mobile multicell networks;user distribution insensitivity;general mobility;session patterns;active user location;channel holding times;stationary distribution;multiroute queueing network;Poisson inputs;arbitrary routing;arbitrary joint probability distributions;decomposition-composition approach;open network;M-M-∞ queues;Dartmouth user mobility traces;Joints;Entropy;Analytical models;Mobile computing;Servers;Mobile communication;Vectors},
doi={10.1109/INFCOM.2013.6566788},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566789,
author={S. Agrawal and P. Chaporkar and R. Udwani},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Call admission control for real-time applications in wireless network},
year={2013},
volume={},
number={},
pages={330-334},
abstract={Supporting real-time applications is paramount to sustaining the growth of wireless networks. Real time applications require strict delay guarantee, i.e., a packet delayed beyond certain predefined value is dropped. Fortunately, depending on the codec used, real-time applications can sustain some loss gracefully. Aim of an admission control algorithm is to make sure that when a new flow is admitted, its and other existing flows' packet loss on account of deadline violation is below their respective acceptable limit. The problem of admission control has been studied extensively for wireline networks. However, this analysis does not extend to wireless case on account of fading. Here, we consider a wireless network with TDMA based MAC, and for this network obtain a scalable admission control algorithm.},
keywords={codecs;radio networks;telecommunication congestion control;time division multiple access;call admission control;real-time applications;wireless network;delay guarantee;deadline violation;admission control problem;TDMA based MAC;scalable admission control algorithm;codec;Admission control;Real-time systems;Delays;Algorithm design and analysis;Wireless networks;Time division multiple access},
doi={10.1109/INFCOM.2013.6566789},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566790,
author={J. J. Jaramillo and L. Ying},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Distributed admission control without knowledge of the capacity region},
year={2013},
volume={},
number={},
pages={335-339},
abstract={We consider the problem of distributed admission control without knowledge of the capacity region in single-hop wireless networks, for flows that require a pre-specified bandwidth from the network. We present an optimization framework that allows us to design a scheduler and resource allocator, and by properly choosing a suitable utility function in the resource allocator, we prove that existing flows can be served with a prespecified bandwidth, while the link requesting admission can determine the largest rate that it can get such that it does not interfere with the allocation to the existing flows.},
keywords={optimisation;radio networks;resource allocation;scheduling;telecommunication congestion control;distributed admission control;capacity region;single-hop wireless networks;prespecified bandwidth;optimization framework;resource allocator;Admission control;Schedules;Wireless networks;Optimization;Resource management;Bandwidth},
doi={10.1109/INFCOM.2013.6566790},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566791,
author={J. Luo and L. Rao and X. Liu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Data center energy cost minimization: A spatio-temporal scheduling approach},
year={2013},
volume={},
number={},
pages={340-344},
abstract={Cloud computing is supported by an infrastructure known as Internet data center (IDC). As cloud computing thrives, the energy consumption and cost for IDCs are exploding. There is growing interest in energy cost minimization for IDCs in deregulated electricity markets. In this paper we study how to leverage both geographic and temporal variation of energy price to minimize energy cost for distributed IDCs. To this end, we propose a novel spatio-temporal load balancing approach. Using reallife electricity price and workload traces, extensive evaluations demonstrate that the proposed spatio-temporal load balancing approach significantly reduces energy cost for distributed IDCs.},
keywords={cloud computing;computer centres;cost reduction;electricity supply industry deregulation;energy consumption;power aware computing;resource allocation;scheduling;spatiotemporal phenomena;telecommunication power management;data center energy cost minimization;spatio-temporal scheduling approach;cloud computing;Internet data center;energy consumption;deregulated electricity markets;temporal energy price variation;geographic energy price variation;distributed IDC;spatio-temporal load balancing approach;real-life electricity price;Electricity;Load management;Servers;Delays;Portals;Minimization;Internet},
doi={10.1109/INFCOM.2013.6566791},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566792,
author={Z. Zheng and M. Li and X. Xiao and J. Wang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Coordinated resource provisioning and maintenance scheduling in cloud data centers},
year={2013},
volume={},
number={},
pages={345-349},
abstract={Lack of proper maintenance is the root cause of anywhere from a third to a half of downtime events in a cloud data center. To help safeguard the uptime of data centers, regular preventive maintenance must be conducted. During the maintenance time, some accommodated virtual machines (VMs) may be re-provisioned to the other available (backup) resource through migration, and some VMs may be terminated. One way that can allow a data center to perform all necessary preventive maintenance activities without causing too much disruption to VMs is to design an appropriate maintenance schedule. In this paper, given the available resource in a data center and the required maintenance activities with their deadlines, we consider the joint VM resource provisioning and maintenance scheduling problem to maximize the revenue of the data center. We tackle the problem by firstly proposing a heuristic for the resource provisioning under a given maintenance schedule. Using such a heuristic algorithm as the building block, we then propose another heuristic algorithm to solve the joint resource provisioning and maintenance scheduling problem and also derive its upper bound. Extensive simulations have shown that our proposed heuristic algorithms can effectively maximize the revenue of the data center.},
keywords={cloud computing;computer centres;preventive maintenance;scheduling;virtual machines;coordinated resource provisioning;downtime event;cloud data center;virtual machine;resource through migration;preventive maintenance activity;maintenance schedule;VM resource provisioning;heuristic algorithm;maintenance scheduling problem;Servers;Schedules;Heuristic algorithms;Preventive maintenance;Bismuth;Virtual machining},
doi={10.1109/INFCOM.2013.6566792},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566793,
author={Z. Guo and Y. Yang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Multicast fat-tree data center networks with bounded link oversubscription},
year={2013},
volume={},
number={},
pages={350-354},
abstract={Many data center networks (DCNs) adopt a multirooted tree structure called fat-tree, which has the potential to deliver large bisection bandwidth through rich path multiplicity. However, unbalanced traffic load distribution may prevent efficient utilization of such high degree of parallelism. Meanwhile, high bandwidth multicast communication is critical to many data center services and applications. Hence, in this paper we consider multicast traffic load balance problem in fat-tree DCNs from a novel angle, aiming to find the most cost-effective way to build a multicast fat-tree DCN with bounded link oversubscription ratio. First, we present a multi-rate network model to accurately describe the communication environment in a fat-tree DCN. Then, we derive the minimum number of core switches required to achieve bounded link oversubscription ratio under arbitrary multicast traffic. Finally, we provide a comprehensive comparison on the cost of different approaches to building such a multicast fat-tree DCN.},
keywords={computer centres;computer networks;multicast communication;telecommunication switching;trees (mathematics);multicast fat-tree data center networks;multirooted tree structure;large bisection bandwidth;rich path multiplicity;unbalanced traffic load distribution;high bandwidth multicast communication;multicast traffic load balance problem;fat-tree DCN;bounded link oversubscription ratio;core switches;Bandwidth;Servers;Uplink;Downlink;Ports (Computers);Load modeling;Subscriptions;Data center networks;cost;fat-tree;oversubscription;multicast;hose traffic;load balancing;bisection bandwidth},
doi={10.1109/INFCOM.2013.6566793},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566794,
author={R. Cohen and L. Lewin-Eytan and J. Seffi Naor and D. Raz},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Almost optimal virtual machine placement for traffic intense data centers},
year={2013},
volume={},
number={},
pages={355-359},
abstract={The recent growing popularity of cloud-based solutions and the variety of new applications present new challenges for cloud management and resource utilization. In this paper we concentrate on the networking aspect and consider the placement problem of virtual machines (VMs) of applications with intense bandwidth requirements. Optimizing the available network bandwidth is far more complex than optimizing resources like memory or CPU, since every network link may be used by many physical hosts and thus by the VMs residing in these hosts. We focus on maximizing the benefit from the overall communication sent by the VMs to a single designated point in the data center (called the root). This is the typical case when considering a storage area network of applications with intense storage requirements. We formulate a bandwidth-constrained VM placement optimization problem that models this setting. This problem is NP hard, and we present a polynomial-time constant approximation algorithm for its most general version, in which hosts are connected to the root by a general network graph. For more practical cases, in which the network topology is a tree and the revenue is a simple function of the allocated bandwidth, we present improved approximation algorithms that are more efficient in terms of running time. We evaluate the expected performance of our proposed algorithms through a simulation study over traces from a real production data center, providing strong indications to the superiority of our proposed solutions.},
keywords={approximation theory;cloud computing;computational complexity;computer centres;computer network performance evaluation;network theory (graphs);optimisation;resource allocation;storage area networks;telecommunication network topology;telecommunication traffic;trees (mathematics);virtual machines;optimal virtual machine placement problem;traffic intense data centers;cloud management;resource utilization;networking aspect;network bandwidth optimization;physical hosts;application storage area network;bandwidth-constrained VM placement optimization problem;NP-hard problem;polynomial-time constant approximation algorithm;network graph;network topology;bandwidth allocation;running time;performance evaluation;Approximation algorithms;Optimized production technology;Bandwidth;Approximation methods;Routing;Greedy algorithms;Resource management},
doi={10.1109/INFCOM.2013.6566794},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566795,
author={S. Yang and J. Kurose and B. N. Levine},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Disambiguation of residential wired and wireless access in a forensic setting},
year={2013},
volume={},
number={},
pages={360-364},
abstract={Thousands of cases each year of child exploitation on P2P file sharing networks lead from an IP address to a home. A first step upon execution of a search warrant is to determine if the home's open Wi-Fi or the closed wired Ethernet was used for trafficking; in the latter case, a resident user is more likely to be the responsible party. We propose methods that use remotely measured traffic to disambiguate wired and wireless residential medium access. Our practical techniques work across the Internet by estimating the perflow distribution of inter-arrival times for different home access network types. We observe that the change of inter-arrival time distribution is subject to several residentialfactors, including differences between OS network stacks, and cable network mechanisms. We propose a model to explain the observed patterns of inter-arrival times, and we study the ability of supervised learning classifiers to differentiate between wired and wireless access based on these remote traffic measurements.},
keywords={computer network security;digital forensics;home networks;Internet;IP networks;learning (artificial intelligence);pattern classification;peer-to-peer computing;radio access networks;telecommunication traffic;wireless LAN;forensic setting;child exploitation;P2P file sharing network;IP address;search warrant;Wi-Fi;closed wired Ethernet;trafficking;wireless residential medium access;wired residential medium access;Internet;home access network;interarrival time distribution;residential factor;OS network stack;cable network mechanism;supervised learning classifier;remote traffic measurement;Wireless communication;Forensics;Throughput;Entropy;Internet;Linux;Logic gates},
doi={10.1109/INFCOM.2013.6566795},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566796,
author={Y. Guo and L. Yang and X. Ding and J. Han and Y. Liu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={OpenSesame: Unlocking smart phone through handshaking biometrics},
year={2013},
volume={},
number={},
pages={365-369},
abstract={Screen locking/unlocking is important for modern smart phones to avoid the unintentional operations and secure the personal stuff. Once the phone is locked, the user should take a specific action or provide some secret information to unlock the phone. Existing approaches do not support smart phones well due to the deficiency of security, high cost, and poor usability. We collect 200 users' handshaking actions with their smart phones and discover an appealing observation: the shaking pattern of a person is kind of unique, stable and distinguishable. In this paper, we propose OpenSesame, which employs the users' shaking patterns for locking/unlocking. The key feature of our system lies in using four fine-grained and statistic features of handshaking to verify users. Moreover, we utilize support vector machine (SVM) for accurate classification. Results from comprehensive experiments show that our technique is robust compatible across different brands of smart phones, without the need of any specialized hardware.},
keywords={authorisation;biometrics (access control);human computer interaction;mobile computing;pattern classification;smart phones;support vector machines;user interfaces;OpenSesame;smart phone unlocking;handshaking biometrics;screen locking;screen unlocking;unintentional operation avoidance;personal stuff security;security deficiency;high cost;poor usability;handshaking actions;shaking patterns;support vector machine;SVM;authentication;accelerometer;mobile phones;Smart phones;Accelerometers;Intelligent sensors;Magnetic sensors;Shape;Sensor phenomena and characterization;Smart Phone;Security;Privacy;Authentication;Accelerameter},
doi={10.1109/INFCOM.2013.6566796},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566797,
author={M. J. Abdel-Rahman and H. Rahbari and M. Krunz and P. Nain},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Fast and secure rendezvous protocols for mitigating control channel DoS attacks},
year={2013},
volume={},
number={},
pages={370-374},
abstract={The operation of a wireless network relies extensively on exchanging messages over a universally known channel, referred to as the control channel. The network performance can be severely degraded if a jammer launches a denial-of-service (DoS) attack on such a channel. In this paper, we design quorum-based frequency hopping (FH) algorithms that mitigate DoS attacks on the control channel of an asynchronous ad hoc network. Our algorithms can establish unicast as well as multicast communications under DoS attacks. They are fully distributed, do not incur any additional message exchange overhead, and can work in the absence of node synchronization. Furthermore, the multicast algorithms maintain the multicast group consistency. The efficiency of our algorithms is shown by analysis and simulations.},
keywords={ad hoc networks;computer network security;frequency hop communication;jamming;multicast communication;protocols;telecommunication control;fast rendezvous protocols;secure rendezvous protocols;control channel;DoS attack mitigation;denial-of-service attack;wireless network;exchanging messages;jammer;quorum-based frequency hopping algorithms;FH algorithms;asynchronous ad hoc network;multicast communications;message exchange overhead;multicast algorithms;multicast group consistency;High definition video;Jamming;Unicast;Computer crime;Algorithm design and analysis;Synchronization;Robustness},
doi={10.1109/INFCOM.2013.6566797},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566798,
author={L. Li and X. Zhao and G. Xue},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Near field authentication for smart devices},
year={2013},
volume={},
number={},
pages={375-379},
abstract={Near field communication (NFC) systems provide a good location-limited channel so that many security systems can use it to force the participants to stay close to each other. Unfortunately, only a small number of smart devices in the market are equipped with NFC chips that are essential for NFC systems. The purpose of this paper is to provide the same feature, called near field authentication (NFA), without using NFC chips. We propose an easy-to-use system to achieve NFA by using human finger movement on the touch screens of two nearby smart devices. Our system does not need any prior secret information shared between two devices and generates the same high-entropy cryptographic key for both devices in a successful authentication. The efficiency of the system is demonstrated by our evaluation on a Motorola Droid smartphone.},
keywords={cryptography;smart phones;wireless channels;near field authentication system;smart devices;location-limited channel;NFC chips;NFA;easy-to-use system;human finger movement;secret information;high-entropy cryptographic key;Motorola Droid smartphone;Authentication;Feature extraction;Protocols;Cryptography;Performance evaluation;Data mining},
doi={10.1109/INFCOM.2013.6566798},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566799,
author={J. Tapolcai and P. Ho and P. Babarczi and L. Rónyai},
booktitle={2013 Proceedings IEEE INFOCOM},
title={On achieving all-optical failure restoration via monitoring trails},
year={2013},
volume={},
number={},
pages={380-384},
abstract={The paper investigates a novel monitoring trail (m-trail) scenario that can enable any shared protection scheme for achieving all-optical and ultra-fast failure restoration. Given a set of working (W-LPs) and protection (P-LPs) lightpaths, we firstly define the neighborhood of a node, which is a set of links whose failure states should be known to the node in restoration of the corresponding W-LPs. A set of m-trails is routed such that each node can localize any failure in its neighborhood according to the ON-OFF status of the traversing m-trails. Bound analysis is performed on the minimum bandwidth required for the m-trails. Extensive simulation is conducted to verify the proposed scheme.},
keywords={failure analysis;optical fibre networks;telecommunication network reliability;telecommunication network routing;monitoring trail scenario;shared protection scheme;all optical failure restoration;ultra fast failure restoration;working lightpaths;W-LP;protection lightpaths;P-LP;failure states;m-trails;ON-OFF status;bound analysis;Monitoring;Switches;Cost function;Optical sensors;High-speed optical techniques;Testing;Optical fiber networks},
doi={10.1109/INFCOM.2013.6566799},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566800,
author={S. Shirazipourazad and C. Zhou and Z. Derakhshandeh and A. Sen},
booktitle={2013 Proceedings IEEE INFOCOM},
title={On routing and spectrum allocation in spectrum-sliced optical networks},
year={2013},
volume={},
number={},
pages={385-389},
abstract={The orthogonal frequency division multiplexing (OFDM) technology provides an opportunity for efficient resource utilization in optical networks. It allows allocation of multiple sub-carriers to meet traffic demands of varying size. Utilizing OFDM technology, a spectrum efficient and scalable optical transport network called SLICE was proposed recently. The SLICE architecture enables sub-wavelength, super-wavelength resource allocation and multiple rate data traffic that results in efficient use of spectrum. However, the benefit is accompanied by additional complexities in resource allocation. In SLICE architecture, in order to minimize the utilized spectrum, one has to solve the routing and spectrum allocation problem (RSA). In this paper, we focus our attention to RSA and (i) prove that RSA is NP-complete even when the optical network topology is as simple as a chain or a ring, (ii) provide approximation algorithms for RSA when the network topology is a binary tree or a ring, (iii) provide a heuristic for the network with arbitrary topology and measure the effectiveness of the heuristic with extensive simulation. Simulation results demonstrate that our heuristic significantly outperforms several other heuristics proposed recently for RSA.},
keywords={approximation theory;computational complexity;OFDM modulation;optical modulation;resource allocation;telecommunication network routing;telecommunication network topology;spectrum-sliced optical networks;orthogonal frequency division multiplexing technology;OFDM technology;optical transport network;SLICE architecture;super-wavelength resource allocation;subwavelength resource allocation;routing and spectrum allocation problem;RSA problem;NP-complete;optical network topology;approximation algorithms;binary tree;Resource management;Optical fiber networks;Network topology;Routing;Approximation algorithms;Approximation methods;Topology},
doi={10.1109/INFCOM.2013.6566800},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566801,
author={X. Chen and A. Jukan and A. Gumaste},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Multipath de-fragmentation: Achieving better spectral efficiency in elastic optical path networks},
year={2013},
volume={},
number={},
pages={390-394},
abstract={In elastic optical networks, the spectrum consecutive and continuous constraints may cause the so-called spectrum fragmentation issue, degrading spectrum utilization, which is especially critical under dynamic traffic scenarios. In this paper, we propose a novel multipath de-fragmentation method which aggregates spectrum fragments instead of reconfiguring existing spectrum paths. We propose an optimization model based on Integer Linear Programming (ILP) and heuristic algorithms and discuss the practical feasibility of the proposed method. We show that multipath routing is an effective de-fragmentation method, as it improves spectral efficiency and reduces blocking under dynamic traffic conditions. We also show that the differential delay issue does not present an obstacle to the application of multipath de-fragmentation in elastic optical networks.},
keywords={linear programming;multipath channels;telecommunication network routing;telecommunication traffic;multipath defragmentation;spectral efficiency;elastic optical path networks;spectrum consecutive constraints;continuous constraints;spectrum utilization;dynamic traffic scenarios;spectrum fragments;optimization model;integer linear programming;ILP;heuristic algorithms;multipath routing;differential delay;Delays;Routing;Optical fiber networks;Heuristic algorithms;Load modeling;Optical fibers;Optimization},
doi={10.1109/INFCOM.2013.6566801},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566802,
author={Z. Ye and X. Cao and X. Gao and C. Qiao},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A predictive and incremental grooming scheme for time-varying traffic in WDM networks},
year={2013},
volume={},
number={},
pages={395-399},
abstract={Traffic grooming can effectively utilize the transmission capacity of WDM networks by properly multiplexing low-speed traffic flows onto high-capacity wavelength channels. In order to maximize the wavelength resource and cut down network costs associated with, e.g. OEO conversion for time-varying yet predictable traffic, we propose a novel predictive and incremental (PI) traffic grooming scheme, named PI-grooming. A conventional traffic grooming approach for fluctuated traffic is to run an algorithm that (re)assigns the traffic flows to as a few wavelengths as possible based only on the current traffic demands of these flows. This however will lead to a lot of OEO traffic. The proposed PI-grooming considers the existing flow assignment, the current traffic demands, and the expected traffic demands in the near future. We show that, compared with the conventional approach, PI-grooming can effectively minimize the amount of OEO traffic while still using a very small number of wavelengths.},
keywords={optical fibre networks;telecommunication congestion control;telecommunication traffic;wavelength division multiplexing;time varying traffic;WDM networks;low-speed traffic flow;high-capacity wavelength channel;predictive traffic grooming;incremental traffic grooming;PI-grooming;fluctuated traffic;OEO traffic;wavelength division multiplexing;WDM networks;Heuristic algorithms;Prediction algorithms;Optical fiber networks;Bandwidth;Accuracy;Traffic grooming;Time-varying traffic;Predictive and incremental;Look-ahead;WDM networks},
doi={10.1109/INFCOM.2013.6566802},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566803,
author={A. Wang and Z. Zhang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Exact cooperative regenerating codes with minimum-repair-bandwidth for distributed storage},
year={2013},
volume={},
number={},
pages={400-404},
abstract={We give an explicit construction of exact cooperative regenerating codes at the MBCR (minimum bandwidth cooperative regeneration) point. Before the paper, the only known explicit MBCR codes are given with parameters n = d + r and d = k, while our construction applies to all possible values of n, k, d, r. The code has a brief expression in the polynomial form and the data reconstruction is accomplished by bivariate polynomial interpolation. It is a scalar code and operates over a finite field of size q ≥ n. Besides, we establish several subspace properties for linear exact MBCR codes. Based on these properties we prove that linear exact MBCR codes cannot achieve repair-by-transfer.},
keywords={interpolation;linear codes;polynomials;storage management;exact cooperative regenerating code;minimum-repair-bandwidth;distributed storage;minimum bandwidth cooperative regeneration point;polynomial form;data reconstruction;bivariate polynomial interpolation;scalar code;subspace property;linear exact MBCR code;Maintenance engineering;Polynomials;Bandwidth;Interpolation;Vectors;Network coding;Joining processes},
doi={10.1109/INFCOM.2013.6566803},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566804,
author={L. Zhang and C. Wu and Z. Li and C. Guo and M. Chen and F. C. M. Lau},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Moving big data to the cloud},
year={2013},
volume={},
number={},
pages={405-409},
abstract={Cloud computing, rapidly emerging as a new computation paradigm, provides agile and scalable resource access in a utility-like fashion, especially for the processing of big data. An important open issue here is how to efficiently move the data, from different geographical locations over time, into a cloud for effective processing. The de facto approach of hard drive shipping is not flexible, nor secure. This work studies timely, cost-minimizing upload of massive, dynamically-generated, geodispersed data into the cloud, for processing using a MapReducelike framework. Targeting at a cloud encompassing disparate data centers, we model a cost-minimizing data migration problem, and propose two online algorithms, for optimizing at any given time the choice of the data center for data aggregation and processing, as well as the routes for transmitting data there. The first is an online lazy migration (OLM) algorithm achieving a competitive ratio of as low as 2.55, under typical system settings. The second is a randomized fixed horizon control (RFHC) algorithm achieving a competitive ratio of 1+ 1/l+λ κ/λ with a lookahead window of l, where κ and λ are system parameters of similar magnitude.},
keywords={cloud computing;storage management;cloud computing;geographical location;hard drive shipping;MapReducelike framework;cost-minimizing data migration problem;online lazy migration;OLM algorithm;randomized fixed horizon control;RFHC algorithm;competitive ratio;Algorithm design and analysis;Heuristic algorithms;Routing;Cloud computing;Prediction algorithms;Virtual private networks;Optimization},
doi={10.1109/INFCOM.2013.6566804},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566805,
author={Q. Hu and Y. Wang and X. Cao},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Resolve the virtual network embedding problem: A column generation approach},
year={2013},
volume={},
number={},
pages={410-414},
abstract={In this paper, we study the virtual network embedding (VNE) problem in the network virtualization context, which aims at mapping the virtual network requests of the service providers (SPs) to the substrate networks managed by the infrastructure providers (InPs). Given the NP-Completeness of the VNE problem, prior approaches primarily rely on solving/relaxing the link-based Integer Linear Programming (ILP) formulations, which lead to either extensive computational time, or non-optimal solutions. In this paper, for the first time, we present a path-based model for the VNE problem, namely P-VNE. By analyzing the dual formulation of the P-VNE model, we propose a column generation process, with which an optimal solution to the VNE problem can be found efficiently (when embedded into a branch-and-bound framework).},
keywords={computational complexity;integer programming;Internet;linear programming;tree searching;virtualisation;virtual network embedding problem;column generation approach;P-VNE problem;network virtualization context;virtual network request mapping;service providers;SP;infrastructure provider;InP;NP-completeness;link-based integer linear programming formulation;ILP formulations;path-based model;branch-and-bound framework;Internet;Substrates;Computational modeling;Bandwidth;Virtualization;Internet;Polynomials;Indium phosphide},
doi={10.1109/INFCOM.2013.6566805},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566806,
author={Y. Pignolet and S. Schmid and G. Tredan},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Adversarial VNet embeddings: A threat for ISPs?},
year={2013},
volume={},
number={},
pages={415-419},
abstract={This paper demonstrates that virtual networks that are dynamically embedded on a given resource network may constitute a security threat as properties of the infrastructure-typically a business secret-are disclosed. We initiate the study of this new problem and introduce the notion of request complexity which captures the number of virtual network embedding requests needed to fully disclose the infrastructure topology. We derive lower bounds and present algorithms achieving an asymptotically optimal request complexity for the important class of tree and cactus graphs (complexity θ(n)) as well as arbitrary graphs (complexity θ(n2)).},
keywords={communication complexity;computer network security;embedded systems;graph theory;resource allocation;virtual private networks;adversarial VNet embeddings;ISP threat;virtual networks;resource network;security threat;business secret;request complexity;virtual network embedding requests;infrastructure topology;asymptotically optimal request complexity;cactus graphs;Substrates;Topology;Complexity theory;Network topology;Virtualization;Joining processes;Image edge detection},
doi={10.1109/INFCOM.2013.6566806},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566807,
author={X. Bao and Y. Lin and U. Lee and I. Rimac and R. R. Choudhury},
booktitle={2013 Proceedings IEEE INFOCOM},
title={DataSpotting: Exploiting naturally clustered mobile devices to offload cellular traffic},
year={2013},
volume={},
number={},
pages={420-424},
abstract={The proliferation of pictures and videos in the Internet is imposing heavy demands on mobile data networks. Though emerging wireless technologies will provide more bandwidth, the increase in demand will easily consume the additional capacity. To alleviate this problem, we explore the possibility of serving user requests from other mobile devices located geographically close to the user. For instance, when Alice reaches areas with high device density - Data Spots - the cellular operator learns Alice's content request, and guides her device to nearby devices that have the requested content. Importantly, communication between the nearby devices can be mediated by servers, avoiding many of the known problems of pure ad hoc communication. This paper argues this viability through systematic prototyping, measurements, and measurement-driven analysis.},
keywords={cellular radio;Internet;mobile ad hoc networks;mobile handsets;telecommunication traffic;data spotting;naturally clustered mobile devices;offload cellular traffic;Internet;mobile data networks;wireless technologies;mobile devices;Alice content request;ad hoc communication;measurement-driven analysis;IEEE 802.11 Standards;Mobile communication;Performance evaluation;Mobile handsets;Servers;Videos;Area measurement},
doi={10.1109/INFCOM.2013.6566807},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566808,
author={S. Zhou and J. Yang and D. Xu and G. Li and Y. Jin and Z. Ge and M. B. Kosseifi and R. Doverspike and Y. Chen and L. Ying},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Proactive call drop avoidance in UMTS networks},
year={2013},
volume={},
number={},
pages={425-429},
abstract={The rapid advancement of smartphones has instigated tremendous data applications for cell phones. Supporting simultaneous voice and data services in a cellular network is not only desirable but also becoming indispensable. However, if the voice and data are serviced through the same antenna (like the 3G UMTS network), a voice call with data sessions requires better radio connection than a voice-only call. In this paper, we systematically study the coordination between the voice and data transmissions in UMTS networks. From analyzing a large carrier's UMTS network recording data, we first identify the most relevant network measurements/features indicating a potential call drop, then propose a drop-call predictor based on AdaBoost. Moreover, we develop an intelligent call management strategy to voluntarily block data sessions when the voice is predicted to be dropped. Our analysis utilizing real service provider's data sets shows that our proposed scheme can not only predict drop calls with a very high accuracy but also achieve the highest user satisfaction compared to the other existing call management strategies.},
keywords={3G mobile communication;antennas;data communication;smart phones;voice communication;3G UMTS network;proactive call drop avoidance;smart phones;cell phones;voice services;data services;cellular network;antenna;voice transmissions;data transmissions;drop-call predictor;AdaBoost;intelligent call management;user satisfaction;3G mobile communication;Feature extraction;Prediction algorithms;Measurement;Member and Geographic Activities Board committees;Machine learning algorithms;Data communication},
doi={10.1109/INFCOM.2013.6566808},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566809,
author={S. Paris and F. Martisnon and I. Filippini and L. Clien},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A bandwidth trading marketplace for mobile data offloading},
year={2013},
volume={},
number={},
pages={430-434},
abstract={The Radio Access Network (RAN) infrastructure represents the most critical part for capacity planning, which usually accounts for peak traffic conditions. A promising approach to increase the RAN capacity and simultaneously reduce its energy consumption is represented by the opportunistic utilization of third party Wi-Fi access devices. In order to foster the utilization of unexploited Internet connections, we propose a new and open market, where a mobile operator can lease the bandwidth made available by third parties (residential users or private companies) through their access points to increase the network capacity and save large amounts of energy. We formulate the offloading problem as a reverse auction considering the most general case of partial covering of the traffic to be offloaded. We discuss the conditions (i) to offload the maximum amount of data traffic according to the capacity of third party access devices, (ii) to foster the participation of access point owners (individual rationality), and (iii) to prevent market manipulation (incentive compatibility). Finally, we propose a greedy algorithm that solves the offloading problem in polynomial time, even for large-size network scenarios.},
keywords={Internet;polynomials;radio access networks;telecommunication network planning;telecommunication traffic;wireless LAN;bandwidth trading marketplace;mobile data offloading;third party Wi-Fi access device;capacity planning;peak traffic conditions;RAN capacity;energy consumption;Internet connections;mobile operator;network capacity;data traffic;market manipulation;polynomial time;large-size network scenario;radio access network infrastructure;Mobile communication;Mobile computing;Resource management;Wireless communication;Bandwidth;Greedy algorithms;Radio access networks;WiFi Offloading;Heterogeneous Mobile Networks;Auction},
doi={10.1109/INFCOM.2013.6566809},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566810,
author={Y. Im and C. Joe-Wong and S. Ha and S. Sen and T. T. Kwon and M. Chiang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={AMUSE: Empowering users for cost-aware offloading with throughput-delay tradeoffs},
year={2013},
volume={},
number={},
pages={435-439},
abstract={Mobile users face a tradeoff between cost, throughput, and delay in making their offloading decisions. To navigate this tradeoff, we propose AMUSE (Adaptive bandwidth Management through USer-Empowerment), a practical, costaware WiFi offloading system that takes into account a user's throughput-delay tradeoffs and cellular budget constraint. Based on predicted future usage and WiFi availability, AMUSE decides which applications to offload to what times of the day. To practically enforce the assigned rate of each TCP application, we introduce a receiver-side TCP bandwidth control algorithm that adjusts the rate by controlling the TCP advertisement window from the user side. We implement AMUSE on Windows 7 tablets and evaluate its effectiveness with 3G and WiFi usage data obtained from a trial with 25 mobile users. Our results show that AMUSE improves user utility.},
keywords={3G mobile communication;bandwidth allocation;cellular radio;delays;radio receivers;transport protocols;wireless LAN;AMUSE;cost-aware offloading;throughput-delay tradeoffs;mobile users;adaptive bandwidth management through user-empowerment;WiFi offloading system;cellular budget constraint;user throughput-delay tradeoffs;WiFi availability;TCP application;receiver-side TCP bandwidth control algorithm;TCP advertisement;Windows 7 tablets;3G;WiFi usage data;IEEE 802.11 Standards;Bandwidth;Prediction algorithms;Delays;Throughput;Mobile communication;Educational institutions},
doi={10.1109/INFCOM.2013.6566810},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566811,
author={F. Yu and G. Xue and H. Zhu and Z. Hu and M. Li and G. Zhang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Cutting without pain: Mitigating 3G radio tail effect on smartphones},
year={2013},
volume={},
number={},
pages={440-444},
abstract={3G technology has stimulated a wide variety of high-bandwidth applications on smartphones, such as video streaming and content-rich web browsing. Although having those applications mobile is quite appealing, high data rate transmission also poses huge demand for power. It has been revealed that the tail effect in 3G radio operation results in significant energy drain on smartphones. Recent fast dormancy technique can be utilized to remove tails but, without care, can degrades user experience. In this paper, we propose a novel scheme SmartCut, which effectively mitigates the tail effect of radio usage in 3G networks with little side-effect on user experience. The core idea of SmartCut is to utilize the temporal correlation of packet arrivals to predict upcoming data, based on which unnecessary high-power-state tails of radio are cut out leveraging the Fast Dormancy mechanism. Extensive trace-driven simulation results demonstrate the efficacy of SmartCut design. On average, SmartCut can save up to 56.57% energy on average while having little side-effect to user experience.},
keywords={3G mobile communication;smart phones;3G radio tail effect mitigation;smartphones;video streaming;content-rich web browsing;data rate transmission;3G radio operation;energy drain;SmartCut scheme;3G networks;packet arrivals;radio high-power-state tails;fast dormancy mechanism;extensive trace-driven simulation;Entropy;Data communication;Delays;Smart phones;Streaming media;Correlation;Switches},
doi={10.1109/INFCOM.2013.6566811},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566812,
author={R. Lu and X. Lin and Z. Shi and B. Cao and X. Sherman Shen},
booktitle={2013 Proceedings IEEE INFOCOM},
title={IPAD: An incentive and privacy-aware data dissemination scheme in opportunistic networks},
year={2013},
volume={},
number={},
pages={445-449},
abstract={Opportunistic network (OPPNET) is characterized by the intermittent connectivity among mobile nodes from their unpredictable mobility. Although it is promising, there still exist many security and privacy challenges. In this paper, we present an incentive and privacy-aware data dissemination (IPAD) scheme for OPPNETs, not only to exploit how to protect mobile node's identity privacy, location privacy and social profile privacy, but also to provide a secure incentive for privacy-aware data dissemination. Through extensive incentive analysis, we show that only if a source provides a secure incentive strategy, can a data packet be efficiently disseminated in OPPNETs.},
keywords={data privacy;incentive schemes;mobile ad hoc networks;telecommunication security;incentive and privacy-aware data dissemination scheme;IPAD;opportunistic networks;OPPNET;intermittent connectivity;mobile nodes;identity privacy;location privacy;social profile privacy;data packet;mobile ad hoc networks;MANET;Nickel;Privacy;Mobile nodes;Relays;IP networks;Security;Opportunistic network;Data dissemination;Incentive;Privacy-Aware},
doi={10.1109/INFCOM.2013.6566812},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566813,
author={Y. Zhou and W. Zhuang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Beneficial cooperation ratio in multi-hop wireless ad hoc networks},
year={2013},
volume={},
number={},
pages={450-454},
abstract={In this paper, we study the differences of applying cooperation to fully-connected and multi-hop wireless networks, and find out that both the enlarged interference area and link density play a pivotal role in making the beneficial cooperation decision in a multi-hop network. Through characterizing effects of the enlarged interference area and link density on the overall network performance, a beneficial cooperation opportunity can be identified. By employing a randomized scheduling scheme and deriving the interference-free probability of any two links, the expected numbers of concurrent direct and cooperative transmissions can be obtained, where the ratio of these two numbers is defined as the beneficial cooperation ratio. Such a ratio translates the reduced spatial reuse to a requirement of the cooperation gain and provides a guideline for enabling beneficial cooperation on a single-link basis. Finally, the analytical and simulation results demonstrate that the beneficial cooperation criterion for a multi-hop network derived in this paper is more accurate than that in [1].},
keywords={ad hoc networks;cooperative communication;probability;radio links;radiofrequency interference;scheduling;multihop wireless ad hoc networks;cooperation ratio;fully-connected wireless networks;interference area;link density;beneficial cooperation opportunity;randomized scheduling;interference-free probability;concurrent direct transmissions;cooperative transmissions;reduced spatial reuse;Interference;Relays;Spread spectrum communication;Silicon;Transmitters;Receivers;Probability},
doi={10.1109/INFCOM.2013.6566813},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566814,
author={C. Qiu and L. Yu and H. Shen and S. Soltani},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Low-latency multi-flow broadcasts in fading wireless networks},
year={2013},
volume={},
number={},
pages={455-459},
abstract={Cooperative broadcast, in which a packet receiver cooperatively combines received weak signal power from different senders to decode the original packet, has gained increasing attention. However, existing approaches are developed based on the assumption that there is a single flow in the network; thus, they are not suitable for multi-flow broadcasting in which broadcasts are initiated by different nodes and consist of more than one packet at any point in time. In this paper, we aim to achieve low-latency multi-flow broadcast in wireless multihop networks with fading channels. We formulate this problem as a Minimum Slotted Delay Cooperative Broadcast (MSDCB) problem, and prove that it is NP-complete and o(logN) inapproximable. We then propose two heuristic algorithms named PCBHS and PCBH-M to solve MSDCB. Our experimental results show that our algorithms outperform previous methods.},
keywords={cooperative communication;fading channels;optimisation;packet radio networks;radio receivers;low-latency multiflow broadcast;fading wireless network;packet receiver;wireless multihop network;fading channel;minimum slotted delay cooperative broadcast;NP-complete problem;heuristic algorithm;PCBH-S;MSDCB;PCBH-M;Relays;Delays;Heuristic algorithms;Schedules;Fading;Wireless networks;Broadcasting},
doi={10.1109/INFCOM.2013.6566814},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566815,
author={P. E. Santacruz and V. Aggarwal and A. Sabharwal},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Beyond interference avoidance: Distributed sub-network scheduling in wireless networks with local views},
year={2013},
volume={},
number={},
pages={460-464},
abstract={In most wireless networks, nodes have only limited local information about the network state, which includes connectivity and channel state information. With limited local information about the network, each node's knowledge is mismatched, therefore they must make distributed decisions. In this paper, we pose the following question - if every node has network state information only about a small neighborhood, how and when should nodes choose to transmit? While scheduling answers the above question for point-to-point physical layers which are designed for an interference-avoidance paradigm, we look for answers in cases when interference can be embraced by advanced code design, as suggested by results in network information theory. To make progress on this challenging problem, we propose a distributed algorithm which achieves rates higher than interference-avoidance based link scheduling, especially if each node knows more than one hop of network state information.},
keywords={interference suppression;network coding;radio networks;scheduling;wireless channels;distributed subnetwork scheduling;wireless networks;local views;limited local information;channel state information;network state information;point-to-point physical layers;advanced code design;network information theory;interference-avoidance based link scheduling;Interference;Scheduling;Wireless networks;Knowledge engineering;Color;Physical layer;Computer architecture},
doi={10.1109/INFCOM.2013.6566815},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566816,
author={Y. Yang and M. Jin and Y. Zhao and H. Wu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Cut graph based information storage and retrieval in 3D sensor networks with general topology},
year={2013},
volume={},
number={},
pages={465-469},
abstract={We address the problem of in-network information processing, storage, and retrieval in three-dimensional (3D) sensor networks in this research. We propose a geographic location free double-ruling-based scheme for large-scale 3D sensor networks. The proposed approach does not require a 3D sensor network with a regular cube shape or uniform node distribution. Without the knowledge of the geographic location and the distance bound, a data query simply travels along a simple curve with the guaranteed success to retrieve aggregated data through time and space with one or different types across the network. Simulations and comparisons show the proposed approach with low cost and a balanced traffic load.},
keywords={distributed sensors;graph theory;information storage;query processing;cut graph-based information storage;information retrieval;in-network information processing;three-dimensional sensor networks;geographic location free double-ruling-based scheme;large-scale 3D sensor networks;regular cube shape;uniform node distribution;data query;balanced traffic load;Memory;Shape;Wireless sensor networks;Network topology;Topology;Tiles;Routing},
doi={10.1109/INFCOM.2013.6566816},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566817,
author={S. He and X. Gong and J. Zhang and J. Chen and Y. Sun},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Barrier coverage in wireless sensor networks: From lined-based to curve-based deployment},
year={2013},
volume={},
number={},
pages={470-474},
abstract={This paper studies deterministic sensor deployment to ensure barrier coverage in wireless sensor networks. Most of existing work focused on line-based deployment, ignoring a wide spectrum of potential curve-based solutions. We, for the first time, extensively study the sensor deployment under general settings. We first present a condition under which line-based deployment is suboptimal, pointing to the advantage of curve-based deployment. By constructing a contracting mapping, we identify the characteristics for a deployment curve to be optimal. We then design sensor deployment algorithms for the optimal deployment curve by introducing a new notion of distance-continuous. Our findings show that i) when the deployment curve is distance-continuous, the proposed algorithm is optimal in terms of the vulnerability corresponding to the deployment, and ii) when the deployment curve is not distance-continuous, the approximation ratio of the vulnerability corresponding to the deployment by the proposed algorithm to the optimal one is upper bounded by min (π, ||AB||/||AGB|| 2n+√(2-1)/2n), where ||AB||, ||AGB|| and n are constants. Extensive numerical results corroborate our analysis.},
keywords={wireless sensor networks;barrier coverage;wireless sensor networks;lined-based deployment;curve-based deployment;sensor deployment algorithms;Algorithm design and analysis;Wireless sensor networks;Approximation algorithms;Approximation methods;Ad hoc networks;Sun;Educational institutions},
doi={10.1109/INFCOM.2013.6566817},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566818,
author={A. Vergne and L. Decreusefond and P. Martins},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Reduction algorithm for simplicial complexes},
year={2013},
volume={},
number={},
pages={475-479},
abstract={In this paper, we aim at reducing power consumption in wireless sensor networks by turning off supernumerary sensors. Random simplicial complexes are tools from algebraic topology which provide an accurate and tractable representation of the topology of wireless sensor networks. Given a simplicial complex, we present an algorithm which reduces the number of its vertices, keeping its homology (i.e. connectivity, coverage) unchanged. We show that the algorithm reaches a Nash equilibrium, moreover we find both a lower and an upper bounds for the number of vertices removed, the complexity of the algorithm, and the maximal order of the resulting complex for the coverage problem. We also give some simulation results for classical cases, especially coverage complexes simulating wireless sensor networks.},
keywords={game theory;telecommunication network topology;wireless sensor networks;wireless sensor network topology;reduction algorithm;power consumption;supernumerary sensors;random simplicial complexes;algebraic topology;homology;Nash equilibrium;coverage problem;Sensors;Abstracts;Indexes;Wireless sensor networks;Topology;Face;Network topology},
doi={10.1109/INFCOM.2013.6566818},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566819,
author={Y. Wang and Y. He and D. Cheng and Y. Liu and X. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={TriggerCas: Enabling wireless consrucive collisions},
year={2013},
volume={},
number={},
pages={480-484},
abstract={Constructive Interference (CI) proposed in the existing work (e.g., A-MAC [1], Glossy [2]) may degrade the packet reception performance in terms of Packet Reception Ratio (PRR) and Received Signal Strength Indication (RSSI). The packet reception performance of a set of nodes transmitting simultaneously might be no better than that of any single node transmitting individually. In this paper, we redefine CI and propose TriggerCast, a practical wireless architecture which ensures concurrent transmissions of an identical packet to interfere constructively rather than to interfere non-destructively. CI potentially allows orders of magnitude reductions in energy consumption and improvements in link quality. Moreover, we for the first time present a theoretical sufficient condition for generating CI with IEEE 802.15.4 radio: concurrent transmissions with an identical packet should be synchronized at chip level. Meanwhile, co-senders participating in concurrent transmissions should be carefully selected, and the starting instants for the concurrent transmissions should be aligned. Based on the sufficient condition, we propose practical techniques to effectively compensate propagation and radio processing delays. TriggerCast has 95<sup>th</sup> percentile synchronization errors of at most 250ns. Extensive experiments in practical testbeds reveal that TriggerCast significantly improves PRR (from 5% to 70% with 7 concurrent senders, from 50% to 98.3% with 6 senders) and RSSI (about 6dB with 5 senders).},
keywords={radiofrequency interference;radiowave propagation;wireless sensor networks;Zigbee;WSN;wireless sensor networks;radio processing delay compensation;propagation compensation;chip level;identical packet;IEEE 802.15.4 radio;link quality improvement;energy consumption;magnitude reductions;concurrent transmissions;wireless architecture;RSSI;received signal strength indication;PRR;packet reception ratio;packet reception performance degradation;CI;constructive interference;wireless constructive collisions;TriggerCast;Delays;Synchronization;Signal to noise ratio;Receivers;Wireless sensor networks;IEEE 802.15 Standards},
doi={10.1109/INFCOM.2013.6566819},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566820,
author={X. Zheng and J. Yang and Y. Chen and Y. Gan},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Adaptive device-free passive localization coping with dynamic target speed},
year={2013},
volume={},
number={},
pages={485-489},
abstract={Device-free passive localization enables locating targets (e.g., intruders or victims) that do not carry any radio devices nor do they actively participate in the wireless localization process. This is because the wireless environments will get affected when people move into the area, which result in the changes of Received Signal Strength (RSS) of the wireless links. In this paper, we first show that the localization performance degrades significantly when people are moving in dynamic speeds. This is because existing studies in device-free passive localization system have an implicit assumption that the target is moving at a constant speed, which is not always true in practical scenarios. To cope with targets moving with dynamic speeds, we propose an adaptive speed change detection framework including three components: speed change detection, determination of time-window size and adaptive localization. Two speed change detection schemes have been developed to capture the changes of moving speed and adjust the time-window size adaptively to facilitate effective localization. We demonstrate that our framework is flexible to work with any device-free localization method using signal strength. Results from the real experiments confirm that our approach has over 30% improvement on both median and max localization error, under dynamically changing speed of the target.},
keywords={radio links;adaptive device-free passive localization coping;dynamic target speed;radio devices;wireless localization process;wireless environments;received signal strength;wireless links;localization performance;dynamic speeds;adaptive speed change detection framework;time-window size;localization error;Wireless communication;Performance evaluation;Wireless sensor networks;Accuracy;Legged locomotion;Tomography},
doi={10.1109/INFCOM.2013.6566820},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566821,
author={C. Wu and Z. Yang and Y. Zhao and Y. Liu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Footprints elicit the truth: Improving global positioning accuracy via local mobility},
year={2013},
volume={},
number={},
pages={490-494},
abstract={Global Positioning System (GPS) has enabled a number of geographical applications over many years. Quite a lot of location-based services, however, still suffer from considerable positioning errors of GPS (usually 1m to 20m in practice). In this study, we design and implement a high-accuracy global positioning solution based on GPS and human mobility captured by mobile phones. Our key observation is that smart phone-enabled dead reckoning supports accurate but local coordinates of users' trajectories, while GPS provides global but inconsistent coordinates. Considering them simultaneously, we devise techniques to refine the global positioning results by fitting the global positions to the structure of locally measured ones, so the refined positioning results are more likely to elicit the ground truth. We develop a prototype system, named GloCal, and conduct comprehensive experiments in both crowded urban and spacious suburban areas. The evaluation results show that GloCal can achieve 30% improvement on average error with respect to GPS.},
keywords={Global Positioning System;mobile radio;smart phones;global positioning accuracy;local mobility;global positioning system;GPS;location based service;human mobility;mobile phones;smart phone enabled dead reckoning;user trajectory;GloCal;crowded urban area;spacious suburban area;Global Positioning System;Accuracy;Trajectory;Mobile handsets;Sensors;Dead reckoning;Legged locomotion},
doi={10.1109/INFCOM.2013.6566821},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566822,
author={K. Liu and X. Liu and L. Xie and X. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Towards accurate acoustic localization on a smartphone},
year={2013},
volume={},
number={},
pages={495-499},
abstract={Since our daily activities are dominantly indoor, as smart phones emerge as the most popular personal computing companions, major IT companies recently launched aggressive investment on mobile indoor location services and positioning systems, e.g., on iOS or Android mobile devices. However, one major hurdle has not been conquered yet: smart phone-based high-resolution indoor localization. In this paper, we propose a practical solution for accurate ranging and localization based on acoustic communication between anchor nodes with speakers and the microphone on a smartphone. To identify different anchor nodes and enable time-of-arrival (TOA) ranging, we propose approaches for signal modulation, symbol detection and demodulation, synchronization and ranging. Experimental results show that the communication bit-error-rate and ranging accuracy is sufficient for our target applications. The preliminary results of localization demonstrate that our algorithm could achieve highaccuracy of 23cm in the offline mode with a promising potential for realtime smartphone-based indoor localization.},
keywords={acoustic signal detection;demodulation;error statistics;indoor radio;microphones;mobile radio;smart phones;time-of-arrival estimation;acoustic localization;smart phones;personal computing companion;IT companies;mobile indoor location service;positioning system;iOS;Android mobile device;high-resolution indoor localization;acoustic communication;microphone;speakers;time-of-arrival ranging;TOA ranging;signal modulation;symbol detection;symbol demodulation;synchronization;communication bit-error-rate;Distance measurement;Acoustics;Accuracy;Microphones;Bit error rate;Demodulation;Mobile handsets},
doi={10.1109/INFCOM.2013.6566822},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566823,
author={X. Li and J. Teng and Q. Zhai and J. Zhu and D. Xuan and Y. F. Zheng and W. Zhao},
booktitle={2013 Proceedings IEEE INFOCOM},
title={EV-Human: Human localization via visual estimation of body electronic interference},
year={2013},
volume={},
number={},
pages={500-504},
abstract={Human localization is an enabling technology for many mobile applications. As more and more people carry mobile phones with them, we can now localize a person by localizing his mobile phone. However, it is observed that presence of human bodies introduces heavy interference to mobile phone signals. This has been one of the major causes of inaccurate wireless localization for humans. In this paper, we propose using video cameras to help estimate human body's interference on mobile device's signals. We combine human orientation detection and human/phone/AP relative position inference estimation to better measure how a human blocks or reflects wireless signals. We have also developed a signal distortion compensation model. Based on these technologies, we have implemented a human localization system called EV-Human. Real world experiments show that our EV-system can accurately and robustly localize humans.},
keywords={mobile computing;mobile handsets;radiofrequency interference;video cameras;EV-Human;human localization;visual estimation;body electronic interference;human bodies;mobile phone signals;wireless localization;video cameras;human body interference estimation;mobile device signals;human orientation detection;human-phone-AP relative position inference estimation;human blocks;wireless signals;signal distortion compensation model;Interference;Wireless communication;Visualization;Cameras;Mobile communication;Mobile handsets;Biological system modeling},
doi={10.1109/INFCOM.2013.6566823},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566824,
author={Y. Zhao and Y. Liu and T. He and A. V. Vasilakos and C. Hu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={FREDI: Robust RSS-based ranging with multipath effect and radio interference},
year={2013},
volume={},
number={},
pages={505-509},
abstract={Radio Signal Strength (RSS) based ranging is attractive by the low cost and easy deployment. In real environments, its accuracy is severely affected by the multipath effect and the external radio interference. The well-known fingerprint approaches can deal with the issues but introduce too much overhead in dynamic environments. In this paper, we attempt to address the issue along a completely different direction. We propose a new ranging framework called Fredi that exploits the frequency diversity to overcome the multi-path effect solely based on RSS measurements. We design a Discrete Fourier Transformation based algorithm and prove that it has the optimal solution under ideal cases. We further revise the algorithm to be robust to the measurement noises in practice. We implement Fredi on top of the USRP-2 platform and conduct extensive real environments in indoor environments. Experimental results show the superiority performance compared with the traditional methods.},
keywords={discrete Fourier transforms;diversity reception;indoor radio;radiofrequency interference;FREDI;radio signal strength;RSS based ranging;multipath effect;external radio interference;frequency diversity;discrete Fourier transformation;measurement noise;USRP-2 platform;indoor environment;Distance measurement;Frequency measurement;Discrete Fourier transforms;Frequency diversity;Receivers;Algorithm design and analysis;Robustness},
doi={10.1109/INFCOM.2013.6566824},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566825,
author={A. Nahir and A. Orda and D. Raz},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Schedule first, manage later: Network-aware load balancing},
year={2013},
volume={},
number={},
pages={510-514},
abstract={Load balancing in large distributed server systems is a complex optimization problem of critical importance in cloud systems and data centers. Existing schedulers often incur a high overhead in communication when collecting the data required to make the scheduling decision, hence delaying the job request on its way to the executing server. We propose a novel scheme that incurs no communication overhead between the users and the servers upon job arrival, thus removing any scheduling overhead from the job's critical path. Our approach is based on creating several replicas of each job and sending each replica to a different server. Upon the arrival of a replica to the head of the queue at its server, the latter signals the servers holding replicas of that job, so as to remove them from their queues. We show, through analysis and simulations, that this scheme improves the expected queuing overhead over traditional schemes by a factor of 9 (or more) under various load conditions. In addition, we show that our scheme remains efficient even when the inter-server signal propagation delay is significant (relative to the job's execution time). We provide heuristic solutions to the performance degradation that occurs in such cases and show, by simulations, that they efficiently mitigate the detrimental effect of propagation delays. Finally, we demonstrate the efficiency of our proposed scheme in a real-world environment by implementing a load balancing system based on it, deploying the system on the Amazon Elastic Compute Cloud (EC2), and measuring its performance.},
keywords={cloud computing;computer centres;optimisation;queueing theory;resource allocation;network-aware load balancing;distributed server systems;complex optimization problem;cloud systems;data centers;communication overhead;scheduling decision;job critical path;server queue;queuing overhead;inter-server signal propagation delay;heuristic solutions;propagation delays;real-world environment;Amazon Elastic Compute Cloud;EC2;Servers;Load modeling;Delays;Analytical models;Propagation delay;Load management;Queueing analysis},
doi={10.1109/INFCOM.2013.6566825},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566826,
author={K. Wang and M. Lin and F. Ciucua and A. Wierman and C. Lin},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Characterizing the impact of the workload on the value of dynamic resizing in data centers},
year={2013},
volume={},
number={},
pages={515-519},
abstract={Energy consumption imposes a significant cost for data centers; yet much of that energy is used to maintain excess service capacity during periods of predictably low load. Resultantly, there has recently been interest in developing designs that allow the service capacity to be dynamically resized to match the current workload. However, there is still much debate about the value of such approaches in real settings. In this paper, we show that the value of dynamic resizing is highly dependent on statistics of the workload process. In particular, both slow timescale non-stationarities of the workload (e.g., the peak-to-mean ratio) and the fast time-scale stochasticity (e.g., the burstiness of arrivals) play key roles. To illustrate the impact of these factors, we combine optimization-based modeling of the slow time-scale with stochastic modeling of the fast time scale.},
keywords={computer centres;energy consumption;power aware computing;statistical analysis;stochastic programming;workload impact characterization;dynamic data center resizing;optimization-based modeling;fast time-scale stochasticity;slow time scale nonstationarity;workload process;energy consumption;Servers;Delays;Heuristic algorithms;Switches;Optimization;Stochastic processes;Data models},
doi={10.1109/INFCOM.2013.6566826},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566827,
author={Y. Gao and Z. Zeng and X. Liu and P. R. Kumar},
booktitle={2013 Proceedings IEEE INFOCOM},
title={The answer is blowing in the wind: Analysis of powering Internet data centers with wind energy},
year={2013},
volume={},
number={},
pages={520-524},
abstract={Internet-scale data centers (IDCs) have rapidly proliferated to such an extent that their energy consumption and GreenHouse Gas (GHG) emissions have become an important concern to society. As a result, many IDC operators have started using renewable energy, e.g., wind power, to power their data centers. Unfortunately, the utilization of wind energy has stayed at a low ratio due to the intermittent nature of wind. This paper makes the case that it is in fact possible for a distributed IDC system to exploit multiple uncorrelated wind energy sources to significantly reduce the effect of intermittency and nearly achieve “entirely green” cloud-scale services. This result is obtained based on the analysis of real-world wind power traces from 69 wind farms. The idea is to leverage the front-end load dispatching server to send work to the location where wind power is available. We propose a wind-power-aware (WPA) policy that routes jobs based only on the current states of workloads and wind power availabilities in the data centers. We show that with the WPA policy more than 95% of energy consumption in IDCs can in fact be satisfied by wind power, and, secondly, that achieving this does not require the delaying of processing of jobs due to wind availability. We also show that the locations where data centers are placed play an important role in achieving high wind power utilization. Our analysis shows that wind power utilization can generally lie in a range from 44% to 96%, depending on how the locations of wind farms are selected. We propose a method for location selection that uses the coefficient of variation instead of the correlation coefficient, and show that with this method the utilization can lie in the high end of the above range. Finally, we verify these results by simulations that are based on real-world traces for both workloads and wind power generations.},
keywords={computer centres;distributed power generation;power consumption;power generation dispatch;wind power;wind power plants;Internet-scale data centers;energy consumption;greenhouse gas;GHG emissions;renewable energy;wind energy;distributed IDC system;intermittency;entirely green cloud-scale services;wind farms;front-end load dispatching server;wind-power-aware;WPA policy;wind power utilization;coefficient of variation;Wind power generation;Wind farms;Servers;Power demand;Wind energy;Correlation;Green products},
doi={10.1109/INFCOM.2013.6566827},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566828,
author={T. Zhang and F. Wu and C. Qiao},
booktitle={2013 Proceedings IEEE INFOCOM},
title={SPECIAL: A strategy-proof and Efficient multi-channel Auction mechanism for wireless networks},
year={2013},
volume={},
number={},
pages={525-529},
abstract={Efficient wireless channel allocation is becoming a more and more important topic in wireless networking. Dynamic channel allocation is believed to be an effective way to cope with the shortage of wireless channel resource. In this paper, we propose SPECIAL, which is a Strategy-Proof and EffiCIent multi-channel Auction mechanism for wireLess networks. SPECIAL guarantees the strategy-proofness of the channel auction, exploits wireless channels' spatial reusability, and achieves high channel allocation efficiency.},
keywords={channel allocation;game theory;radio networks;wireless networks;wireless channel allocation;SPECIAL;strategy proof and efficient multichannel auction mechanism;Channel allocation;Cost accounting;Resource management;Wireless networks;Vectors;Interference},
doi={10.1109/INFCOM.2013.6566828},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566829,
author={Y. Zhao and S. Wang and S. Xu and X. Wang and X. Gao and C. Qiao},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Load balance vs energy efficiency in traffic engineering: A game Theoretical Perspective},
year={2013},
volume={},
number={},
pages={530-534},
abstract={In this paper, we study the tradeoff between two important traffic engineering objectives: load balance and energy efficiency. Although traditional commonly used multi-objective optimization methods can yield a Pareto efficient solution, they need to construct an aggregate objective function (AOF) or model one of the two objectives as a constraint in the optimization problem formulation. As a result, it is difficult to achieve a fair tradeoff between these two objectives. Accordingly, we induce a Nash bargaining framework which treats the two objectives as two virtual players in a game theoretic model, who negotiate how traffic should be routed in order to optimize both objectives. During the negotiation, each of them announces its performance threat value to reduce its cost, so the model is regarded as a threat value game. Our analysis shows that no agreement can be achieved if each player sets its threat value selfishly. To avoid such a negotiation break-down, we modify the threat value game to have a repeated process and design a mechanism to not only guarantee an agreement, but also generate a fair solution. In addition, the insights from this work are also useful for achieving a fair tradeoff in other multi-objective optimization problems.},
keywords={energy conservation;game theory;Pareto optimisation;resource allocation;telecommunication traffic;load balance;energy efficiency;traffic engineering;game theoretical perspective;traffic engineering objective;multiobjective optimization method;Pareto efficient solution;aggregate objective function;AOF;Nash bargaining framework;threat value game;Energy efficiency;Optimization;Games;Load modeling;Routing;Telecommunication traffic;Energy consumption;Traffic Engineering;Load Balance;Energy Efficiency;Nash Bargaining;Multi-Objective Optimization},
doi={10.1109/INFCOM.2013.6566829},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566830,
author={X. Luo and H. Tembine},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Evolutionary coalitional games for random access control},
year={2013},
volume={},
number={},
pages={535-539},
abstract={In this paper we consider a random access system where each user can be in two modes of operation, has a packet or not and the set of users which have a packet is available to a shared medium. We propose an evolving coalitional game theory to analyze the system outcomes. Unlike classical coalitional approaches that assume that coalitional structures are fixed and formed with cost-free, we explain how coalitions can be formed in a fully distributed manner using evolutionary dynamics and coalitional combined fully distributed payoff and strategy (CODIPAS) learning. We introduce the concept of evolutionarily stable coalitional structure (ESCS), which is, when it is formed it is resilient by small perturbation of strategies. We show that (i) the formation and the stability of coalitions depend mainly on the cost of making a coalition compared to the benefit of cooperation, (ii) the grand coalition can be unstable and a localized coalitional structure is formed as an evolutionarily stable coalitional structure. When the core is empty, the coalitional CODIPAS scheme selects one of the stable sets. Finally, we discuss the convergence and complexity of the proposed coalitional CODIPAS learning in access control with different users' activities.},
keywords={evolutionary computation;game theory;multi-access systems;radio networks;localized coalitional structure;ESCS;evolutionarily stable coalitional structure;CODIPAS learning;coalitional combined fully distributed payoff and strategy;evolutionary dynamics;random access control;coalitional game theory;evolutionary coalitional games;Games;Convergence;Vectors;Game theory;Stability analysis;Access control;Resource management},
doi={10.1109/INFCOM.2013.6566830},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566831,
author={C. Wang and Y. Chen and H. Wei and K. J. R. Liu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Optimal pricing in stochastic scalable video coding multicasting system},
year={2013},
volume={},
number={},
pages={540-544},
abstract={Heterogeneous multimedia content delivery over wireless networks is an important yet challenging issue. A promising solution is combining multicasting and scalable video coding (SVC) techniques via cross-layer design which has been shown to be effectively solution in the literature. Nevertheless, most existing works on SVC multicasting system focus on the static scenarios. In addition, the economic value of SVC multicasting system has seldom been explored. In this work, we study a subscription-based SVC multicasting system with stochastic user arrival and heterogeneous user preferences. A stochastic framework based on Multi-dimensional Markov Decision Process (M-MDP) is proposed to study the negative network externality existing in the proposed system. A game-theoretic analysis is conducted to understand the rational demands from heterogeneous users the subscription economic model. We show that the optimal pricing strategy which maximizes the expected revenue of the service provider can be derived through dynamic iterative updating techniques. Moreover, the overall user's valuation on the system is maximized under such an optimal pricing strategy. Finally, the solution efficiency is evaluated through simulations.},
keywords={game theory;iterative methods;Markov processes;multimedia communication;radio networks;video coding;optimal pricing;stochastic scalable video coding multicasting system;heterogeneous multimedia content delivery;wireless network;cross-layer design;subscription-based SVC multicasting system;stochastic user arrival;heterogeneous user preference;multidimensional Markov decision process;M-MDP;game-theoretic analysis;subscription economic model;dynamic iterative updating technique;Subscriptions;Pricing;Multicast communication;Streaming media;Static VAr compensators;Resource management;Games},
doi={10.1109/INFCOM.2013.6566831},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566832,
author={Y. Kanizo and D. Hay and I. Keslassy},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Palette: Distributing tables in software-defined networks},
year={2013},
volume={},
number={},
pages={545-549},
abstract={In software-defined networks (SDNs), the network controller first formulates abstract network-wide policies, and then implements them in the forwarding tables of network switches. However, fast SDN tables often cannot scale beyond a few hundred entries. This is because they typically include wildcards, and therefore are implemented using either expensive and power-hungry TCAMs, or complex and slow data structures. This paper presents the Palette distribution framework for decomposing large SDN tables into small ones and then distributing them across the network, while preserving the overall SDN policy semantics. Palette helps balance the sizes of the tables across the network, as well as reduce the total number of entries by sharing resources among different connections. It copes with two NP-hard optimization problems: Decomposing a large SDN table into equivalent subtables, and distributing the subtables such that each connection traverses each type of subtable at least once. To implement the Palette distribution framework, we introduce graph-theoretical formulations and algorithms, and show that they achieve close-to-optimal results in practice.},
keywords={data structures;software radio;distributing tables;software-defined networks;network controller;network-wide policies;network switches;TCAM;data structures;Palette distribution framework;Color;Control systems;Semantics;Protocols;Access control;Computer languages;Monitoring},
doi={10.1109/INFCOM.2013.6566832},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566833,
author={Q. Li and M. Xu and M. Chen},
booktitle={2013 Proceedings IEEE INFOCOM},
title={NSFIB construction amp;amp; aggregation with next hop of strict partial order},
year={2013},
volume={},
number={},
pages={550-554},
abstract={The Internet global routing tables have been expanding at a dramatic and increasing rate. In this paper, we propose the next hop of strict partial order to construct and aggregate the Nexthop-Selectable FIB (NSFIB). We control the path stretch caused by NSFIB aggregation by setting an upper limit number of next hops. According to our simulation, our aggregation algorithms shrink the FIB to 5-15%, compared with 20-60% of single-nexthop FIB aggregation algorithms; our method works very well in controlling the path stretch.},
keywords={Internet;telecommunication network routing;NSFIB construction;NSFIB aggregation;strict partial order;Internet global routing table;next hop selectable FIB;aggregation algorithm;path stretch;forwarding information base;Topology;Routing;Heuristic algorithms;Aggregates;Complexity theory;Internet;Network topology},
doi={10.1109/INFCOM.2013.6566833},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566834,
author={D. G. Garao and G. Maier and A. Pattavina},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Modular architectures of optical multi-stage switching networks},
year={2013},
volume={},
number={},
pages={555-559},
abstract={Future switching and interconnection fabrics inside switching equipment, high-performance computers and datacenters will require more throughput and more energy efficiency. Optical technology provides many opportunities of improvement of both features compared to electronic counterparts. This work defines a procedure to design the architecture of optical multistage switching networks. Modularity of the implementation is the primary concern, allowing for the construction of a genericsize fabric by the simple cascading of multiple stage-modules. In this paper we show in details the application of the approach to a family of banyan networks. The designed architecture can be exploited for various implementation technologies, as, for instance, integrated optics with micro-ring resonators, free-space optics with 2-D MEMS, networks on chip.},
keywords={computer centres;energy conservation;multistage interconnection networks;optical computing;optical interconnections;optical switches;switching networks;modular architecture;interconnection fabrics;switching equipment;high-performance computer;data center;energy efficiency;optical multistage switching networks;generic-size fabric;multiple stage-module;banyan network;microring resonator;free-space optics;2D MEMS;networks on chip;Optical switches;Optical resonators;Optical device fabrication;Optical network units;Optical interconnections},
doi={10.1109/INFCOM.2013.6566834},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566835,
author={E. Chai and K. G. Shin and S. Lee and J. Lee and R. Etkin},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Defeating heterogeneity in wireless multicast networks},
year={2013},
volume={},
number={},
pages={560-564},
abstract={The growing demand for real-time streaming video on portable devices has increased the importance of multimedia multicast in mobile wireless networks. A defining characteristic of such multicast networks is its heterogeneity in both the channel states and the MIMO capabilities of its clients. However, current wireless multicast schemes adapt poorly to such heterogeneity. We introduce Procrustes, a multimedia multicast scheme that is built upon a novel PHY-layer rateless code. Unlike bit-level rateless codes (such as Raptor [14] codes), Procrustes clients automatically adjust the PSNR of the received multicast video stream to match both the instantaneous channel state and the number of active receive antennas. We demonstrate the performance of Procrustes in a simulated environment.},
keywords={antenna arrays;mobile handsets;multicast communication;radio networks;receiving antennas;video streaming;wireless channels;wireless multicast networks;real-time video streaming;mobile wireless networks;MIMO capabilities;Procrustes;multimedia multicast scheme;PHY-layer rateless code;bit-level rateless codes;PSNR;received multicast video stream;instantaneous channel state;active receive antennas;Bit rate;Antennas;Transmitters;OFDM;Streaming media;MIMO;PSNR},
doi={10.1109/INFCOM.2013.6566835},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566836,
author={C. Chai and Y. Shih and A. Pang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A spectrum-sharing rewarding framework for co-channel hybrid access femtocell networks},
year={2013},
volume={},
number={},
pages={565-569},
abstract={With the explosive growth of mobile data traffic, the femtocell technology is one of the proper solutions to enhance mobile service quality and system capacity for cellular networks. However, one of the key problems for femtocell deployment is to find appropriate access control in which mobile operators and users are willing to be involved. Among all kinds of access control modes, the hybrid access mode is considered as the most promising one, which allows femtocells to provide preferential access to femtocell owners and subscribers while other public users can access femtocells with certain restriction. Since all femtocell owners are selfish, how to provide enough incentives to the owners for sharing their femtocell resources is challenging. In this paper, we construct an economic framework for mobile operator and femtocell users by a game theoretical analysis and introduce the concept of profit sharing to provide a positive cycle to sustain the femtocell service. In this framework, a femtocell game is formulated, where the femtocell owners determine the proportion of femtocell resources shared with public users while the operator can maximize its own benefit by determining the ratio of revenue distribution to femtocell owners. The existence of the Nash equilibrium of the game is analyzed. Extensive simulations are conducted to show that the profit of the operator can be maximized while the service requirements of users can be maintained by the proposed framework.},
keywords={access control;femtocellular radio;game theory;radio spectrum management;telecommunication services;telecommunication traffic;spectrum-sharing rewarding framework;cochannel hybrid access femtocell networks;mobile data traffic;mobile service quality enhancement;system capacity;mobile operators;access control mode;game theoretical analysis;femtocell resources;revenue distribution ratio;femtocell owner;Nash equilibrium;profit sharing concept;hybrid access mode;cellular networks;Games;Macrocell networks;Interference;Femtocell networks;Signal to noise ratio;Access control;Femtocell;game theory;hybrid access mode;profit sharing},
doi={10.1109/INFCOM.2013.6566836},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566837,
author={R. Southwell and X. Chen and J. Huang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={QoS satisfaction games for spectrum sharing},
year={2013},
volume={},
number={},
pages={570-574},
abstract={Today's wireless networks are facing tremendous growth and many applications have more demanding quality of service (QoS) requirements than ever before. However, there is only a finite amount of wireless resources (such as spectrum) that can be used to satisfy these demanding requirements. We present a general QoS satisfaction game framework for modeling the issue of distributed spectrum sharing to meet QoS requirements. Our study is motivated by the observation that finding globally optimal spectrum sharing solutions with QoS guarantees is NP hard. We show that the QoS satisfaction game has the finite improvement property, and the users can self-organize into a pure Nash equilibrium in polynomial time. By bounding the price of anarchy, we demonstrate that the worst case pure Nash equilibrium can be close to the global optimal solution when users' QoS demands are not too diverse.},
keywords={game theory;optimisation;quality of service;radio networks;radio spectrum management;wireless networks;quality of service;general QoS satisfaction game;distributed spectrum sharing;globally optimal spectrum sharing solutions;NP hard prolems;finite improvement property;Nash equilibrium;Games;Quality of service;Nash equilibrium;Wireless communication;Integrated circuits;Interference;Streaming media},
doi={10.1109/INFCOM.2013.6566837},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566838,
author={S. Bodas and B. Sadiq},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Polynomial-complexity, low-delay scheduling for SCFDMA-based wireless uplink networks},
year={2013},
volume={},
number={},
pages={575-579},
abstract={Uplink scheduling/resource allocation under the single-carrier FDMA constraint is investigated, taking into account the queuing dynamics at the transmitters. Under the single-carrier constraint, the problem of MaxWeight scheduling, as well as that of determining if a given number of packets can be served from all the users, are shown to be NP-complete. Finally, a matching-based scheduling algorithm is presented that requires only a polynomial number of computations per timeslot, and in the case of a system with large bandwidth and user population, provably provides a good delay (small-queue) performance, even under the single-carrier constraint. In summary, the results in first part of the paper support the recent push to remove SCFDMA from the Standards, whereas those in the second part present a way of working around the single-carrier constraint if it remains in the Standards.},
keywords={computational complexity;frequency division multiple access;polynomials;queueing theory;radio networks;scheduling;polynomial-complexity;low-delay scheduling;SCFDMA-based wireless uplink networks;uplink scheduling-resource allocation;single-carrier FDMA constraint;queuing dynamics;transmitters;MaxWeight scheduling;NP-complete;matching-based scheduling algorithm;user population;Servers;Barium;Uplink;Resource management;OFDM;Wireless communication;Radio spectrum management;Uplink scheduling;single-carrier FDMA;Batch-and-allocate},
doi={10.1109/INFCOM.2013.6566838},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566839,
author={Q. Wang and M. Liu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={When simplicity meets optimality: Efficient transmission power control with stochastic energy harvesting},
year={2013},
volume={},
number={},
pages={580-584},
abstract={We consider the optimal transmission power control of a single wireless node with stochastic energy harvesting and an infinite/saturated queue with the objective of maximizing a certain reward function, e.g., the total data rate. We develop simple control policies that achieve near optimal performance in the finite-horizon case with finite energy storage. The same policies are shown to be asymptotically optimal in the infinite horizon case for sufficiently large energy storage. Such policies are typically difficult to directly obtain using a Markov Decision Process (MDP) formulation or through a dynamic programming framework due to the computational complexity. We relate our results to those obtained in the unsaturated regime, and highlight a type of threshold-based policies that is universally optimal.},
keywords={computational complexity;Markov processes;power control;queueing theory;radio networks;stochastic processes;transmission power control;stochastic energy harvesting;single wireless node;infinite-saturated queue;control policies;finite energy storage;infinite horizon case;Markov decision process formulation;MDP formulation;computational complexity;threshold-based policies;Batteries;Energy harvesting;Wireless communication;Throughput;Power control;Markov processes;Optimization},
doi={10.1109/INFCOM.2013.6566839},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566840,
author={M. Gorlatova and R. Margolies and J. Sarik and G. Stanje and J. Zhu and B. Vigraham and M. Szczodrak and L. Carloni and P. Kinget and I. Kymissis and G. Zussman},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Prototyping energy harvesting active networked tags (EnHANTs)},
year={2013},
volume={},
number={},
pages={585-589},
abstract={This paper focuses on a new type of wireless devices in the domain between RFIDs and sensor networks - Energy Harvesting Active Networked Tags (EnHANTs). Future EnHANTs will be small, flexible, and self-powered devices that can be attached to objects that are traditionally not networked (e.g., books, toys, clothing), thereby providing the infrastructure for novel tracking applications. We present the design considerations for the EnHANT prototypes, developed over the past 3 years. The prototypes harvest indoor light energy using custom organic solar cells, communicate and form multihop networks using ultralow-power Ultra-Wideband Impulse Radio (UWB-IR) transceivers, and adapt their communications and networking patterns to the energy harvesting and battery states. We also describe a small scale EnHANTs testbed that uniquely allows evaluating different algorithms with trace-based light energy inputs.},
keywords={energy harvesting;radiofrequency identification;solar cells;telecommunication power supplies;wireless sensor networks;energy harvesting active networked tags;EnHANT;wireless device;RFID;wireless sensor network;tracking application;indoor light energy;organic solar cell;multihop network;ultralow power impulse radio transceiver;ultrawideband impulse radio transceiver;battery state;light energy input;Energy harvesting;Prototypes;Photovoltaic cells;Transceivers;Photovoltaic systems;Multiaccess communication;Energy adaptive networking;energy harvesting;ultra-low-power communications;organic solar cells;ultrawideband (UWB);cross-layer},
doi={10.1109/INFCOM.2013.6566840},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566841,
author={N. Michelusi and L. Badia and R. Carli and L. Corradini and M. Zorzi},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Impact of battery degradation on optimal management policies of harvesting-based wireless sensor devices},
year={2013},
volume={},
number={},
pages={590-594},
abstract={Harvesting-Based Wireless Sensor Devices are increasingly being deployed in today's sensor networks, due to their demonstrated advantages in terms of prolonged lifetime and autonomous operation. However, irreversible degradation mechanisms jeopardize battery lifetime, calling for intelligent management policies, which minimize the impact of these phenomena while guaranteeing a minimum Quality of Service (QoS). This paper explores a mathematical characterization of harvesting-based battery-powered sensor devices, focusing on the impact of the battery discharge policy on the irreversible degradation of the storage capacity. A general framework based on Markov chains which captures the battery degradation process is proposed. Based on such model, it is shown that a degradationaware policy significantly improves the lifetime of the sensor compared to "greedy" operation policies, while guaranteeing the minimum required QoS.},
keywords={Markov processes;quality of service;telecommunication network management;telecommunication network reliability;wireless sensor networks;battery degradation impact;optimal management policies;harvesting-based wireless sensor devices;wireless sensor networks;autonomous operation;irreversible degradation mechanisms;battery lifetime;intelligent management policies;quality of service;QoS;harvesting-based battery-powered sensor devices;battery discharge policy;Markov chains;battery degradation process;greedy operation policies;Batteries;Degradation;Quality of service;Wireless sensor networks;Energy harvesting;Markov processes;Wireless communication;Battery management;Energy harvesting;Lifetime estimation;Markov processes;Wireless sensor networks},
doi={10.1109/INFCOM.2013.6566841},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566842,
author={B. Kim and S. Ren and M. van der Schaar and J. Lee},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Bidirectional energy trading for residential load scheduling and electric vehicles},
year={2013},
volume={},
number={},
pages={595-599},
abstract={Electric vehicles (EVs) will play an important role in the future smart grid because of their capabilities of storing electrical energy in their batteries during off-peak hours and supplying the stored energy to the power grid during peak hours. In this paper, we consider a power system with an aggregator and multiple customers with EVs and propose a novel electricity load scheduling which, unlike previous works, jointly considers the load scheduling for appliances and the energy trading using EVs. Specifically, we allow customers to determine how much energy to purchase from or to sell to the aggregator while taking into consideration the load demands of their residential appliances and the associated electricity bill. Under the assumption of the collaborative system where the customers agree to maximize the social welfare of the power system, we develop an optimal distributed load scheduling algorithm that maximizes the social welfare. Through numerical results, we show when the energy trading leads to an increase in the social welfare in various usage scenarios.},
keywords={electric vehicles;load (electric);scheduling;bidirectional energy trading;residential load scheduling;electric vehicles;smart grid;electrical energy;off-peak hours;EV;residential appliances;social welfare;power system;distributed load scheduling algorithm;Home appliances;Batteries;Electricity;Smart grids;Energy consumption;Scheduling},
doi={10.1109/INFCOM.2013.6566842},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566843,
author={S. Shang and P. Cuff and P. Hui and S. Kulkarni},
booktitle={2013 Proceedings IEEE INFOCOM},
title={An upper bound on the convergence time for quantized consensus},
year={2013},
volume={},
number={},
pages={600-604},
abstract={We analyze a class of distributed quantized consensus algorithms for arbitrary networks. In the initial setting, each node in the network has an integer value. Nodes exchange their current estimate of the mean value in the network, and then update their estimate by communicating with their neighbors in a limited capacity channel in an asynchronous clock setting. Eventually, all nodes reach consensus with quantized precision. We start the analysis with a special case of a distributed binary voting algorithm, then proceed to the expected convergence time for the general quantized consensus algorithm proposed by Kashyap et al. We use the theory of electric networks, random walks, and couplings of Markov chains to derive an O(N<sup>3</sup> log N) upper bound for the expected convergence time on an arbitrary graph of size N, improving on the state of art bound of O(N<sup>4</sup> log N) for binary consensus and O(N<sup>5</sup>) for quantized consensus algorithms. Our result is not dependent on the graph topology. Simulations are performed to validate the analysis.},
keywords={convergence of numerical methods;graph theory;Markov processes;network analysis;telecommunication network topology;distributed quantized consensus algorithms;arbitrary networks;network mean value;limited capacity channel;asynchronous clock setting;quantized precision;distributed binary voting algorithm;convergence time;electric networks theory;random walks;Markov chains couplings;graph topology;Convergence;Upper bound;Algorithm design and analysis;Markov processes;Peer-to-peer computing;Simulation;Clocks;Distributed quantized consensus;gossip;convergence time},
doi={10.1109/INFCOM.2013.6566843},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566844,
author={L. Zhang and X. Li and Y. Liu and T. Jung},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Verifiable private multi-party computation: Ranging and ranking},
year={2013},
volume={},
number={},
pages={605-609},
abstract={The existing work on distributed secure multi-party computation, e.g., set operations, dot product, ranking, focus on the privacy protection aspects, while the verifiability of user inputs and outcomes are neglected. Most of the existing works assume that the involved parties will follow the protocol honestly. In practice, a malicious adversary can easily forge his/her input values to achieve incorrect outcomes or simply lie about the computation results to cheat other parities. In this work, we focus on the problem of verifiable privacy preserving multiparty computation. We thoroughly analyze the attacks on existing privacy preserving multi-party computation approaches and design a series of protocols for dot product, ranging and ranking, which are proved to be privacy preserving and verifiable. We implement our protocols on laptops and mobile phones. The results show that our verifiable private computation protocols are efficient both in computation and communication.},
keywords={computer network security;cryptographic protocols;data privacy;mobile handsets;verifiable private multiparty computation;distributed secure multiparty computation;privacy protection;malicious adversary;privacy preserving multiparty computation;dot product protocol;ranging protocol;ranking protocol;verifiable private computation protocol;Protocols;Privacy;Distance measurement;Encryption;Portable computers;Vectors;Verifiability;Privacy;Multi-party Computation;Ranking;Ranging;Dot Product},
doi={10.1109/INFCOM.2013.6566844},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566845,
author={Z. Xu and C. Wang and Q. Wang and K. Ren and L. Wang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Proof-carrying cloud computation: The case of convex optimization},
year={2013},
volume={},
number={},
pages={610-614},
abstract={Cloud computing provides a “pay-per-use” utility service which offers the customer the economical access to large amount of computing resources with minimal management overhead. Despite the tremendous benefits, computation outsourcing also eliminates the customer's ultimate control over the data computation process, which makes securing cloud computation an imperative and challenging task, especially in the aspect of integrity verification. To address these challenges, in this paper we propose to research on integrity verification mechanisms for secure outsourced computations in cloud computing. In particular, we focus on outsourcing the widely applicable engineering optimization problem, i.e., convex optimization, and aim to investigate efficient integrity verification mechanisms using application-specific techniques. Our security design does not require the use of heavy cryptographic tools. Instead, we leverage the inherent structure of the optimization problems and the proof-carrying characteristics of the solving algorithms to achieve efficient integrity verification. The proposed design provides substantial computational savings on the customer side and introduce marginal overhead on the cloud side. We further prove its correctness and soundness. The extensive experiments under real cloud environment show our mechanisms ensure strong integrity assurance with high efficiency on both the customer and cloud sides and are readily applicable in practice.},
keywords={cloud computing;cryptography;data integrity;optimisation;outsourcing;proof-carrying cloud computation;convex optimization;pay-per-use utility service;economical access;computing resources;management overhead;computation outsourcing;data computation process;cloud computation securing;engineering optimization problem;integrity verification mechanisms;application-specific techniques;cryptographic tools;security design;integrity verification;cloud environment;Convex functions;Outsourcing;Optimization;Cryptography;Algorithm design and analysis;Educational institutions;Cloud Computing;Security;Computation Outsourcing;Convex Optimization},
doi={10.1109/INFCOM.2013.6566845},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566846,
author={S. Roos and T. Strufe},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A contribution to analyzing and enhancing Darknet routing},
year={2013},
volume={},
number={},
pages={615-619},
abstract={Routing in Darknets, membership concealing overlays for pseudonymous communication, like for instance Freenet, is insufficiently analyzed, barely understood, and highly inefficient. These systems at higher performance are promising privacy preserving solutions for social applications. This paper contributes a realistic analytical model and a novel routing algorithm with provable polylog expected routing length. Using the model, we additionally prove that this can not be achieved by Freenet's routing. Simulations support that our proposed algorithm achieves a better performance than Freenet for realistic network sizes.},
keywords={telecommunication network routing;darknet routing;pseudonymous communication;privacy preserving solutions;social applications;routing algorithm;Freenet routing;Routing;Peer-to-peer computing;Topology;Network topology;Analytical models;Social network services;Privacy},
doi={10.1109/INFCOM.2013.6566846},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566847,
author={Y. Guo and A. L. Stolyar and A. Walid},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Shadow-routing based dynamic algorithms for virtual machine placement in a network cloud},
year={2013},
volume={},
number={},
pages={620-628},
abstract={We consider a shadow routing based approach to the problem of real-time adaptive placement of virtual machines (VM) in large data centers (DC) within a network cloud. Such placement in particular has to respect vector packing constraints on the allocation of VMs to host physical machines (PM) within a DC, because each PM can potentially serve multiple VMs simultaneously. Shadow routing is attractive in that it allows a large variety of system objectives and/or constraints to be treated within a common framework (as long as the underlying optimization problem is convex). Perhaps even more attractive feature is that the corresponding algorithm is very simple to implement, it runs continuously, and adapts automatically to changes in the VM demand rates, changes in system parameters, etc., without the need to re-solve the underlying optimization problem “from scratch”. In this paper we focus on the minmax-DC-load problem. Namely, we propose a combined VM-toDC routing and VM-to-PM assignment algorithm, referred to as Shadow scheme, which minimizes the maximum of appropriately defined DC utilizations. We prove that the Shadow scheme is asymptotically optimal (as one of its parameters goes to 0). Simulation confirms good performance and high adaptivity of the algorithm. Favorable performance is also demonstrated in comparison with a baseline algorithm based on VMware implementation [7], [8]. We also propose a simplified - “more distributed” - version of the Shadow scheme, which performs almost as well in simulations.},
keywords={cloud computing;computer centres;convex programming;minimax techniques;resource allocation;virtual machines;virtualisation;shadow-routing based dynamic algorithms;virtual machine placement;network cloud;data centers;real-time adaptive placement problem;vector packing constraints;VM allocation;physical machines;optimization problem;VM demand rate change;system parameter change;minmax-DC-load problem;VM-to-DC routing algorithm;VM-to-PM assignment algorithm;DC utilizations;VMware implementation;resource management;cloud service providers;cloud computing;virtualization technology;Routing;Vectors;Computational modeling;Algorithm design and analysis;Steady-state;Virtual machining;Servers},
doi={10.1109/INFCOM.2013.6566847},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566848,
author={H. Yanagisawa and T. Osogami and R. Raymond},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Dependable virtual machine allocation},
year={2013},
volume={},
number={},
pages={629-637},
abstract={The difficulty in allocating virtual machines (VMs) on servers stems from the requirement that sufficient resources (such as CPU capacity and network bandwidth) must be available for each VM in the event of a failure or maintenance work as well as for temporal fluctuations of resource demands, which often exhibit periodic patterns. We propose a mixed integer programming approach that considers the fluctuations of the resource demands for optimal and dependable allocation of VMs. At the heart of the approach are techniques for optimally partitioning the time-horizon into intervals of variable lengths and for reliably estimating the resource demands in each interval. We show that our new approach allocates VMs successfully in a cloud computing environment in a financial company, where the dependability requirement is strict and there are various types of VMs exist.},
keywords={cloud computing;finance;integer programming;virtual machines;dependable virtual machine allocation;temporal fluctuations;mixed integer programming approach;cloud computing environment;financial company;dependability requirement;VM;Servers;Maintenance engineering;Resource management;Cloud computing;Fault tolerance;Fault tolerant systems;server consolidation;capacity planning;fault tolerance;mixed integer programming;dynamic programming},
doi={10.1109/INFCOM.2013.6566848},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566849,
author={L. E. Li and V. Liaghat and H. Zhao and M. Hajiaghayi and D. Li and G. Wilfong and Y. R. Yang and C. Guo},
booktitle={2013 Proceedings IEEE INFOCOM},
title={PACE: Policy-Aware Application Cloud Embedding},
year={2013},
volume={},
number={},
pages={638-646},
abstract={The emergence of new capabilities such as virtualization and elastic (private or public) cloud computing infrastructures has made it possible to deploy multiple applications, on demand, on the same cloud infrastructure. A major challenge to achieve this possibility, however, is that modern applications are typically distributed, structured systems that include not only computational and storage entities, but also policy entities (e.g., load balancers, firewalls, intrusion prevention boxes). Deploying applications on a cloud infrastructure without the policy entities may introduce substantial policy violations and/or security holes. In this paper, we present PACE: the first systematic framework for Policy-Aware Application Cloud Embedding. We precisely define the policy-aware, cloud application embedding problem, study its complexity and introduce simple, efficient, online primal-dual algorithms to embed applications in cloud data centers. We conduct evaluations using data from a real, large campus network and a realistic data center topology to evaluate the feasibility and performance of PACE. We show that deployment in a cloud without considering in-network policies may lead to a large number of policy violations (e.g., using tree routing as a way to enforce in-network policies may observe up to 91% policy violations). We also show that our embedding algorithms are very efficient by comparing with a good online fractional embedding algorithm.},
keywords={cloud computing;computer centres;trees (mathematics);PACE;policy-aware application cloud embedding;virtualization;elastic cloud computing infrastructure;private cloud computing infrastructure;public cloud computing infrastructure;policy entities;load balancer;firewall;intrusion prevention boxes;policy violation;security holes;online primal-dual algorithm;cloud data center;campus network;data center topology;in-network policies;tree routing;online fractional embedding algorithm;Middleboxes;Routing;Security;Bandwidth;Virtual machining;Network topology;Topology},
doi={10.1109/INFCOM.2013.6566849},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566850,
author={M. Alicherry and T. V. Lakshman},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Optimizing data access latencies in cloud systems by intelligent virtual machine placement},
year={2013},
volume={},
number={},
pages={647-655},
abstract={Many cloud applications are data intensive requiring the processing of large data sets and the MapReduce/Hadoop architecture has become the de facto processing framework for these applications. Large data sets are stored in data nodes in the cloud which are typically SAN or NAS devices. Cloud applications process these data sets using a large number of application virtual machines (VMs), with the total completion time being an important performance metric. There are many factors that affect the total completion time of the processing task such as the load on the individual servers, the task scheduling mechanism, communication and data access bottlenecks, etc. One dominating factor that affects completion times for data intensive applications is the access latencies from processing nodes to data nodes. Ideally, one would like to keep all data access local to minimize access latency but this is often not possible due to the size of the data sets, capacity constraints in processing nodes which constrain VMs from being placed in their ideal location and so on. When it is not possible to keep all data access local, one would like to optimize the placement of VMs so that the impact of data access latencies on completion times is minimized. We address this problem of optimized VM placement - given the location of the data sets, we need to determine the locations for placing the VMs so as to minimize data access latencies while satisfying system constraints. We present optimal algorithms for determining the VM locations satisfying various constraints and with objectives that capture natural tradeoffs between minimizing latencies and incurring bandwidth costs. We also consider the problem of incorporating inter-VM latency constraints. In this case, the associated location problem is NP-hard with no effective approximation within a factor of 2 - ϵ for any ϵ > 0. We discuss an effective heuristic for this case and evaluate by simulation the impact of the various tradeoffs in the optimization objectives.},
keywords={cloud computing;computational complexity;virtual machines;data access latency optimization;cloud systems;intelligent virtual machine placement;cloud applications;large data set processing;MapReduce-Hadoop architecture;data nodes;SAN devices;NAS devices;VM;data intensive applications;processing nodes;capacity constraints;optimal algorithms;latency minimization;bandwidth costs;interVM latency constraints;NP-hard problem;Bandwidth;Virtual machining;Approximation algorithms;Optimization;Measurement;Approximation methods;Minimization},
doi={10.1109/INFCOM.2013.6566850},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566851,
author={Z. Gu and Q. Hua and Y. Wang and F. C. M. Lau},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Reducing information gathering latency through Mobile Aerial Sensor Network},
year={2013},
volume={},
number={},
pages={656-664},
abstract={Gathering information in a sensing field of interest is a fundamental task in wireless sensor networks. Current methods either use multihop forwarding to the sink via stationary nodes or use mobile sinks to traverse the sensing field. The multihop forwarding method intrinsically has the energy hole problem and the mobile sinks method has a large gathering latency due to its low mobility velocity. In addition, all the mobile sinks methods assume unlimited power supply and memory which is unrealistic in practice. In this paper, we propose a new approach for information gathering through a Mobile Aerial Sensor Network (MASN). We adopt the Hive-Drone model [5] where a centralized station (Hive) responsible for serving and recharging Micro-Aerial Vehicle (MAV) sensor nodes (Drones) is strategically placed in the sensing field. We then face the challenges of how to control the mobility of each MAV and devising interference-free scheduling for wireless transmissions that can substantially reduce the latency. We present a family of algorithms with constant memory to reduce both gathering latency, which is the duration from dispatching the MAVs to the moment when all the sensed information are gathered at the central station, and information latency, which is the duration from when some information is sensed to when it is received by the station. We also consider how to extend the single Hive to multiple Hives for monitoring an arbitrarily large area. Extensive simulation results corroborate our theoretical analysis.},
keywords={mobile radio;scheduling;space vehicles;wireless sensor networks;information gathering latency reduction;mobile aerial sensor network;wireless sensor networks;multihop forwarding;stationary nodes;multihop forwarding method;energy hole problem;mobile sink method;mobility velocity;unlimited power supply;MASN;hive-drone model;centralized station;microaerial vehicle sensor nodes;MAV sensor nodes;interference-free scheduling;wireless transmissions;constant memory;MAV dispatching;Sensors;Mobile communication;Monitoring;Interference;Wireless sensor networks;Mobile computing;Wireless communication;Micro-Aerial Vehicle;Information Gathering;Sensor Networks;Gathering Latency;Information Latency},
doi={10.1109/INFCOM.2013.6566851},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566852,
author={S. Han and Y. Noh and U. Lee and M. Gerla},
booktitle={2013 Proceedings IEEE INFOCOM},
title={M-FAMA: A multi-session MAC protocol for reliable underwater acoustic streams},
year={2013},
volume={},
number={},
pages={665-673},
abstract={Mobile underwater networking is a developing technology for monitoring and exploring the Earth's oceans. For effective underwater exploration, multimedia communications such as sonar images and low resolution videos are becoming increasingly important. Unlike terrestrial RF communication, underwater networks rely on acoustic waves as a means of communication. Unfortunately, acoustic waves incur long propagation delays that typically lead to low throughput especially in protocols that require receiver feedback such as multimedia stream delivery. On the positive side, the long propagation delay permits multiple packets to be “pipelined” concurrently in the underwater channel, improving the overall throughput and enabling applications that require sustained bandwidth. To enable session multiplexing and pipelining, we propose the Multi-session FAMA (M-FAMA) algorithm. M-FAMA leverages passively-acquired local information (i.e., neighboring nodes' propagation delay maps and expected transmission schedules) to launch multiple simultaneous sessions. M-FAMA's greedy behavior is controlled by a Bandwidth Balancing algorithm that guarantees max-min fairness across multiple contending sources. Extensive simulation results show that M-FAMA significantly outperforms existing MAC protocols in representative streaming applications.},
keywords={access protocols;acoustic receivers;acoustic streaming;acoustic waves;underwater acoustic communication;multisession MAC protocol;reliable underwater acoustic stream;mobile underwater networking;multimedia communication;RF communication;underwater exploration;sonar image;low resolution video;terrestrial RF communication;acoustic wave;receiver feedback;propagation delay;multiple packet;underwater channel;session multiplexing;pipelining;multisession FAMA algorithm;M-FAMA algorithm;bandwidth balancing algorithm;multiple contending source;Delays;Propagation delay;Receivers;Protocols;Bandwidth;Schedules;Throughput;Underwater;AUV;SEA Swarm;Medium Access Control;Concurrent Transmission;CSMA},
doi={10.1109/INFCOM.2013.6566852},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566853,
author={X. Dong and M. C. Vuran},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Environment aware connectivity for wireless underground sensor networks},
year={2013},
volume={},
number={},
pages={674-682},
abstract={Wireless underground sensor networks (WUSNs) consist of sensors that are buried in and communicate through soil. The channel quality of WUSNs is strongly impacted by environmental parameters such soil moisture. Thus, the communication range of the nodes and the network connectivity vary over time. To address the challenges in underground communication, above ground nodes are deployed to maintain connectivity. In this paper, the connectivity of WUSNs under varying environmental conditions is captured by modeling the cluster size distribution under sub-critical conditions and through a novel aboveground communication coverage model for underground clusters. The resulting connectivity model is utilized to analyze two communication schemes: transmit power control and environment-aware routing, which maintain connectivity while reducing energy consumption. It is shown that transmit power control can maintain network connectivity under all soil moisture values at the cost of energy consumption. Utilizing relays based on soil moisture levels can decrease this energy consumption. A composite of both approaches is also considered to analyze the tradeoff between connectivity and energy consumption.},
keywords={telecommunication network routing;underground communication;wireless sensor networks;environment aware connectivity;wireless underground sensor network;WUSN;channel quality;soil moisture;cluster size distribution;transmit power control;environment-aware routing;energy consumption;Soil moisture;Lattices;Approximation methods;Analytical models;Energy consumption;Topology},
doi={10.1109/INFCOM.2013.6566853},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566854,
author={Y. Zhu and Z. Jiang and Z. Peng and M. Zuba and J. Cui and H. Chen},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Toward practical MAC design for underwater acoustic networks},
year={2013},
volume={},
number={},
pages={683-691},
abstract={Recently, various Medium Access Control (MAC) protocols have been proposed for underwater acoustic networks. These protocols have significantly improved the performance of MAC layer in theory. However, two critical characteristics, low transmission rates and long preambles, found in the commercial modem-based real systems, drastically degrade the performance of existing MAC protocols in the real world. A new practical MAC design is demanded. Toward a proper approach, this paper analyzes the impact of the two newly found modem characteristics on the random access-based MAC and handshakebased MAC, which are two major types of MAC protocols for underwater acoustic networks. We further develop the nodal throughput and collision probability models for representative solutions of these two MAC protocol types. Based on the analysis, we believe time sharing-based MAC is very promising. Along this line, we propose a time sharing-based MAC and analyze its nodal throughput. Both analytical and simulation results show that the time sharing-based solution can achieve significantly better performance.},
keywords={access protocols;probability;underwater acoustic communication;MAC design;underwater acoustic networks;medium access control protocol;MAC protocol;transmission rates;commercial modem-based real system;collision probability model;representative solution;time sharing-based MAC;handshake-based MAC;Media Access Protocol;Modems;Throughput;Propagation delay;Delays;Underwater acoustics},
doi={10.1109/INFCOM.2013.6566854},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566855,
author={G. Kuperman and E. Modiano},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Network protection with guaranteed recovery times using recovery domains},
year={2013},
volume={},
number={},
pages={692-700},
abstract={We consider the problem of providing network protection that guarantees the maximum amount of time that flow can be interrupted after a failure. This is in contrast to schemes that offer no recovery time guarantees, such as IP rerouting, or the prevalent local recovery scheme of Fast ReRoute, which often over-provisions resources to meet recovery time constraints. To meet these recovery time guarantees, we provide a novel and flexible solution by partitioning the network into failure-independent “recovery domains”, where within each domain, the maximum amount of time to recover from a failure is guaranteed. We show the recovery domain problem to be NP-Hard, and develop an optimal solution in the form of an MILP for both the case when backup capacity can and cannot be shared. This provides protection with guaranteed recovery times using up to 45% less protection resources than local recovery. We demonstrate that the network-wide optimal recovery domain solution can be decomposed into a set of easier to solve subproblems. This allows for the development of flexible and efficient solutions, including an optimal algorithm using Lagrangian relaxation, which simulations show to converge rapidly to an optimal solution. Additionally, an algorithm is developed for when backup sharing is allowed. For dynamic arrivals, this algorithm performs better than the solution that tries to greedily optimize for each incoming demand.},
keywords={computational complexity;failure analysis;integer programming;IP networks;linear programming;telecommunication network routing;network protection;guaranteed recovery times;recovery domains;IP rerouting;fast reroute prevalent local recovery scheme;over-provisions resources;recovery time constraints;failure-independent recovery domains;NP-Hard;MILP;protection resources;network-wide optimal recovery domain solution;Lagrangian relaxation;backup sharing;dynamic arrivals;Routing;Multiprotocol label switching;Switches;Delays;Time factors;Resource management;Heuristic algorithms},
doi={10.1109/INFCOM.2013.6566855},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566856,
author={Z. Zhang and Z. Guo and Y. Yang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Bounded-reorder packet scheduling in optical cut-through switch},
year={2013},
volume={},
number={},
pages={701-709},
abstract={Energy efficiency of optical packet switches (OPS) is the key to ensure the profitability of backbone network providers. However, due to lack of optical random access buffer, most optical packet switches rely on electronic buffer to resolve output contention, which requires power-hungry O/E/O conversion for all packets. The recently proposed optical cut-through (OpCut) switch holds a great potential in achieving high energy efficiency, as it allows optical packets to cut through the switch in optical domain whenever possible. The energy efficiency of OpCut switch hinges on the cut-through ratio, which is the percentage of packets that cut through the switch optically. On the other hand, it is generally desirable to maintain packet order in a switch. To achieve in-order transmission, an optical packet needs to be converted to electronic form and buffered when an earlier packet from the same flow is still in the buffer, which may lead to a low cut-through ratio. In the meanwhile, the Internet is designed to accommodate a certain degree of packet reorder, which is very common in practice due to path multiplicity. In this paper, we introduce a novel reorder metric, reorder degree, to accurately describe the extent of packet reordering, and propose a flow management scheme to bound the reorder degree of transmitted flows. We then design an efficient packet scheduling algorithm that significantly increases the cutthrough ratio of the OpCut switch while allowing a small degree of out-of-order transmission. Our extensive simulation results show that the cut-through ratio can be drastically increased with only a very small reorder degree.},
keywords={optical switches;packet switching;bounded reorder packet scheduling;optical cut-through switch;optical packet switch;backbone network provider;optical random access buffer;OpCut switch;cut-through ratio;reorder metric;reorder degree;packet reordering;flow management;efficient packet scheduling algorithm;out-of-order transmission;Optical switches;Optical buffering;Optical packet switching;Scheduling algorithms;Indexes;Ports (Computers);OpCut switch;Power consumption;Energy efficiency;O/E/O conversion;Cut-through ratio;Reorder degree;Reorder bound;Packet scheduling},
doi={10.1109/INFCOM.2013.6566856},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566857,
author={A. Fréchette and F. B. Shepherd and M. K. Thottan and P. J. Winzer},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Shortest path versus multi-hub routing in networks with uncertain demand},
year={2013},
volume={},
number={},
pages={710-718},
abstract={We study a class of robust network design problems motivated by the need to scale core networks to meet increasingly dynamic capacity demands. Past work has focused on designing the network to support all hose matrices (all matrices not exceeding marginal bounds at the nodes). This model may be too conservative if additional information on traffic patterns is available. Another extreme is the fixed demand model, where one designs the network to support peak point-to-point demands. We introduce a capped hose model to explore a broader range of traffic matrices which includes the above two as special cases. It is known that optimal designs for the hose model are always determined by single-hub routing, and for the fixed-demand model are based on shortest-path routing. We shed light on the wider space of capped hose matrices in order to see which traffic models are more shortest path-like as opposed to hub-like. To address the space in between, we use hierarchical multi-hub routing templates, a generalization of hub and tree routing. In particular, we show that by adding peak capacities into the hose model, the single-hub tree-routing template is no longer cost-effective. This initiates the study of a class of robust network design (RND) problems restricted to these templates. Our empirical analysis is based on a heuristic for this new hierarchical RND problem. We also propose that it is possible to define a routing indicator that accounts for the strengths of the marginals and peak demands and use this information to choose the appropriate routing template. We benchmark our approach against other well-known routing templates, using representative carrier networks and a variety of different capped hose traffic demands, parameterized by the relative importance of their marginals as opposed to their point-to-point peak demands. This study also reveals conditions under which multi-hub routing gives improvements over single-hub and shortest-path routings.},
keywords={telecommunication network routing;telecommunication traffic;shortest path routing;uncertain demand;robust network design problems;dynamic capacity demands;traffic patterns;fixed demand model;peak point-to-point demands;traffic matrices;single-hub routing;fixed-demand model;capped hose matrices;multihub routing templates;single-hub tree-routing template;robust network design;empirical analysis;hierarchical RND problem;routing indicator;routing template;capped hose traffic demands;capped hose traffic demands;Routing;Hoses;Virtual private networks;Robustness;Vegetation;Measurement;Heuristic algorithms},
doi={10.1109/INFCOM.2013.6566857},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566858,
author={T. Bansal and B. Chen and P. Sinha},
booktitle={2013 Proceedings IEEE INFOCOM},
title={DISCERN: Cooperative whitespace scanning in practical environments},
year={2013},
volume={},
number={},
pages={719-727},
abstract={Cognitive radio devices opportunistically operate on whitespace channels, provided those channels are not in use by the primary users. This opportunistic reusing of channels requires secondary users to perform fast and efficient sensing to determine the unused channels. Although individual secondary clients may be unwilling to frequently sense all the channels, their density could be exploited for tasking the individual devices to collaboratively extract useful information on spectrum usage. It is critical to determine how the sensing tasks should be assigned to different secondary users. This is particularly challenging in practical networks due to the variability in the sensing accuracy of different users that may arise because of multipath effects on the signal and varying distances from the primary transmitters. Further, presence of multiple Primary Users on the same channel makes it challenging to select the best users for sensing. Finally, to reduce the sensing overhead, it is beneficial to limit the number of channel sensing tasks that can be performed within a given time period. We propose a novel metric that captures the sensing accuracy of a given sensing assignment. Using our metric, we design an algorithm DISCERN for computing the sensing assignment that maximizes the sensing accuracy. Our algorithm is the first to take into account the limitations in practical networks. Our work is motivated by experimental measurements. Tracedriven simulations show that DISCERN increases the sensing accuracy by at least 30%. Theoretical analysis shows that the sensing assignment computed by DISCERN performs within 63% of the exponential-time optimal solution.},
keywords={cognitive radio;cooperative communication;multipath channels;radio transmitters;DISCERN;cooperative whitespace scanning;practical environments;cognitive radio devices;whitespace channels;unused channels;secondary clients;multipath effects;primary transmitters;sensing overhead;channel sensing tasks;exponential-time optimal solution;Sensors;Accuracy;Scattering;Measurement;Channel estimation;Correlation;Base stations},
doi={10.1109/INFCOM.2013.6566858},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566859,
author={L. Sun and W. Wang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Understanding Blackholes in large-scale Cognitive Radio Networks under generic failures},
year={2013},
volume={},
number={},
pages={728-736},
abstract={It has been demonstrated that in wireless networks, Blackholes, which are typically generated by isolated node failures, and augmented by failure correlations, can easily result in devastating impact on network performance. Therefore, many solutions, such as routing protocols and restoration algorithms, are proposed to deal with Blackholes by identifying alternative paths to bypass these holes such that the effect of Blackholes can be mitigated. These advancements are based on an underlying premise that there exists at least one alternative path in the network. However, such a hypothesis remains an open question. In other words, we do not know whether the network is resilient to Blackholes or whether an alternative path exists. The answer to this question can complement our understanding of designing routing protocols, as well as topology evolution in the presence of random failures. In order to address this issue, we focus on the topology of Cognitive Radio Networks (CRNs) because of their phenomenal benefits in improving spectrum efficiency through opportunistic communications. Particularly, we first define two metrics, namely the failure occurrence probability p and failure connection function g(·), to characterize node failures and their spreading properties, respectively. Then we prove that each Blackhole is exponentially bounded based on percolation theory. By mapping failure spreading using a branching process, we further derive an upper bound on the expected size of Blackholes. With the observations from our analysis, we are able to find a sufficient condition for a resilient CRN in the presence of Blackholes through analysis and simulations.},
keywords={cognitive radio;failure analysis;routing protocols;telecommunication network topology;Blackholes;large-scale cognitive radio networks;generic failures;wireless networks;isolated node failures;failure correlations;routing protocols;restoration algorithms;network performance;random failures;CRN topology;opportunistic communications;failure occurrence probability;failure connection function;spreading properties;branching process;Interference;Lattices;Network topology;Topology;Explosions;Wireless networks;Analytical models},
doi={10.1109/INFCOM.2013.6566859},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566860,
author={Y. Gwon and H. T. Kung},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Scaling network-based spectrum analyzer with constant communication cost},
year={2013},
volume={},
number={},
pages={737-745},
abstract={We propose a spectrum analyzer that leverages many networked commodity sensor nodes, each of which samples its portion in a wideband spectrum. The sensors operate in parallel and transmit their measurements over a wireless network without performing any significant computations such as FFT. The measurements are forwarded to the backend of the system where spectrum analysis takes place. In particular, we propose a solution that compresses the raw measurements in a simple random linear projection and combines the compressed measurements from multiple sensors in-network. As a result, we achieve a substantial reduction in the network bandwidth requirement to operate the proposed system. We discover that the overall communication cost can be independent of the number of sensors and is affected only by sparsity of discretized spectrum under analysis. This principle founds the basis for a claim that our network-based spectrum analyzer can scale up the number of sensor nodes to process a very wide spectrum block potentially having a GHz bandwidth. We devise a novel recovery algorithm that systematically undoes compressive encoding and in-network combining done to the raw measurements, incorporating the least squares and I1-minimization decoding used in compressive sensing, and demonstrate that the algorithm can effectively restore an accurate estimate of the original data suitable for finegrained spectrum analysis. We present mathematical analysis and empirical evaluation of the system with software-defined radios.},
keywords={compressed sensing;decoding;least squares approximations;minimisation;radio networks;radio spectrum management;software radio;scaling network-based spectrum analyzer;constant communication cost;networked commodity sensor nodes;wideband spectrum;wireless network;FFT;random linear projection;compressed measurements;GHz bandwidth;recovery algorithm;compressive encoding;raw measurements;least square analysis;l1-minimization decoding;compressive sensing;fine-grained spectrum analysis;mathematical analysis;software-defined radio;discretized spectrum analysis;Bandwidth;Compressed sensing;Decoding;Base stations;Atmospheric measurements;Particle measurements;Time-domain analysis},
doi={10.1109/INFCOM.2013.6566860},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566861,
author={I. Chuang and H. Wu and K. Lee and Y. Kuo},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Alternate hop-and-wait channel rendezvous method for cognitive radio networks},
year={2013},
volume={},
number={},
pages={746-754},
abstract={Cognitive Radio (CR) is an emerging technology developed to improve the utilization of licensed spectra, and a promising solution to alleviate the spectrum shortage problem. In cognitive radio networks (CRNs), if users want to establish communication links with neighbors, they need to rendezvous on an available channel. However, it is infeasible to employ a common control channel (CCC) in a licensed spectrum, because the CCC will be congested by primary users and degrade the performance of CRNs by the single point of failure problem. Therefore, the channel hopping (CH) based methods without CCC support are usually adopted to establish rendezvous in CRNs. This kind of rendezvous is called blind rendezvous and generally estimated by the following criteria: 1) asynchronous rendezvous support, 2) guaranteed rendezvous, 3) asymmetric model support, 4) multi-user/multi-hop scenario support, and 5) short time-to-rendezvous (TTR). Most existing blind rendezvous methods fail to fully satisfy these criteria or have considerable TTR which is significantly increased with the number of available channels. In this paper, the alternate hop-and-wait CH method is proposed to solve these problems. Furthermore, each user has a unique alternating sequence of HOP and WAIT modes. The theoretical analysis results have confirmed that the proposed method satisfies all evaluation criteria mentioned above and provides shorter TTR. According to simulation results, the performance of the proposed method even is two times better than Jump-Stay algorithm (JS), which is the most efficient blind rendezvous method as we know, under the asymmetric model which is much critical for CRNs.},
keywords={cognitive radio;failure analysis;radio links;wireless channels;hop-and-wait channel rendezvous method;cognitive radio networks;licensed spectra;spectrum shortage problem;CRN;communication links;common control channel;CCC;failure problem;channel hopping based methods;asynchronous rendezvous support;guaranteed rendezvous;asymmetric model support;multiuser-multihop scenario support;short time-to-rendezvous;TTR;blind rendezvous methods;hop-and-wait CH method;jump-stay algorithm;JS algorithm;Transforms;Cognitive radio;Simulation;Educational institutions;Algorithm design and analysis;Analytical models;Synchronization;cognitive radio;blind rendezvous;channel hopping},
doi={10.1109/INFCOM.2013.6566861},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566862,
author={W. Dong and S. Rallapalli and R. Jana and L. Qiu and K. K. Ramakrishnan and L. Razoumov and Y. Zhang and T. W. Cho},
booktitle={2013 Proceedings IEEE INFOCOM},
title={iDEAL: Incentivized dynamic cellular offloading via auctions},
year={2013},
volume={},
number={},
pages={755-763},
abstract={The explosive growth of cellular traffic and its highly dynamic nature often make it increasingly expensive for a cellular service provider to provision enough cellular resources to support the peak traffic demands. In this paper, we propose iDEAL, a novel auction-based incentive framework that allows a cellular service provider to leverage resources from third-party resource owners on demand by buying capacity whenever needed through reverse auctions. iDEAL has several distinctive features: (i) iDEAL explicitly accounts for the diverse spatial coverage of different resources and can effectively foster competition among third-party resource owners in different regions, resulting in significant savings to the cellular service provider. (ii) iDEAL provides revenue incentives for third-party resource owners to participate in the reverse auction and be truthful in the bidding process. (iii) iDEAL is provably efficient. (iv) iDEAL effectively guards against collusion. (v) iDEAL effectively copes with the dynamic nature of traffic demands. In addition, iDEAL has useful extensions that address important practical issues. Extensive evaluation based on real traces from a large US cellular service provider clearly demonstrates the effectiveness of our approach. We further demonstrate the feasibility of iDEAL using a prototype implementation.},
keywords={cellular radio;electronic commerce;mobile computing;telecommunication traffic;traffic demands;cellular service provider;third-party resource;buying capacity;auction-based incentive framework;peak traffic demands;cellular resources;cellular traffic growth;incentivized dynamic cellular offloading;iDEAL;Decision support systems},
doi={10.1109/INFCOM.2013.6566862},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566863,
author={H. Cai and I. Koprulu and N. B. Shroff},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Exploiting double opportunities for deadline based content propagation in wireless networks},
year={2013},
volume={},
number={},
pages={764-772},
abstract={In this paper, we focus on mobile wireless networks comprising of a powerful communication center and a multitude of mobile users. We investigate the propagation of deadline-based content in the wireless network characterized by heterogeneous (time-varying and user-dependent) wireless channel conditions, heterogeneous user mobility, and where communication could occur in a hybrid format (e.g., directly from the central controller or by exchange with other mobiles in a peer-to-peer manner). We show that exploiting double opportunities, i.e., both time-varying channel conditions and mobility, can result in substantial performance gains. We develop a class of double opportunistic multicast schedulers and prove their optimality in terms of both utility and fairness under heterogeneous channel conditions and user mobility. Extensive simulation results are provided to demonstrate that these algorithms can not only substantially boost the throughput of all users (e.g., by 50% to 150%), but also achieve different consideration of fairness among individual users and groups of users.},
keywords={mobility management (mobile radio);multicast communication;time-varying channels;mobile wireless networks;communication center;mobile users;deadline based content propagation;heterogeneous wireless channel conditions;heterogeneous user mobility;hybrid format;double opportunistic multicast schedulers;throughput;Mobile communication;Throughput;Tin;Mobile computing;Aggregates;Wireless networks},
doi={10.1109/INFCOM.2013.6566863},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566864,
author={B. Błaszczyszyn and M. K. Karray and H. P. Keeler},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Using Poisson processes to model lattice cellular networks},
year={2013},
volume={},
number={},
pages={773-781},
abstract={An almost ubiquitous assumption made in the stochastic-analytic approach to study of the quality of user-service in cellular networks is Poisson distribution of base stations, often completed by some specific assumption regarding the distribution of the fading (e.g. Rayleigh). The former (Poisson) assumption is usually (vaguely) justified in the context of cellular networks, by various irregularities in the real placement of base stations, which ideally should form a lattice (e.g. hexagonal) pattern. In the first part of this paper we provide a different and rigorous argument justifying the Poisson assumption under sufficiently strong lognormal shadowing observed in the network, in the evaluation of a natural class of the typical-user service-characteristics (including path-loss, interference, signal-to-interference ratio, spectral efficiency). Namely, we present a Poisson-convergence result for a broad range of stationary (including lattice) networks subject to log-normal shadowing of increasing variance. We show also for the Poisson model that the distribution of all these typical-user service characteristics does not depend on the particular form of the additional fading distribution. Our approach involves a mapping of 2D network model to 1D image of it “perceived” by the typical user. For this image we prove our Poisson convergence result and the invariance of the Poisson limit with respect to the distribution of the additional shadowing or fading. Moreover, in the second part of the paper we present some new results for Poisson model allowing one to calculate the distribution function of the SINR in its whole domain. We use them to study and optimize the mean energy efficiency in cellular networks.},
keywords={cellular radio;stochastic processes;model lattice cellular networks;ubiquitous assumption;stochastic-analytic approach;Poisson distribution;base stations;fading distribution;lognormal shadowing;typical-user service-characteristics;Poisson-convergence;stationary networks;Poisson model;2D network model mapping;1D image;Poisson convergence;SINR;Shadow mapping;Interference;Signal to noise ratio;Propagation losses;Fading;Base stations;Convergence;Wireless cellular networks;Poisson;Hexagonal;convergence;shadowing;fading;spectral/energy efficiency;optimization},
doi={10.1109/INFCOM.2013.6566864},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566865,
author={W. Wang and J. Zhang and Q. Zhang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Cooperative cell outage detection in Self-Organizing femtocell networks},
year={2013},
volume={},
number={},
pages={782-790},
abstract={The vision of Self-Organizing Networks (SON) has been drawing considerable attention as a major axis for the development of future networks. As an essential functionality in SON, cell outage detection is developed to autonomously detect macrocells or femtocells that are inoperative and unable to provide service. Previous cell outage detection approaches have mainly focused on macrocells while the outage issue in the emerging femtocell networks is less discussed. However, due to the two-tier macro-femto network architecture and the small coverage nature of femtocells, it is challenging to enable outage detection functionality in femtocell networks. Based on the observation that spatial correlations among users can be extracted to cope with these challenges, this paper proposes a Cooperative femtocell Outage Detection (COD) architecture which consists of a trigger stage and a detection stage. In the trigger stage, we design a trigger mechanism that leverages correlation information extracted through collaborative filtering to efficiently trigger the detection procedure without inter-cell communications. In the detection stage, to improve the detection accuracy, we introduce a sequential cooperative detection rule to process the spatially and temporally correlated user statistics. In particular, the detection problem is formulated as a sequential hypothesis testing problem, and the analytical results on the detection performance are derived. Numerical studies for a variety of femtocell deployments and configurations demonstrate that COD outperforms the existing scheme in both communication overhead and detection accuracy.},
keywords={collaborative filtering;cooperative communication;correlation methods;femtocellular radio;self-organizing femtocell networks;SON;cooperative cell outage detection;two-tier macro-femto network architecture;spatial correlations;cooperative femtocell outage detection;COD;correlation information;collaborative filtering;intercell communications;sequential hypothesis testing problem;Computer architecture;Correlation;Macrocell networks;Microprocessors;Handover;Femtocell networks},
doi={10.1109/INFCOM.2013.6566865},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566866,
author={B. Gu and X. Li and G. Li and A. C. Champion and Z. Chen and F. Qin and D. Xuan},
booktitle={2013 Proceedings IEEE INFOCOM},
title={D2Taint: Differentiated and dynamic information flow tracking on smartphones for numerous data sources},
year={2013},
volume={},
number={},
pages={791-799},
abstract={With smartphones' meteoric growth in recent years, leaking sensitive information from them has become an increasingly critical issue. Such sensitive information can originate from smartphones themselves (e.g., location information) or from many Internet sources (e.g., bank accounts, emails). While prior work has demonstrated information flow tracking's (IFT's) effectiveness at detecting information leakage from smartphones, it can only handle a limited number of sensitive information sources. This paper presents a novel IFT tagging strategy using differentiated and dynamic tagging. We partition information sources into differentiated classes and store them in fixed-length tags. We adjust tag structure based on time-varying received information sources. Our tagging strategy enables us to track at runtime numerous information sources in multiple classes and rapidly detect information leakage from any of these sources. We design and implement D2Taint, an IFT system using our tagging strategy on real-world smartphones. We experimentally evaluate D2Taint's effectiveness with 84 real-world applications downloaded from Google Play. D2Taint reports that over 80% of them leak data to third-party destinations; 14% leak highly sensitive data. Our experimental evaluation using a standard benchmark tool illustrates D2Taint's effectiveness at handling many information sources on smartphones with moderate runtime and space overhead.},
keywords={mobile computing;security of data;smart phones;tracking;D2Taint;differentiated information flow tracking;dynamic information flow tracking;smartphone;sensitive information leaking;IFT tagging strategy;differentiated tagging;dynamic tagging;information source partitioning;tag structure;information leakage detection;Google Play;Internet source;Tagging;Smart phones;Switches;Sensitivity;Security;Runtime;Androids},
doi={10.1109/INFCOM.2013.6566866},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566867,
author={J. Sun and R. Zhang and Y. Zhang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Privacy-preserving spatiotemporal matching},
year={2013},
volume={},
number={},
pages={800-808},
abstract={The explosive growth of mobile-connected and location-aware devices makes it possible to have a new way of establishing trust relationships, which we coin as spatiotemporal matching. In particular, a mobile user could very easily maintain his spatiotemporal profile recording his continuous whereabouts in time, and the level of his spatiotemporal profile matching that of the other user can be translated into the level of trust they two can have in each other. Since spatiotemporal profiles contain very sensitive personal information, privacy-preserving spatiotemporal matching is needed to ensure that as little information as possible about the spatiotemporal profile of either matching participant is disclosed beyond the matching result. We propose a cryptographic solution based on Private Set Intersection Cardinality and a more efficient non-cryptographic solution involving a novel use of the Bloom filter. We thoroughly analyze both solutions and compare their efficacy and efficiency via detailed simulation studies.},
keywords={data privacy;data structures;mobile computing;pattern matching;private key cryptography;privacy-preserving spatiotemporal matching;spatiotemporal profile matching;cryptographic solution;private set intersection cardinality;noncryptographic solution;Bloom filter;mobile-connected devices;location-aware devices;Spatiotemporal phenomena;Protocols;Indexes;Privacy;Mobile handsets;Accuracy;Estimation},
doi={10.1109/INFCOM.2013.6566867},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566868,
author={S. Dai and A. Tongaonkar and X. Wang and A. Nucci and D. Song},
booktitle={2013 Proceedings IEEE INFOCOM},
title={NetworkProfiler: Towards automatic fingerprinting of Android apps},
year={2013},
volume={},
number={},
pages={809-817},
abstract={Network operators need to have a clear visibility into the applications running in their network. This is critical for both security and network management. Recent years have seen an exponential growth in the number of smart phone apps which has complicated this task. Traditional methods of traffic classification are no longer sufficient as the majority of this smart phone app traffic is carried over HTTP/HTTPS. Keeping up with the new applications that come up everyday is very challenging and time-consuming. We present a novel technique for automatically generating network profiles for identifying Android apps in the HTTP traffic. A network profile consists of fingerprints, i.e., unique characteristics of network behavior, that can be used to identify an app. To profile an Android app, we run the app automatically in an emulator and collect the network traces. We have developed a novel UI fuzzing technique for running the app such that different execution paths are exercised, which is necessary to build a comprehensive network profile. We have also developed a light-weight technique, for extracting fingerprints, that is based on identifying invariants in the generated traces. We used our technique to generate network profiles for thousands of apps. Using our network profiles we were able to detect the presence of these apps in real-world network traffic logs from a cellular provider.},
keywords={cellular radio;feature extraction;fingerprint identification;fuzzy set theory;mobility management (mobile radio);smart phones;telecommunication security;telecommunication traffic;transport protocols;NetworkProfiler;automatic fingerprinting;Android Apps;network operators;network management;network security;network traffic classification method;smart phone application traffic;HTTP-HTTPS traffic;network behavior characteristics;emulator;network traces;UI fuzzing technique;fingerprint extraction;network traffic logs;cellular provider;Androids;Humanoid robots;Smart phones;Fingerprint recognition;Servers;Mobile communication;Internet},
doi={10.1109/INFCOM.2013.6566868},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566869,
author={M. H. R. Khouzani and S. Sen and N. B. Shroff},
booktitle={2013 Proceedings IEEE INFOCOM},
title={An economic analysis of regulating security investments in the Internet},
year={2013},
volume={},
number={},
pages={818-826},
abstract={Regulating the ISPs to adopt more security measures has been proposed as an effective method in mitigating the threats of attacks in the Internet. However, economic incentives of the ISPs and the network effects of security measures can lead to an under-investment in their adoption. We study the potential gains in a network's social utility when a regulator implements a monitoring and penalizing mechanism on the outbound threat activities of autonomous systems (ASes). We then show how freeriding can render regulations futile if the subset of ASes under the regulator's authority is smaller than a threshold. Finally, we show how heterogeneity of the ASes affect the responses of the ISPs and discuss how the regulator can leverage such information to improve the overall effectiveness of different security policies.},
keywords={Internet;security of data;economic analysis;security investment;Internet;ISP;economic incentive;autonomous system;security policy;Security;Regulators;Monitoring;Internet;Investment;Economics;Time measurement},
doi={10.1109/INFCOM.2013.6566869},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566870,
author={Q. Huang and Y. Tao and F. Wu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={SPRING: A Strategy-proof and Privacy preserving spectrum auction mechanism},
year={2013},
volume={},
number={},
pages={827-835},
abstract={The problem of dynamic spectrum redistribution has been extensively studied in recent years. Auction is believed to be one of the most effective tools to solve this problem. A great number of strategy-proof auction mechanisms have been proposed to improve spectrum allocation efficiency by stimulating bidders to truthfully reveal their valuations of spectrum, which are the private information of bidders. However, none of these approaches protects bidders' privacy. In this paper, we present SPRING, which is the first Strategy-proof and PRivacy preservING spectrum auction mechanism. We not only rigorously prove the properties of SPRING, but also extensively evaluate its performance. Our evaluation results show that SPRING achieves good spectrum redistribution efficiency with low overhead.},
keywords={data privacy;radio spectrum management;SPRING;privacy preserving spectrum auction mechanism;dynamic spectrum redistribution;strategy-proof auction mechanism;bidder private information;bidder privacy protection;Privacy;Springs;Resource management;Encryption;Cost accounting;Interference},
doi={10.1109/INFCOM.2013.6566870},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566871,
author={S. Sheng and M. Liu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Profit incentive in a secondary spectrum market: A contract design approach},
year={2013},
volume={},
number={},
pages={836-844},
abstract={In this paper we formulate a contract design problem where a primary license holder wishes to profit from its excess spectrum capacity by selling it to potential secondary users/buyers. It needs to determine how to optimally price the excess spectrum so as to maximize its profit, knowing that this excess capacity is stochastic in nature, does not come with exclusive access, and cannot provide deterministic service guarantees to a buyer. At the same time, buyers are of different types, characterized by different communication needs, tolerance for the channel uncertainty, and so on, all of which a buyer's private information. The license holder must then try to design different contracts catered to different types of buyers in order to maximize its profit. We address this problem by adopting as a reference a traditional spectrum market where the buyer can purchase exclusive access with fixed/deterministic guarantees. We fully characterize the optimal solution in the cases where there is a single buyer type, and when multiple types of buyers share the same, known channel condition as a result of the primary user activity. In the most general case we construct an algorithm that generates a set of contracts in a computationally efficient manner, and show that this set is optimal when the buyer types satisfy a monotonicity condition.},
keywords={channel capacity;contracts;incentive schemes;spread spectrum communication;monotonicity condition;primary user activity;fixed-deterministic guarantees;buyer private information;channel uncertainty;communication needs;deterministic service;exclusive access;spectrum capacity;primary license holder;contract design approach;secondary spectrum market;profit incentive;Bandwidth;Licenses;Uncertainty;Stochastic processes;Numerical models},
doi={10.1109/INFCOM.2013.6566871},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566872,
author={R. Berry and M. Honig and T. Nguyen and V. Subramanian and H. Zhou and R. Vohra},
booktitle={2013 Proceedings IEEE INFOCOM},
title={On the nature of revenue-sharing contracts to incentivize spectrum-sharing},
year={2013},
volume={},
number={},
pages={845-853},
abstract={In a limited form cellular providers have long shared spectrum in the form of roaming agreements. The primary motivation for this has been to extend the coverage of a wireless carrier's network into regions where it has no infrastructure. As devices and infrastructure become more agile, such sharing could be done on a much faster time-scale and have advantages even when two providers both have coverage in a given area, e.g., by enabling one provider to acquire “overflow” capacity from another provider during periods of high demand. This may provide carriers with an attractive means to better meet their rapidly increasing bandwidth demands. On the other hand, the presence of such a sharing agreement could encourage providers to underinvest in their networks, resulting in poorer performance. We adapt the newsvendor model from the operations management literature to model such a situation and to gain insight into these trade-offs. In particular, we analyze the structure of revenue-sharing contracts that incentivize both capacity sharing and increased access for end-users.},
keywords={cellular radio;incentive schemes;radio spectrum management;revenue-sharing contracts;incentivize spectrum-sharing;cellular providers;shared spectrum;roaming agreements;wireless carrier network;sharing agreement;overflow capacity;newsvendor model;operation management literature;capacity sharing;Investment;Joints;Nash equilibrium;Contracts;Games;Distribution functions;Roaming},
doi={10.1109/INFCOM.2013.6566872},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566873,
author={H. Xu and B. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Joint request mapping and response routing for geo-distributed cloud services},
year={2013},
volume={},
number={},
pages={854-862},
abstract={Many cloud services are running on geographically distributed datacenters for better reliability and performance. We consider the emerging problem of joint request mapping and response routing with distributed datacenters in this paper. We formulate the problem as a general workload management optimization. A utility function is used to capture various performance goals, and the location diversity of electricity and bandwidth costs are realistically modeled. To solve the large-scale optimization, we develop a distributed algorithm based on the alternating direction method of multipliers (ADMM). Following a decomposition-coordination approach, our algorithm allows for a parallel implementation in a datacenter where each server solves a small sub-problem. The solutions are coordinated to find an optimal solution to the global problem. Our algorithm converges to near optimum within tens of iterations, and is insensitive to step sizes. We empirically evaluate our algorithm based on real-world workload traces and latency measurements, and demonstrate its effectiveness compared to conventional methods.},
keywords={cloud computing;computer centres;computer network reliability;distributed algorithms;power aware computing;telecommunication network routing;telecommunication power management;Web services;geo-distributed cloud services;geographically distributed datacenters;joint request mapping and response routing problem;workload management optimization;utility function;electricity location diversity;bandwidth costs;large-scale optimization;distributed algorithm;alternating direction method of multipliers;ADMM;decomposition-coordination approach;parallel datacenter implementation;real-world workload traces;latency measurements;Routing;Algorithm design and analysis;Servers;Optimization;Bandwidth;Electricity;Accuracy},
doi={10.1109/INFCOM.2013.6566873},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566874,
author={E. J. Rosensweig and D. S. Menasche and J. Kurose},
booktitle={2013 Proceedings IEEE INFOCOM},
title={On the steady-state of cache networks},
year={2013},
volume={},
number={},
pages={863-871},
abstract={Over the past few years Content-Centric Networking, a networking model in which host-to-content communication protocols are introduced, has been gaining much attention. A central component of such an architecture is a large-scale interconnected caching system. To date, the way these Cache Networks operate and perform is still poorly understood. In this work, we demonstrate that certain cache networks are non-ergodic in that their steady-state characterization depends on the initial state of the system. We then establish several important properties of cache networks, in the form of three independently-sufficient conditions for a cache network to comprise a single ergodic component. Each property targets a different aspect of the system - topology, admission control and cache replacement policies. Perhaps most importantly we demonstrate that cache replacement can be grouped into equivalence classes, such that the ergodicity (or lack-thereof) of one policy implies the same property holds for all policies in the class.},
keywords={cache storage;content management;network topology;peer-to-peer computing;cache network steady state;content centric networking;host-to-content communication protocol;interconnected caching system;sufficient condition;ergodic component;admission control;cache replacement policy;network topology;Steady-state;Markov processes;Topology;Network topology;Routing;Admission control;Delays},
doi={10.1109/INFCOM.2013.6566874},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566875,
author={Z. Zhou and F. Liu and H. Jin and B. Li and B. Li and H. Jiang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={On arbitrating the power-performance tradeoff in SaaS clouds},
year={2013},
volume={},
number={},
pages={872-880},
abstract={In this paper, we present an analytical framework for characterizing and optimizing the power-performance tradeoff in Software-as-a-Service (SaaS) cloud platforms. Our objectives are two-fold: (1) We maximize the operating profit when serving heterogeneous SaaS applications with unpredictable user requests, and (2) we minimize the power consumption when processing user requests. To achieve these objectives, we take advantage of Lyapunov Optimization techniques to design and analyze an optimal control framework to make online decisions on request admission control, routing, and virtual machine (VMs) scheduling. In particular, our control framework can be flexibly extended to incorporate various design choices and practical requirements of a data-center in the cloud, such as enforcing a certain power budget for improving the performance (dollar) per watt. Our mathematical analyses and simulations have demonstrated both the optimality (in terms of a cost-effective power-performance tradeoff) and system stability (in terms of robustness and adaptivity to time-varying and bursty user requests) achieved by our proposed control framework.},
keywords={cloud computing;computer centres;Lyapunov methods;optimal control;optimisation;scheduling;telecommunication congestion control;telecommunication network routing;virtual machines;system stability;mathematical analyses;power budget;datacenter;virtual machine scheduling;routing;request admission control;online decision making;optimal control framework;Lyapunov optimization techniques;power consumption minimization;unpredictable user requests;heterogeneous SaaS applications;operating profit;SaaS cloud platforms;software-as-a-service cloud platforms;power-performance tradeoff;Servers;Power demand;Throughput;Admission control;Routing;Control systems;Computational modeling},
doi={10.1109/INFCOM.2013.6566875},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566876,
author={W. Wei and H. T. Gao and F. Xu and Q. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Fast Mencius: Mencius with low commit latency},
year={2013},
volume={},
number={},
pages={881-889},
abstract={Mencius is a protocol for general state machine replication that tolerates crash failures. It has high performance in wide-area networks. However, the commit latency of Mencius is limited by the slowest replica. This paper presents Fast Mencius, a crash fault-tolerant state machine replication protocol, which enhances Mencius with Active Revoke and Multi-instance Propose. Active Revoke allows the non-slow replicas to proceed without being delayed by the slowest replica, while Multi-instance Propose enables the slow replicas to have their proposals chosen by the replicated state machine. Our evaluation shows that in presence of slow replicas, Fast Mencius's commit latency is significantly lower than that of Mencius, and it also achieves high throughput.},
keywords={fault tolerant computing;finite state machines;protocols;wide area networks;fast Mencius;low-commit latency;crash failures;crash fault-tolerant state machine replication protocol;active revoke;multiinstance propose;slowest replica;Delays;Protocols;Computer crashes;Optimization;Detectors;Throughput;Proposals},
doi={10.1109/INFCOM.2013.6566876},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566877,
author={W. Luo and Y. Qiao and S. Chen},
booktitle={2013 Proceedings IEEE INFOCOM},
title={An efficient protocol for RFID multigroup threshold-based classification},
year={2013},
volume={},
number={},
pages={890-898},
abstract={RFID technology has many applications such as object tracking, automatic inventory control, and supply chain management. They can be used to identify individual objects or count the population of each type of objects in a deployment area, no matter whether the objects are passports, retail products, books or even humans. Most existing work adopts a “flat” RFID system model and performs functions of collecting tag IDs, estimating the number of tags, or detecting the missing tags. However, in practice, tags are often attached to objects of different groups, which may represent a different product type in a warehouse, a different book category in a library, etc. An interesting problem, called multigroup threshold-based classification, is to determine whether the number of objects in each group is above or below a prescribed threshold value. Solving this problem is important for inventory tracking applications. If the number of groups is very large, it will be inefficient to measure the groups one at a time. The best existing solution for multigroup threshold-based classification is based on generic group testing, whose design is however geared towards detecting a small number of populous groups. Its performance degrades quickly when the number of groups above the threshold become large. In this paper, we propose a new classification protocol based on logical bitmaps. It achieves high efficiency by measuring all groups in a mixed fashion. In the meantime, we show that the new method is able to perform threshold-based classification with an accuracy that can be pre-set to any desirable level, allowing tradeoff between time efficiency and accuracy.},
keywords={protocols;radiofrequency identification;efficient protocol;RFID multigroup threshold-based classification;RFID technology;object tracking;automatic inventory control;supply chain management;retail products;RFID system model;logical bitmaps;classification protocol;generic group testing;inventory tracking applications;multigroup threshold-based classification;tag ID;Protocols;Radiofrequency identification;Sociology;Accuracy;Maximum likelihood estimation},
doi={10.1109/INFCOM.2013.6566877},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566878,
author={M. Chen and W. Luo and Z. Mo and S. Chen and Y. Fang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={An efficient tag search protocol in large-scale RFID systems},
year={2013},
volume={},
number={},
pages={899-907},
abstract={Radio frequency identification (RFID) technology has many applications in inventory management, supply chain, product tracking, transportation and logistics. One research issue of practical importance is to search for a particular group of tags in a large-scale RFID system. Time efficiency is a core factor that must be taken into consideration when designing a tag search protocol to ensure scalability. In this paper, we design a new technique called filtering vector, which can significantly reduce transmission overhead during search process, thereby shortening search time. Based on this technique, we propose an iterative tag search protocol. In each round, we filter out some tags and eventually terminate the search process when the search result meets the accuracy requirement. The simulation results demonstrate that our protocol performs much better than the best existing ones.},
keywords={protocols;radiofrequency identification;telecommunication network reliability;tag search protocol;large-scale RFID system;radio frequency identification technology;inventory management;supply chain;product tracking;transportation;logistics;scalability;filtering vector;transmission overhead reduction;search process;Vectors;Protocols;Radiofrequency identification;Search problems;Cats;Arrays;Servers},
doi={10.1109/INFCOM.2013.6566878},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566879,
author={Y. Zheng and M. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={ZOE: Fast cardinality estimation for large-scale RFID systems},
year={2013},
volume={},
number={},
pages={908-916},
abstract={Estimating the RFID cardinality with accuracy guarantee is an important task in large-scale RFID systems. This paper proposes a fast RFID cardinality estimation scheme. The proposed Zero-One Estimator (ZOE) protocol rapidly converges to optimal parameter settings and achieves high estimation efficiency. ZOE significantly improves the cardinality estimation efficiency, achieving 3x performance gain compared with existing protocols. Meanwhile, ZOE guarantees arbitrary accuracy requirement without imposing computation and memory overhead at RFID tags. Due to the simplicity and robustness, the ZOE protocol provides reliable cardinality estimation even over noisy channel. We implement a prototype system using the USRP software defined radio and Intel WISP RFID tags. We also evaluate the performance of ZOE with extensive simulations. The evaluation of ZOE shows encouraging results in terms of estimation accuracy, time efficiency, as well as robustness.},
keywords={protocols;radiofrequency identification;software radio;fast cardinality estimation;large-scale RFID systems;zero-one estimator protocol;USRP software defined radio;Intel WISP RFID tags;Estimation;Protocols;Accuracy;RFID tags;Channel estimation;Probabilistic logic},
doi={10.1109/INFCOM.2013.6566879},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566880,
author={Y. Zheng and M. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={P-MTI: Physical-layer Missing Tag Identification via compressive sensing},
year={2013},
volume={},
number={},
pages={917-925},
abstract={RFID systems are emerging platforms that support a variety of pervasive applications. The problem of identifying missing tag in RFID systems has attracted wide attention due to its practical importance. This paper presents P-MTI: a Physical-layer Missing Tag Identification scheme which effectively makes use of the lower layer information and dramatically improves operational efficiency. Unlike conventional approaches, P-MTI looks into the aggregated responses instead of focusing on individual tag responses and extracts useful information from physical layer symbols. P-MTI leverages the sparsity of missing tag events and reconstructs tag responses through compressive sensing. We implement P-MTI and prototype the system based on the USRP software defined radio and Intel WISP platform which demonstrates the efficacy. We also evaluate the performance of P-MTI with extensive simulations and compare with previous approaches under various scenarios. The evaluation shows promising results of P-MTI in terms of identification accuracy, time efficiency, as well as robustness over noisy channels.},
keywords={compressed sensing;radiofrequency identification;software radio;P-MTI;physical-layer missing tag identification;compressive sensing;RFID systems;physical layer symbol;tag response;USRP software defined radio;Intel WISP platform;identification accuracy;time efficiency;noisy channel;Physical layer;Monitoring;Noise;Compressed sensing;Vectors;RFID tags},
doi={10.1109/INFCOM.2013.6566880},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566881,
author={G. Kuperman and E. Modiano},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Providing protection in multi-hop wireless networks},
year={2013},
volume={},
number={},
pages={926-934},
abstract={We consider the problem of providing protection against failures in wireless networks subject to interference constraints. Typically, protection in wired networks is provided through the provisioning of backup paths. This approach has not been previously considered in the wireless setting due to the prohibitive cost of backup capacity. However, we show that in the presence of interference, protection can often be provided with no loss in throughput. This is due to the fact that after a failure, links that previously interfered with the failed link can be activated, thus leading to a “recapturing” of some of the lost capacity. We provide both an ILP formulation for the optimal solution, as well as algorithms that perform close to optimal. More importantly, we show that providing protection in a wireless network uses as much as 72% less protection resources as compared to similar protection schemes designed for wired networks, and that in many cases, no additional resources for protection are needed.},
keywords={protection;telecommunication security;wireless mesh networks;backup capacity;ILP formulation;optimal solution;protection resources;wired networks;multihop wireless mesh networks;Interference constraints;Wireless networks;Schedules;Throughput;Routing},
doi={10.1109/INFCOM.2013.6566881},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566882,
author={J. Tapolcai and G. Rétvári},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Router virtualization for improving IP-level resilience},
year={2013},
volume={},
number={},
pages={935-943},
abstract={IP-level failure protection based on the IP Fast ReRoute/Loop-Free Alternates (LFA) specification has become industrial requirement recently. The success of LFA lies in its inherent simplicity, but this comes at the expense of letting certain failure scenarios go unprotected. Realizing full failure coverage with LFA so far has only been possible through completely reengineering the network around LFA-compliant design patterns. In this paper, we show that attaining high LFA coverage is possible without any alteration to the installed IP infrastructure, by introducing a carefully designed virtual overlay on top of the physical network that provides LFAs to otherwise unprotected routers. We study the problem of how to provision the overlay to maximize LFA coverage, we find that this problem is NPcomplete, and we give Integer Linear Programs to solve it. We also propose novel methods to work-around the limitations of current LFA implementations concerning Shared Risk Link Groups (SRLGs), which might be of independent interest. Our numerical evaluations suggest that router virtualization is an efficient tool for improving LFA-based resilience in real topologies.},
keywords={computer network reliability;integer programming;IP networks;linear programming;numerical analysis;overlay networks;telecommunication network routing;router virtualization;IP-level resilience improvement;IP-level failure protection;fast reroute-loop-free alternate specification;LFA specification;LFA-compliant design pattern;IP infrastructure;virtual overlay design;integer linear program;shared risk link group;SRLG;numerical analysis;unprotected routers;Topology;IP networks;Routing protocols;Virtualization;Network topology;Optimization;Substrates;IP Fast ReRoute;Loop-Free Alternates;router virtualization},
doi={10.1109/INFCOM.2013.6566882},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566883,
author={J. Yallouz and A. Orda},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Tunable QoS-aware network survivability},
year={2013},
volume={},
number={},
pages={944-952},
abstract={Coping with network failures has been recognized as an issue of major importance in terms of social security, stability and prosperity. It has become clear that current networking standards fall short of coping with the complex challenge of surviving failures. The need to address this challenge has become a focal point of networking research. In particular, the concept of tunable survivability offers major performance improvements over traditional approaches. Indeed, while the traditional approach is to provide full (100%) protection against network failures through disjoint paths, it was realized that this requirement is too restrictive in practice. Tunable survivability provides a quantitative measure for specifying the desired level (0%-100%) of survivability and offers flexibility in the choice of the routing paths. Previous work focused on the simpler class of “bottleneck” criteria, such as bandwidth. In this study, we focus on the important and much more complex class of additive criteria, such as delay and cost. First, we establish some (in part, counter-intuitive) properties of the optimal solution. Then, we establish efficient algorithmic schemes for optimizing the level of survivability under additive end-to-end QoS bounds. Subsequently, through extensive simulations, we show that, at the price of negligible reduction in the level of survivability, a major improvement (up to a factor of 2) is obtained in terms of end-to-end QoS performance. Finally, we exploit the above findings in the context of a network design problem, in which we need to best invest a given “budget” for improving the performance of the network links.},
keywords={computer network management;computer network reliability;quality of service;telecommunication network routing;tunable network survivability;QoS aware network survivability;network failure;tunable survivability;quantitative measure;routing path;bottleneck criteria;network delay;additive end-to-end QoS bound;network design problem;network link budget;network link cost;Quality of service;Additives;Delays;Maximum likelihood detection;Standards;Optimization},
doi={10.1109/INFCOM.2013.6566883},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566884,
author={S. Li and Z. Zheng and E. Ekici and N. B. Shroff},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Maximizing social welfare in operator-based Cognitive Radio Networks under spectrum uncertainty and sensing inaccuracy},
year={2013},
volume={},
number={},
pages={953-961},
abstract={In Cognitive Radio Networks (CRNs), secondary users (SUs) are allowed to opportunistically access the unused/under-utilized channels of primary users (PUs). To utilize spectrum resources efficiently, an auction scheme is often applied where an operator serves as an auctioneer and accepts spectrum requests from SUs. Most existing works on spectrum auctions assume that the operator has perfect knowledge of PU activities. In practice, however, it is more likely that the operator only has statistical information of the PU traffic when it is trading a spectrum hole, and it is acquiring more accurate information in real time. In this paper, we distinguish PU channels that are under the control of the operator, where accurate channel states are revealed in real-time, and channels that the operator acquires from PUs out of its control, where a sense-before-use paradigm has to be followed. Considering both spectrum uncertainty and sensing inaccuracy, we study the social welfare maximization problem for serving SUs with various levels of delay tolerance. We first model the problem as a finite horizon Markov decision process when the operator knows all spectrum requests in advance, and propose an optimal dynamic programming based algorithm. We then investigate the case when spectrum requests are submitted online, and propose a greedy algorithm that is 1/2-competitive for homogeneous channels and is comparable to the offline algorithm for more general settings. We further extend the online algorithm to an online auction scheme, which ensures incentive compatibility for the SUs and also provides a way for trading off social welfare and revenue.},
keywords={cognitive radio;delay tolerant networks;dynamic programming;greedy algorithms;Markov processes;radio networks;radio spectrum management;telecommunication traffic;online auction scheme;optimal dynamic programming based algorithm;finite horizon Markov decision process;delay tolerance;social welfare maximization problem;sense-before-use paradigm;PU traffic;SU;auction scheme;spectrum resource;primary user;secondary user;CRN;sensing inaccuracy;spectrum uncertainty;operator-based cognitive radio network;Decision support systems},
doi={10.1109/INFCOM.2013.6566884},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566885,
author={C. Jiang and Y. Chen and Y. Yang and C. Wang and K. J. R. Liu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Dynamic Chinese Restaurant Game in cognitive radio networks},
year={2013},
volume={},
number={},
pages={962-970},
abstract={In a cognitive radio network with mobility, secondary users can arrive at and leave the primary users' licensed networks at any time. After arrival, secondary users are confronted with channel access under the uncertain primary channel state. On one hand, they have to estimate the channel state, i.e., the primary users' activities, through performing spectrum sensing and learning from other secondary users' sensing results. On the other hand, they need to predict subsequent secondary users' access decisions to avoid competition when accessing the ”spectrum hole”. In this paper, we propose a Dynamic Chinese Restaurant Game to study such a learning and decision making problem in cognitive radio networks. We introduce a Bayesian learning based method for secondary users to learn the channel state and propose a Multi-dimensional Markov Decision Process based approach for secondary users to make optimal channel access decisions. Finally, we conduct simulations to verify the effectiveness and efficiency of the proposed scheme.},
keywords={cognitive radio;game theory;dynamic Chinese restaurant game;cognitive radio networks;secondary users;primary users;channel access;spectrum sensing;decision making;Bayesian learning based method;multi-dimensional Markov decision process;Sensors;Games;Bayes methods;Cognitive radio;Channel estimation;Markov processes;Educational institutions;Chinese Restaurant Game;Bayesian Learning;Markov Decision Process;Cognitive Radio;Game Theory},
doi={10.1109/INFCOM.2013.6566885},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566886,
author={W. Li and X. Cheng and T. Jing and X. Xing},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Cooperative multi-hop relaying via network formation games in cognitive radio networks},
year={2013},
volume={},
number={},
pages={971-979},
abstract={The cooperation between the primary and the secondary users has attracted a lot of attention in cognitive radio networks. However, most existing research mainly focuses on the single-hop relay selection for a primary transmitter-receiver pair, which might not be able to fully explore the benefit brought by cooperative transmissions. In this paper, we study the problem of multi-hop relay selection by applying the network formation game. In order to mitigate interference and reduce delay, we propose a cooperation framework FTCO by considering the spectrum sharing in both the time and the frequency domain. Then we formulate the multi-hop relay selection problem as a network formation game, in which the multi-hop relay path is computed via performing the primary player's strategies in the form of link operations. We also devise a distributed dynamic algorithm PRADA to obtain a global-path stable network. Finally, we conduct extensive numerical experiments and our results indicate that cooperative multi-hop relaying can significantly benefit both the primary and the secondary network, and that the network graph resulted from our PRADA algorithm can achieve the global-path stability.},
keywords={cognitive radio;cooperative communication;game theory;interference suppression;network theory (graphs);radio receivers;radio spectrum management;radio transmitters;relay networks (telecommunication);time-frequency analysis;cooperative multihop relay selection;network formation game;cognitive radio network;primary user;secondary user;single hop relay selection;primary transmitter-receiver pair;cooperative transmission;interference mitigation;cooperation framework;FTCO;spectrum sharing;frequency domain analysis;time domain analysis;primary player strategy;distributed dynamic algorithm;global path stable network;primary network;network graph;PRADA algorithm;Relays;Games;Heuristic algorithms;Spread spectrum communication;Bit rate;Delays;Receivers;Cognitive radio networks;cooperative multi-hop relaying;network formation game;global-path stable network},
doi={10.1109/INFCOM.2013.6566886},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566887,
author={S. Bu and F. R. Yu and Y. Qian},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Energy-efficient cognitive heterogeneous networks powered by the smart grid},
year={2013},
volume={},
number={},
pages={980-988},
abstract={Rapidly rising energy costs and increasingly rigid environmental standards have led to an emerging trend of addressing the “energy efficiency” aspect of mobile cellular networks. Cognitive heterogeneous mobile networks are considered as important techniques to improve the energy efficiency. However, most existing works do not consider the power grid, which provides electricity to cellular networks. Currently, the power grid is experiencing a significant shift from the traditional grid to the smart grid. In the smart grid environment, only considering energy efficiency may not be sufficient, since the dynamics of the smart grid will have significant impacts on mobile networks. In this paper, we study cognitive heterogeneous mobile networks in the smart grid environment. Unlike most existing studies on cognitive networks, where only the radio spectrum is sensed, our cognitive networks sense not only the radio spectrum environment but also the smart grid environment, based on which power allocation and interference management are performed. We formulate the problems of electricity price decision, energy-efficient power allocation and interference management as a three-level Stackelberg game. A homogeneous Bertrand game with asymmetric costs is used to model price decisions made by the electricity retailers. A backward induction method is used to analyze the proposed Stackelberg game. Simulation results show that our proposed scheme can significantly reduce operational expenditure and CO2emissions in cognitive heterogeneous mobile networks.},
keywords={cellular radio;cognitive radio;energy conservation;smart power grids;telecommunication power management;energy efficient cognitive heterogeneous network;environmental standards;mobile cellular networks;cognitive heterogeneous mobile networks;energy efficiency;smart grid environment;radio spectrum environment;interference management;electricity price decision;energy efficient power allocation;Stackelberg game;homogeneous Bertrand game;electricity retailer;backward induction method;Electricity;Femtocells;Smart grids;Interference;Games;Mobile computing;Mobile communication;Energy efficiency;heterogeneous networks;smart grid},
doi={10.1109/INFCOM.2013.6566887},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566888,
author={D. De Vleeschauwer and H. Viswanathan and A. Beck and S. Benno and G. Li and R. Miller},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Optimization of HTTP adaptive streaming over mobile cellular networks},
year={2013},
volume={},
number={},
pages={898-997},
abstract={Video streaming, in particular, hypertext transfer protocol based (HTTP) adaptive streaming (HAS) of video, is expected to be a dominant application over mobile networks in the near future. The observation that the base station can alter the video quality requested by a HAS client to its server by controlling the over-the-air throughput from the base station to the client implies that the base station can jointly maximize aggregate video quality of all the HAS flows and throughput of data flows that it serves. We formulate a utility maximization problem that separately takes into account different utility functions for video and data flows and show that the utility maximization can be achieved through an algorithm, we term adaptive guaranteed bit rate (AGBR), wherein target bit rates are calculated for each HAS flow and passed on to an underlying minimum rate proportional fair scheduler that schedules resources across all the flows. This approach has the advantage that it retains the existing scheduling function in the base station with a separate function to compute the target bit rates for the video flows allowing them to only change slowly over time in order to avoid frequent video quality changes. Through analytical modeling and simulations we show that the proposed algorithm can achieve required fairness among the video flows as well as automatically and fairly adapt video quality with increasing congestion thereby preventing data flow throughput starvation.},
keywords={cellular radio;transport protocols;video streaming;analytical simulations;analytical modeling;scheduling function;minimum rate proportional fair scheduler;HAS flow;AGBR;adaptive guaranteed bit rate;utility maximization problem;video quality;HAS client;mobile networks;hypertext transfer protocol based adaptive streaming;mobile cellular networks;HTTP adaptive streaming;Streaming media;Throughput;Bit rate;Base stations;Optimization;Video recording;Quality assessment;scheduling;resource allocation;mobile networks;3GPP;LTE;video;HAS;GBR},
doi={10.1109/INFCOM.2013.6566888},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566889,
author={E. Aryafar and A. Keshavarz-Haddad and M. Wang and M. Chiang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={RAT selection games in HetNets},
year={2013},
volume={},
number={},
pages={998-1006},
abstract={We study the dynamics of network selection in heterogeneous wireless networks (HetNets). Users in such networks selfishly select the best radio access technology (RAT) with the objective of maximizing their own throughputs. We propose two general classes of throughput models that capture the basic properties of random access (e.g., Wi-Fi) and scheduled access (e.g., WiMAX, LTE, 3G) networks. Next, we formulate the problem as a non-cooperative game, and study its convergence, efficiency, and practicality. Our results reveal that: (i) Singleclass RAT selection games converge to Nash equilibria, while an improvement path can be repeated infinitely with a mixture of classes. We next introduce a hysteresis mechanism in RAT selection games, and prove that with appropriate hysteresis policies, convergence can still be guaranteed; (ii) We analyze the Pareto-efficiency of the Nash equilibria of these games. We derive the conditions under which Nash equilibria are Paretooptimal, and we quantify the distance of Nash equilibria with respect to the set of Pareto-dominant points when the conditions are not satisfied; (iii) Finally, with extensive measurement-driven simulations we show that RAT selection games converge to Nash equilibria in a small number of steps, and hence are amenable to practical implementation. We also investigate the impact of noisy throughput measurements, and propose solutions to handle them.},
keywords={convergence;game theory;Pareto optimisation;radio access networks;telecommunication congestion control;telecommunication network management;network selection;heterogeneous wireless networks;HetNets;radio access technology;throughput models;random access networks;scheduled access networks;Nash equilibria;hysteresis mechanism;RAT selection games;Pareto efficiency;Pareto-dominant points;Throughput;Games;Switches;Convergence;Hysteresis;IEEE 802.11 Standards;Rats},
doi={10.1109/INFCOM.2013.6566889},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566890,
author={A. Sridharan and J. Bolot},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Location patterns of mobile users: A large-scale tudy},
year={2013},
volume={},
number={},
pages={1007-1015},
abstract={The opportunities to understand human-mobility have increased significantly of late with the rapid adoption of wireless devices that report locations frequently. In this work<sup>1</sup>, we utilize one such rich data-set comprising of nationwide call data records from several million users to analyze and understand their location patterns. We define a location pattern as the set of locations visited by a user, which roughly speaking, can be considered to be the footprint of the user. Such an analysis is useful since it allows insight into aspects such as the range covered by a user, general direction and major routes of travel, characterization of geographic areas etc.,. These in turn are useful inputs for network planning, traffic planning and mobility models. We propose a systematic methodology that utilizes geometric structures like the Minimum Area Rectangle, line segmentation and clustering techniques to extract meaningful information for location patterns and apply it to our large data-set. Based on this we report on aspects such as the size and orientation of footprints, length of major routes as well as characterize and compare locales based on movement patterns. Finally, we identify some key features of location patterns that can be modeled very well with a single statistical distribution, the Double Pareto LogNormal (DPLN) distribution regardless of locale.},
keywords={mobile radio;Pareto distribution;statistical distributions;telecommunication network planning;telecommunication traffic;mobile users;human-mobility;wireless devices;location patterns;network planning;traffic planning;mobility models;minimum area rectangle;line segmentation;clustering techniques;information extraction;statistical distribution;double Pareto lognormal distribution;Poles and towers;Clustering algorithms;Cities and towns;Shape;Approximation methods;Trajectory;Planning},
doi={10.1109/INFCOM.2013.6566890},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566891,
author={Y. Sun and C. E. Koksal and S. Lee and N. B. Shroff},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Network control without CSI using rateless codes for downlink cellular systems},
year={2013},
volume={},
number={},
pages={1016-1024},
abstract={Wireless network scheduling and control techniques (e.g., opportunistic scheduling) rely heavily on access to Channel State Information (CSI). However, obtaining this information is costly in terms of bandwidth, time, and power, and could result in large overhead. Therefore, a critical question is how to optimally manage network resources in the absence of such information. To that end, we develop a cross-layer solution for downlink cellular systems with imperfect (and possibly no) CSI at the transmitter. We use rateless codes to resolve channel uncertainty. To keep the decoding complexity low, we explicitly incorporate time-average block-size constraints, and aim to maximize the system utility. The block-size of a rateless code is determined by both the network control decisions and the unknown CSI of many time slots. Therefore, unlike standard utility maximization problems, this problem can be viewed as a constrained partial observed Markov decision problem (CPOMDP), which is known to be hard due to the “curse of dimensionality.” However, by using a modified Lyapunov drift method, we develop a dynamic network control scheme, which yields a total network utility within O(1/Lav) of utility-optimal point achieved by infinite block-size channel codes, where Lavis the enforced value of the time-average block-size of rateless codes. This opens the door of being able to trade complexity/delay for performance gains in the absence of accurate CSI. Our simulation results show that the proposed scheme improves the network throughput by up to 68% over schemes that use fixed-rate codes.},
keywords={cellular radio;channel coding;computational complexity;decision theory;Markov processes;optimisation;resource allocation;scheduling;telecommunication control;telecommunication network management;network throughput improvement;fixed-rate codes;performance gains;infinite block-size channel codes;utility-optimal point;total network utility;dynamic network control scheme;Lyapunov drift method;curse-of-dimensionality;CPOMDP;constrained partial observed Markov decision problem;time slots;network control decisions;system utility maximization;time-average block-size constraints;low decoding complexity;channel uncertainty;cross-layer solution;network resource management;CSI;channel state information;opportunistic scheduling;downlink cellular systems;rateless codes;wireless network control technique;wireless network scheduling technique;Receivers;Transmitters;Complexity theory;Mutual information;Maximum likelihood decoding;Downlink},
doi={10.1109/INFCOM.2013.6566891},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566892,
author={K. D. Bowers and A. Juels and R. L. Rivest and E. Shen},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Drifting Keys: Impersonation detection for constrained devices},
year={2013},
volume={},
number={},
pages={1025-1033},
abstract={We introduce Drifting Keys (DKs), a simple new approach to detecting device impersonation. DKs enable detection of complete compromise by an attacker of the device and its secret state, e.g., cryptographic keys. A DK evolves within a device randomly over time. Thus an attacker will create DKs that randomly diverge from those in the original, valid device over time, alerting a trusted verifier to the attack. DKs may be transmitted unidirectionally from a device, eliminating interaction between the device and verifier. Device emissions of DK values can be quite compact - even just a single bit - and DK evolution and emission require minimal computation. Thus DKs are well suited for highly constrained devices, such as sensors and hardware authentication tokens. We offer a formal adversarial model for DKs, and present a simple scheme that we prove essentially optimal (undominated) for a natural class of attack timelines. We explore application of this scheme to one-time passcode authentication tokens. Using the logs of a large enterprise, we experimentally study the effectiveness of DKs in detecting the compromise of such tokens.},
keywords={cryptography;trusted computing;drifting keys;constrained device impersonation detection;complete compromise detection;secret state;cryptographic keys;trusted verifier;device emissions;DK values;DK evolution;formal adversarial model;one-time passcode authentication tokens;Authentication;Forgery;Cryptography;Cloning;Sensors;Synchronization},
doi={10.1109/INFCOM.2013.6566892},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566893,
author={L. Shi and S. Yu and W. Lou and Y. T. Hou},
booktitle={2013 Proceedings IEEE INFOCOM},
title={SybilShield: An agent-aided social network-based Sybil defense among multiple communities},
year={2013},
volume={},
number={},
pages={1034-1042},
abstract={Lacking trusted central authority, distributed systems have received serious security threats from Sybil attack, where an adversary forges identities of more than one node and attempts to control the system. By utilizing the real-world trust relationships between users, social network-based defense schemes have been proposed to mitigate the impact of Sybil attacks. These solutions are mostly built on the assumption that the social network graph can be partitioned into two loosely linked regions - a tightly connected non-Sybil region and a Sybil region. Although such an assumption may hold in certain settings, studies have shown that the real-world social connections tend to divide users into multiple inter-connected small worlds instead of a single uniformly connected large region. Given this fact, the applicability of existing schemes would be greatly undermined for inability to distinguish Sybil users from valid ones in the small non-Sybil regions. This paper addresses this problem and presents SybilShield, the first protocol that defends against Sybil attack utilizing multi-community social network structure in real world. Our scheme leverages the sociological property that the number of cutting edges between a non-Sybil community and a Sybil community, which represent human-established trust relationships, is much smaller than that among non-Sybil communities. With the help of agent nodes, SybilShield greatly reduces false positive rate of non-Sybils among multiple communities, while effectively identifying Sybil nodes. Analytical results prove the superiority of SybilShield. Our experiments on a real-world social network graph with 100,000 nodes also validate the effectiveness of SybilShield.},
keywords={distributed processing;graph theory;multi-agent systems;network theory (graphs);security of data;trusted computing;SybilShield;agent-aided social network-based Sybil defense;trusted central authority;distributed system;serious security threat;adversary;identity forging;system control;Sybil attack impact mitigation;social network graph partitioning;loosely linked region;tightly connected nonSybil region;social connection;multiple interconnected small world;single uniformly connected large region;Sybil users;protocol;multicommunity social network structure;sociological property;human-established trust relationship;agent node;Communities;Social network services;Peer-to-peer computing;Routing protocols;Routing;Image edge detection},
doi={10.1109/INFCOM.2013.6566893},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566894,
author={Z. Ling and J. Luo and K. Wu and X. Fu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Protocol-level hidden server discovery},
year={2013},
volume={},
number={},
pages={1043-1051},
abstract={Tor hidden services are commonly used to provide a TCP based service to users without exposing the hidden server's IP address in order to achieve anonymity and anti-censorship. However, hidden services are currently abused in various ways. Illegal content such as child pornography has been discovered on various Tor hidden servers. In this paper, we propose a protocollevel hidden server discovery approach to locate the Tor hidden server that hosts the illegal website. We investigate the Tor hidden server protocol and develop a hidden server discovery system, which consists of a Tor client, a Tor rendezvous point, and several Tor entry onion routers. We manipulate Tor cells, the basic transmission unit over Tor, at the Tor rendezvous point to generate a protocol-level feature at the entry onion routers. Once our controlled entry onion routers detect such a feature, we can confirm the IP address of the hidden server. We conduct extensive analysis and experiments to demonstrate the feasibility and effectiveness of our approach.},
keywords={client-server systems;computer network security;IP networks;telecommunication network routing;transport protocols;protocol-level hidden server discovery;TCP-based service;IP address;hidden services;illegal content;Tor hidden server localisation;illegal Web site;Tor hidden server protocol;Tor client;Tor rendezvous point;Tor entry onion routers;Tor cells;transmission unit;anonymous communication;Servers;Relays;Routing protocols;IP networks;Correlation;Timing;Anonymous Communication;Tor;Hidden Service},
doi={10.1109/INFCOM.2013.6566894},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566895,
author={L. Chen and H. W. Lim and G. Yang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Cross-domain password-based authenticated key exchange revisited},
year={2013},
volume={},
number={},
pages={1052-1060},
abstract={We revisit the problem of cross-domain secure communication between two users belonging to different security domains within an open and distributed environment. Existing approaches presuppose that either the users are in possession of public key certificates issued by a trusted certificate authority (CA), or the associated domain authentication servers share a long-term secret key. In this paper, we propose a four-party password-based authenticated key exchange (4PAKE) protocol that takes a different approach from previous work. The users are not required to have public key certificates, but they simply reuse their login passwords they share with their respective domain authentication servers. On the other hand, the authentication servers, assumed to be part of a standard PKI, act as ephemeral CAs that “certify” some key materials that the users can subsequently exchange and agree on a session key. Moreover, we adopt a compositional approach. That is, by treating any secure two-party password-based key exchange protocol and two-party asymmetric-key based key exchange protocol as black boxes, we combine them to obtain a generic and provably secure 4PAKE protocol.},
keywords={cryptographic protocols;public key cryptography;telecommunication security;cross-domain password-based authenticated key exchange;cross-domain secure communication;public key certificates;trusted certificate;domain authentication servers;long-term secret key;four-party password-based authenticated key exchange protocol;two-party password-based key exchange protocol;two-party asymmetric-key based key exchange protocol;Protocols;Servers;Public key;Authentication;Electronic mail;Materials;Password-based protocol;key exchange;cross-domain;client-to-client},
doi={10.1109/INFCOM.2013.6566895},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566896,
author={C. Joe-Wong and S. Sen and S. Ha},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Offering supplementary wireless technologies: Adoption behavior and offloading benefits},
year={2013},
volume={},
number={},
pages={1061-1069},
abstract={To alleviate the congestion caused by rapid growth in demand for mobile data, ISPs have begun encouraging users to offload some of their traffic onto a supplementary, better quality network technology, e.g., offloading from 3G or 4G to WiFi and femtocells. With the growing popularity of such offerings, a deeper understanding of the underlying economic principles and their impact on technology adoption is necessary. To this end, we develop a model for user adoption of a base wireless technology and a bundle of the base plus a supplementary technology. In our model, individual users make their adoption decisions based on several factors, including the technologies' intrinsic qualities, throughput degradation due to congestion externalities from other subscribers, and the flat access rates that an ISP charges. We study the adoption dynamics and show that they converge to a unique equilibrium for a given set of exogenously determined system parameters. In particular, we characterize the occurrence of interesting adoption behaviors, including a possible decrease in the adoption of the supplementary technology as its coverage increases. Similar behaviors occur at an ISP's profit-maximizing prices and the optimal coverage area for the supplementary technology. To account for the potential benefits from offloading in practice, we collect 3G and WiFi usage and location data from twenty mobile users. We then use this data to numerically investigate the profit-maximizing adoption levels when an ISP accounts for its cost of deploying the supplemental technology and savings from offloading traffic onto this technology.},
keywords={3G mobile communication;4G mobile communication;femtocellular radio;Internet;numerical analysis;pricing;telecommunication traffic;wireless LAN;supplementary wireless technology;mobile data;Internet service provider;quality network technology;4G mobile communication;3G mobile communication;WiFi;femtocellular radio;technology adoption behavior;user adoption;adoption dynamics;ISP profit-maximizing prices;profit-maximizing adoption levels;flat access rates;numerical analysis;Throughput;Femtocells;IEEE 802.11 Standards;Degradation;Economics;Cost accounting;Wireless communication},
doi={10.1109/INFCOM.2013.6566896},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566897,
author={L. Duan and J. Huang and J. Walrand},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Economic analysis of 4G network upgrade},
year={2013},
volume={},
number={},
pages={1070-1078},
abstract={As the successor to the 3G standard, 4G provides much higher data rates to address cellular users' ever-increasing demands for high-speed multimedia communications. This paper analyzes the cellular operators' timing of network upgrades and models that users can switch operators and services. Being the first to upgrade 3G to 4G service, an operator increases his market share but takes more risk or upgrade cost because 4G technology matures over time. This paper first studies a 4G monopoly market with one dominant operator and some small operators, where the monopolist decides his upgrade time by trading off increased market share and upgrade cost. The paper also considers a 4G competition market and develops a game theoretic model for studying operators' interactions. The analysis shows that operators select different upgrade times to avoid severe competition. One operator takes the lead to upgrade, using the benefit of a larger market share to compensate for the larger cost of an early upgrade. This result matches well with many industry observations of asymmetric 4G upgrades. The paper further shows that the availability of 4G upgrade may decrease both operators' profits due to increased competition. Perhaps surprisingly, the profits can increase with the upgrade cost.},
keywords={4G mobile communication;economics;marketing;multimedia communication;economic analysis;4G network upgrade;multimedia communication;cellular operator;4G monopoly market;4G competition market;operator interaction;market share;upgrade cost;Switches;Biological system modeling;Monopoly;Quality of service;Timing;Games;Industries},
doi={10.1109/INFCOM.2013.6566897},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566898,
author={V. Pacifici and G. Dán},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Content-peering dynamics of autonomous caches in a content-centric network},
year={2013},
volume={},
number={},
pages={1079-1087},
abstract={A future content-centric Internet would likely consist of autonomous systems (ASes) just like today's Internet. It would thus be a network of interacting cache networks, each of them optimized for local performance. To understand the influence of interactions between autonomous cache networks, in this paper we consider ASes that maintain peering agreements with each other for mutual benefit, and engage in content-level peering to leverage each others' cache contents. We propose a model of the interaction and the coordination between the caches managed by peering ASes. We address whether stable and efficient content-level peering can be implemented without explicit coordination between the neighboring ASes or alternatively, whether the interaction needs to rely on explicit announcements of content reachability in order for the system to be stable. We show that content-level peering leads to stable cache configurations, both with and without coordination. If the ASes do coordinate, then coordination that avoids simultaneous updates by peering ISPs provides faster and more cost efficient convergence to a stable configuration. Furthermore, if the content popularity estimates are inaccurate, content-level peering is likely to lead to cost efficient cache allocations. We validate our analytical results using simulations on the measured peering topology of more than 600 ASes.},
keywords={cache storage;computer network performance evaluation;convergence;Internet;optimisation;reachability analysis;telecommunication network topology;content-centric network;content-centric Internet;autonomous systems;interacting cache networks;autonomous cache networks;peering agreements;peering ASes;content reachability;cache configurations;peering ISP;convergence;content popularity estimation;cost efficient cache allocations;peering topology;autonomous cache content-peering dynamics;cache management;optimisation;Resource management;Internet;Nash equilibrium;Convergence;Routing;Protocols;Numerical models},
doi={10.1109/INFCOM.2013.6566898},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566899,
author={L. Duan and J. Huang and B. Shou},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Optimal pricing for local and global WiFi markets},
year={2013},
volume={},
number={},
pages={1088-1096},
abstract={This paper analyzes two pricing schemes commonly used in WiFi markets: flat-rate pricing and usage-based pricing. The flat-free pricing encourages users to achieve the maximum WiFi usage and targets at users with high valuations in mobile Internet access, whereas the usage-based pricing is flexible to attract more users - even those with low valuations. First, we show that for a local provider, the flat-rate pricing provides more revenue than the usage-based pricing, which is consistent with the common practice in today's local markets. Second, we study how Skype may work with many local WiFi providers to provide a global WiFi service. We formulate the interactions between Skype, local providers, and users as a two-stage dynamic game. In Stage I, Skype bargains with each local provider to determine the global Skype WiFi service price and revenue sharing agreement; in Stage II, local users and travelers decide whether and how to use local or Skype WiFi service. Our analysis discovers two key insights behind Skype's current choice of usage-based pricing for its global WiFi service: to avoid severe competition with local providers and attract travelers to the service. We further show that at the equilibrium, Skype needs to share the majority of his revenue with a local provider to compensate the local provider's revenue loss due to competition. When there are more travelers or fewer local users, the competition between Skype and a local provider becomes less severe, and Skype can give away less revenue and reduce its usage-based price to attract more users.},
keywords={game theory;pricing;wireless LAN;optimal pricing;local WiFi market;global WiFi market;flat-rate pricing;usage-based pricing;mobile Internet access;Skype;two-stage dynamic game;revenue sharing agreement;IEEE 802.11 Standards;Pricing;Cost accounting;Internet;Games;Elasticity;Educational institutions},
doi={10.1109/INFCOM.2013.6566899},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566900,
author={D. Tsilimantos and J. Gorce and E. Altman},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Stochastic analysis of energy savings with sleep mode in OFDMA wireless networks},
year={2013},
volume={},
number={},
pages={1097-1105},
abstract={The issue of energy efficiency (EE) in Orthogonal Frequency-Division Multiple Access (OFDMA) wireless networks is discussed in this paper. Our interest is focused on the promising concept of base station (BS) sleep mode, introduced recently as a key feature in order to dramatically reduce network energy consumption. The proposed technical approach fully exploits the properties of stochastic geometry, where the number of active cells is reduced in a way that the outage probability, or equivalently the signal to interference plus noise (SINR) distribution, remains the same. The optimal EE gains are then specified with the help of a simplified but yet realistic BS power consumption model. Furthermore, the authors extend their initial work by studying a non-singular path loss model in order to verify the validity of the analysis and finally, the impact on the achieved user capacity is investigated. In this context, the significant contribution of this paper is the evaluation of the theoretically optimal energy savings of sleep mode, with respect to the decisive role that the BS power profile plays.},
keywords={energy conservation;frequency division multiple access;geometry;OFDM modulation;power consumption;probability;radiofrequency interference;stochastic processes;telecommunication power management;stochastic analysis;energy efficiency;EE;orthogonal frequency-division multiple access wireless network;OFDMA wireless network;base station;sleep mode;network energy consumption;stochastic geometry;active cells;outage probability;signal to interference plus noise;SINR distribution;BS power consumption model;nonsingular path loss model;user capacity;optimal energy saving;BS power profile;Interference;Power demand;Signal to noise ratio;Mathematical model;Analytical models;Stochastic processes;Optimized production technology},
doi={10.1109/INFCOM.2013.6566900},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566901,
author={R. M. Karthik and A. Chakrapani},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Practical algorithm for power efficient DRX configuration in next generation mobiles},
year={2013},
volume={},
number={},
pages={1106-1114},
abstract={In this paper, we consider wireless communication systems where user equipments (UEs) monitor packet-data traffic characteristics and adopt Discontinuous Reception (DRX) to conserve battery power. With DRX, the receiver circuitry is configured to toggle between on (active) and off (inactive) states for specified durations, depending on the packet arrival process. Arriving at an appropriate DRX configuration remains a challenging research issue, especially when multiple applications generate traffic. Our objective in this work is to provide a practical mechanism for selecting a suitable DRX configuration. First, we derive an analytical expression for the expected maximum time (delay) required for a packet arriving during the offduration to be serviced for any arrival process. Second, we obtain an estimate for the on-duration, T<sub>on</sub>* for which the expected delay is below a certain threshold. Using T<sub>on</sub>*, we compute the active duration, T<sub>active</sub> in each DRX cycle by considering the timers specified in 3<sup>rd</sup> Generation Partnership Project (3GPP) Release 10 and for a given interarrival time distribution between packets. Finally, using our analysis, we propose a pragmatic algorithm and show how to select an appropriate DRX configuration which will lead to high power efficiency with acceptable buffer requirements. Through extensive analysis and simulations both with general arrival processes and real-time traces, we show that our algorithm can lead to significant extension in battery life at the UE.},
keywords={3G mobile communication;next generation networks;telecommunication traffic;power efficient DRX configuration;next generation mobiles;wireless communication systems;user equipments;UE;packet-data traffic characteristics;discontinuous reception;battery power;packet arrival process;active duration;DRX cycle;3rd Generation Partnership Project;3GPP;pragmatic algorithm;general arrival processes;real-time traces;Tin;Equations;Mathematical model;Delays;Batteries;Joints;Monitoring},
doi={10.1109/INFCOM.2013.6566901},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566902,
author={R. Vaze},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Competitive ratio analysis of online algorithms to minimize packet transmission time in energy harvesting communication system},
year={2013},
volume={},
number={},
pages={115-1123},
abstract={The design of online algorithms for minimizing packet transmission time is considered for single-user Gaussian channel and two-user Gaussian multiple access channel (GMAC) powered by natural renewable sources. The most general case of arbitrary energy arrivals is considered where neither the future energy arrival instants or amount, nor their distribution is known. The online algorithm adaptively changes the transmission rate according to the causal energy arrival information, so as to minimize the packet transmission time. For a minimization problem, the utility of an online algorithm is tested by finding its competitive ratio or competitiveness that is the maximum of the ratio of the gain of the online algorithm and the optimal offline algorithm over all input sequences. We derive a lower bound that shows that competitive ratio of any online algorithm is at least 1.38 for single-user Gaussian channel and 1.356 for GMAC. A `lazy' transmission policy that chooses its transmission power to minimize the transmission time assuming that no further energy arrivals are going to occur in future is shown to be strictly two-competitive for both the single-user Gaussian channel and the GMAC.},
keywords={energy harvesting;Gaussian channels;minimisation;multi-access systems;telecommunication power management;transmission power;lazy transmission policy;optimal offline algorithm;competitiveness;minimization problem;causal energy arrival information;transmission rate;natural renewable sources;GMAC;two-user Gaussian multiple access channel;single-user Gaussian channel;packet transmission time minimization;online algorithm design;energy harvesting communication system;competitive ratio analysis;Algorithm design and analysis;Energy harvesting;Optimization;Upper bound;Channel models;Indexes;Communication systems},
doi={10.1109/INFCOM.2013.6566902},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566903,
author={C. Liu and K. Sundaresan and M. Jiang and S. Rangarajan and G. Chang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={The case for re-configurable backhaul in cloud-RAN based small cell networks},
year={2013},
volume={},
number={},
pages={1124-1132},
abstract={Small cells have become an integral component in meeting the increased demand for cellular network capacity. Cloud radio access networks (C-RAN) have been proposed as an effective means to harness the capacity benefits of small cells at reduced capital and operational expenses. With the baseband units (BBUs) separated from the radio access units (RAUs) and moved to the cloud for centralized processing, the backhaul between BBUs and RAUs forms a key component of any C-RAN. In this work, we argue that a one-one mapping of BBUs to RAUs is highly sub-optimal, thereby calling for a functional decoupling of the BBU pool from the RAUs. Further, the backhaul architecture must be made re-configurable to allow the mapping between BBUs and RAUs to be flexible and changed dynamically so as to not just optimize RAN performance but also energy consumption in the BBU pool. Towards this end, we design and implement the first OFDMA-based C-RAN test-bed with a reconfigurable backhaul that allows 4 BBUs to connect flexibly with 4 RAUs using radio-over-fiber technology. We demonstrate the feasibility of our system over a 10 km separation between the BBU pool and RAUs. Further, real world experiments with commercial off-the-shelf WiMAX clients reveal the performance benefits of our reconfigurable backhaul in catering effectively to heterogeneous user (static and mobile clients) and traffic profiles, while also delivering energy benefits in the BBU pool.},
keywords={cellular radio;frequency division multiple access;OFDM modulation;radio access networks;radio-over-fibre;WiMax;reconfigurable backhaul;cloud-RAN;small cell network;cellular network capacity;cloud radio access network;baseband unit;radio access unit;RAU;functional decoupling;energy consumption;OFDMA;C-RAN test-bed;radio-over-fiber technology;off-the-shelf WiMAX client;static client;mobile client;Optical switches;Computer architecture;Microprocessors;Optical receivers;WiMAX;Mobile communication;Baseband},
doi={10.1109/INFCOM.2013.6566903},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566904,
author={S. Tang and Q. Huang and X. Li and D. Wu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Smoothing the energy consumption: Peak demand reduction in smart grid},
year={2013},
volume={},
number={},
pages={1133-1141},
abstract={Assume that a set of Demand Response Switch (DRS) devices are deployed in smart meters for autonomous demand side management within one house. The DRS devices are able to sense and control the activity of each appliance. We propose a set of appliance scheduling algorithms to 1) minimize the peak power consumption under a fixed delay requirement, and 2) minimize the delay under a fixed peak demand constraint. For both problems, we first prove that they are NP-Hard. Then we propose a set of approximation algorithms with constant approximation ratios. We conduct extensive simulations using both real-life appliance energy consumption data trace and synthetic data to evaluate the performance of our algorithms. Extensive evaluations verify that the schedules obtained by our methods significantly reduce the peak demand or delay compared with naive greedy algorithm or randomized algorithm.},
keywords={demand side management;energy consumption;optimisation;scheduling;smart power grids;peak demand reduction;demand response switch;DRS devices;demand side management;peak power consumption;delay;NP-hard;energy consumption data trace;peak demand;greedy algorithm;randomized algorithm;Schedules;Delays;Minimization;Approximation algorithms;Energy consumption;Home appliances;Strips},
doi={10.1109/INFCOM.2013.6566904},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566905,
author={Y. Huang and S. Mao and R. M. Nelms},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Adaptive electricity scheduling in microgrids},
year={2013},
volume={},
number={},
pages={1142-1150},
abstract={Microgrid (MG) is a promising component for future smart grid (SG) deployment. The balance of supply and demand of electric energy is one of the most important requirements of MG management. In this paper, we present a novel framework for smart energy management based on the concept of quality-of-service in electricity (QoSE). Specifically, the resident electricity demand is classified into basic usage and quality usage. The basic usage is always guaranteed by the MG, while the quality usage is controlled based on the MG state. The microgrid control center (MGCC) aims to minimize the MG operation cost and maintain the outage probability of quality usage, i.e., QoSE, below a target value, by scheduling electricity among renewable energy resources, energy storage systems, and macrogrid. The problem is formulated as a constrained stochastic programming problem. The Lyapunov optimization technique is then applied to derive an adaptive electricity scheduling algorithm by introducing the QoSE virtual queues and energy storage virtual queues. The proposed algorithm is an online algorithm since it does not require any statistics and future knowledge of the electricity supply, demand and price processes. We derive several "hard" performance bounds for the proposed algorithm, and evaluate its performance with trace-driven simulations. The simulation results demonstrate the efficacy of the proposed electricity scheduling algorithm.},
keywords={adaptive scheduling;distributed power generation;Lyapunov methods;renewable energy sources;smart power grids;stochastic programming;microgrids;smart grid deployment;electric energy;smart energy management;quality-of-service;microgrid control center;outage probability;renewable energy resources;energy storage systems;macrogrid;stochastic programming;Lyapunov optimization technique;adaptive electricity scheduling algorithm;QoSE virtual queues;energy storage virtual queues;Decision support systems;Zinc},
doi={10.1109/INFCOM.2013.6566905},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566906,
author={Y. Gao and Y. Chen and C. Wang and K. J. R. Liu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A contract-based approach for ancillary services in V2G networks: Optimality and learning},
year={2013},
volume={},
number={},
pages={1151-1159},
abstract={With the foreseeable large scale deployment of electric vehicles (EVs) and the development of vehicle-to-grid (V2G) technologies, it is possible to provide ancillary services to the power grid in a cost efficient way, i.e., through the bidirectional power flow of EVs. A key issue in such kind of schemes is how to stimulate a large number of EVs to act coordinately to achieve the service request. This is challenging since EVs are self-interested and generally have different preferences toward charging and discharging based on their own constraints. In this paper, we propose a contract-based mechanism to tackle this challenge. Through the design of an optimal contract, the aggregator can provide incentives for EVs to participate in ancillary services to power grid, match the aggregated energy rate with the service request and maximize its own profits. We prove that under mild conditions, the optimal contract-based mechanism takes a very simple form, i.e., the aggregator only needs to publish an optimal unit price to EVs, which is determined based on the statistical distribution of EVs' preferences. We then consider a more practical scenario where the aggregator has no prior knowledge regarding the statistical distribution and study how should the aggregator learn the optimal unit price from its interactions with EVs. Simulation results are shown to verify the effectiveness of the proposed contract-based mechanism.},
keywords={contracts;electric vehicles;power grids;pricing;statistical distributions;contract-based approach;V2G networks;electric vehicles;EV;vehicle-to-grid technologies;power grid;bidirectional power flow;service request;optimal contract-based mechanism;statistical distribution;optimal unit price;Contracts;Power grids;Batteries;Integrated circuits;Statistical distributions;Optimization;Incentive schemes},
doi={10.1109/INFCOM.2013.6566906},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566907,
author={X. Liu and K. Ren and Y. Yuan and Z. Li and Q. Wang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Optimal budget deployment strategy against power grid interdiction},
year={2013},
volume={},
number={},
pages={1160-1168},
abstract={Power network is one of the most critical infrastructures in a nation and is always a target of attackers. Recently, many schemes are proposed to protect the security of power systems. However, most of existing works did not consider the component attacking cost and ignored the relationship between the budget deployed on the component and its attacking cost. To address this problem, in this paper we introduce the concept of budget-cost function, which describes the dynamic characteristics of component attacking cost, and propose a new model to protect power grid against intentional attacks. In our model, the attackers have limited attacking capacity and aim to maximize the damage of attacks. On the other hand, the defenders aim to find the optimal strategy of the budget deployment to limit the damage to an expected level. We formulate the above problem as a nonlinear optimization problem and solve it by employing the primal-dual interior-point method. To the author's best knowledge, this is the first work which analyzes the optimal budget deployment strategy based on budget-cost function. Simulations on the IEEE 5-bus system demonstrate the correctness and effectiveness of the proposed model and algorithms. The results provide a basis of budget investment for power systems.},
keywords={budgeting;nonlinear programming;power grids;power system economics;power system protection;power system security;optimal budget deployment strategy;power grid interdiction;power network;power system security protection;budget-cost function concept;dynamic component attacking cost characteristics;intentional attacks;limited attacking capacity;attack damage maximization;nonlinear optimization problem;primal-dual interior-point method;IEEE 5-bus system;budget investment;power systems;Optimization;Power systems;Computational modeling;Generators;Vectors;Linear matrix inequalities;Load modeling;attacking cost;budget-cost function;candidate line combination;optimal strategy;primal-dual interior-point method;power system security;redundant line combination},
doi={10.1109/INFCOM.2013.6566907},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566908,
author={X. Gong and J. Zhang and D. Cochran},
booktitle={2013 Proceedings IEEE INFOCOM},
title={When target motion matters: Doppler coverage in radar sensor networks},
year={2013},
volume={},
number={},
pages={1169-1177},
abstract={Radar sensors, which actively transmit radio waves and collect RF energy scattered by objects in the environment, offer a number of advantages over purely passive sensors. An important issue in radar is that the transmitted energy may be scattered by objects that are not of interest as well as objects of interest (e.g., targets). The detection performance of radar systems is affected by such clutter as well as noise. Further, in many applications, clutter can be substantially stronger than the signals of interest. To combat the effect of clutter, a popular method is to take advantage of the Doppler frequency shift (DFS) extracted from the echo signal due to the relative motion of a target with respect to the radar. Unfortunately, a sensor coverage model that only depends on the distance to a target would fail to capture the DFS. In this paper, we set forth the concept of Doppler coverage for a network of spatially distributed radars. Specifically, a target is said to be Doppler-covered if, regardless of its direction of motion, there exists some radar in the network whose signalto-noise ratio (SNR) is sufficiently high and the DFS at that radar is sufficiently large. Based on the Doppler coverage model, we first propose an efficient method to characterize Dopplercovered regions for arbitrarily deployed radars. Then we design an algorithm for deriving the minimum radar density required to achieve Doppler coverage in a region under any polygonal deployment pattern, and further apply it to investigate the regular triangle based deployment.},
keywords={Doppler radar;Doppler shift;radar signal processing;radiowaves;target motion;radar sensor network;radio waves;RF energy;passive sensor;radar system;Doppler frequency shift;echo signal;Doppler sensor coverage model;signal to noise ratio;SNR;radar density;polygonal deployment pattern;triangle based deployment;Doppler effect;Doppler radar;Silicon;Sensors;Radar detection;Clutter;radar sensor network;Doppler effect;deterministic deployment;critical sensor density},
doi={10.1109/INFCOM.2013.6566908},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566909,
author={Y. Shen and D. T. Nguyen and M. T. Thai},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Adaptive approximation algorithms for hole healing in hybrid wireless sensor networks},
year={2013},
volume={},
number={},
pages={1178-1186},
abstract={Region coverage and network connectivity are among the most important problems for the quality of service in wireless sensor networks. Unfortunately, due to the sensor failures and hostile environments, such as active volcanic regions or battle fields, the emergence of coverage holes and disconnections among sensors is unavoidable. One way to handle this problem is to deploy mobile sensors in the network, which is called hybrid sensor networks, so that these mobile sensors can be relocated to heal the holes or maintain the network connectivity. However, because of the low-power of mobile sensors, it is extremely challenging to design a fast and effective movement schedule for mobile sensors to (1) maintain both the region coverage and network connectivity at any time, and (2) minimize the moving energy consumption. In this paper, we develop an adaptive algorithm, AHCH algorithm, to adaptively heal the holes with the guarantee of network connectivity without recomputing from scratch. By comparing AHCH algorithm with the optimal solution at each time-slot, we show its expected adaptive approximation ratio as O(log |M|) with |M| mobile sensors in some special cases. In more general cases, we extend our AHCH algorithm to InAHCH and GenAHCH algorithms, handling insufficient mobile sensors as well as disconnected regions, along with the proof of their corresponding theoretical adaptive approximation ratios. The experimental evaluation shows the effectiveness of our proposed algorithms with respect to both low energy consumption and hole healing latency.},
keywords={approximation theory;mobile communication;quality of service;wireless sensor networks;adaptive approximation algorithms;hole healing;hybrid wireless sensor networks;region coverage;network connectivity;quality of service;sensor failures;coverage holes;mobile sensors;energy consumption;AHCH algorithm;O(log |M|);GenAHCH algorithms;InAHCH algorithms;Mobile communication;Approximation algorithms;Measurement;Schedules;Energy consumption;Approximation methods;Mobile computing;Sensor Coverage;Approximation Algorithms;Hybrid Sensor Networks},
doi={10.1109/INFCOM.2013.6566909},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566910,
author={L. Wu and H. Du and W. Wu and D. Li and J. Lv and W. Lee},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Approximations for Minimum Connected Sensor Cover},
year={2013},
volume={},
number={},
pages={1187-1194},
abstract={Given a requested area, the Minimum Connected Sensor Cover problem is to find a minimum number of sensors such that their communication ranges induce a connected graph and their sensing ranges cover the requested area. Several polynomial-time approximation algorithms have been designed previously in the literature. Their best known performance ratio is O(r ln n) where r is the link radius of the sensor network and n is the number of sensors. In this paper, we will present two polynomial-time approximation algorithms. The first one is a random algorithm, with probability 1 - ε, producing an approximation solution with performance ratio O(log3n log log n), independent from r. The second one is a deterministic approximation with performance ratio O(r), independent from n.},
keywords={approximation theory;computational complexity;deterministic algorithms;graph theory;wireless sensor networks;minimum connected sensor cover;connected graph;polynomial-time approximation algorithms;wireless sensor network;random algorithm;probability 1-ε algorithm;deterministic approximation algorithm;link radius;Sensors;Approximation methods;Steiner trees;Approximation algorithms;Algorithm design and analysis;Measurement;Educational institutions},
doi={10.1109/INFCOM.2013.6566910},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566911,
author={Z. Yu and J. Teng and X. Li and D. Xuan},
booktitle={2013 Proceedings IEEE INFOCOM},
title={On wireless network coverage in bounded areas},
year={2013},
volume={},
number={},
pages={1195-1203},
abstract={In this paper, we study the problem of wireless coverage in bounded areas. Coverage is one of the fundamental requirements of wireless networks. There has been considerable research on optimal coverage of infinitely large areas. However, in the real world, the deployment areas of wireless networks are always geographically bounded. It is a much more challenging and significant problem to find optimal deployment patterns to cover bounded areas. In this paper, we approach this problem starting from the development of tight lower bounds on the number of nodes needed to cover a bounded area. Then we design several deployment patterns for different kinds of convex and concave shapes such as rectangles and L-shapes. These patterns require only few more nodes than the theoretical lower bound, and can achieve efficient coverage. We have also carefully addressed and evaluated practical conditions such as coverage modeling and connectivity regarding our deployment patterns.},
keywords={radio networks;wireless network coverage;bounded areas;optimal deployment patterns;convex shapes;concave shapes;L-shapes;Honeycomb structures;Wireless networks;Wireless sensor networks;Tiles;Shape;Approximation methods},
doi={10.1109/INFCOM.2013.6566911},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566912,
author={L. Huang and J. Walrand},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A Benes packet network},
year={2013},
volume={},
number={},
pages={1204-1212},
abstract={Benes networks are constructed with simple switch modules and have many advantages, including small latency and requiring only an almost linear number of switch modules. As circuit-switches, Benes networks are rearrangeably non-blocking, which implies that they are full-throughput as packet switches, with suitable routing. Routing in Benes networks can be done by time-sharing permutations. However, this approach requires centralized control of the switch modules and statistical knowledge of the traffic arrivals. We propose a backpressure-based routing scheme for Benes networks, combined with end-to-end congestion control. This approach achieves the maximal utility of the network and requires only four queues per module, independently of the size of the network.},
keywords={packet radio networks;packet switching;queueing theory;telecommunication congestion control;telecommunication network routing;telecommunication traffic;Benes packet network;circuit-switches;switch module;packet switches;time-sharing permutation;centralized control;statistical knowledge analysis;traffic arrivals;backpressure-based routing scheme;end-to-end congestion control;queues per module;Servers;Routing;Algorithm design and analysis;Resource management;Scheduling algorithms;Optical switches;Benes Network;Dynamic Control;Stochastic Network Optimization;Queueing},
doi={10.1109/INFCOM.2013.6566912},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566913,
author={Y. Liu and B. Zhang and L. Wang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={FIFA: Fast incremental FIB aggregation},
year={2013},
volume={},
number={},
pages={1-9},
abstract={The fast growth of global routing table size has been causing concerns that the Forwarding Information Base (FIB) will not be able to fit in existing routers' expensive line-card memory, and upgrades will lead to higher cost for network operators and customers. FIB Aggregation, a technique that merges multiple FIB entries into one, is probably the most practical solution since it is a software solution local to a router, and does not require any changes to routing protocols or network operations. While previous work on FIB aggregation mostly focuses on reducing table size, this work focuses on algorithms that can update compressed FIBs quickly and incrementally. Quick update is critical to routers because they have very limited time to process routing updates without impacting packet delivery performance. We have designed three algorithms: FIFA-S for smallest table size, FIFA-T for shortest running time, and FIFA-H for both small tables and short running time, and operators can use the one best suited to their needs. These algorithms significantly improve over existing work in terms of reducing routers' computation overhead and limiting impact on the forwarding plane while maintaining a good compression ratio.},
keywords={computer network performance evaluation;IP networks;routing protocols;FIFA;fast incremental FIB aggregation;global routing table size;forwarding information base;router line-card memory;multiple FIB entry merging;routing protocols;network operations;compressed FIB update;routing update;packet delivery performance;FIFA-S algorithm;FIFA-T algorithm;running time;router computation overhead reduction;forwarding plane impact reduction;compression ratio maintenance;Routing;Routing protocols;Algorithm design and analysis;Binary trees;Merging;Educational institutions;Software},
doi={10.1109/INFCOM.2013.6566913},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566914,
author={L. Luo and G. Xie and K. Salamatian and S. Uhlig and L. Mathy and Y. Xie},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A trie merging approach with incremental updates for virtual routers},
year={2013},
volume={},
number={},
pages={1222-1230},
abstract={Virtual routers are increasingly being studied, as an important building block to enable network virtualization. In a virtual router platform, multiple virtual router instances coexist, each having its own FIB (Forwarding Information Base). In this context, memory scalability and route updates are two major challenges. Existing approaches addressed one of these challenges but not both. In this paper, we present a trie merging approach, which compactly represents multiple FIBs by a merged trie and a table of next-hop-pointer arrays to achieve good memory scalability, while supporting fast incremental updates by avoiding the use of leaf pushing during merging. Experimental results show that storing the merged trie requires limited memory space, e.g., we only need 10MB memory space to store the merged trie for 14 full FIBs from IPv4 core routers, achieving a memory reduction by 87% when compared to the total size of the individual tries. We implement our approach in an SRAM (Static Random Access Memory)-based lookup pipeline. Using our approach, an on-chip SRAM-based lookup pipeline with 5 external stages is sufficient to store the 14 full IPv4 FIBs. Furthermore, our approach can guarantee a minimum update overhead of one write bubble per update, as well as a high lookup throughput of one lookup per clock cycle, which corresponds to a throughput of 251 million lookups per second in the implementation.},
keywords={IP networks;SRAM chips;telecommunication network routing;virtualisation;trie merging approach;incremental updates;network virtualization;virtual router platform;FIB;forwarding information base;memory scalability;route updates;next-hop-pointer arrays;memory scalability;leaf pushing;IPv4 core routers;memory reduction;static random access memory;on-chip SRAM-based lookup pipeline;write bubble;lookup throughput;Arrays;Merging;Memory management;Scalability;IP networks;Routing},
doi={10.1109/INFCOM.2013.6566914},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566915,
author={O. Rottenstreich and M. Radan and Y. Cassuto and I. Keslassy and C. Arad and T. Mizrahi and Y. Revah and A. Hassidim},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Compressing forwarding tables},
year={2013},
volume={},
number={},
pages={1231-1239},
abstract={With the rise of datacenter virtualization, the number of entries in forwarding tables is expected to scale from several thousands to several millions. Unfortunately, such forwarding table sizes can hardly be implemented today in on-chip memory. In this paper, we investigate the compressibility of forwarding tables. We first introduce a novel forwarding table architecture with separate encoding in each column. It is designed to keep supporting fast random accesses and fixed-width memory words. Then, we suggest an encoding whose memory requirement per row entry is guaranteed to be within a small additive constant of the optimum. Next, we analyze the common case of two-column forwarding tables, and show that such tables can be presented as bipartite graphs. We deduce graph-theoretical bounds on the encoding size. We also introduce an algorithm for optimal conditional encoding of the second column given an encoding of the first one. In addition, we explain how our architecture can handle table updates. Last, we evaluate our suggested encoding techniques on synthetic forwarding tables as well as on real-life tables.},
keywords={computer centres;computer networks;data compression;encoding;graph theory;storage management;telecommunication network routing;forwarding table compression;data center virtualization;forwarding table size;on-chip memory;forwarding table architecture;random access;fixed-width memory words;memory requirement;row entry;two-column forwarding table;bipartite graph;graph-theoretical bounds;encoding size;optimal conditional encoding;table update;encoding technique;Encoding;Optimization;Dictionaries;Additives;Servers;Approximation methods;System-on-chip},
doi={10.1109/INFCOM.2013.6566915},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566916,
author={Q. Yan and M. Li and F. Chen and T. Jiang and W. Lou and Y. T. Hou and C. Lu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Non-parametric passive traffic monitoring in cognitive radio networks},
year={2013},
volume={},
number={},
pages={1240-1248},
abstract={Passive monitoring by distributed wireless sniffers has been used to strategically capture the network traffic, as the basis of automatic network diagnosis. However, the traditional monitoring techniques fall short in cognitive radio networks (CRNs) due to the much larger number of channels to be monitored, and the secondary users' channel availability uncertainty imposed by primary user activities. To better serve CRNs, we propose a systematic passive monitoring framework for traffic collection using a limited number of sniffers in WiFi like CRNs. We jointly consider primary user activity and secondary user channel access pattern to optimize the traffic capturing strategy. In particular, we exploit a non-parametric density estimation method to learn and predict secondary users' access pattern in an online fashion, which rapidly adapts to the users' dynamic behaviors and supports accurate estimation of merged access patterns from multiple users. We also design near-optimal monitoring algorithms that maximize two levels of quality-of-monitoring goals respectively, based on the predicted channel access patterns. The simulations and experiments show that our proposed framework outperforms the existing schemes significantly.},
keywords={cognitive radio;computerised monitoring;radio networks;telecommunication traffic;wireless channels;wireless LAN;nonparametric passive traffic monitoring;distributed wireless sniffer;automatic network traffic diagnosis;cognitive radio network;CRN;primary user activity;systematic passive monitoring framework;Wi-Fi;secondary user channel access pattern;traffic capturing strategy;nonparametric density estimation method;near-optimal monitoring algorithm;quality-of-monitoring goal;Monitoring;Channel estimation;Estimation;Sensors;Switches;Inspection;Data models},
doi={10.1109/INFCOM.2013.6566916},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566917,
author={Y. Liu and M. Liu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={To stay or to switch: Multiuser dynamic channel access},
year={2013},
volume={},
number={},
pages={1249-1257},
abstract={In this paper we study opportunistic spectrum access (OSA) policies in a multiuser multichannel random access setting, where users perform channel probing and switching in order to obtain better channel condition or higher instantaneous transmission quality. However, unlikely many prior works in this area, including channel probing and switching policies for a single user to exploit spectral diversity, and probing and access policies for multiple users over a single channel to exploit temporal and multiuser diversity, in this study we consider the collective switching of multiple users over multiple channels. In addition, we consider finite arrivals, i.e., users are not assumed to always have data to send and demand for channel follow a certain arrival process. Under such a scenario, the users' ability to opportunistically exploit temporal diversity (the temporal variation in channel quality over a single channel) and spectral diversity (quality variation across multiple channels at a give time) is greatly affected by the level of congestion in the system. We investigate the optimal decision process in this case, and evaluate the extent to which congestion affects potential gains from opportunistic dynamic channel switching.},
keywords={diversity reception;multi-access systems;multiuser channels;telecommunication switching;opportunistic dynamic channel switching;optimal decision process;channel quality;temporal variation;temporal diversity;finite arrival;multiple use;collective switching;multiuser diversity;spectral diversity;switching policies;channel condition;channel probing;multiuser multichannel random access;opportunistic spectrum access policy;multiuser dynamic channel access;Switches;Sensors;Throughput;Data communication;Delays;Wireless communication;Diversity methods},
doi={10.1109/INFCOM.2013.6566917},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566918,
author={W. Afifi and M. Krunz},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Exploiting self-interference suppression for improved spectrum awareness/efficiency in cognitive radio systems},
year={2013},
volume={},
number={},
pages={1258-1266},
abstract={Inspired by recent developments in full-duplex communications, we propose and study new modes of operation for cognitive radios with the goal of achieving improved primary user (PU) detection and/or secondary user (SU) throughput. Specifically, we consider an opportunistic PU/SU setting in which the SU is equipped with partial/complete self-interference suppression (SIS), enabling it to transmit and receive/sense at the same time. Following a brief sensing period, the SU can operate in either simultaneous transmit-and-sense (TS) mode or simultaneous transmit-and-receive (TR) mode. We analytically study the performance metrics for the two modes, namely the detection and false-alarm probabilities, the PU outage probability, and the SU throughput. From this analysis, we evaluate the sensing-throughput tradeoff for both modes. Our objective is to find the optimal sensing and transmission durations for the SU that maximize its throughput subject to a given outage probability. We also explore the spectrum awareness/efficiency tradeoff that arises from the two modes by determining an efficient adaptive strategy for the SU link. This strategy has a threshold structure, which depends on the PU traffic load. Our study considers both perfect and imperfect sensing as well as perfect/imperfect SIS.},
keywords={cognitive radio;interference suppression;probability;radio spectrum management;telecommunication traffic;self-interference suppression;spectrum efficiency;cognitive radio system;full-duplex communication;primary user detection;secondary user throughput;opportunistic PU-SU setting;simultaneous transmit-and-sense mode;TS mode;simultaneous transmit-and-receive mode;TR mode;performance metrics;false-alarm probability;PU outage probability;SU throughput;sensing-throughput tradeoff;spectrum awareness-efficiency tradeoff;adaptive strategy;PU traffic load;Sensors;Throughput;High definition video;Measurement;Signal to noise ratio;Interference;Receivers},
doi={10.1109/INFCOM.2013.6566918},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566919,
author={X. Sheng and J. Tang and C. Gao and W. Zhang and C. Wang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Leveraging load migration and basestaion consolidation for green communications in virtualized Cognitive Radio Networks},
year={2013},
volume={},
number={},
pages={1267-1275},
abstract={With wireless resource virtualization, multiple Mobile Virtual Network Operators (MVNOs) can be supported over a shared physical wireless network and traffic loads in a Base Station (BS) can be easily migrated to more power-efficient BSs in its neighborhood such that idle BSs can be turned off or put into sleep to save power. In this paper, we propose to leverage load migration and BS consolidation for green communications and consider a power-efficient network planning problem in virtualized Cognitive Radio Networks (CRNs) with the objective of minimizing total power consumption while meeting traffic load demand of each MVNO. First, we present a Mixed Integer Linear Programming (MILP) to provide optimal solutions. Then we present a general optimization framework to guide algorithm design, which solves two subproblems, channel assignment and load allocation, in sequence. For channel assignment, we present a (Δ1)-approximation algorithm (where Δ is the maximum number of BSs a BS can potentially interfere with). For load allocation, we present a polynomial-time optimal algorithm for a special case where BSs are power-proportional as well as two effective heuristic algorithms for the general case. In addition, we present an effective heuristic algorithm that jointly solves the two subproblems. It has been shown by extensive simulation results that the proposed algorithms produce close-to-optimal solutions, and moreover, achieve over 45% power savings compared to a baseline algorithm that does not migrate loads or consolidate BSs.},
keywords={cognitive radio;communication complexity;integer programming;linear programming;radio networks;telecommunication network planning;telecommunication traffic;virtualisation;polynomial-time optimal algorithm;MILP;mixed integer linear programming;power-efficient network planning;traffic loads;physical wireless network;MVNO;mobile virtual network operators;wireless resource virtualization;virtualized cognitive radio networks;green communications;base staion consolidation;leveraging load migration;Resource management;Wireless communication;Virtualization;Wireless sensor networks;Power demand;Approximation algorithms;Optimization;Green wireless communications;virtualization;cognitive radio;basestation consolidation;load migration},
doi={10.1109/INFCOM.2013.6566919},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566920,
author={Y. Ma and T. Nandagopal and K. P. N. Puttaswamy and S. Banerjee},
booktitle={2013 Proceedings IEEE INFOCOM},
title={An ensemble of replication and erasure codes for cloud file systems},
year={2013},
volume={},
number={},
pages={1276-1284},
abstract={Geographically distributed storage is an important method of ensuring high data availability in cloud computing and storage systems. With the increasing demand for moving file systems to the cloud, current methods of providing such enterprise-grade resiliency are very inefficient. For example, replication based methods incur large storage cost though they provide low access latencies. While erasure coded schemes reduce storage cost, they are associated with large access latencies and high bandwidth cost. In this paper, we propose a novel scheme named CAROM, an ensemble of replication and erasure codes, to provide resiliency in cloud file systems with high efficiency. While maintaining the same consistency semantics seen in today's cloud file systems, CAROM provides the benefit of low bandwidth cost, low storage cost, and low access latencies. We perform a large-scale evaluation using real-world file system traces and demonstrate that CAROM outperforms replication based schemes in storage cost by up to 60% and erasure coded schemes in bandwidth cost by up to 43%, while maintaining low access latencies close to those in replication based schemes.},
keywords={cloud computing;costing;error correction codes;redundancy;storage management;access latencies;storage cost;bandwidth cost;cloud file system resiliency;CAROM;cloud storage systems;cloud computing systems;data availability;geographically distributed storage;erasure codes;replication codes;Bandwidth;Encoding;Availability;Cloud computing;Semantics;Reed-Solomon codes;Servers},
doi={10.1109/INFCOM.2013.6566920},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566921,
author={M. V. Barbera and S. Kosta and A. Mei and J. Stefa},
booktitle={2013 Proceedings IEEE INFOCOM},
title={To offload or not to offload? The bandwidth and energy costs of mobile cloud computing},
year={2013},
volume={},
number={},
pages={1285-1293},
abstract={The cloud seems to be an excellent companion of mobile systems, to alleviate battery consumption on smartphones and to backup user's data on-the-fly. Indeed, many recent works focus on frameworks that enable mobile computation offloading to software clones of smartphones on the cloud and on designing cloud-based backup systems for the data stored in our devices. Both mobile computation offloading and data backup involve communication between the real devices and the cloud. This communication does certainly not come for free. It costs in terms of bandwidth (the traffic overhead to communicate with the cloud) and in terms of energy (computation and use of network interfaces on the device). In this work we study the fmobile software/data backupseasibility of both mobile computation offloading and mobile software/data backups in real-life scenarios. In our study we assume an architecture where each real device is associated to a software clone on the cloud. We consider two types of clones: The off-clone, whose purpose is to support computation offloading, and the back-clone, which comes to use when a restore of user's data and apps is needed. We give a precise evaluation of the feasibility and costs of both off-clones and back-clones in terms of bandwidth and energy consumption on the real device. We achieve this through measurements done on a real testbed of 11 Android smartphones and an equal number of software clones running on the Amazon EC2 public cloud. The smartphones have been used as the primary mobile by the participants for the whole experiment duration.},
keywords={cloud computing;energy consumption;mobile computing;smart phones;mobile cloud computing bandwidth;mobile cloud computing energy cost;battery consumption;Android smartphones software clones;mobile computation offloading;cloud-based backup systems;mobile software-data backup;off-clone;back-clone;energy consumption;Amazon EC2 public cloud;Smart phones;Mobile communication;Cloning;IEEE 802.11 Standards;Batteries;Software;Bandwidth},
doi={10.1109/INFCOM.2013.6566921},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566922,
author={L. Pamies-Juarez and A. Datta and F. Oggier},
booktitle={2013 Proceedings IEEE INFOCOM},
title={RapidRAID: Pipelined erasure codes for fast data archival in distributed storage systems},
year={2013},
volume={},
number={},
pages={1294-1302},
abstract={To achieve reliability in distributed storage systems, data has usually been replicated across different nodes. However the increasing volume of data to be stored has motivated the introduction of erasure codes, a storage efficient alternative to replication, particularly suited for archival in data centers, where old datasets (rarely accessed) can be erasure encoded, while replicas are maintained only for the latest data. Many recent works consider the design of new storage-centric erasure codes for improved repairability. In contrast, this paper addresses the migration from replication to encoding: traditionally erasure coding is an atomic operation in that a single node with the whole object encodes and uploads all the encoded pieces. Although large datasets can be concurrently archived by distributing individual object encodings among different nodes, the network and computing capacity of individual nodes constrain the archival process due to such atomicity. We propose a new pipelined coding strategy that distributes the network and computing load of single-object encodings among different nodes, which also speeds up multiple object archival. We further present RapidRAID codes, an explicit family of pipelined erasure codes which provides fast archival without compromising either data reliability or storage overheads. Finally, we provide a real implementation of RapidRAID codes and benchmark its performance using both a cluster of 50 nodes and a set of Amazon EC2 instances. Experiments show that RapidRAID codes reduce a single object's coding time by up to 90%, while when multiple objects are encoded concurrently, the reduction is up to 20%.},
keywords={computer centres;distributed databases;forward error correction;pipeline processing;pipelined erasure codes;fast data archival;distributed storage systems;data centers;storage-centric erasure codes;atomic operation;pipelined coding strategy;single-object encodings;RapidRAID codes;data reliability;storage overheads;Amazon EC2 instances;Encoding;Distributed databases;Redundancy;Fault tolerant systems;Pipelines;archival;migration;erasure codes;distributed storage},
doi={10.1109/INFCOM.2013.6566922},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566923,
author={Y. Hua and B. Xiao and X. Liu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={NEST: Locality-aware approximate query service for cloud computing},
year={2013},
volume={},
number={},
pages={1303-1311},
abstract={Cloud computing applications face the challenges of dealing with a huge volume of data that needs the support of fast approximate queries to enhance system scalability and improve quality of service, especially when users are not aware of exact query inputs. Locality-Sensitive Hashing (LSH) can support the approximate queries that unfortunately suffer from imbalanced load and space inefficiency among distributed data servers, which severely limits the query accuracy and incurs long query latency between users and cloud servers. In this paper, we propose a novel scheme, called NEST, which offers ease-of-use and cost-effective approximate query service for cloud computing. The novelty of NEST is to leverage cuckoo-driven locality-sensitive hashing to find similar items that are further placed closely to obtain load-balancing buckets in hash tables. NEST hence carries out flat and manageable addressing in adjacent buckets, and obtains constant-scale query complexity even in the worst case. The benefits of NEST include the increments of space utilization and fast query response. Theoretical analysis and extensive experiments in a large-scale cloud testbed demonstrate the salient properties of NEST to meet the needs of approximate query service in cloud computing environments.},
keywords={cloud computing;computational complexity;file organisation;quality of service;query processing;resource allocation;locality-aware approximate query service;fast approximate queries;system scalability enhancement;quality of service improvement;LSH;imbalanced load;space inefficiency;distributed data servers;query accuracy;query latency;cloud servers;NEST;cloud computing;cuckoo-driven locality-sensitive hashing;load balancing buckets;hash tables;constant-scale query complexity;space utilization;fast query response;large-scale cloud testbed;Artificial neural networks;Complexity theory;Cloud computing;Educational institutions;Vectors;Standards;Servers},
doi={10.1109/INFCOM.2013.6566923},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566924,
author={R. Laufer and L. Kleinrock},
booktitle={2013 Proceedings IEEE INFOCOM},
title={On the capacity of wireless CSMA/CA multihop networks},
year={2013},
volume={},
number={},
pages={1312-1320},
abstract={Due to a poor understanding of the interactions among transmitters, wireless multihop networks have commonly been stigmatized as unpredictable in nature. Even elementary questions regarding the throughput limitations of these networks cannot be answered in general. In this paper we investigate the behavior of wireless multihop networks using carrier sense multiple access with collision avoidance (CSMA/CA). Our goal is to understand how the transmissions of a particular node affect the medium access, and ultimately the throughput, of other nodes in the network. We introduce a theory which accurately models the behavior of these networks and show that, contrary to popular belief, their performance is easily predictable and can be described by a system of equations. Using the proposed theory, we provide the analytical expressions necessary to fully characterize the capacity region of any wireless CSMA/CA multihop network. We show that this region is nonconvex in general and entirely agnostic to the probability distributions of all network parameters, depending only on their expected values.},
keywords={carrier sense multiple access;radio networks;statistical distributions;wireless CSMA/CA multihop network capacity;transmitters;wireless multihop network behavior modelling;carrier sense multiple access with collision avoidance;probability distributions;Transmitters;Multiaccess communication;Throughput;Wireless communication;Spread spectrum communication;Steady-state;Radiation detectors},
doi={10.1109/INFCOM.2013.6566924},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566925,
author={W. Liu and K. Lu and J. Wang and Y. Qian and L. Huang and J. Liu and D. O. Wu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={On the throughput-delay trade-off in large-scale MANETs with a generalized i.i.d. mobility model},
year={2013},
volume={},
number={},
pages={1321-1329},
abstract={In mobile ad hoc networks (MANETs), it is important to understand the throughput-delay trade-off (TD trade-off) problem in large-scale scenarios. In the literature, the TD tradeoff problem has been studied extensively and many of them are based on the independent and identically distributed (i.i.d.) mobility model, in which each node can randomly move to any place in the network, after every time slot. Although the i.i.d. model has been widely used, it cannot fully represent MANETs in which nodes change positions less frequently. To characterize such MANETs, in this paper, we propose a generalized i.i.d. (g.i.i.d.) mobility model, in which each node moves once after every 1/f (0 <; f ≤ 1) time slots, and remains static between two moves. To investigate the TD trade-off under the g.i.i.d. model, we develop a novel multi-relay multi-hop (MRMH) scheme that exploits the opportunities of multi-hop transmissions when the network is static. Furthermore, to enable the multi-hop transmissions, we construct a new percolation highway system, which has not been used in the TD trade-off analysis for MANETs. Using the proposed MRMH scheme, we develop and prove constructive bounds for throughput and delay in MANETs with different scales of f. Our constructive bound is asymptotically optimal for f = 1 (i.e., the i.i.d. model).},
keywords={mobile ad hoc networks;mobility management (mobile radio);throughput-delay trade-off;large-scale MANET;generalized mobility model;mobile ad hoc networks;identically distributed mobility model;TD trade-off;multirelay multihop scheme;MRMH scheme;multihop transmissions;constructive bounds;Road transportation;Throughput;Delays;Ad hoc networks;Mobile computing;Relays;Educational institutions},
doi={10.1109/INFCOM.2013.6566925},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566926,
author={H. Zeng and Y. Shi and Y. T. Hou and W. Lou and S. Kompella and S. F. Midkiff},
booktitle={2013 Proceedings IEEE INFOCOM},
title={On interference alignment for multi-hop MIMO networks},
year={2013},
volume={},
number={},
pages={1330-1338},
abstract={Interference alignment (IA) is a major advance in information theory. Despite its rapid advance in the information theory community, most results on IA remain point-to-point or single-hop and there is a lack of advance of IA in the context of multi-hop wireless networks. The goal of this paper is to make a concrete step toward advancing IA technique in multi-hop MIMO networks. We present an IA model consisting of a set of constraints at a transmitter and a receiver that can be used to determine a subset of interfering streams for IA. Based on this IA model, we develop an IA optimization framework for a multihop MIMO network. For performance evaluation, we compare the performance of a network throughput optimization problem under our proposed IA framework and the same problem when IA is not employed. Simulation results show that the use of IA can significantly decrease the DoF consumption for IC, thereby improving network throughput.},
keywords={interference suppression;MIMO communication;optimisation;radio links;radio receivers;radio transmitters;interference alignment;multihop MIMO network;information theory;point-to-point hop;multihop wireless network;transmitter;receiver;IA optimization;network throughput;Receivers;Transmitters;MIMO;Spread spectrum communication;Vectors;Integrated circuits;Interference},
doi={10.1109/INFCOM.2013.6566926},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566927,
author={G. Pei and A. K. S. Vullikanti},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Distributed approximation algorithms for maximum link scheduling and local broadcasting in the physical interference model},
year={2013},
volume={},
number={},
pages={1339-1347},
abstract={In this paper, we develop the first rigorous distributed algorithm for link scheduling in the SINR model under any length-monotone sub-linear power assignments. Our algorithms give constant factor approximation guarantees, matching the bounds of the sequential algorithms for these problems, with provable bounds on the running time in terms of the graph topology. We also study a related and fundamental problem of local broadcasting for uniform power levels, and obtain similar bounds. These problems are much more challenging in the SINR model than in the more standard graph based interference models, because of the non-locality of the SINR model. Our algorithms are randomized and crucially rely on physical carrier sensing for the distributed communication steps. We find that the specific wireless device capability of duplex/halfduplex communication significantly impacts the performance. Our main technique involves the distributed computation of affectance and a construct called a ruling, which are likely to be useful in other scheduling problems in the SINR model. We also study the empirical performance of our algorithms, and find that the performance depends on the topology, and the approximation ratio is very close to the best sequential algorithm.},
keywords={approximation theory;broadcasting;distributed algorithms;graph theory;radio networks;radiofrequency interference;scheduling;telecommunication links;distributed computation;duplex/halfduplex communication;wireless device capability;distributed communication steps;physical carrier sensing;graph based interference models;graph topology;sequential algorithms;factor approximation;length-monotone sub-linear power assignments;SINR model;physical interference model;local broadcasting;maximum link scheduling;distributed approximation algorithms;Interference;Signal to noise ratio;Approximation algorithms;Distributed algorithms;Computational modeling;Silicon;Approximation methods},
doi={10.1109/INFCOM.2013.6566927},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566928,
author={M. Gowda and S. Sen and R. R. Choudhury and S. Lee},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Cooperative packet recovery in enterprise WLANs},
year={2013},
volume={},
number={},
pages={1348-1356},
abstract={Cooperative packet recovery has been widely investigated in wireless networks, where corrupt copies of a packet are combined to recover the original packet. While previous work such as MRD (Multi Radio Diversity) and Soft apply combining to bits and bit-confidences, combining at the symbol level has been avoided. The reason is rooted in the prohibitive overhead of sharing raw symbol information between different APs of an enterprise WLAN. We present Epicenter that overcomes this constraint, and combines multiple copies of incorrectly received “symbols” to infer the actual transmitted symbol. Our core finding is that symbols need not be represented in full fidelity - coarse representation of symbols can preserve most of their diversity, while substantially lowering the overhead. We then develop a rate estimation algorithm that actually exploits symbol level combining. Our USRP/GNURadio testbed confirms the viability of our ideas, yielding 40% throughput gain over Soft, and 25-90% over 802.11. While the gains are modest, we believe that they are realistic, and available with minimal modifications to today's EWLAN systems.},
keywords={cooperative communication;packet radio networks;wireless LAN;EWLAN systems;USRP/GNURadio testbed;symbol level combining;rate estimation algorithm;Multi Radio Diversity;wireless networks;enterprise WLAN;cooperative packet recovery;Diversity reception;Throughput;Vectors;Algorithm design and analysis;Modulation;IEEE 802.11 Standards;Estimation},
doi={10.1109/INFCOM.2013.6566928},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566929,
author={J. Tang and Y. Cheng},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Selfish misbehavior detection in 802.11 based wireless networks: An adaptive approach based on Markov decision process},
year={2013},
volume={},
number={},
pages={1357-1365},
abstract={The open and distributed nature of the IEEE 802.11 based wireless networks provides selfish users the opportunity to to gain an unfair share of the network throughput by manipulating the protocol parameters, say, using a smaller contention window. In this paper, we propose an adaptive approach for real-time detection of such selfish misbehavior. An adaptive detector is necessary in practice, as it needs to deal with different misbehaving scenarios where the number of selfish users and the contention windows exploited by each selfish user are different. In this paper, we first design a basic misbehavior detector based on the non-parametric cumulative sum (CUSUM) test. While the basic detector can be modeled with a Markov chain, we further resort to the Markov decision process (MDP) technique to enhance the basic detector to an adaptive design. In particular, we develop a novel reward function based on which the optimal policy of the MDP can be determined. The optimal policy indicates how the adaptive detector should operate at each state. Another important feature of our detector is that it enables an effective iterative method to detect multiple misbehaving nodes. We present thorough simulation results to confirm the accuracy of our analysis, and demonstrate the efficiency of the adaptive detector compared to a static solution.},
keywords={iterative methods;Markov processes;protocols;radio networks;wireless LAN;selfish misbehavior detection;IEEE 802.11 based wireless networks;adaptive approach;Markov decision process;protocol parameters;smaller contention window;adaptive detector;CUSUM test;nonparametric cumulative sum;Markov chain;Markov decision process technique;MDP technique;optimal policy;iterative method;Detectors;IEEE 802.11 Standards;Markov processes;Protocols;Probability;Delays;Mathematical model},
doi={10.1109/INFCOM.2013.6566929},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566930,
author={Y. Zhang and Q. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={HoWiES: A holistic approach to ZigBee assisted WiFi energy savings in mobile devices},
year={2013},
volume={},
number={},
pages={1366-1374},
abstract={We propose HoWiES, a system that saves energy consumed by WiFi interfaces in mobile devices with the assistance of ZigBee radios. The core component of HoWiES is a WiFiZigBee message delivery scheme that enables WiFi radios to convey different messages to ZigBee radios in mobile devices. Based on the WiFi-ZigBee message delivery scheme, we design three protocols that target at three WiFi energy saving opportunities in scanning, standby and wakeup respectively. We have implemented the HoWiES system with two mobile devices platforms and two AP platforms. Our real-world experimental evaluation shows that our system can convey thousands of different messages from WiFi radios to ZigBee radios with an accuracy over 98%, and our energy saving protocols, while maintaining the comparable wakeup delay to that of the standard 802.11 power save mode, save 88% and 85% of energy consumed in scanning state and standby state respectively.},
keywords={mobile radio;protocols;wireless LAN;Zigbee;HoWiES;ZigBee assisted WiFi energy savings;mobile devices platforms;energy consumption saving;WiFi interfaces;ZigBee radios;WiFi-ZigBee message delivery scheme;WiFi energy saving opportunities;scanning opportunity;standby opportunity;wakeup opportunity;AP platforms;WiFi radios;standard 802.11 power save mode;energy saving protocols;IEEE 802.11 Standards;Zigbee;Power demand;Protocols;Smart phones;Decoding},
doi={10.1109/INFCOM.2013.6566930},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566931,
author={P. Huang and X. Yang and L. Xiao},
booktitle={2013 Proceedings IEEE INFOCOM},
title={WiFi-BA: Choosing arbitration over backoff in high speed multicarrier wireless networks},
year={2013},
volume={},
number={},
pages={1375-1383},
abstract={Advancements in wireless communication techniques have increased the wireless physical layer (PHY) data rates by hundreds of times in a dozen years. The high PHY data rates, however, have not been translated to commensurate throughput gains due to overheads incurred by medium access control (MAC) and PHY convergence procedure. At high PHY data rates, the time used for collision avoidance (CA) at MAC layer and the time used for PHY convergence procedure can easily exceed the time used for transmission of an actual data frame. Recent work intends to reduce the CA overhead by reducing the backoff time slot size. However, the method introduces more collisions in presence of hidden terminals because the tiny backoff slots can no longer de-synchronize hidden terminals, leading to persistent collisions among hidden terminals. As collision detection (CD) in wireless communication became feasible recently, some protocols migrate random backoff from the time domain to the frequency domain, but they fail to address the introduced high collision probability. We investigate the practical issues of CD in the frequency domain and introduce a binary mapping scheme to reduce the collision probability. Based on the binary mapping, a bitwise arbitration (BA) mechanism is devised to grant only one transmitter the permission to initiate data transmission in a contention. With the low collision probability achieved in a short bounded arbitration phase, the throughput is significantly improved by our proposed WiFi-BA. Because collisions are unlikely to happen, unfairness caused by capture effect of radios is also reduced. The bitwise arbitration mechanism can further be set to let high priority messages get through unimpeded, making WiFi-BA suitable for real time prioritized communication. We validate the effectiveness of WiFi-BA through implementation on FPGA of USRP E110. Performance evaluation demonstrates that WiFi-BA is more efficient than current Wi-Fi solutions.},
keywords={access protocols;frequency-domain analysis;probability;time-domain analysis;wireless LAN;WiFi-BA;high speed multicarrier wireless networks;wireless communication techniques;wireless physical layer data rates;PHY data rates;medium access control;PHY convergence procedure;collision avoidance;MAC layer;backoff time slot size;collision detection;random backoff;time domain;frequency domain;high collision probability;binary mapping scheme;bitwise arbitration mechanism;low collision probability;real time prioritized communication;FPGA;USRP E110;Data communication;OFDM;Frequency-domain analysis;Binary codes;Wireless communication;IEEE 802.11 Standards;Interference},
doi={10.1109/INFCOM.2013.6566931},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566932,
author={X. Liu and J. Cao and S. Tang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Fault tolerant complex event detection in WSNs: A case study in structural health monitoring},
year={2013},
volume={},
number={},
pages={1384-1392},
abstract={Reliably detecting event in the presence of faulty nodes, particularly nodes with faulty readings is a fundamental task in wireless sensor networks (WSNs). Existing fault-tolerant event detection schemes usually 'mask' the effect of faulty readings through high-level fusion techniques. However, in some applications such as structural health monitoring (SHM) and volcano monitoring, detecting the events of interest requires lowlevel data collaboration from multiple sensors. This implies that the effect of faulty readings cannot be masked once they are involved into event detection. Nodes with faulty readings must be firstly detected and removed from the system. Unfortunately, most existing techniques to detect faulty nodes can only take boolean or scalar data as input while in these applications, data generated from each sensor is a sequence of dynamic data. In this paper, we address these issues using an example of SHM. Detecting event in SHM (i.e. structural damage) requires low level collaboration from multiple sensors, and each sensor generates a sequence of dynamic vibrational data. We proposed a fault-tolerant event detection scheme in SHM called FTED. In FTED, three novel techniques are proposed: (1) distributed extraction of features for faulty node detection, (2) iterative faulty node detection (I-FUND), and (3) distributed event detection. In particular, I-FUND takes vector as input and can even handle the 'element mismatch problem' where comparable elements in vectors are located at unknown different positions. The effectiveness of FTED is demonstrated through both simulations and real experiments.},
keywords={condition monitoring;fault tolerance;structural engineering;wireless sensor networks;fault tolerant complex event detection;WSN;structural health monitoring;reliably detecting event;wireless sensor network;high level fusion technique;volcano monitoring;data collaboration;multiple sensors;faulty reading;scalar data;dynamic data;dynamic vibrational data;fault tolerant event detection;FTED;iterative faulty node detection;distributed event detection;element mismatch problem;Sensors;Shape;Event detection;Feature extraction;Vectors;Fault tolerance},
doi={10.1109/INFCOM.2013.6566932},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566933,
author={L. Cheng and Y. Gu and T. He and J. Niu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Dynamic switching-based reliable flooding in low-duty-cycle wireless sensor networks},
year={2013},
volume={},
number={},
pages={1393-1401},
abstract={Reliable flooding in wireless sensor networks (WSNs) is desirable for a broad range of applications and network operations, and has been extensively investigated. However, relatively little work has been done for reliable flooding in lowduty-cycle WSNs with unreliable wireless links. It is a challenging problem to efficiently ensure 100% flooding coverage considering the combined effects of low-duty-cycle operation and unreliable wireless transmission. In this work, we propose a novel dynamic switching-based reliable flooding (DSRF) framework, which is designed as an enhancement layer to provide efficient and reliable delivery for a variety of existing flooding tree structures in lowduty-cycle WSNs. The key novelty of DSRF lies in the dynamic switching decision making when encountering a transmission failure, where a flooding tree structure is dynamically adjusted based on the packet reception results for energy saving and delay reduction. DSRF is distinctive from existing works in that it explores both poor links and good links on demand. Through comprehensive performance comparisons, we demonstrate that, compared with the flooding protocol without DSRF enhancement, DSRF effectively reduces the flooding delay and the total number of packet transmission by 12% 25% and 10% 15%, respectively. Remarkably, the achieved performance is close to the theoretical lower bound.},
keywords={decision making;failure analysis;radio links;telecommunication network reliability;telecommunication switching;wireless sensor networks;dynamic switching-based reliable flooding;low-duty-cycle wireless sensor networks;network operations;low duty-cycle WSN;unreliable wireless links;low-duty-cycle operation;unreliable wireless transmission;dynamic switching-based reliable flooding framework;DSRF framework;enhancement layer;flooding tree structures;dynamic switching decision making;transmission failure;packet reception;energy saving;delay reduction;packet transmission;Switches;Wireless sensor networks;Schedules;Reliability;Receivers;Dynamic scheduling;Synchronization},
doi={10.1109/INFCOM.2013.6566933},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566934,
author={I. Koutsopoulos},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Optimal incentive-driven design of participatory sensing systems},
year={2013},
volume={},
number={},
pages={1402-1410},
abstract={Participatory sensing has emerged as a novel paradigm for data collection and collective knowledge formation about a state or condition of interest, sometimes linked to a geographic area. In this paper, we address the problem of incentive mechanism design for data contributors for participatory sensing applications. The service provider receives service queries in an area from service requesters and initiates an auction for user participation. Upon request, each user reports its perceived cost per unit of amount of participation, which essentially maps to a requested amount of compensation for participation. The participation cost quantifies the dissatisfaction caused to user due to participation. This cost is considered to be private information for each device, as it strongly depends on various factors inherent to it, such as the energy cost for sensing, data processing and transmission to the closest point of wireless access, the residual battery level, the number of concurrent jobs at the device processor, the required bandwidth to transmit data and the related charges of the mobile network operator, or even the user discomfort due to manual effort to submit data. Hence, participants have strong motive to mis-report their cost, i.e. declare a higher cost that the actual one, so as to obtain higher payment. We seek a mechanism for user participation level determination and payment allocation which is most viable for the provider, that is, it minimizes the total cost of compensating participants, while delivering a certain quality of experience to service requesters. We cast the problem in the context of optimal reverse auction design, and we show how the different quality of submitted information by participants can be tracked by the service provider and used in the participation level and payment selection procedures. We derive a mechanism that optimally solves the problem above, and at the same time it is individually rational (i.e., it motivates users to participate) and incentive-compatible (i.e. it motivates truthful cost reporting by participants). Finally, a representative participatory sensing case study involving parameter estimation is presented, which exemplifies the incentive mechanism above.},
keywords={artificial intelligence;commerce;cost reduction;data acquisition;incentive schemes;mobile radio;quality of experience;quality of service;query processing;wireless sensor networks;optimal incentive driven design;participatory sensing system;data collection;incentive mechanism design;service provider;service query;service requester;participation cost;residual battery level;mobile network operator;user discomfort;user participation level determination;payment allocation;total cost minimization;participation compensation;quality of experience;optimal reverse auction design;payment selection procedure;parameter estimation;Sensors;Bayes methods;Resource management;Vectors;Air pollution;Atmospheric measurements;Quality of service},
doi={10.1109/INFCOM.2013.6566934},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566935,
author={H. Liu and S. Hu and W. Zheng and Z. Xie and S. Wang and P. Hui and T. Abdelzaher},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Efficient 3G budget utilization in mobile participatory sensing applications},
year={2013},
volume={},
number={},
pages={1411-1419},
abstract={This paper explores efficient 3G budget utilization in mobile participatory sensing applications. 1 Distinct from previous research work that either rely on limited WiFi access points or assume the availability of unlimited 3G communication capability, we offer a more practical participatory sensing system that leverages potential 3G budgets that participants contribute at will, and uses it efficiently customized for the needs of multiple participatory sensing applications with heterogeneous sensitivity to environmental changes. We address the challenge that the information of data generation and WiFi encounters is not a priori knowledge, and propose an online decision making algorithm that takes advantage of participants' historical data. We also develop a heuristic algorithm to consume less energy and reduce the storage overhead while maintaining efficient 3G budget utilization. Experimental results from a 30-participant deployment demonstrate that, even when the budget is as small as 2.5% of a popular data plan, these two algorithms achieve higher utility of uploaded data compared to the baseline solution, especially, they increase the utility of received data by 151.4% and 137.8% for those sensitive applications.},
keywords={3G mobile communication;data communication;decision making;wireless LAN;3G budget utilization;mobile participatory sensing applications;WiFi access points;unlimited 3G communication capability;heterogeneous sensitivity;environmental changes;data generation;online decision making;heuristic algorithm;30-participant deployment;sensitive applications;Sensors;IEEE 802.11 Standards;Smart phones;Heuristic algorithms;Mobile communication;Vehicles;Wireless communication},
doi={10.1109/INFCOM.2013.6566935},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566936,
author={S. Gangam and P. Sharma and S. Fahmy},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Pegasus: Precision hunting for icebergs and anomalies in network flows},
year={2013},
volume={},
number={},
pages={1420-1428},
abstract={Accurate online network monitoring is crucial for detecting attacks, faults, and anomalies, and determining traffic properties across the network. With high bandwidth links and consequently increasing traffic volumes, it is difficult to collect and analyze detailed flow records in an online manner. Traditional solutions that decouple data collection from analysis resort to sampling and sketching to handle large monitoring traffic volumes. We propose a new system, Pegasus, to leverage commercially available co-located compute and storage devices near routers and switches. Pegasus adaptively manages data transfers between monitors and aggregators based on traffic patterns and user queries. We use Pegasus to detect global icebergs or global heavy-hitters. Icebergs are flows with a common property that contribute a significant fraction of network traffic. For example, DDoS attack detection is an iceberg detection problem with a common destination IP. Other applications include identification of “top talkers,” top destinations, and detection of worms and port scans. Experiments with Abilene traces, sFlow traces from an enterprise network, and deployment of Pegasus as a live monitoring service on PlanetLab show that our system is accurate and scales well with increasing traffic and number of monitors.},
keywords={computer network performance evaluation;computer network security;supervisory programs;system monitoring;Pegasus;network flows;online network monitoring;co-located compute-storage devices;adaptive data transfer management;monitors;aggregators;traffic patterns;user queries;global icebergs;global heavy-hitter detection;network traffic;DDoS attack detection;iceberg detection problem;top talkers identification;top destination detection;worm detection;port scan detection;Abilene traces;sFlow traces;enterprise network;live monitoring service;PlanetLab;Monitoring;Ports (Computers);Computer crime;Accuracy;Blades;IP networks;Bandwidth},
doi={10.1109/INFCOM.2013.6566936},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566937,
author={A. Hassidim and D. Raz and M. Segalov and A. Shaqed},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Network utilization: The flow view},
year={2013},
volume={},
number={},
pages={1429-1437},
abstract={Building and operating a large backbone network can take months or even years, and it requires a substantial investment. Therefore, there is an economical drive to increase the utilization of network resources (links, switches, etc.) in order to improve the cost efficiency of the network. At the same time, the utilization of network components has a direct impact on the performance of the network and its resilience to failure, and thus operational considerations are a critical aspect of the decision regarding the desired network load and utilization. However, the actual utilization of the network resources is not easy to predict or control. It depends on many parameters like the traffic demand and the routing scheme (or Traffic Engineering if deployed), and it varies over time and space. As a result it is very difficult to actually define real network utilization and to understand the reasons for this utilization. In this paper we introduce a novel way to look at the network utilization. Unlike traditional approaches that consider the average link utilization, we take the flow perspective and consider the network utilization in terms of the growth potential of the flows in the network. After defining this new Flow Utilization, and discussing how it differs from common definitions of network utilization, we study ways to efficiently compute it over large networks. We then show, using real backbone data, that Flow Utilization is very useful in identifying network state and evaluating performance of TE algorithms.},
keywords={network parameters;telecommunication network management;telecommunication network routing;telecommunication traffic;TE algorithms;network state;flow utilization;growth potential;network utilization;routing scheme;traffic demand;network load;operational considerations;cost efficiency;network resources;Vectors;Google;Packet loss;Planning;Monitoring},
doi={10.1109/INFCOM.2013.6566937},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566938,
author={G. Hasslinger and A. Schwahn and F. Hartleb},
booktitle={2013 Proceedings IEEE INFOCOM},
title={2-State (semi-)Markov processes beyond Gilbert-Elliott: Traffic and channel models based on 2 lt;sup gt;nd lt;/sup gt; order statistics},
year={2013},
volume={},
number={},
pages={1438-1446},
abstract={Two-state Markov models are applied in many performance evaluation studies as a simple form of autocorrelated processes, starting with Gilbert-Elliott channels for the analysis of transmission protocols subject to error bursts. We derive an explicit formula for the 2nd order statistics of 2state semi-Markov processes in order to adapt them to correlated traffic and error processes. State and transition specific distribution functions are included in a general representation covering the special cases being usually studied in the literature. The results reveal the influence of model parameters on short and long term dependency and give rise to a straightforward procedure for parameter adaption. In general, 2-state models provide a 2-dimensional fitting space, whereas special 2-state cases often have only one parameter left to fit the shape of the 2nd order statistics. In our evaluation of IP packet measurements on aggregation links we experienced that adaptations by general 2state Markov models achieve a much closer fit to the traffic variability in different time scales than self-similar processes.},
keywords={computer network performance evaluation;correlation methods;higher order statistics;Internet;IP networks;Markov processes;telecommunication channels;telecommunication traffic;transport protocols;Internet traffic measurement;traffic variability;IP packet measurements;two-dimensional fitting space;parameter adaption;short term dependency;long term dependency;model parameters;transition specific distribution function;state specific distribution function;error process;two-state semiMarkov process;explicit formula;transmission protocols;Gilbert-Elliott channels;autocorrelated processes;performance evaluation;two-state Markov models;second order statistics;channel models;traffic models;Markov processes;Adaptation models;Fitting;IP networks;Mathematical model;Time measurement;Equations;2-state (semi-)Markov;Gilbert-Elliott;self-similar processes;Internet traffic measurement;traffic variability;autocorrelation;2nd order statistics},
doi={10.1109/INFCOM.2013.6566938},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566939,
author={Y. Liu and W. Chen and Y. Guan},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Near-optimal approximate membership query over time-decaying windows},
year={2013},
volume={},
number={},
pages={1447-1455},
abstract={There has been a long history of finding a spaceefficient data structure to support approximate membership queries, started from Bloom's work in the 1970's. Given a set A of n items and an additional item x from the same universe U of a size m ≫ n, we want to distinguish whether x ∈ A or not, using small (limited) space. The solutions for the membership query are needed for many network applications, such as cache directory, load-balancing, security, etc. If A is static, there exist optimal algorithms to find a randomized data structure to represent A using only (1+ o(1))n log 1/δ bits, which only allows for a small false positive δ but no false negative. However, existing optimal algorithms are not practical for many Internet applications, e.g., social network services, peer-to-peer systems, network traffic monitoring, etc. They are too spaceand time-expensive due to the frequent changes in the set A, because all items are needed to recompute the optimal data structure for each change using a linear running time. In this paper, we propose a novel data structure to support the approximate membership query in the time-decaying window model. In this model, items are inserted one-by-one over a data stream, and we want to determine whether an item is among the most recent w items for any given window size w ≤ n. Our data structure only requires O(n(log 1/δ+logn)) bits and O(1) running time. We also prove a non-trivial space lower bound, i.e. (n - δm) log(n - δm) bits, which guarantees that our data structure is near-optimal. Our data structure has been evaluated using both synthetic and real data sets.},
keywords={computational complexity;data structures;Internet;query processing;randomised algorithms;resource allocation;near-optimal approximate membership query;space-efficient data structure;network applications;cache directory;load-balancing;optimal algorithms;randomized data structure;Internet applications;social network services;peer-to-peer systems;network traffic monitoring;linear running time;time-decaying window model;data stream;nontrivial space lower bound;Data structures;Dictionaries;Data models;Radiation detectors;Algorithm design and analysis;Internet;Security},
doi={10.1109/INFCOM.2013.6566939},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566940,
author={Y. Dai and J. Wu and C. Xin},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Virtual backbone construction for cognitive radio networks without common control channel},
year={2013},
volume={},
number={},
pages={1456-1464},
abstract={The advantages of virtual backbones have been proven in wireless networks. In cognitive radio networks (CRNs), virtual backbones can also play a critical role in both routing and data transport. However, the virtual backbone construction for CRNs is more challenging than for traditional wireless networks because of the opportunistic spectrum access. Moreover, when no common control channel is available to exchange the control information, this problem is even more difficult. In this paper, we propose a novel approach for constructing virtual backbones in CRNs, without relying on a common control channel. Our approach first utilizes the geographical information to let the nodes of a CRN self-organize into cells. Next, the nodes in each cell form into clusters, and a virtual backbone is established over the cluster heads. The virtual backbone is then applied to carry out the end-to-end data transmission. The proposed virtual backbone construction approach requires only limited exchange of control messages. It is efficient and highly adaptable under the opportunistic spectrum access. We evaluate our approach through extensive simulations.},
keywords={cognitive radio;data communication;radio networks;radio spectrum management;virtual backbone construction;cognitive radio networks;common control channel;wireless networks;CRN;end-to-end data transmission;opportunistic spectrum access;Data communication;Wireless networks;Availability;Cognitive radio;Global Positioning System;Routing;Organizations;Cognitive radio networks;self-organization;virtual backbone construction;end-to-end data transmission},
doi={10.1109/INFCOM.2013.6566940},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566941,
author={X. Xing and T. Jing and Y. Huo and H. Li and X. Cheng},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Channel quality prediction based on Bayesian inference in cognitive radio networks},
year={2013},
volume={},
number={},
pages={1465-1473},
abstract={The problem of channel quality prediction in cognitive radio networks is investigated in this paper. First, the spectrum sensing process is modeled as a Non-Stationary Hidden Markov Model (NSHMM), which captures the fact that the channel state transition probability is a function of the time interval the primary user has stayed in the current state. Then the model parameters, which carry the information about the expected duration of the channel states and the spectrum sensing accuracy (detection accuracy and false alarm probability) of the SU, are estimated via Bayesian inference with Gibbs sampling. Finally, the estimated NSHMM parameters are employed to design a channel quality metric according to the predicted channel idle duration and spectrum sensing accuracy. Extensive simulation study has been performed to investigate the effectiveness of our design. The results indicate that channel ranking based on the proposed channel quality prediction mechanism captures the idle state duration of the channel and the spectrum sensing accuracy of the SUs, and provides more high quality transmission opportunities and higher successful transmission rates at shorter spectrum waiting times for dynamic spectrum access.},
keywords={Bayes methods;cognitive radio;hidden Markov models;parameter estimation;radio spectrum management;sampling methods;wireless channels;Bayesian inference;cognitive radio network;spectrum sensing process;nonstationary hidden Markov model;channel state transition probability;spectrum sensing accuracy;detection accuracy;false alarm probability;Gibbs sampling;NSHMM parameter estimation;channel quality metric design;channel idle duration;channel ranking;channel quality prediction mechanism;dynamic spectrum access;Hidden Markov models;Sensors;Bayes methods;Probability distribution;Channel estimation;Accuracy;Cognitive radio;Channel quality prediction;cognitive radio networks;non-stationary HMM;Bayesian inference},
doi={10.1109/INFCOM.2013.6566941},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566942,
author={Y. Zhao and M. Song and C. Xin},
booktitle={2013 Proceedings IEEE INFOCOM},
title={FMAC: A fair MAC protocol for coexisting cognitive radio networks},
year={2013},
volume={},
number={},
pages={1474-1482},
abstract={Cognitive radio is viewed as a disruptive technology innovation to improve spectrum efficiency. The deployment of coexisting cognitive radio networks, however, raises a great challenge to the medium access control (MAC) protocol design. While there have been many MAC protocols developed for cognitive radio networks, most of them have not considered the coexistence of cognitive radio networks, and thus do not provide a mechanism to ensure fair and efficient coexistence of cognitive radio networks. In this paper, we introduce a novel MAC protocol, termed fairness-oriented media access control (FMAC), to address the dynamic availability of channels and achieve fair and efficient coexistence of cognitive radio networks. Different from the existing MACs, FMAC utilizes a three-state spectrum sensing model to distinguish whether a busy channel is being used by a primary user or a secondary user from an adjacent cognitive radio network. As a result, secondary users from coexisting cognitive radio networks are able to share the channel together, and hence to achieve fair and efficient coexistence. We develop an analytical model using two-level Markov chain to analyze the performance of FMAC including throughput and fairness. Numerical results verify that FMAC is able to significantly improve the fairness of coexisting cognitive radio networks while maintaining a high throughput.},
keywords={access protocols;cognitive radio;innovation management;Markov processes;radio spectrum management;wireless channels;two-level Markov chain;primary user;secondary user;three-state spectrum sensing model;dynamic availability;fairness-oriented media access control;medium access control protocol design;spectrum efficiency;disruptive technology innovation;coexisting cognitive radio networks;fair MAC protocol;FMAC;Sensors;Media Access Protocol;Cognitive radio;Switches;Throughput;Markov processes;IEEE 802.11 Standards},
doi={10.1109/INFCOM.2013.6566942},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566943,
author={M. Ozger and O. B. Akan},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Event-driven spectrum-aware clustering in cognitive radio sensor networks},
year={2013},
volume={},
number={},
pages={1483-1491},
abstract={Wireless sensor networks (WSN) with dynamic spectrum access (DSA) capability, namely cognitive radio sensor networks (CRSN), is a promising solution for spectrum scarcity problem. Despite improvement in spectrum utilization by DSA capability, energy-efficient solutions for CRSN are required due to resource-constrained nature of CRSN inherited from WSN. Clustering is an efficient way to decrease energy consumption. Existing clustering approaches for WSN are not applicable in CRSN and existing solutions for cognitive radio networks are not suitable for sensor networks. In this paper, we propose an event-driven clustering protocol which forms temporal cluster for each event in CRSN. Upon detection of an event, we determine eligible nodes for clustering according to local position of nodes between event and sink. Cluster-heads are selected among eligible nodes according to node degree, available channels and distance to the sink in their neighborhood. They select one-hop members for maximizing the number of two-hop neighbors that are accessible by one-hop neighbors through cluster channels to increase connectivity between clusters. Clusters are between event and sink and are no longer available after the end of the event. This avoids energy consumption due to unnecessary cluster formation and maintenance overheads. Performance evaluation reveals that our solution is energy-efficient with a delay due to spontaneous cluster formation.},
keywords={cognitive radio;energy conservation;pattern clustering;protocols;radio spectrum management;telecommunication power management;wireless channels;wireless sensor networks;event-driven spectrum-aware clustering;cognitive radio sensor networks;wireless sensor networks;WSN;DSA capability;dynamic spectrum access capability;CRSN;spectrum scarcity problem;spectrum utilization;energy-efficient solutions;energy consumption reduction;event-driven clustering protocol;temporal cluster;event detection;clustering node determination;cluster-head selection;two-hop neighbors;one-hop members;one-hop neighbors;cluster channels;maintenance overheads;unnecessary cluster formation;performance evaluation;Clustering algorithms;Cognitive radio;Wireless sensor networks;Protocols;Energy consumption;Availability},
doi={10.1109/INFCOM.2013.6566943},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566944,
author={A. Khanafer and M. Kodialam and K. P. N. Puttaswamy},
booktitle={2013 Proceedings IEEE INFOCOM},
title={The constrained Ski-Rental problem and its application to online cloud cost optimization},
year={2013},
volume={},
number={},
pages={1492-1500},
abstract={Cloud service providers (CSPs) enable tenants to elastically scale their resources to meet their demands. In fact, there are various types of resources offered at various price points. While running applications on the cloud, a tenant aiming to minimize cost is often faced with crucial trade-off considerations. For instance, upon each arrival of a query, a web application can either choose to pay for CPU to compute the response fresh, or pay for cache storage to store the response so as to reduce the compute costs of future requests. The SkiRental problem abstracts such scenarios where a tenant is faced with a to-rent-or-to-buy trade-off; in its basic form, a skier should choose between renting or buying a set of skis without knowing the number of days she will be skiing. In this paper, we introduce a variant of the classical SkiRental problem in which we assume that the skier knows the first (or second) moment of the distribution of the number of ski days in a season. We demonstrate that utilizing this information leads to achieving the best worst-case expected competitive ratio (CR) performance. Our method yields a new class of randomized algorithms that provide arrivals-distribution-free performance guarantees. Further, we apply our solution to a cloud file system and demonstrate the cost savings obtained in comparison to other competing schemes. Simulations illustrate that our scheme exhibits robust average-cost performance that combines the best of the well-known deterministic and randomized schemes previously proposed to tackle the Ski-Rental problem.},
keywords={cloud computing;cost reduction;deterministic algorithms;optimisation;pricing;randomised algorithms;constrained ski-rental problem;online cloud cost optimization;cloud service providers;CSP;cost minimization;query arrival;Web application;CPU;cache storage;response storage;response computation;cost reduction;to-rent-or-to-buy trade-off;ski buying;worst-case expected competitive ratio performance;worst-case expected CR performance;randomized algorithms;arrival-distribution-free performance guarantees;cloud file system;cost savings;robust average-cost performance;deterministic schemes;first-moment-constrained ski-rental problem;second-moment-constrained ski-rental problem;Games;Algorithm design and analysis;Game theory;Optimization;Snow;Cache storage;Standards},
doi={10.1109/INFCOM.2013.6566944},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566945,
author={D. Niu and B. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={An efficient distributed algorithm for resource allocation in large-scale coupled systems},
year={2013},
volume={},
number={},
pages={1501-1509},
abstract={In modern large-scale systems, fast distributed resource allocation and utility maximization are becoming increasingly important. Traditional solutions to such problems rely on primal/dual decomposition and gradient methods, whose convergence is sensitive to the choice of the stepsize and may not be sufficient to satisfy the requirement of large-scale real-time applications. We propose a new iterative approach to distributed resource allocation in coupled systems. Without complicating message-passing, the new approach is robust to parameter choices and expedites convergence by exploiting problem structures. We theoretically analyze the asynchronous algorithm convergence conditions, and empirically evaluate its benefits in a case of cloud network resource reservation based on real-world data.},
keywords={distributed processing;gradient methods;resource allocation;efficient distributed algorithm;fast distributed resource allocation;utility maximization;large-scale coupled systems;gradient methods;iterative approach;message-passing;asynchronous algorithm convergence conditions;cloud network resource reservation;Convergence;Resource management;Jacobian matrices;Gradient methods;Vectors;Algorithm design and analysis;Cost function},
doi={10.1109/INFCOM.2013.6566945},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566946,
author={H. Zhang and B. Li and H. Jiang and F. Liu and A. V. Vasilakos and J. Liu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A framework for truthful online auctions in cloud computing with heterogeneous user demands},
year={2013},
volume={},
number={},
pages={1510-1518},
abstract={The paradigm of cloud computing has spontaneously prompted a wide interest in market-based resource allocation mechanisms by which a cloud provider aims at efficiently allocating cloud resources among potential users. Among these mechanisms, auction-style pricing policies, as they can effectively reflect the underlying trends in demand and supply for the computing resources, have attracted a research interest recently. This paper conducts the first work on a framework for truthful online cloud auctions where users with heterogeneous demands could come and leave on the fly. Our framework desirably supports a variety of design requirements, including (1) dynamic design for timely reflecting fluctuation of supply-demand relations, (2) joint design for supporting the heterogeneous user demands, and (3) truthful design for discouraging bidders from cheating behaviors. Concretely speaking, we first design a novel bidding language, wherein users' heterogeneous demands are generalized to regulated and consistent forms. Besides, building on top of our bidding language we propose COCA, an incentive-Compatible (truthful) Online Cloud Auction mechanism based on two proposed guidelines. Our theoretical analysis shows that the worst-case performance of COCA can be well-bounded. Further, in simulations the performance of COCA is seen to be comparable to the well-known off-line Vickrey-Clarke-Groves (VCG) mechanism [11].},
keywords={cloud computing;commerce;pricing;research and development;resource allocation;supply and demand;trusted computing;truthful online auctions;cloud computing;heterogeneous user demands;market-based resource allocation mechanisms;cloud provider;auction-style pricing policies;research interest;design requirements;dynamic design;supply-demand relations;bidders;cheating behaviors;COCA;incentive-compatible online cloud auction mechanism;well-known off-line Vickrey-Clarke-Groves mechanism;VCG;Resource management;Cost accounting;Pricing;Educational institutions;Cloud computing;Computer science;Buildings},
doi={10.1109/INFCOM.2013.6566946},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566947,
author={H. Roh and C. Jung and W. Lee and D. Du},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Resource pricing game in geo-distributed clouds},
year={2013},
volume={},
number={},
pages={1519-1527},
abstract={Cloud computing enables larger classes of application service providers to distribute their services to world-wide users in multiple regions without their own private data centers. Heterogeneity and resource limitation of geo-graphically distributed cloud data centers impose application service providers to have incentives to optimize their computing resource usage while guaranteeing some level of quality of service. Recent studies proposed various techniques for optimization of computing resource usage from cloud users (or application service providers) perspective with little consideration of competition. In addition, optimization efforts of application service providers motivate cloud service providers owning multiple geo-distributed clouds to decide their computing resource prices considering their efforts. In this context, we formulate this problem for cloud service providers as a game of resource pricing in geo-distributed clouds. One of the main challenges in this problem is how to model the best responses of application service providers, given resource price information of clouds in non-overlapped regions. We propose a novel concave game to describe the quantity competition among application service providers reducing payment while guaranteeing fair service delay to end users. Furthermore, we optimize the prices of computing resources to converge to the equilibrium. In addition, we show several characteristics of the equilibrium point and discuss their implications to design computing resource markets for geo-distributed clouds.},
keywords={cloud computing;game theory;pricing;quality of service;resource allocation;resource pricing game;geodistributed cloud;cloud computing;application service provider;private data center;heterogeneity;resource limitation;geographically distributed cloud data center;computing resource usage optimization;quality of service;computing resource price;resource price information;nonoverlapped region;concave game;payment;service delay;computing resource market;Games;Pricing;Cloud computing;Distributed databases;Time factors;Optimization;Quality of service},
doi={10.1109/INFCOM.2013.6566947},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566948,
author={B. Ji and C. Joo and N. B. Shroff},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Exploring the inefficiency and instability of Back-Pressure algorithms},
year={2013},
volume={},
number={},
pages={1528-1536},
abstract={In this paper, we focus on the issue of stability in multihop wireless networks under flow-level dynamics, and explore the inefficiency and instability of the celebrated Back-Pressure algorithms. It has been well-known that the Back-Pressure (or Max-Weight) algorithms achieve queue stability and throughput optimality in a wide variety of scenarios. Yet, these results all rely on the assumptions that the set of flows is fixed, and that all the flows are long-lived and keep injecting packets into the network. Recently, in the presence of flow-level dynamics, where flows arrive and request to transmit a finite amount of packets, it has been shown that the Max-Weight algorithms may not guarantee stability due to channel fading or inefficient spatial reuse. However, these observations are made only for single-hop traffic, and thus have resulted in partial solutions that are limited to the single-hop scenarios. An interesting question is whether straightforward extensions of the previous solutions to the known instability problems would achieve throughput optimality in multihop traffic setting. To answer the question, we explore potential inefficiency and instability of the Back-Pressure algorithms, and provide interesting examples that are useful to obtain insights into developing an optimal solution. We also conduct simulations to further illustrate the instability issue of the Back-Pressure algorithms in various scenarios. Our study reveals that new types of inefficiencies may arise in the settings with multihop traffic due to underutilization of the link capacity or inefficient routing, and the stability problem becomes more challenging than in the single-hop traffic counterpart.},
keywords={queueing theory;radio links;radio networks;stability;telecommunication network routing;telecommunication traffic;routing;link capacity;multihop traffic setting;instability problem;single-hop traffic;channel fading;throughput optimality;queue stability;max-weight algorithm;celebrated back-pressure algorithm;flow-level dynamics;multihop wireless network;Heuristic algorithms;Schedules;Routing;Delays;Dynamic scheduling;Scheduling algorithms;Throughput},
doi={10.1109/INFCOM.2013.6566948},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566949,
author={S. Moharir and S. Shakkottai},
booktitle={2013 Proceedings IEEE INFOCOM},
title={MaxWeight vs. BackPressure: Routing and scheduling in multi-channel relay networks},
year={2013},
volume={},
number={},
pages={1537-1545},
abstract={We study routing and scheduling algorithms for relay-assisted, multi-channel downlink wireless networks (e.g., OFDM-based cellular systems with relays). Over such networks, while it is well understood that the BackPressure algorithm is stabilizing (i.e., queue lengths do not become arbitrarily large), its performance (e.g., delay, buffer usage) can be poor. In this paper, we study an alternative - the MaxWeight algorithm - variants of which are known to have good performance in a single-hop setting. In a general relay setting however, MaxWeight is not even stabilizing (and thus can have very poor performance). In this paper, we study an iterative MaxWeight algorithm for routing and scheduling in downlink multi-channel relay networks. We show that, surprisingly, the iterative MaxWeight algorithm can stabilize the system in several large-scale instantiations of this setting (e.g., general arrivals with full-duplex relays, bounded arrivals with half-duplex relays). Further, using both many-channel large-deviations analysis and simulations, we show that iterative MaxWeight outperforms the BackPressure algorithm from a queue-length/delay perspective.},
keywords={cellular radio;iterative methods;OFDM modulation;scheduling;telecommunication channels;telecommunication network routing;buffer usage;delay;queue-length-delay perspective;many-channel large-deviations simulations;many-channel large-deviations analysis;half-duplex relays;bounded arrivals;general arrivals;full-duplex relays;downlink multichannel relay networks;iterative MaxWeight algorithm;BackPressure algorithm;OFDM-based cellular systems;relay-assisted multichannel downlink wireless networks;routing algorithms;scheduling algorithms;multichannel relay networks;Relays;Downlink;Routing;Resource management;Indexes;Delays;Algorithm design and analysis;Wireless Scheduling and Routing;Downlink Relay Networks},
doi={10.1109/INFCOM.2013.6566949},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566950,
author={H. Zeng and Y. Shi and Y. T. Hou and W. Lou},
booktitle={2013 Proceedings IEEE INFOCOM},
title={An efficient DoF scheduling algorithm for multi-hop MIMO networks},
year={2013},
volume={},
number={},
pages={1564-1554},
abstract={Degree-of-Freedom (DoF)-based model is a simple yet powerful tool to analyze MIMO's spatial multiplexing (SM) and interference cancellation (IC) capabilities in a multi-hop network. Recently, a new DoF model was proposed and was shown to achieve the same rate region as the matrix-based model (under SM and IC). The essence of this new DoF model is a novel node ordering concept, which eliminates potential duplication of DoF allocation for IC. In this paper, we investigate DoF scheduling for a multi-hop MIMO network based on this new DoF model. Specifically, we study how to perform DoF allocation among the nodes for SM and IC so as to maximize the minimum rate among a set of sessions. We formulate this problem as a mixed integer linear programming (MILP) and develop an efficient DoF scheduling algorithm to solve it. We show that our algorithm is amenable to local implementation and has polynomial time complexity. More importantly, it guarantees the feasibility of final solution (upon algorithm termination), despite that node ordering establishment and adjustment are performed locally. Simulation results show that our algorithm can offer a result that is close to an upper bound found by CPLEX solver, thus showing that the result found by our algorithm is highly competitive.},
keywords={integer programming;interference suppression;linear programming;MIMO communication;space division multiplexing;DoF scheduling algorithm;multihop MIMO network;degree-of-freedom-based model;spatial multiplexing;interference cancellation;matrix-based model;node ordering concept;DoF allocation;mixed integer linear programming;MILP;polynomial time complexity;CPLEX solver;Interference;Resource management;Integrated circuit modeling;MIMO;Random access memory;Spread spectrum communication},
doi={10.1109/INFCOM.2013.6566950},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566951,
author={H. Seferoglu and E. Modiano},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Diff-Max: Separation of routing and scheduling in backpressure-based wireless networks},
year={2013},
volume={},
number={},
pages={1555-1563},
abstract={Backpressure routing and scheduling, with throughput-optimal operation guarantee, is a promising technique to improve throughput in wireless multi-hop networks. Although backpressure is conceptually viewed as layered, the decisions of routing and scheduling are made jointly, which imposes several challenges in practice. In this work, we present Diff-Max, an approach that separates routing and scheduling and has three strengths: (i) Diff-Max improves throughput significantly, (ii) the separation of routing and scheduling makes practical implementation easier by minimizing cross-layer operations; i.e., routing is implemented in the network layer and scheduling is implemented in the link layer, and (iii) the separation of routing and scheduling leads to modularity; i.e., routing and scheduling are independent modules in Diff-Max, and one can continue to operate even if the other does not. Our approach is grounded in a network utility maximization (NUM) formulation and its solution. Based on the structure of Diff-Max, we propose two practical schemes: Diff-subMax and wDiff-subMax. We demonstrate the benefits of our schemes through simulation in ns-2.},
keywords={radio networks;scheduling;telecommunication network routing;Diff-Max;scheduling;backpressure-based wireless networks;backpressure routing;wireless multihop networks;cross-layer operations;network utility maximization;NUM formulation;Routing;Scheduling;Scheduling algorithms;Algorithm design and analysis;Vectors;Wireless networks;Joints},
doi={10.1109/INFCOM.2013.6566951},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566952,
author={H. Li and K. Wu and Q. Zhang and L. M. Ni},
booktitle={2013 Proceedings IEEE INFOCOM},
title={CUTS: Improving channel utilization in both time and spatial domains in WLANs},
year={2013},
volume={},
number={},
pages={1564-1572},
abstract={Improving channel utilization is a well-known issue in wireless networks. In traditional point-to-point wireless communication, significant efforts had been made by the existing study on enhancing the utilization of the channel access time. However, in the emerging wireless network using MU-MIMO, considering only the time domain in channel utilization is not sufficient. As multiple transmitters are allowed to transmit packets simultaneously to the same AP, allowing more antennas at AP would lead to higher channel utilization. Thus the channel utilization in MU-MIMO should consider both time and spatial domains, i.e., the channel access time and the antenna usage, which has not been considered in the existing methods. In this paper, we point out that the fundamental problem is lacking of the antenna information of contention nodes in channel contention. To address this issue, we propose a new MAC-PHY architecture design, CUTS, to utilize the channel in both domains. Particularly, CUTS adopts interference nulling to attach the antenna information in channel contention. Meanwhile, techniques such as channel contention in frequency domain and ACK in frequency domain using self-jamming are adopted. Through the software defined radio based real experiments and extensive simulations, we demonstrate the feasibility of our design and illustrate that CUTS provides better channel utilization with the gain over IEEE 802.11 reaching up to 470%.},
keywords={antenna arrays;interference suppression;MIMO communication;software radio;time-frequency analysis;wireless channels;wireless LAN;CUTS;channel utilization;spatial domains;WLAN;time domains;wireless networks;point-to-point wireless communication;channel access time;MU-MIMO;multiple transmitters;AP;antenna usage;contention node antenna information;MAC-PHY architecture design;interference nulling;channel contention;frequency domain;ACK;self-jamming;software defined radio;IEEE 802.11;Transmitting antennas;Receiving antennas;Frequency-domain analysis;Wireless networks;Interference},
doi={10.1109/INFCOM.2013.6566952},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566953,
author={J. Herzen and R. Merz and P. Thiran},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Distributed spectrum assignment for home WLANs},
year={2013},
volume={},
number={},
pages={1573-1581},
abstract={We consider the problem of jointly allocating channel center frequencies and bandwidths for IEEE 802.11 wireless LANs (WLANs). The bandwidth used on a link affects significantly both the capacity experienced on this link and the interference produced on neighboring links. Therefore, when jointly assigning both center frequencies and channel widths, there is a trade-off between interference mitigation and the potential capacity offered on each link. We study this tradeoff and we present SAW (spectrum assignment for WLANs), a decentralized algorithm that finds efficient configurations. SAW is tailored for 802.11 home networks. It is distributed, online and transparent. It does not require a central coordinator and it constantly adapts the spectrum usage without disrupting network traffic. A key feature of SAW is that the access points (APs) need only a few out-of-band measurements in order to make spectrum allocation decisions. Despite being completely decentralized, the algorithm is self-organizing and provably converges towards efficient spectrum allocations. We evaluate SAW using both simulation and a deployment on an indoor testbed composed of off-the-shelf 802.11 hardware. We observe that it dramatically increases the overall network efficiency and fairness.},
keywords={home networks;radio links;radio spectrum management;radiofrequency interference;radiofrequency measurement;wireless channels;wireless LAN;distributed spectrum assignment;home WLAN;channel center frequencies allocation;IEEE 802.11 wireless LAN;channel widths;interference mitigation;SAW;IEEE 802.11 home networks;network traffic;access points;AP;out-of-band measurements;spectrum allocation decisions;indoor testbed;off-the-shelf 802.11 hardware;network efficiency;Bandwidth;Interference;Surface acoustic waves;IEEE 802.11 Standards;Resource management;Monitoring;Radio spectrum management},
doi={10.1109/INFCOM.2013.6566953},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566954,
author={O. Bejarano and E. W. Knightly},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Virtual MISO triggers in Wi-Fi-like networks},
year={2013},
volume={},
number={},
pages={1582-1590},
abstract={Virtual Multiple-Input Single-Output (vMISO) systems distribute multi-antenna diversity capabilities between a sending and a cooperating node. vMISO has the potential to vastly improve wireless link reliability and bit error rates by exploiting spatial diversity. In this paper, we present the first design and experimental evaluation of vMISO triggers (when to invoke vMISO rather than traditional transmission) in Wi-Fi networking environments. We consider the joint effect of gains obtained at the physical layer with MAC and network-scale factors and show that 802.11 MAC mechanisms represent a major bottleneck to realizing gains that can be attained by a vMISO PHY. In contrast, we show how vMISO alters node interconnectivity and coordination and therefore can vastly transform the network throughput distribution in beneficial ways that are not described merely by vMISO link gains. Moreover, we show how to avoid triggering vMISO when the increased spatial footprint of the new cooperator would excessively hinder other flows' performance. In this paper, we build the first multi-flow vMISO testbed and explore the trigger criteria that are essential to attain substantial gains in a fully integrated vMISO system. We find that the largest gains are achieved by a largely isolated flow (gains of 110%) whereas cooperator interference and contention effects are pronounced in larger topologies, limiting typical gains to 14%.},
keywords={access protocols;diversity reception;error statistics;probability;telecommunication standards;wireless LAN;virtual MISO triggers;Wi-Fi-like networks;virtual multiple-input single-output systems;distribute multi-antenna diversity;wireless link reliability;bit error rates;spatial diversity;Wi-Fi networking environments;physical layer;network-scale factors;802.11 MAC mechanisms;cooperator interference;contention effects;Protocols;Throughput;Signal to noise ratio;Receivers;Gain;Fading;Modulation},
doi={10.1109/INFCOM.2013.6566954},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566955,
author={Y. Yao and L. Rao and X. Liu and X. Zhou},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Delay analysis and study of IEEE 802.11p based DSRC safety communication in a highway environment},
year={2013},
volume={},
number={},
pages={1591-1599},
abstract={As a key enabling technology for the next generation inter-vehicle safety communications, The IEEE 802.11p protocol is currently attracting much attention. Many inter-vehicle safety communications have stringent real-time requirements on broadcast messages to ensure drivers have enough reaction time toward emergencies. Most existing studies only focus on the average delay performance of IEEE 802.11p, which only contains very limited information of the real capacity for inter-vehicle communication. In this paper, we propose an analytical model, showing the performance of broadcast under IEEE 802.11p in terms of the mean, deviation and probability distribution of the MAC access delay. Comparison with the NS-2 simulations validates the accuracy of the proposed analytical model. In addition, we show that the exponential distribution is a good approximation to the MAC access delay distribution. Numerical analysis indicates that the QoS support in IEEE 802.11p can provide relatively good performance guarantee for higher priority messages while fails to meet the real-time requirements of the lower priority messages.},
keywords={access protocols;automated highways;emergency management;exponential distribution;next generation networks;numerical analysis;quality of service;radio broadcasting;road safety;road vehicles;telecommunication network reliability;vehicular ad hoc networks;delay analysis;DSRC safety communication;highway environment;next generation intervehicle safety communication;IEEE 802.11p protocol;real-time requirement;broadcast message;driver reaction time;emergencies;average delay performance;broadcast performance;mean;deviation;probability distribution;NS-2 simulation;exponential distribution;MAC access delay distribution;numerical analysis;QoS support;priority message;VANET;Safety;Delays;Vehicles;Markov processes;Analytical models;Protocols;Quality of service},
doi={10.1109/INFCOM.2013.6566955},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566956,
author={Y. Zheng and N. B. Shroff and P. Sinha},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A new analytical technique for designing provably efficient MapReduce schedulers},
year={2013},
volume={},
number={},
pages={1600-1608},
abstract={With the rapid increase in size and number of jobs that are being processed in the MapReduce framework, efficiently scheduling jobs under this framework is becoming increasingly important. We consider the problem of minimizing the total flowtime of a sequence of jobs in the MapReduce framework, where the jobs arrive over time and need to be processed through both Map and Reduce procedures before leaving the system. We show that for this problem for non-preemptive tasks, no on-line algorithm can achieve a constant competitive ratio (defined as the ratio between the completion time of the online algorithm to the completion time of the optimal non-causal off-line algorithm). We then construct a slightly weaker metric of performance called the efficiency ratio. An online algorithm is said to achieve an efficiency ratio of γ when the flow-time incurred by that scheduler divided by the minimum flow-time achieved over all possible schedulers is almost surely less than or equal to γ. Under some weak assumptions, we then show a surprising property that, for the flow-time problem, any work-conserving scheduler has a constant efficiency ratio in both preemptive and nonpreemptive scenarios. More importantly, we are able to develop an online scheduler with a very small efficiency ratio (2), and through simulations we show that it outperforms the state-of-the-art schedulers.},
keywords={data handling;parallel algorithms;parallel programming;scheduling;online scheduler;constant efficiency ratio;work-conserving scheduler;flow-time problem;optimal noncausal off-line algorithm;completion time;online algorithm;nonpreemptive tasks;job scheduling;MapReduce framework;MapReduce schedulers;Delays;Algorithm design and analysis;Scheduling algorithms;Schedules;Writing},
doi={10.1109/INFCOM.2013.6566956},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566957,
author={W. Wang and K. Zhu and L. Ying and J. Tan and L. Zhang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Map task scheduling in MapReduce with data locality: Throughput and heavy-traffic optimality},
year={2013},
volume={},
number={},
pages={1609-1617},
abstract={Scheduling map tasks to improve data locality is crucial to the performance of MapReduce. Many works have been devoted to increasing data locality for better efficiency. However, to the best of our knowledge, fundamental limits of MapReduce computing clusters with data locality, including the capacity region and theoretical bounds on the delay performance, have not been studied. In this paper, we address these problems from a stochastic network perspective. Our focus is to strike the right balance between data-locality and load-balancing to simultaneously maximize throughput and minimize delay. We present a new queueing architecture and propose a map task scheduling algorithm constituted by the Join the Shortest Queue policy together with the MaxWeight policy. We identify an outer bound on the capacity region, and then prove that the proposed algorithm stabilizes any arrival rate vector strictly within this outer bound. It shows that the algorithm is throughput optimal and the outer bound coincides with the actual capacity region. Further, we study the number of backlogged tasks under the proposed algorithm, which is directly related to the delay performance based on Little's law. We prove that the proposed algorithm is heavy-traffic optimal, i.e., it asymptotically minimizes the number of backlogged tasks as the arrival rate vector approaches the boundary of the capacity region. Therefore, the proposed algorithm is also delay optimal in the heavy-traffic regime.},
keywords={minimisation;parallel algorithms;queueing theory;resource allocation;scheduling;software architecture;vectors;map task scheduling algorithm;MapReduce computing clusters;data locality improvement;throughput optimality;heavy-traffic optimality;stochastic network perspective;load-balancing;throughput maximization;delay minimization;queueing architecture;join-the-shortest queue policy;MaxWeight policy;actual capacity region;arrival rate vector stabilization;backlogged task number minimization;Little law;delay performance;Scheduling algorithms;Vectors;Throughput;Delays;Routing;Markov processes;Computational modeling},
doi={10.1109/INFCOM.2013.6566957},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566958,
author={J. Tan and X. Meng and L. Zhang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Coupling task progress for MapReduce resource-aware scheduling},
year={2013},
volume={},
number={},
pages={1618-1626},
abstract={Schedulers are critical in enhancing the performance of MapReduce/Hadoop in presence of multiple jobs with different characteristics and performance goals. Though current schedulers for Hadoop are quite successful, they still have room for improvement: map tasks (MapTasks) and reduce tasks (ReduceTasks) are not jointly optimized, albeit there is a strong dependence between them. This can cause job starvation and unfavorable data locality. In this paper, we design and implement a resource-aware scheduler for Hadoop. It couples the progresses of MapTasks and ReduceTasks, utilizing Wait Scheduling for ReduceTasks and Random Peeking Scheduling for MapTasks to jointly optimize the task placement. This mitigates the starvation problem and improves the overall data locality. Our extensive experiments demonstrate significant improvements in job response times.},
keywords={resource allocation;task placement;random peeking scheduling;wait scheduling;ReduceTasks;MapTasks;Hadoop;MapReduce resource-aware scheduling;coupling task progress;Couplings;Delays;Heart beat;Processor scheduling;Synchronization;Time factors;Instruction sets},
doi={10.1109/INFCOM.2013.6566958},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566959,
author={J. Tan and S. Meng and X. Meng and L. Zhang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Improving ReduceTask data locality for sequential MapReduce jobs},
year={2013},
volume={},
number={},
pages={1627-1635},
abstract={Improving data locality for MapReduce jobs is critical for the performance of large-scale Hadoop clusters, embodying the principle of moving computation close to data for big data platforms. Scheduling tasks in the vicinity of stored data can significantly diminish network traffic, which is crucial for system stability and efficiency. Though the issue on data locality has been investigated extensively for MapTasks, most of the existing schedulers ignore data locality for ReduceTasks when fetching the intermediate data, causing performance degradation. This problem of reducing the fetching cost for ReduceTasks has been identified recently. However, the proposed solutions are exclusively based on a greedy approach, relying on the intuition to place ReduceTasks to the slots that are closest to the majority of the already generated intermediate data. The consequence is that, in presence of job arrivals and departures, assigning the ReduceTasks of the current job to the nodes with the lowest fetching cost can prevent a subsequent job with even better match of data locality from being launched on the already taken slots. To this end, we formulate a stochastic optimization framework to improve the data locality for ReduceTasks, with the optimal placement policy exhibiting a threshold-based structure. In order to ease the implementation, we further propose a receding horizon control policy based on the optimal solution under restricted conditions. The improved performance is further validated through simulation experiments and real performance tests on our testbed.},
keywords={data handling;predictive control;scheduling;stochastic programming;ReduceTask data locality improvement;sequential MapReduce jobs;large-scale Hadoop clusters;moving computation principle;task scheduling;network traffic;MapTask;intermediate data fetching;fetching cost reduction;greedy approach;job arrivals;job departure;stochastic optimization framework;optimal placement policy;threshold-based structure;receding horizon control policy;Bismuth;Virtual machining;Network topology;Random variables;Indexes;Optimization;System performance},
doi={10.1109/INFCOM.2013.6566959},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566960,
author={L. Xie and Y. Shi and Y. T. Hou and W. Lou and H. D. Sherali and S. F. Midkiff},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Bundling mobile base station and wireless energy transfer: Modeling and optimization},
year={2013},
volume={},
number={},
pages={1636-1644},
abstract={Wireless energy transfer is a promising technology to fundamentally address energy and lifetime problems in a wireless sensor network (WSN). On the other hand, it has been well recognized that a mobile base station has significant advantages over a static one. In this paper, we study the interesting problem of co-locating the mobile base station on the wireless charging vehicle (WCV). The goal is to minimize energy consumption of the entire system while ensuring none of the sensor nodes runs out of energy. We develop a mathematical model for this complex problem. Instead of studying the general problem formulation (OPT-t), which is time-dependent, we show that it is sufficient to study a special subproblem (OPT-s) which only involves space-dependent variables. Subsequently, we develop a provably near-optimal solution to OPT-s. The novelty of this research mainly resides in the development of several solution techniques to tackle a complex problem that is seemingly intractable at first glance. In addition to addressing a challenging and interesting problem in a WSN, we expect the techniques developed in this research can be applied to address other related networking problems involving time-dependent movement, flow routing, and energy consumption.},
keywords={electric vehicles;minimisation;mobile radio;telecommunication network routing;telecommunication power management;wireless sensor networks;networking problems;flow routing;time-dependent movement;space-dependent variables;OPT-s;special subproblem;OPT-t;general problem formulation;mathematical model;sensor nodes;energy consumption minimization;WCV;wireless charging vehicle;colocation problem;WSN;wireless sensor network;lifetime problem;energy problem;wireless energy transfer;mobile base station bundling;Energy consumption;Wireless sensor networks;Base stations;Routing;Mobile communication;Wireless communication;Vehicles},
doi={10.1109/INFCOM.2013.6566960},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566961,
author={X. Fang and H. Gao and J. Li and Y. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Application-aware data collection in Wireless Sensor Networks},
year={2013},
volume={},
number={},
pages={1645-1653},
abstract={Data sharing for data collection among multiple applications is an efficient way to reduce the communication cost of Wireless Sensor Networks (WSNs). This paper is the first work to introduce the interval data sharing problem which is to investigate how to transmit as less data as possible over the network, and meanwhile the transmitted data satisfies the requirements of all the applications. Different from current studies where each application requires a single data sampling during each task, we study the problem where each application requires a continuous interval of data sampling in each task instead. The proposed problem is a nonlinear nonconvex optimization problem. In order to lower the high complexity for solving a nonlinear nonconvex optimization problem in resource restricted sensor nodes, a 2-factor approximation algorithm whose time complexity is O(n<sup>2</sup>) and memory complexity is O(n) is provided. A special instance of this problem is also analyzed. This special instance can be solved with a dynamic programming algorithm in polynomial time, which gives an optimal result in O(n<sup>2</sup>) time complexity and O(n) memory complexity. We evaluate the proposed algorithms with TOSSIM, a widely used simulation tool in WSNs. Theoretical analysis and simulation results both demonstrate the effectiveness of the proposed algorithms.},
keywords={wireless sensor networks;application-aware data collection;wireless sensor networks;data sharing;data sampling;nonlinear nonconvex optimization problem;resource restricted sensor nodes;Approximation algorithms;Approximation methods;Heuristic algorithms;Time complexity;Wireless sensor networks;Optimization},
doi={10.1109/INFCOM.2013.6566961},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566962,
author={L. Kong and M. Xia and X. Liu and M. Wu and X. Liu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Data loss and reconstruction in sensor networks},
year={2013},
volume={},
number={},
pages={1654-1662},
abstract={Reconstructing the environment in cyber space by sensory data is a fundamental operation for understanding the physical world in depth. A lot of basic scientific work (e.g., nature discovery, organic evolution) heavily relies on the accuracy of environment reconstruction. However, data loss in wireless sensor networks is common and has its special patterns due to noise, collision, unreliable link, and unexpected damage, which greatly reduces the accuracy of reconstruction. Existing interpolation methods do not consider these patterns and thus fail to provide a satisfactory accuracy when missing data become large. To address this problem, this paper proposes a novel approach based on compressive sensing to reconstruct the massive missing data. Firstly, we analyze the real sensory data from Intel Indoor, GreenOrbs, and Ocean Sense projects. They all exhibit the features of spatial correlation, temporal stability and low-rank structure. Motivated by these observations, we then develop an environmental space time improved compressive sensing (ESTICS) algorithm to optimize the missing data estimation. Finally, the extensive experiments with real-world sensory data shows that the proposed approach significantly outperforms existing solutions in terms of reconstruction accuracy. Typically, ESTICS can successfully reconstruct the environment with less than 20% error in face of 90% missing data.},
keywords={compressed sensing;signal reconstruction;wireless sensor networks;data loss;sensory data;wireless sensor networks;compressive sensing;massive missing data reconstruction;Intel Indoor project;GreenOrbs project;Ocean Sense project;spatial correlation;temporal stability;low rank structure;ESTICS algorithm;Wireless sensor networks;Compressed sensing;Interpolation;Estimation;Ocean temperature;Temperature sensors},
doi={10.1109/INFCOM.2013.6566962},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566963,
author={G. Li and J. Li and H. Gao},
booktitle={2013 Proceedings IEEE INFOCOM},
title={ε-Approximation to data streams in sensor networks},
year={2013},
volume={},
number={},
pages={1663-1671},
abstract={The rapid development in processor, memory, and radio technology have contributed to the furtherance of decentralized sensor networks of small, inexpensive nodes that are capable of sensing, computation, and wireless communication. Due to the characteristic of limited communication bandwidth and other resource constraints of sensor networks, an important and practical demand is to compress time series data generated by sensor nodes with precision guarantee in an online manner. Although a large number of data compression algorithms have been proposed to reduce data volume, their offline characteristic or super-linear time complexity prevents them from being applied directly on time series data generated by sensor nodes. To remedy the deficiencies of previous methods, we propose an optimal online algorithm GDPLA for constructing a disconnected piecewise linear approximation representation of a time series which guarantees that the vertical distance between each real data point and the corresponding fit line is less than or equal to ε. GDPLA not only generates the minimum number of segments to approximate a time series with precision guarantee, but also only requires linear time O(n) bounded by a constant coefficient 6, where unit 1 denotes the time complexity of comparing the slopes of two lines. The low cost characteristic of our method makes it the popular choice for resource-constrained sensor networks. Extensive experiments on a real dataset have been conducted to demonstrate the superior compression performance of our approach.},
keywords={approximation theory;time series;wireless sensor networks;ε-approximation;data streams;radio technology;decentralized sensor networks;inexpensive nodes;wireless communication;limited communication bandwidth;resource constraints;time series data compression;sensor nodes;data volume reduction;super-linear time complexity;GDPLA optimal online algorithm;disconnected piecewise linear approximation representation;data point;fit line;resource-constrained sensor networks;Time series analysis;Piecewise linear approximation;Approximation algorithms;Time complexity;Linear approximation;Arrays},
doi={10.1109/INFCOM.2013.6566963},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566964,
author={O. Argon and Y. Shavitt and U. Weinsberg},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Inferring the periodicity in large-scale Internet measurements},
year={2013},
volume={},
number={},
pages={1672-1680},
abstract={Many Internet events exhibit periodical patterns. Such events include the availability of end-hosts, usage of internetwork links for balancing load and cost of transit, traffic shaping during peak hours, etc. Internet monitoring systems that collect huge amount of data can leverage periodicity information for improving resource utilization. However, automatic periodicity inference is a non trivial task, especially when facing measurement “noise”. In this paper we present two methods for assessing the periodicity of network events and inferring their periodical patterns. The first method uses Power Spectral Density for inferring a single dominant period that exists in a signal representing the sampling process. This method is highly robust to noise, but is most useful for single-period processes. Thus, we present a novel method for detecting multiple periods that comprise a single process, using iterative relaxation of the time-domain autocorrelation function. We evaluate these methods using extensive simulations, and show their applicability on real Internet measurements of end-host availability and IP address alternations.},
keywords={computer network performance evaluation;Internet;resource allocation;telecommunication traffic;end-host availability;IP address alternations;iterative relaxation;time-domain autocorrelation function;sampling process;power spectral density;automatic periodicity inference;resource utilization;Internet monitoring systems;traffic shaping;transit cost;load balancing;internetwork links;Internet events;large-scale Internet measurements;Phase noise;Harmonic analysis;Internet;Discrete Fourier transforms;Robustness;Time-domain analysis},
doi={10.1109/INFCOM.2013.6566964},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566965,
author={X. Fan and J. Heidemann and R. Govindan},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Evaluating anycast in the domain name system},
year={2013},
volume={},
number={},
pages={1681-1689},
abstract={IP anycast is a central part of production DNS. While prior work has explored proximity, affinity and load balancing for some anycast services, there has been little attention to third-party discovery and enumeration of components of an anycast service. Enumeration can reveal abnormal service configurations, benign masquerading or hostile hijacking of anycast services, and help characterize anycast deployment. In this paper, we discuss two methods to identify and characterize anycast nodes. The first uses an existing anycast diagnosis method based on CHAOS-class DNS records but augments it with traceroute to resolve ambiguities. The second proposes Internet-class DNS records which permit accurate discovery through the use of existing recursive DNS infrastructure. We validate these two methods against three widely-used anycast DNS services, using a very large number (60k and 300k) of vantage points, and show that they can provide excellent precision and recall. Finally, we use these methods to evaluate anycast deployments in top-level domains (TLDs), and find one case where a third-party operates a server masquerading as a root DNS anycast node as well as a noticeable proportion of unusual DNS proxies. We also show that, across all TLDs, up to 72% use anycast.},
keywords={Internet;IP networks;anycast evaluation;domain name system;IP anycast;production DNS;anycast services;third-party discovery;abnormal service configurations;benign masquerading;hostile hijacking;anycast services;CHAOS-class DNS records;Internet-class DNS records;recursive DNS infrastructure;top-level domains;TLD;root DNS anycast node;DNS proxies;Servers;Chaos;Peer-to-peer computing;IP networks;Standards;Extraterrestrial measurements;Routing},
doi={10.1109/INFCOM.2013.6566965},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566966,
author={Y. Chen and G. M. Lee and N. Duffield and L. Qiu and J. Wang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Event detection using customer care calls},
year={2013},
volume={},
number={},
pages={1690-1698},
abstract={Customer care calls serve as a direct channel for a service provider to learn feedbacks from their customers. They reveal details about the nature and impact of major events and problems observed by customers. By analyzing the customer care calls, a service provider can detect important events to speed up problem resolution. However, automating event detection based on customer care calls poses several significant challenges. First, the relationship between customers' calls and network events is blurred because customers respond to an event in different ways. Second, customer care calls can be labeled inconsistently across agents and across call centers, and a given event naturally give rise to calls spanning a number of categories. Third, many important events cannot be detected by looking at calls in one category. How to aggregate calls from different categories for event detection is important but challenging. Lastly, customer care call records have high dimensions (e.g., thousands of categories in our dataset). In this paper, we propose a systematic method for detecting events in a major cellular network using customer care call data. It consists of three main components: (i) using a regression approach that exploits temporal stability and low-rank properties to automatically learn the relationship between customer calls and major events, (ii) reducing the number of unknowns by clustering call categories and using L1norm minimization to identify important categories, and (iii) employing multiple classifiers to enhance the robustness against noise and different response time. For the detected events, we leverage Twitter social media to summarize them and to locate the impacted regions. We show the effectiveness of our approach using data from a large cellular service provider in the US.},
keywords={call centres;customer satisfaction;customer services;pattern clustering;regression analysis;social networking (online);customer feedbacks;problem resolution;automatic event detection;network events;customer care call records;systematic method;cellular network;regression approach;temporal stability;low-rank properties;call category clustering;L1norm minimization;Twitter social media;US cellular service provider;Testing;Principal component analysis;Training;Noise;Scalability;Time factors;Measurement},
doi={10.1109/INFCOM.2013.6566966},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566967,
author={M. Malboubi and C. Vu and C. Chuah and P. Sharma},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Decentralizing network inference problems with Multiple-Description Fusion Estimation (MDFE)},
year={2013},
volume={},
number={},
pages={1699-1707},
abstract={Two forms of network inference (or tomography) problems have been studied rigorously: (a) traffic matrix estimation or completion based on link-level traffic measurements, and (b) link-level loss or delay inference based on end-to-end measurements. These problems are often posed as underdetermined linear inverse (UDLI) problems and solved in a centralized manner, where all the measurements are collected at a central node, which then applies a variety of inference techniques to estimate the attributes of interest. This paper proposes a novel framework for decentralizing these large-scale UDLI network inference problems by intelligently partitioning it into smaller sub-problems and solving them independently and in parallel. The resulting estimates, referred to as multiple descriptions, can then be fused together to compute the global estimate. We apply this Multiple Description and Fusion Estimation (MDFE) framework to three classical problems: traffic matrix estimation, traffic matrix completion, and loss inference. Using real topologies and traces, we demonstrate how MDFE can speed up computation time while maintaining (even improving) the estimation accuracy and how it enhances robustness against noise and failures. We also show that our MDFE framework is compatible with a variety of existing inference techniques used to solve the UDLI problems.},
keywords={delays;inference mechanisms;Internet;matrix algebra;telecommunication traffic;decentralizing network inference problem;multiple-description fusion estimation;MDFE;link-level traffic measurements;link-level loss;delay inference;end-to-end measurement;under-determined linear inverse;UDLI network inference problem;traffic matrix estimation;traffic matrix completion;MDFE framework;inference technique;Internet;Estimation;Partitioning algorithms;Accuracy;Loss measurement;Robustness;Complexity theory;Clustering algorithms},
doi={10.1109/INFCOM.2013.6566967},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566968,
author={A. Giannoulis and P. Patras and E. W. Knightly},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Mobile Access of Wide-Spectrum Networks: Design, deployment and experimental evaluation},
year={2013},
volume={},
number={},
pages={1708-1716},
abstract={Wireless networks increasingly utilize diverse spectral bands that exhibit vast differences in both transmission range and usage. In this work, we present MAWS (Mobile Access of Wide-Spectrum Networks), the first scheme designed for mobile clients to evaluate and select both APs and spectral bands in wide-spectrum networks. Because of the potentially vast number of spectrum and AP options, scanning may be prohibitive. Consequently, our key technique is for clients to infer channel quality and spectral usage for their current location and bands using limited measurements collected in other bands and at other locations. We experimentally evaluate MAWS via a widespectrum network that we deploy, a testbed providing access to four bands at 700 MHz, 900 MHz, 2.4 GHz and 5 GHz. To the best of our knowledge, the spectrum of these bands is the widest to be spanned to date by a single operational access network. A key finding of our evaluation is that under a diverse set of operating conditions, mobile clients can accurately predict their performance without a direct measurement at their current location and spectral bands.},
keywords={mobile radio;radio access networks;radio spectrum management;wireless networks;mobile access of wide-spectrum networks;MAWS;mobile clients;AP;single operational access network;access point;channel quality;frequency 700 MHz;frequency 2.4 GHz;frequency 5 GHz;frequency 900 MHz;Mobile communication;Delays;Frequency measurement;Channel estimation;Mobile computing;Interference},
doi={10.1109/INFCOM.2013.6566968},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566969,
author={H. Li and C. Wu and Z. Li and F. C. M. Lau},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Socially-optimal multi-hop secondary communication under arbitrary primary user mechanisms},
year={2013},
volume={},
number={},
pages={1717-1725},
abstract={In a cognitive radio system, licensed primary users can lease idle spectrum to secondary users for monetary remuneration. Secondary users acquire available spectrum for their data delivery needs, with the goal of achieving high throughput and low spectrum charges. Maximizing such a net utility (throughput utility minus spectrum cost) is a central problem faced by a multihop secondary network. Optimal decision making is challenging, since it involves multiple data flows, cross-layer coordination, and economic constraints (budgets of sources). The picture is further complicated by the inter-play between secondary data communication and primary spectrum leasing mechanisms. This work is the first to investigate the full spectrum of socially optimal secondary user communication. We design a social welfare maximization framework for multi-session multi-hop secondary data dissemination based on Lyapunov optimization techniques. A salient feature of the framework is that it takes any given primary user mechanism as input, and produces correspondingly a dynamic, distributed rate control, routing, and spectrum allocation and pricing protocol that can achieve longterm maximization of the overall system utility. Through rigorous theoretical analysis, we prove that our online protocol can achieve a social welfare that is arbitrarily close to the offline optimum, with only finite buffer space requirement at each secondary user, and guarantee of no buffer overflow. Empirical studies are conducted to examine the performance of the protocol.},
keywords={cognitive radio;decision making;Lyapunov methods;optimisation;finite buffer space requirement;online protocol;system utility;pricing protocol;spectrum allocation;distributed rate control;Lyapunov optimization technique;multisession multihop secondary data dissemination;social welfare maximization framework;secondary user communication;economic constraints;cross layer coordination;multiple data flow;optimal decision making;multihop secondary network;net utility;data delivery needs;monetary remuneration;idle spectrum;cognitive radio system;arbitrary primary user mechanism;socially optimal multihop secondary communication;Heuristic algorithms;Protocols;Optimization;Routing;Throughput;Unicast;Algorithm design and analysis},
doi={10.1109/INFCOM.2013.6566969},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566970,
author={B. Cao and J. W. Mark and Q. Zhang and R. Lu and X. Lin and X. Sherman Shen},
booktitle={2013 Proceedings IEEE INFOCOM},
title={On optimal communication strategies for cooperative cognitive radio networking},
year={2013},
volume={},
number={},
pages={1726-1734},
abstract={This work is concerned with enhancement of spectrum-energy efficiency whereby a primary user (PU) engages secondary users (SUs) to relay its transmission in an energy-aware cognitive radio network, i.e., forming a cooperative cognitive radio network (CCRN). The cooperation framework in CCRN can be multiple two-hop relaying with or without PU's direct link transmission using an amplify-and-forward or decode- and-forward mode. In the energy-aware CCRN, an individual cooperating partner attempts to maximize its own utility. The partner selection and parameter optimization, led by the PU, are formulated as two Stackelberg games, namely a sum-constrained power allocation game for two-phase cooperation and a power control game for three-phase cooperation, respectively. Unique Nash Equilibrium is proved and achieved in analytical format for each game. The optimal communication strategy is chosen which achieves the maximum PU utility among different optimal communication strategies. Moreover, an implementation scheme is presented to perform the partner selection and parameter optimization based on the analytical results. Theoretical analysis and performance evaluation show that the proposed CCRN model is a promising framework under which the PU's utility is maximized, while the relaying SUs can attain acceptable utilities.},
keywords={amplify and forward communication;cognitive radio;cooperative communication;decode and forward communication;game theory;relay networks (telecommunication);optimal communication strategies;cooperative cognitive radio networking;spectrum-energy efficiency;primary user;PU;secondary users;SU;energy-aware cognitive radio network;CCRN;multiple two-hop relaying;decode- and-forward mode;amplify-and-forward mode;partner selection;parameter optimization;Stackelberg games;sum-constrained power allocation game;two-phase cooperation;power control game;three-phase cooperation;unique Nash equilibrium;Relays;Games;Throughput;Optimization;Resource management;Silicon;Cognitive radio},
doi={10.1109/INFCOM.2013.6566970},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566971,
author={S. Jeon and M. Gastpar},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Capacity scaling of cognitive networks: Beyond interference-limited communication},
year={2013},
volume={},
number={},
pages={1735-1743},
abstract={The capacity scaling laws of two overlaid networks sharing the same wireless resources with different priorities are investigated. The primary network is assumed to operate in an order-optimal fashion to achieve its standalone capacity scaling law. The secondary “cognitive” network must keep its interference to the primary network below a certain threshold while at the same time maximizing its own throughput scaling law based on cognition information. The existing scaling results for cognitive networks inherently assume multihop communication treating all other signals except from a single intended transmitter as noise. By contrast, in this paper, a general coding model is considered without any specific physical layer coding assumptions. Therefore, this paper provides a general framework for comprehensive understanding of fundamental limits on the capacity scaling laws of cognitive networks. For the extended network model, the capacity scaling laws of both the primary and secondary networks are completely characterized. For the dense network model, an improved throughput scaling law is achieved by inducing cooperation within the secondary network. In both cases, it turns out that the conventional multihop approach is in general quite suboptimal.},
keywords={cognitive radio;encoding;interference suppression;radio transmitters;secondary cognitive network capacity scaling;interference-limited communication;overlaid networks;primary network;order-optimal fashion;standalone capacity scaling law;cognition information;multihop communication;single intended transmitter;general coding model;physical layer coding assumptions;extended network model;Throughput;Interference;Protocols;Wireless communication;Physical layer;Upper bound;Spread spectrum communication},
doi={10.1109/INFCOM.2013.6566971},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566972,
author={Y. Jin and Y. Yi and G. Kesidis and F. Kocak and J. Shin},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Hybrid client-server and peer-to-peer caching systems with selfish peers},
year={2013},
volume={},
number={},
pages={1744-1752},
abstract={This paper considers a hybrid peer-to-peer (p2p) system, a dynamic distributed caching system with an authoritative server dispensing contents only if the contents fail to be found by searching an unstructured peer-to-peer (p2p) system. We study the case when some peers may not be fully cooperative in the search process and examine the impact of various noncooperative behaviors on the querying load on the server as the peer population size increases. We categorize selfish peers into three classes: impatient peers that directly query the server without searching the p2p system, non-forwarders that refuse to forward query requests, and non-resolvers that refuse to share contents. It is shown that in the hybrid p2p system, impatient and/or nonforwarding behaviors prevent the system from scaling well because of the high server load, while the system scales well under the non-resolving selfish peers. Our study implies that the hybrid p2p system does not mandate an incentive mechanism for content sharing, which is in stark contrast to unstructured p2p systems, where incentivizing peers to share contents is known to be a key factor for the system's scalability.},
keywords={cache storage;client-server systems;peer-to-peer computing;query processing;client-server system;peer-to-peer caching system;hybrid P2P system;dynamic distributed caching system;authoritative server;unstructured peer-to-peer system;peer noncooperative behavior;server querying load;peer population size;impatient peer;nonforwarder;query request forwarding;nonresolvers;content sharing;impatient behavior;nonforwarding behavior;server load;nonresolving selfish peers;incentive mechanism;peer incentive;system scalability;Servers;Peer-to-peer computing;Scalability;Probabilistic logic;Markov processes;Waste materials;Sociology},
doi={10.1109/INFCOM.2013.6566972},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566973,
author={F. Baccelli and F. Mathieu and I. Norros and R. Varloot},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Can P2P networks be super-scalable?},
year={2013},
volume={},
number={},
pages={1753-1761},
abstract={We propose a new model for peer-to-peer networking which takes the network bottlenecks into account beyond the access. This model can cope with key features of P2P networking like degree or locality constraints together with the fact that distant peers often have a smaller rate than nearby peers. Using a network model based on rate functions, we give a closed form expression of peers download performance in the system's fluid limit, as well as approximations for the other cases. Our results show the existence of realistic settings for which the average download time is a decreasing function of the load, a phenomenon that we call super-scalability.},
keywords={peer-to-peer computing;P2P networks;peer-to-peer networking;degree constraints;locality constraints;rate functions;system fluid limit;peers download performance closed form expression;superscalability phenomenon;Fluids;Peer-to-peer computing;Mathematical model;Bandwidth;Load modeling;Scalability;Availability},
doi={10.1109/INFCOM.2013.6566973},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566974,
author={Z. Yao and D. B. H. Cline and D. Loguinov},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Unstructured P2P link lifetimes redux},
year={2013},
volume={},
number={},
pages={1762-1770},
abstract={We revisit link lifetimes in random P2P graphs under dynamic node failure and create a unifying stochastic model that generalizes the majority of previous efforts in this direction. We not only allow non-exponential user lifetimes and age-dependent neighbor selection, but also cover both active and passive neighbor-management strategies, model the lifetimes of incoming and outgoing links, derive churn-related message volume of the system, and obtain the distribution of transient in/out degree at each user. We then discuss the impact of design parameters on overhead and resilience of the network.},
keywords={graph theory;peer-to-peer computing;stochastic processes;unstructured P2P;link lifetimes redux;random P2P graphs;dynamic node failure;stochastic model;neighbor-management strategies;Peer-to-peer computing;Resilience;Delays;Random variables;Shape;Linear approximation;Educational institutions},
doi={10.1109/INFCOM.2013.6566974},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566975,
author={K. Mokhtarian and H. Jacobsen},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Minimum-delay overlay multicast},
year={2013},
volume={},
number={},
pages={1771-1779},
abstract={Delivering delay-sensitive data to a group of receivers with minimum latency is a fundamental problem for various distributed applications. In this paper, we study multicast routing with minimum end-to-end delay to the receivers. The delay to each receiver in a multicast tree consist of the time that the data spends in overlay links as well as the latency incurred at each overlay node, which has to send out a piece of data several times over a finite-capacity network connection. The latter portion of the delay, which is proportional to the degree of nodes in the tree, can be a significant portion of the total delay as we show in the paper. Yet, it is often ignored or only partially addressed by previous multicast algorithms. We formulate the actual delay to the receivers in a multicast tree and consider minimizing the average and the maximum delay in the tree. We show the NP-hardness of these problems and prove that they cannot be approximated in polynomial time to within any reasonable approximation ratio. We then present a number of efficient algorithms to build a multicast tree in which the average or the maximum delay is minimized. These algorithms cover a wide range of overlay sizes for both versions of our problem. The effectiveness of our algorithms is demonstrated through comprehensive experiments on different real-world datasets, and using various overlay network models. The results confirm that our algorithms can achieve much lower delays (up to 60% less) and up to orders of magnitude faster running times (i.e., supporting larger scales) than previous minimum-delay multicast approaches.},
keywords={multicast communication;telecommunication network routing;trees (mathematics);minimum-delay overlay multicast;delay-sensitive data;receivers;multicast routing;end-to-end delay;multicast tree;finite-capacity network connection;Delays;Routing;Receivers;Approximation algorithms;Algorithm design and analysis;Approximation methods;Overlay networks},
doi={10.1109/INFCOM.2013.6566975},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566976,
author={H. Chen and Y. Chen and D. H. Summerville and Z. Su},
booktitle={2013 Proceedings IEEE INFOCOM},
title={An optimized design of reconfigurable PSD accelerator for online shrew DDoS attacks detection},
year={2013},
volume={},
number={},
pages={1780-1787},
abstract={Shrew Distributed Denial-of-Service (DDoS) attacks are stealthy, concealing their malicious activities in normal traffic. Although it is difficult to detect shrew DDoS attacks in the time domain, the existent energy exposes them in frequency domain. For this purpose, online Power Spectral Density (PSD) analysis necessitates real-time PSD data conversion. In this paper, an optimized FPGA based accelerator for real-time PSD conversion is proposed, which is based on our innovative component-reusable Auto-Correlation (AC) algorithm and the adapted 2N-point real-valued Discrete Fourier Transform (DFT) algorithm. Further optimization is achieved through the exploration of algorithm characteristics and hardware parallelism for this case. Evaluation results from both simulation and synthesis are provided. The overall design can be easily placed in a Xilinx Virtex2 Pro FGPA.},
keywords={computer network security;discrete Fourier transforms;electronic data interchange;field programmable gate arrays;frequency-domain analysis;parallel processing;reconfigurable architectures;telecommunication traffic;reconfigurable PSD accelerator optimized design;online shrew DDoS attack detection;shrew distributed denial-of-service attacks;malicious activities;normal traffic;frequency domain;online power spectral density analysis;real-time PSD data conversion;optimized FPGA based accelerator;component-reusable auto-correlation algorithm;AC algorithm;2N-point real-valued discrete Fourier transform algorithm;DFT algorithm;hardware parallelism;Xilinx Virtex2 Pro FGPA;Discrete Fourier transforms;Algorithm design and analysis;Computer crime;Field programmable gate arrays;Frequency-domain analysis;Real-time systems;Convolution},
doi={10.1109/INFCOM.2013.6566976},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566977,
author={W. Chen and Y. Liu and Y. Guan},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Cardinality change-based early detection of large-scale cyber-attacks},
year={2013},
volume={},
number={},
pages={1788-1796},
abstract={Cyber-attacks are happening every day, with a variety of behaviors and objects. For example, email spammers may compromise computers to sign-up millions of email accounts for sending spam emails; during worm spreading, each infected host may try to connect to many hosts to further spread the worm, etc. However, many such large-scale and often distributed cyber-attacks share a common characteristic that the activities involved in them result in changes in the cardinality of attack traffic. Examples include: the cardinality of the accounts signed up by a compromised host often increases in spam email delivery scenarios, and the cardinality of the connections made from a host may increase in worm spreading scenarios. In this paper, we focus on changes in the cardinality of the network/attack traffic that may indicate on-going cyber-attacks. We formulate this problem as cardinality-based change point detection in distributed streams of attack traffic, and develop a nonparametric error-bounded scheme for it. Our scheme supports the capability of merging information collected from multiple monitoring points to detect large-scale attacks. Also, our scheme uses small space as well as constant processing time, which makes it applicable for spaceconstrained network or security systems. We have conducted experiments using both real-world traces and synthetic data. Experimental results and theoretical analysis show that our scheme can detect changes in the cardinality within given time and error bounds. We expect the solutions of this work will be deployed as a building block in network and security monitoring systems to detect large distributed cyber attacks.},
keywords={nonparametric statistics;pattern recognition;security of data;unsolicited e-mail;cardinality change-based early detection;large-scale cyber-attack;email spammers;email accounts;spam email sending;worm spreading;infected host;distributed cyber-attack;attack traffic cardinality;spam email delivery scenario;connection cardinality;cardinality-based change point detection;distributed stream;nonparametric error-bounded scheme;information merging;space-constrained network;security system;error bound;network monitoring system;security monitoring system;large distributed cyber attack detection;Monitoring;Grippers;Radiation detectors;Electronic mail;Merging;IP networks;Time series analysis},
doi={10.1109/INFCOM.2013.6566977},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566978,
author={G. Tian and Z. Duan and T. Baumeister and Y. Dong},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A traceback attack on Freenet},
year={2013},
volume={},
number={},
pages={1797-1805},
abstract={Freenet is a popular peer to peer anonymous network, with the objective to provide the anonymity of both content publishers and retrievers. Despite more than a decade of active development and deployment and the adoption of well-established cryptographic algorithms in Freenet, it remains unanswered how well the anonymity objective of the initial Freenet design has been met. In this paper we develop a traceback attack on Freenet, and show that the originating machine of a content request message in Freenet can be identified; that is, the anonymity of a content retriever can be broken, even if a single request message has been issued by the retriever. We present the design of the traceback attack, and perform Emulab-based experiments to confirm the feasibility and effectiveness of the attack. With randomly chosen content requesters (and random contents stored in the Freenet testbed), the experiments show that, for 24% to 43% of the content request messages, we can identify their originating machines. We also briefly discuss potential solutions to address the developed traceback attack. Despite being developed specifically on Freenet, the basic principles of the traceback attack and solutions have important security implications for similar anonymous content sharing systems.},
keywords={computer network security;cryptography;peer-to-peer computing;peer-to-peer anonymous network;content publisher anonymity;content retriever anonymity;cryptographic algorithms;content request message;traceback attack design;Emulab-based experiments;random content requester selection;random content storage;anonymous content sharing systems;Freenet testbed;Peer-to-peer computing;Routing;Monitoring;Probes;Security;Algorithm design and analysis;Educational institutions},
doi={10.1109/INFCOM.2013.6566978},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566979,
author={H. Zhang and J. Shi and L. Ye and X. Du},
booktitle={2013 Proceedings IEEE INFOCOM},
title={PPBD: A piracy preventing system for BT DHT networks},
year={2013},
volume={},
number={},
pages={1806-1814},
abstract={In this paper, we study several important issues that can be used to prevent pirated content propagation in BitTorrent (BT) Distributed Hash-Tables (DHT) networks. We design a system called PPBD to stop pirated content propagation by utilizing several attacking methods. First, the system can efficiently deal with massive concurrent connections to reduce bandwidth consumption, schedule peers to cooperate and optimize the protection methods according to clients. Second, we construct two mathematical models for BT DHT attacks, and we theoretically analyze the system performance. Third, we take into account some countermeasures of different BT clients and make corresponding optimizations of our PPBD system. Our realworld experiments show that: (1) our system can extend the download duration at least three times by the fake-block attacking method and it is more effective in a small swarm; (2) DHT index poison and routing pollution methods can limit the sharing swarm to a small swarm.},
keywords={computer crime;computer network reliability;computer network security;cryptography;Internet;mathematical analysis;peer-to-peer computing;telecommunication network routing;piracy preventing system;BT DHT network;pirated content propagation;BitTorrent;distributed hash-table network;system design;concurrent connection;bandwidth consumption reduction;peer scheduling;protection method;mathematical model;BT DHT attack;system performance;BT client;optimization;PPBD system;download duration;fake-block attacking method;DHT index poison;routing pollution;Peer-to-peer computing;Indexes;Pollution;Routing;Toxicology;Bandwidth;Crawlers;Peer-to-peer networking;BitTorren;DHT;piracy prevention},
doi={10.1109/INFCOM.2013.6566979},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566980,
author={G. Alfano and M. Garetto and E. Leonardi},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Content-centric wireless networks with limited buffers: When mobility hurts},
year={2013},
volume={},
number={},
pages={1815-1823},
abstract={We analyze throughput-delay scaling laws of mobile ad-hoc networks under a content-centric traffic scenario, where users are mainly interested in retrieving contents cached by other nodes. We assume limited buffer size available at each node and Zipf-like content popularity. We consider nodes uniformly visiting the network area according to a random-walk mobility model, whose flight size is varied from the typical distance among the nodes (quasi-static case) up to the edge length of the network area (reshuffling mobility model). Our main findings are i) the best throughput-delay trade-offs are achieved in the quasi-static case: increasing the mobility degree of nodes leads to worse and worse performance; ii) the best throughput-delay trade-offs can be recovered by power control (i.e., by adapting the transmission range to the content) even in the complete reshuffling case.},
keywords={mobile ad hoc networks;mobility management (mobile radio);radio networks;telecommunication traffic;content-centric wireless networks;limited buffers;throughput-delay scaling laws;mobile ad hoc networks;content-centric traffic scenario;Zipf-like content;random-walk mobility model;Delays;Throughput;Mobile communication;Receivers;Aggregates;Indexes;Mobile computing},
doi={10.1109/INFCOM.2013.6566980},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566981,
author={S. Y. Han and N. B. Abu-Ghazaleh and D. Lee},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Double Regression: Efficient spatially correlated path loss model for wireless network simulation},
year={2013},
volume={},
number={},
pages={1824-1832},
abstract={The accuracy of wireless network packet simulation critically depends on the quality of the wireless channel models. These models directly affect the fundamental network characteristics, such as link quality, transmission range, and capture effect, as well as their dynamic variation in time and space. Path loss is the stationary component of the channel model affected by the shadowing in the environment. Existing path loss models are inaccurate, require very high measurement or computational overhead, and/or often cannot be made to represent a given environment. The paper contributes a flexible path loss model that uses a novel approach for spatially coherent interpolation from available nearby channels to allow accurate and efficient modeling of path loss. We show that the proposed model, called Double Regression (DR), generates a correlated space, allowing both the sender and the receiver to move without abrupt change in path loss. Combining DR with a traditional temporal fading model, such as Rayleigh fading, provides an accurate and efficient channel model that we integrate with the NS-2 simulator. We use measurements to validate the accuracy of the model for a number of scenarios. We also show that there is substantial impact on simulation behavior (e.g., up to 600% difference in throughput for simple scenarios) when path loss is modeled accurately.},
keywords={fading channels;radiofrequency measurement;regression analysis;double regression;spatially correlated path loss model;wireless network packet simulation;wireless channel models;fundamental network characteristics;flexible path loss model;DR;temporal fading model;NS-2 simulator;Loss measurement;Correlation;Receivers;Computational modeling;Fading;Shadow mapping;Channel models},
doi={10.1109/INFCOM.2013.6566981},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566982,
author={H. Al-Zubaidy and J. Liebeherr and A. Burchard},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A (min, ×) network calculus for multi-hop fading channels},
year={2013},
volume={},
number={},
pages={1833-1841},
abstract={A fundamental problem for the delay and backlog analysis across multi-hop paths in wireless networks is how to account for the random properties of the wireless channel. Since the usual statistical models for radio signals in a propagation environment do not lend themselves easily to a description of the available service rate, the performance analysis of wireless networks has resorted to higher-layer abstractions, e.g., using Markov chain models. In this work, we propose a network calculus that can incorporate common statistical models of fading channels and obtain statistical bounds on delay and backlog across multiple nodes. We conduct the analysis in a transfer domain, which we refer to as the SNR domain, where the service process at a link is characterized by the instantaneous signal-to-noise ratio at the receiver. We discover that, in the transfer domain, the network model is governed by a dioid algebra, which we refer to as (min, ×) algebra. Using this algebra we derive the desired delay and backlog bounds. An application of the analysis is demonstrated for a simple multi-hop network with Rayleigh fading channels.},
keywords={algebra;calculus;Markov processes;Rayleigh channels;statistical analysis;(min, ×) network calculus;multihop fading channels;wireless channel;statistical models;radio signals;propagation environment;higher-layer abstractions;Markov chain models;transfer domain;SNR domain;instantaneous signal-to-noise ratio;(min, ×) algebra;Rayleigh fading channels;Signal to noise ratio;Fading;Calculus;Transforms;Servers;Delays;Algebra},
doi={10.1109/INFCOM.2013.6566982},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566983,
author={M. Li and P. Li and M. Pan and J. Sun},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Economic-robust transmission opportunity auction in multi-hop wireless networks},
year={2013},
volume={},
number={},
pages={1842-1850},
abstract={The rapid growth of wireless devices and services exacerbates the problem of spectrum scarcity in wireless networks. Recently, spectrum auction has emerged as one of the most promising techniques to enhance spectrum utilization and mitigate this problem. Although there exist some works studying spectrum auction, most of them are designed for single-hop communications, and it is usually not clear whom a winning user communicates with. Moreover, most previous auction schemes only focus on satisfying the incentive compatibility property, also called truthfulness, but ignore another two critical properties: individual rationality, and budget balance. Thus, they may not be economic-robust. In this paper, we propose a transmission opportunity auction scheme, called TOA, which can support multi-hop data traffic, ensure economic-robustness, and generate high revenue for the auctioneer. Specifically, in TOA, instead of spectrum bands as in traditional spectrum auction schemes, users bid for transmission opportunities (TOs). A TO is defined as the permit of data transmission on a specific link using a certain band, i.e., a link-band pair. The TOA scheme is composed of three procedures: TO allocation, TO scheduling, and pricing, which are performed sequentially and iteratively until the aforementioned goals are reached. We prove that TOA is economic-robust, and conduct extensive simulations to show its effectiveness and efficiency.},
keywords={data communication;radio networks;radio spectrum management;telecommunication traffic;time-of-arrival estimation;multihop data traffic;incentive compatibility property;spectrum utilization enhancement;data transmission;link-band pair;TO allocation;TO pricing;TO scheduling;transmission opportunities;TOA;individual rationality;budget balance;single-hop communications;spectrum auction;spectrum scarcity;wireless services;wireless devices;multihop wireless networks;economic-robust transmission opportunity auction;Resource management;Pricing;Interference;Integrated circuits;Wireless networks;Relays;Scheduling},
doi={10.1109/INFCOM.2013.6566983},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566984,
author={D. M. Shila and Y. Cheng},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Capacity gain through power enhancement in multi-radio multi-channel wireless networks},
year={2013},
volume={},
number={},
pages={1851-1859},
abstract={The main focus of this paper is to show theoretically that power is a crucial factor in multi-radio multi-channel (MR-MC) wireless networks and hence by judiciously leveraging the power, one can realize a considerable gain on the capacity for MR-MC wireless networks. Such a capacity gain through power enhancement is revealed by our new insights of a co-channel enlarging effect. In particular, when the number of available channels (c) in a network is larger than that necessary for enabling the maximum set of simultaneous transmissions (c̃), allocating transmissions to those additional c-c̃ channels could enlarge the distance between the co-channel transmissions; the larger co-channel distance then allows a higher transmission power for higher link capacity. The finding of this paper specifically indicate that by exploiting the co-channel enlarging effect with power, one can realize the following gain on the capacity for MR-MC wireless networks: (i) In the channel-constraint region (c̃ <; c <; nφ/2), if each node augments its power from the minimum Pminto Pminc/c̃α/2, then a gain of Θ(log(c/c̃)α/2) is achieved; (ii) In the power-constraint region (c ≥ nφ/2), if each node sends at the maximum power level, Pmax= Pmin.nKor Pmin.2nφ/2, depending on the power availability at a node, then a gain of Θ(log n) or Θ(n) is achieved, respectively.},
keywords={channel capacity;cochannel interference;radio networks;wireless channels;capacity gain;power enhancement;multiradio multichannel wireless networks;MR-MC wireless networks;cochannel enlarging effect;cochannel distance;cochannel transmissions;link capacity;channel-constraint region;power-constraint region;cochannel interference;Interference;Wireless networks;Receivers;Signal to noise ratio;Channel models;Bandwidth},
doi={10.1109/INFCOM.2013.6566984},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566985,
author={H. Li and J. Xie},
booktitle={2013 Proceedings IEEE INFOCOM},
title={GaS: A gateway scheduling-based handoff scheme in single-radio infrastructure wireless mesh networks},
year={2013},
volume={},
number={},
pages={1860-1868},
abstract={Fast handoff support is a basic requirement for an Internet-based wireless mesh network (WMN), aiming to guarantee mobile users to be continuously connected to the Internet, regardless of their physical locations or moving trajectory. Due to the multi-hop transmission of network-layer handoff signaling packets, handoff performance in WMNs can be largely degraded by the increasing number of wireless hops as well as the channel access contentions between data and signaling packets in the mesh backbone. However, these issues are ignored in existing handoff solutions and multi-channel medium access control schemes. In this paper, we address the seamless handoff support from a different perspective and propose a gateway scheduling-based handoff scheme in single-radio multi-hop WMNs. Our proposed handoff scheme can realize single-hop handoff signaling packet transmissions and eliminate the channel contentions between data and signaling packets. Simulation results show that the total handoff delay is improved significantly using our proposed gateway scheduling-based handoff scheme under various scenarios, as compared to existing handoff solutions in multi-hop WMNs. In addition, due to the single-hop transmission of signaling packets, the signaling overhead in the wireless mesh backbone can be substantially reduced.},
keywords={access protocols;Internet;internetworking;scheduling;telecommunication signalling;wireless mesh networks;gateway scheduling-based handoff scheme;single-radio infrastructure wireless mesh networks;handoff support;Internet-based wireless mesh network;WMN;physical locations;multihop transmission;handoff performance;wireless hops;signaling packets;data packets;handoff solutions;multichannel medium access control schemes;seamless handoff support;single-radio multi-hop WMNs;channel contentions;single-hop transmission;signaling packets;signaling overhead;wireless mesh backbone;GaS;Delays;Wireless communication;Internet;Schedules;Logic gates;Directional antennas;Manganese},
doi={10.1109/INFCOM.2013.6566985},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566986,
author={R. K. Sheshadri and D. Koutsonikolas},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Comparison of routing metrics in 802.11n wireless mesh networks},
year={2013},
volume={},
number={},
pages={1869-1877},
abstract={We conduct the first experimental study of the performance of link quality-based routing metrics in an 802.11n wireless mesh network (WMN). Link quality-based metrics have been shown to significantly outperform the traditional hopcount metric but they have only been evaluated over legacy 802.11a/b/g radios. The new 802.11n standard introduces a number of enhancements at the MAC and PHY layers (MIMO technology, channel bonding, frame aggregation, short guard interval, and more aggressive modulation and coding schemes) marking the beginning of a new generation of 802.11 radios. Our study in a 21-node indoor 802.11n WMN testbed reveals that the gains of link quality-based metrics over the hopcount metric in legacy 802.11 WMNs do not carry over in 802.11n MIMO WMNs. We analyze the causes of this behavior and make recommendations for the design of new routing metrics in 802.11n WMNs.},
keywords={access protocols;MIMO communication;modulation coding;telecommunication links;telecommunication network routing;wireless LAN;wireless mesh networks;IEEE 802.11n MIMO WMN;hopcount metric;modulation scheme;coding scheme;short guard interval;frame aggregation;channel bonding;PHY layers;MAC;link quality-based routing metrics;IEEE 802.11n wireless mesh networks;IEEE 802.11n Standard;Throughput;IEEE 802.11g Standard;Routing;Bit rate;Barium},
doi={10.1109/INFCOM.2013.6566986},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566987,
author={Z. Feng and G. Papageorgiou and S. V. Krishnamurthy and R. Govindan and T. L. Porta},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Trading off distortion for delay for video transmissions in wireless networks},
year={2013},
volume={},
number={},
pages={1878-1886},
abstract={The end-user experience in viewing a video depends on the distortion; however, also of importance is the delay experienced by the packets of the video flow since it impacts the timeliness of the information contained and the playback rate at the receiver. Unfortunately, these performance metrics are in conflict with each other in a wireless network. Packet losses can be minimized by perfectly avoiding interference by separating transmissions in time or frequency; however, this decreases the rate at which transmissions occur, and this increases delay. Relaxing the requirement for interference avoidance can lead to packet losses and thus increase distortion, but can decrease the delay for those packets that are delivered. In this paper, we investigate this trade-off between distortion and delay for video. To understand the trade-off between video quality and packet delay, we develop an analytical framework that accounts for characteristics of the network (e.g. interference, channel variations) and the video content (motion level), assuming as a basis, a simple channel access policy that provides flexibility in managing the interference in the network. We validate our model via extensive simulations. Surprisingly, we find that the trade-off depends on the specific features of the video flow: it is better to trade-off high delay for low distortion with fast motion video, but not with slow motion video. Specifically, for an increase in PSNR (a metric that quantifies distortion) from 20 to 25 dB, the penalty in terms of the increase in mean delay with fast motion video is 91 times that with slow motion video. Our simulation results further quantify the trade-offs in various scenarios.},
keywords={interference suppression;radio networks;radiofrequency interference;video communication;wireless channels;video transmission;wireless network;end-user experience;performance metrics;packet loss minimisation;interference avoidance;distortion;video quality;packet delay;channel access policy;interference management;PSNR;Delays;Interference;Streaming media;Signal to noise ratio;Packet loss;Video recording},
doi={10.1109/INFCOM.2013.6566987},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566988,
author={S. T. Maguluri and R. Srikant},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Scheduling jobs with unknown duration in clouds},
year={2013},
volume={},
number={},
pages={1887-1895},
abstract={We consider a stochastic model of jobs arriving at a cloud data center. Each job requests a certain amount of CPU, memory, disk space, etc. Job sizes (durations) are also modeled as random variables, with possibly unbounded support. These jobs need to be scheduled non preemptively on servers. The jobs are first routed to one of the servers when they arrive and are queued at the servers. Each server then chooses a set of jobs from its queues so that it has enough resources to serve all of them simultaneously. This problem has been studied previously under the assumption that job sizes are known and upper bounded, and an algorithm was proposed which stabilizes traffic load in a diminished capacity region. Here, we present a load balancing and scheduling algorithm that is throughput optimal, without assuming that job sizes are known or are upper bounded.},
keywords={cloud computing;computer centres;network servers;processor scheduling;resource allocation;stochastic processes;telecommunication traffic;cloud job scheduling algorithm;stochastic job model;cloud data center;job requests;CPU;job sizes;random variables;servers;job queues;traffic load stabilization;diminished capacity region;load balancing algorithm;Servers;Schedules;Throughput;Routing;Markov processes;Scheduling;Vectors},
doi={10.1109/INFCOM.2013.6566988},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566989,
author={F. Huang and A. Anandkumar},
booktitle={2013 Proceedings IEEE INFOCOM},
title={FCD: Fast-concurrent-distributed load balancing under switching costs and imperfect observations},
year={2013},
volume={},
number={},
pages={1896-1904},
abstract={The problem of distributed load balancing among m agents operating in an n-server slotted system is considered. A randomized local search mechanism, FCD (fast, concurrent and distributed) algorithm, is implemented concurrently by each agent associated with a user. It involves switching to a different server with a certain exploration probability and then backtracking with a probability proportional to the ratio of the measured loads in the two servers (in consecutive time slots). The exploration and backtracking operations are executed concurrently by users in local alternating time slots. To ensure that users do not switch to other servers asymptotically, each user chooses the exploration probability to be decaying polynomially with time for decaying rate β ∈ [0.5, 1]. The backtracking decision is then based on an estimate of the server load which is computed based on local information. Thus, FCD algorithm does not require synchronization or coordination with other users. The main contribution of this work, besides the FCD algorithm, is the analysis of the convergence time for the system to be approximately balanced, i.e. to reach an c-Nash equilibrium. We show that the system reaches an c-Nash equilibrium in expected time O (max {n log n/ϵ + n1/β, (n3/m3log n2/ϵ)1/β}) when m > n2. This implies that the convergence rate is robust with large scale system(large user population), and is not affected by imperfect measurements of the server load. We also extend our analysis to open systems where users arrive and depart from a system with an initial load of m users. We allow for general time-dependent arrival processes (including heavy-tailed processes) and consider a uniform and a load-oblivious routing of the arrivals to the servers. A wide class of departure processes including load-dependent departures from the servers is also allowed. Our analysis demonstrates that it is possible to design fast, concurrent and distributed load balancing mechanisms in large multi-agent systems via randomized local search.},
keywords={cloud computing;computational complexity;multi-agent systems;resource allocation;FCD;fast-concurrent-distributed load balancing;switching costs;n-server slotted system;randomized local search mechanism;exploration probability;decaying rate;load-dependent departures;multi-agent systems;Servers;Convergence;Load management;Switches;Open systems;Games;Nash equilibrium},
doi={10.1109/INFCOM.2013.6566989},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566990,
author={J. Schneider and S. Schmid},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Optimal bounds for online page migration with generalized migration costs},
year={2013},
volume={},
number={},
pages={1905-1913},
abstract={This paper attends to a generalized version of the classic page migration problem where migration costs are not necessarily given by the migration distance only, but may depend on prior migrations, or on the available bandwidth along the migration path. Interestingly, this problem cannot be viewed from a Metrical Task System (MTS) perspective, despite the generality of MTS: The corresponding MTS has an unbounded state space and, thus, an unbounded competitive ratio. Nevertheless, we are able to present an optimal online algorithm for a wide range of problem variants, improving the best upper bounds known so far for more specific problems. For example, we present a tight bound of Θ(log n/log log n) for the competitive ratio of the virtual server migration problem introduced recently.},
keywords={computational complexity;graph theory;network servers;optimisation;virtual machines;optimal bound;online page migration;generalized migration cost;migration distance;migration path;metrical task system;MTS;unbounded state space;unbounded competitive ratio;optimal online algorithm;tight bound;virtual server migration problem;Servers;Algorithm design and analysis;Gravity;Cost function;Extraterrestrial measurements;Bandwidth;Upper bound},
doi={10.1109/INFCOM.2013.6566990},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566991,
author={Y. Rochman and H. Levy and E. Brosh},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Resource placement and assignment in distributed network topologies},
year={2013},
volume={},
number={},
pages={1914-1922},
abstract={We consider the problem of how to place and efficiently utilize resources in network environments. The setting consists of a regionally organized system which must satisfy regionally varying demands for various resources. The operator aims at placing resources in the regions as to minimize the cost of providing the demands. Examples of systems falling under this paradigm are 1) A peer supported Video on Demand service where the problem is how to place various video movies, and 2) A cloud-based system consisting of regional server-farms, where the problem is where to place various contents or end-user services. The main challenge posed by this paradigm is the need to deal with an arbitrary multi-dimensional (high-dimensionality) stochastic demand. We show that, despite this complexity, one can optimize the system operation while accounting for the full demand distribution. We provide algorithms for conducting this optimization and show that their complexity is pretty small, implying they can handle very large systems. The algorithms can be used for: 1) Exact system optimization, 2) deriving lower bounds for heuristic based analysis, and 3) Sensitivity analysis. The importance of the model is demonstrated by showing that an alternative analysis which is based on the demand means only, may, in certain cases, achieve performance that is drastically worse than the optimal one.},
keywords={optimisation;resource allocation;sensitivity analysis;telecommunication network topology;resource placement;resource assignment;distributed network topology;resource utilization;regionally organized system;video on demand service;video movies;cloud-based system;regional server-farms;content services;end-user services;multidimensional stochastic demand;exact system optimization;heuristic based analysis;sensitivity analysis;Algorithm design and analysis;Complexity theory;Servers;Stochastic processes;Streaming media;Motion pictures;Network topology},
doi={10.1109/INFCOM.2013.6566991},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566992,
author={S. Laitrakun and E. J. Coyle},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Optimizing the collection of local decisions for time-constrained distributed detection in WSNs},
year={2013},
volume={},
number={},
pages={1923-1931},
abstract={We consider a distributed detection application for a fusion center that has a limited time to make a global decision by collecting, weighting, and fusing local decisions made by nodes in a wireless sensor network that uses a random access channel. When this time is not long enough to collect decisions from all nodes in the network, a strategy is needed for collecting those with the highest reliability. This is accomplished with an easily implemented modification of the random access protocol: the collection time is divided into frames and only nodes with a particular range of reliabilities compete for the channel within each frame. Nodes with the most reliable decisions attempt transmission in the first frame; nodes with the next most reliable set of decisions attempt in the next frame; etc. We formulate an optimization problem that determines the reliability interval that defines who attempts in each frame in order to minimize the Detection Error Probability (DEP) at the fusion center. When the noise distribution affecting nodes' local decisions is continuous, symmetric, unimodal, and has a monotone likelihood ratio, reliability thresholds that maximize the channel throughput in each frame are optimal when the ratio of the collection time to the number of nodes is small. The number of frames that should be used depends on whether the average reliability or the worst-case reliability of local decisions in each frame is used to determine the DEP.},
keywords={decision theory;error statistics;optimisation;protocols;sensor fusion;signal detection;telecommunication network reliability;wireless channels;wireless sensor networks;local decision collection optimization;time-constrained distributed detection application;WSN;fusion center;node local decision fusion;wireless sensor network;random access channel;random access protocol;collection time;reliability interval;detection error probability;DEP;noise distribution;monotone likelihood ratio;reliability thresholds;channel throughput maximization;Reliability;Approximation methods;Frequency modulation;Wireless sensor networks;Sensors;Throughput;Random variables},
doi={10.1109/INFCOM.2013.6566992},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566993,
author={S. Guo and C. Wang and Y. Yang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Mobile data gathering with Wireless Energy Replenishment in rechargeable sensor networks},
year={2013},
volume={},
number={},
pages={1932-1940},
abstract={The emerging wireless energy transfer technology enables charging sensor batteries in a wireless sensor network (WSN) and maintaining perpetual operation of the network. Recent breakthrough in this area has opened up a new dimension to the design of sensor network protocols. In the meanwhile, mobile data gathering has been considered as an efficient alternative to data relaying in WSNs. However, time variation of recharging rates in wireless rechargeable sensor networks imposes a great challenge in obtaining an optimal data gathering strategy. In this paper, we propose a framework of joint Wireless Energy Replenishment and anchor-point based Mobile Data Gathering (WerMDG) in WSNs by considering various sources of energy consumption and time-varying nature of energy replenishment. To that end, we first determine the anchor point selection and the sequence to visit the anchor points. We then formulate the WerMDG problem into a network utility maximization problem which is constrained by flow conversation, energy balance, link and battery capacity and the bounded sojourn time of the mobile collector. Furthermore, we present a distributed algorithm composed of cross-layer data control, scheduling and routing subalgorithms for each sensor node, and sojourn time allocation subalgorithm for the mobile collector at different anchor points. Finally, we give extensive numerical results to verify the convergence of the proposed algorithm and the impact of utility weight on network performance.},
keywords={routing protocols;telecommunication power management;wireless sensor networks;wireless energy replenishment;rechargeable sensor networks;wireless energy transfer technology;sensor network protocol;optimal data gathering strategy;anchor point based mobile data gathering;WerMDG;energy consumption;time-varying nature;anchor point selection;flow conversation;energy balance;distributed algorithm;cross layer data control;scheduling subalgorithm;routing subalgorithm;Wireless sensor networks;Batteries;Mobile communication;Optimization;Robot sensing systems;Wireless communication;Distributed databases;Mobile data gathering;energy replenishment;distributed algorithm;rechargeable sensor networks},
doi={10.1109/INFCOM.2013.6566993},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566994,
author={D. Gong and Y. Yang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Low-latency SINR-based data gathering in wireless sensor networks},
year={2013},
volume={},
number={},
pages={1941-1949},
abstract={Data gathering is a fundamental operation for various applications of wireless sensor networks (WSNs), where sensor nodes sense information and forward data to a sink node via multihop wireless communications. Typically, data in a WSN is relayed over a tree topology to the sink for effective data gathering. A number of tree-based data gathering schemes have been proposed in the literature, most of which aim at maximizing network lifetime. However, the timeliness and reliability of gathered data are also of great importance to many applications in WSNs. To achieve low-latency, high-reliability data gathering in WSNs, in this paper, we construct a data gathering tree based on a reliability model, schedule data transmissions for the links on the tree and assign transmitting power to each link accordingly. Since the reliability of a link is highly related to its signal to interference plus noise ratio (SINR), the SINR of all the currently used links on the data gathering tree should be greater than a threshold to guarantee high reliability. We formulate the joint problem of tree construction, link scheduling and power assignment for data gathering into an optimization problem, with the objective of minimizing data gathering latency. We show the problem is NP-hard and divide the problem into two subproblems: Construction of a low-latency data gathering tree; Jointly link scheduling and power assignment for the data gathering tree. We then propose a polynomial heuristic algorithm for each subproblem and conduct extensive simulations to verify the effectiveness of the proposed algorithms. Our simulation results show that the proposed algorithms achieve much lower data gathering latency than existing data gathering strategies while guaranteeing high reliability.},
keywords={data communication;scheduling;telecommunication network reliability;telecommunication network topology;time division multiple access;trees (mathematics);wireless sensor networks;wireless sensor networks;WSN;multihop wireless communications;tree topology;network lifetime;low latency data gathering;reliability model;signal to interference plus noise ratio;SINR;tree construction;link scheduling;power assignment;data gathering latency;polynomial heuristic algorithm;Interference;Wireless sensor networks;Signal to noise ratio;Reliability;Optimization;Data models;Wireless communication;Wireless sensor networks (WSNs);data gathering;link scheduling;power assignment;SINR constraint},
doi={10.1109/INFCOM.2013.6566994},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566995,
author={Y. Yi and R. Li and F. Chen and A. X. Liu and Y. Lin},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A digital watermarking approach to secure and precise range query processing in sensor networks},
year={2013},
volume={},
number={},
pages={1950-1958},
abstract={Two-tiered wireless sensor networks offer good scalability, efficient power usage, and space saving. However, storage nodes are more attractive to attackers than sensors because they store sensor collected data and processing sink issued queries. A compromised storage node not only reveals sensor collected data, but also may reply incomplete or wrong query results. In this paper, we propose QuerySec, a protocol that enables storage nodes to process queries correctly while prevents them from revealing both data from sensors and queries from the sink. To protect privacy, we propose an order preserving function-based scheme to encode both sensor collected data and sink issued queries, which allows storage nodes to process queries correctly without knowing the actual values of both data and queries. To preserve integrity, we proposed a link watermarking scheme, where data items are formed into a link by the watermarks embedded in them so that any deletion in query results can be detected.},
keywords={query processing;watermarking;wireless sensor networks;digital watermarking;query processing;wireless sensor networks;attackers;sensors;QuerySec;storage nodes;link watermarking scheme;Watermarking;Data privacy;Privacy;Cryptography;Silicon;Protocols;Upper bound},
doi={10.1109/INFCOM.2013.6566995},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566996,
author={P. Huang and M. J. Tonnemacher and Y. Du and D. Rajan and J. Camp},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Towards scalable network emulation: Channel accuracy versus implementation resources},
year={2013},
volume={},
number={},
pages={1959-1967},
abstract={Channel emulators are valuable tools for controllable and repeatable wireless experimentation. Often, however, the high cost of such emulators preclude their widespread usage, especially in large-scale wireless networks. Moreover, existing channel emulators offer either very realistic channels for simplistic topologies or complex topologies with highly-abstracted, low-fidelity channels. To bridge the gap in offering a low-cost channel emulation solution which can scale to a large network size, in this paper, we study the tradeoff in channel emulation fidelity versus the hardware resources consumed using both analytical modeling and FPGA-based implementation. To reduce the memory footprint of our design, we optimize our channel emulation using an iterative structure to generate the Rayleigh fading channel. In addition, the channel update rate and word length selection are also evaluated in the paper which greatly improve the efficiency of implementation. We then extend our analysis of a single channel to understand how the implementation scales for the emulation of a large-scale wireless network, showing that up to 24 vehicular channels can be emulated in real-time on a single Virtex-4 FPGA.},
keywords={field programmable gate arrays;iterative methods;Rayleigh channels;telecommunication network topology;scalable network emulation;channel accuracy;implementation resources;repeatable wireless experimentation;controllable wireless experimentation;large-scale wireless networks;simplistic topologies;highly-abstracted channels;low-fidelity channels;low-cost channel emulation solution;hardware resources;memory footprint;iterative structure;Rayleigh fading channel;word length selection;large-scale wireless network;Virtex-4 FPGA-based implementation;Fading;Emulation;Field programmable gate arrays;Hardware;Wireless communication;Optimization;Accuracy},
doi={10.1109/INFCOM.2013.6566996},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566997,
author={M. Gjoka and M. Kurant and A. Markopoulou},
booktitle={2013 Proceedings IEEE INFOCOM},
title={2.5K-graphs: From sampling to generation},
year={2013},
volume={},
number={},
pages={1968-1976},
abstract={Understanding network structure and having access to realistic graphs plays a central role in computer and social networks research. In this paper, we propose a complete, practical methodology for generating graphs that resemble a real graph of interest. The metrics of the original topology we target to match are the joint degree distribution (JDD) and the degree-dependent average clustering coefficient (c̅(k)). We start by developing efficient estimators for these two metrics based on a node sample collected via either independence sampling or random walks. Then, we process the output of the estimators to ensure that the target metrics are realizable. Finally, we propose an efficient algorithm for generating topologies that have the exact target JDD and a c̅(k) close to the target. Extensive simulations using real-life graphs show that the graphs generated by our methodology are similar to the original graph with respect to, not only the two target metrics, but also a wide range of other topological metrics. Furthermore, our generator is order of magnitudes faster than state-of-the-art techniques.},
keywords={graph theory;network theory (graphs);pattern clustering;sampling methods;statistical distributions;graph sampling;state-of-the-art techniques;topological metrics;real-life graphs;topology generation;target metrics;random walks;independence sampling;node sample;degree-dependent average clustering coefficient;JDD;joint degree distribution;graph generation;social network research;computer network research;realistic graphs;network structure;Estimation;Measurement;Joints;Clustering algorithms;Social network services;Topology;Network topology},
doi={10.1109/INFCOM.2013.6566997},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566998,
author={T. Kuo and K. C. Lin and M. Tsai},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Maximizing submodular set function with connectivity constraint: Theory and application to networks},
year={2013},
volume={},
number={},
pages={1977-1985},
abstract={In this paper, we investigate the wireless network deployment problem, which seeks the best deployment of a given limited number of wireless routers. We found that many goals for network deployment, such as maximizing the number of covered users or areas, or the total throughput of the network, can be modelled with the submodular set function. Specifically, given a set of routers, the goal is to find a set of locations S, each of which is equipped with a router, such that S maximizes a predefined submodular set function. However, this deployment problem is more difficult than the traditional maximum submodular set function problem, e.g., the maximum coverage problem, because it requires all the deployed routers to form a connected network. In addition, deploying a router in different locations might consume different costs. To address these challenges, this paper introduces two approximation algorithms, one for homogeneous deployment cost scenarios and the other for heterogeneous deployment cost scenarios. Our simulations, using synthetic data and real traces of census in Taipei, show that the proposed algorithms achieve a better performance than other heuristics.},
keywords={approximation theory;radio networks;telecommunication network routing;connectivity constraint;wireless network deployment problem;wireless router;network throughput;submodular set function maximization;maximum coverage problem;connected network;router deployment;approximation algorithm;homogeneous deployment cost scenario;heterogeneous deployment cost scenario;census;Taipei;Approximation algorithms;Approximation methods;Logic gates;Algorithm design and analysis;TV;Optimized production technology},
doi={10.1109/INFCOM.2013.6566998},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6566999,
author={Y. Zhu and B. Li and Z. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Core-selecting combinatorial auction design for secondary spectrum markets},
year={2013},
volume={},
number={},
pages={1986-1994},
abstract={In a secondary spectrum market, the utility of a secondary user often depends on not only whether it wins, but also which channels it wins. Combinatorial auctions are a natural fit here to allow secondary users to bid for combinations of channels. In this context, the VCG mechanism constitutes a generic auction that uniquely guarantees both truthfulness and efficiency, but it is vulnerable to shill bidding and generates low revenue. In this paper, without compromising efficiency, we propose to design core-selecting auctions instead, which resolves VCG's vulnerability and improves seller revenue. We prove that in a secondary spectrum market, the revenue gleaned from a core-selecting auction is at least that of the VCG mechanism, and shills are not profitable to bidders. Employing linear programming and quadratic programming techniques, we design two payment rules suitable for our core-selecting auction, which aim to minimize the incentives of bidders to deviate from truthful-telling. Our extensive simulation results show that the revenues can be largely increased due to spectrum sharing.},
keywords={commerce;linear programming;quadratic programming;radio spectrum management;wireless channels;spectrum sharing;linear programming techniques;quadratic programming techniques;VCG mechanism;seller revenue;VCG vulnerability;generic auction;secondary user;secondary spectrum markets;core-selecting combinatorial auction design;Channel allocation;Resource management;Vectors;Wireless communication;Cost accounting;Robustness;Economics},
doi={10.1109/INFCOM.2013.6566999},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567000,
author={X. Feng and Q. Zhang and J. Zhang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Hybrid pricing for TV white space database},
year={2013},
volume={},
number={},
pages={1995-2003},
abstract={According to the recent rulings of the Federal Communications Commission (FCC), TV white spaces (TVWS) can now be accessed by secondary users (SUs) after a list of vacant TV channels is obtained via a geo-location database. Proper business models are essential for database operators to manage the cost of maintaining geo-location databases. Database access can be simultaneously priced under two different schemes: the registration scheme and the service plan scheme. In the registration scheme, the database reserves part of the TV bandwidth for registered White Space Devices (WSD) in a soft-license way. In the service plan scheme, WSDs are charged according to their queries. In this paper, we investigate the business model for the TVWS database under a hybrid pricing scheme. We consider the scenario where a database operator employs both the registration scheme and the service plan scheme to serve the SUs. The SUs' choices of different pricing schemes are modeled as a non-cooperative game and we derive distributed algorithms to achieve the Nash Equilibrium (NE). Considering the NE of the SUs, the database operator optimally determines the pricing parameters for both pricing schemes in terms of bandwidth reservation, registration fee and query plans.},
keywords={business data processing;database management systems;game theory;pricing;query processing;television;hybrid pricing;TV white space database;Federal Communications Commission;FCC;TVWS;geo-location database;business models;database access;white space devices;WSD;query processing;noncooperative game;distributed algorithms;Nash equilibrium;Pricing;Databases;Bandwidth;Contracts;TV;Games;Cost accounting},
doi={10.1109/INFCOM.2013.6567000},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567001,
author={Y. Chen and L. Duan and J. Huang and Q. Zhang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Balance of revenue and social welfare in FCC's spectrum allocation},
year={2013},
volume={},
number={},
pages={2004-2012},
abstract={To accommodate users' ever-increasing traffic in wireless broadband services, the Federal Communications Commission (FCC) in the U.S. is considering allocating additional spectrum to the wireless market. There are two major directions: licensed (e.g. 3G) and unlicensed services (e.g. Wi-Fi). On the one hand, 3G service can realize a high spectrum efficiency and provide ubiquitous connection. On the other hand, the Wi-Fi service (often with limited coverage) can provide users with high-speed local connections, but is subject to uncontrollable interferences. Regarding spectrum allocation, prior studies only focused on revenue maximization. However, one of FCC's missions is to better improve all wireless users' utilities. This motivates us to design a spectrum allocation scheme that jointly considers social welfare and revenue. In this paper, we formulate the interactions among the FCC, typical 3G and Wi-Fi operators, and the endusers as a three-stage dynamic game and derive the equilibrium of the entire game. Compared to the benchmark case where the FCC only maximizes its revenue, the consideration of social welfare will encourage the FCC to allocate more spectrum to the service which lacks spectrum to better serve its users. Such consideration for the social welfare, to our delight, brings limited revenue loss for the FCC.},
keywords={3G mobile communication;radio spectrum management;wireless LAN;revenue;social welfare;FCC;spectrum allocation;wireless broadband service;Federal Communications Commission;Wi-Fi;3G service;high-speed local connection;IEEE 802.11 Standards;FCC;Resource management;Games;Wireless communication;Economics;Pricing},
doi={10.1109/INFCOM.2013.6567001},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567002,
author={P. Lin and X. Feng and Q. Zhang and M. Hamdi},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Groupon in the Air: A three-stage auction framework for Spectrum Group-buying},
year={2013},
volume={},
number={},
pages={2013-2021},
abstract={Spectrum auction is widely applied in spectrum redistributions, especially under the dynamic spectrum management context. However, due to the high price asked by the spectrum holders, secondary users (SUs) with limited budget cannot benefit from such auction directly. Motivated by the recent group-buying behaviors in the Internet based service, we advocate that SUs can be grouped together to take part in the spectrum auction as a whole to increase their chances to win the channel. The cost and benefit of the won spectrum are then shared evenly among the SUs within the group. None of the existing auction models can be applied in this scenario due to three unique challenges: how can a group leader select the winning SUs and charge them fairly and efficiently; how to guarantee truthfulness of users' bids; how to match the heterogeneous channels to groups when one group would like to buy at most one channel. In this paper, we propose TASG, a Three-stage Auction framework for Spectrum Group-buying to address the above challenges and enable group-buying behaviors among SUs. In the first stage, we propose an algorithm to decide the group members and bids for the channels. In the second stage, we conduct auction between the group leaders and the spectrum holder, with a novel winner determination algorithm. In the third stage, the group leaders further distribute spectrum and bills to the SUs in the group. TASG possesses good properties such as truthfulness, individual rationality, improved system efficiency, and computational tractability.},
keywords={radio spectrum management;spectrum auction;dynamic spectrum management context;secondary users;SU;Internet based service;heterogeneous channels;TASG;group leaders;spectrum holder;novel winner determination algorithm;three-stage auction framework for spectrum group-buying;Tin;Algorithm design and analysis;Vectors;Time complexity;Wireless communication;Biological system modeling;Aggregates},
doi={10.1109/INFCOM.2013.6567002},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567003,
author={P. M. Comar and L. Liu and S. Saha and P. Tan and A. Nucci},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Combining supervised and unsupervised learning for zero-day malware detection},
year={2013},
volume={},
number={},
pages={2022-2030},
abstract={Malware is one of the most damaging security threats facing the Internet today. Despite the burgeoning literature, accurate detection of malware remains an elusive and challenging endeavor due to the increasing usage of payload encryption and sophisticated obfuscation methods. Also, the large variety of malware classes coupled with their rapid proliferation and polymorphic capabilities and imperfections of real-world data (noise, missing values, etc) continue to hinder the use of more sophisticated detection algorithms. This paper presents a novel machine learning based framework to detect known and newly emerging malware at a high precision using layer 3 and layer 4 network traffic features. The framework leverages the accuracy of supervised classification in detecting known classes with the adaptability of unsupervised learning in detecting new classes. It also introduces a tree-based feature transformation to overcome issues due to imperfections of the data and to construct more informative features for the malware detection task. We demonstrate the effectiveness of the framework using real network data from a large Internet service provider.},
keywords={Internet;invasive software;learning (artificial intelligence);tree data structures;supervised learning;unsupervised learning;zero-day malware detection;security threats;Internet today;payload encryption;obfuscation methods;polymorphic capabilities;layer 3 network traffic features;layer 4 network traffic features;tree-based feature transformation;Internet service provider;Malware;Feature extraction;Kernel;Support vector machines;Training;Payloads;Unsupervised learning},
doi={10.1109/INFCOM.2013.6567003},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567004,
author={S. A. Ahmadzadeh and G. Agnew},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Turbo covert channel: An iterative framework for covert communication over data networks},
year={2013},
volume={},
number={},
pages={2031-2039},
abstract={Inspired by the challenges of designing a robust, and undetectable covert channel, in this paper we introduce a design methodology for timing covert channels that achieve provable polynomial-time undetectability. This means that the covert channel can not be detected by any polynomial-time statistical test that analyzes the samples of the covert traffic and the legitimate traffic. The proposed framework is based on modeling the covert channel as a differential communication channel, and the formulation for modulation/demodulation processes that are derived according to the communication model. The proposed scheme incorporates a trellis structure in modulating the covert message. The trellis structure is also used at the covert receiver to perform iterative demodulation/decoding of the covert message that significantly enhances the channel reliability. In addition, the paper presents an adaptive modulation strategy that improves the channel robustness without compromising the stealthiness of the channel. The combination of the adaptive modulation and the trellis structure gives the covert channel considerable flexibility and low error rate at the covert receiver. In fact, performance analysis of the channel reveals that the proposed covert communication scheme withstands extremely high levels of network noise and adversarial disruption, while it maintains an outstanding undetectability level and covert rate.},
keywords={adaptive modulation;communication complexity;demodulation;error statistics;iterative decoding;radio receivers;radiofrequency interference;statistical testing;telecommunication network reliability;telecommunication traffic;trellis codes;turbo codes;wireless channels;turbo covert channel;iterative framework;data network;design methodology;timing covert channel;polynomial-time undetectability;polynomial-time statistical test;covert traffic;legitimate traffic;covert channel modeling;differential communication channel;modulation/demodulation process;communication model;trellis structure;covert message;covert receiver;iterative demodulation;iterative decoding;channel reliability;adaptive modulation strategy;channel robustness;error rate;channel performance analysis;covert communication scheme;network noise;undetectability level;covert rate;Receivers;Delays;Transmitters;Noise;Demodulation},
doi={10.1109/INFCOM.2013.6567004},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567005,
author={J. Daly and A. X. Liu and E. Torng},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A difference resolution approach to compressing Access Control Lists},
year={2013},
volume={},
number={},
pages={2040-2048},
abstract={Access Control Lists (ACLs) are the core of many networking and security devices. As new threats and vulnerabilities emerge, ACLs on routers and firewalls are getting larger. Therefore, compressing ACLs is an important problem. In this paper, we propose a new approach, called Diplomat, to ACL compression. The key idea is to transform higher dimensional target patterns into lower dimensional patterns by dividing the original pattern into a series of hyperplanes and then resolving differences between two adjacent hyperplanes by adding rules that specify the differences. This approach is fundamentally different from prior ACL compression algorithms and is shown to be very effective. We implemented Diplomat and conducted side-by-side comparison with the prior Firewall Compressor algorithm on real life classifiers. The experimental results show that Diplomat outperforms Firewall Compressor most of the time, often by a considerable margin. In particular, on our largest ACLs, Diplomat has an average improvement ratio over Firewall Compressor of 30.6%.},
keywords={authorisation;data compression;firewalls;difference resolution approach;access control list compression;networking device;security device;router;diplomat;ACL compression algorithm;firewall compressor;Color;Dynamic programming;Heuristic algorithms;Merging;Approximation methods;Approximation algorithms;Security},
doi={10.1109/INFCOM.2013.6567005},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567006,
author={O. Rottenstreich and I. Keslassy and A. Hassidim and H. Kaplan and E. Porat},
booktitle={2013 Proceedings IEEE INFOCOM},
title={On finding an optimal TCAM encoding scheme for packet classification},
year={2013},
volume={},
number={},
pages={2049-2057},
abstract={Hardware-based packet classification has become an essential component in many networking devices. It often relies on TCAMs (ternary content-addressable memories), which need to compare the packet header against a set of rules. But efficiently encoding these rules is not an easy task. In particular, the most complicated rules are range rules, which usually require multiple TCAM entries to encode them. However, little is known on the optimal encoding of such non-trivial rules. In this work, we take steps towards finding an optimal encoding scheme for every possible range rule. We first present an optimal encoding for all possible generalized extremal rules. Such rules represent 89% of all non-trivial rules in a typical real-life classification database. We also suggest a new method of simply calculating the optimal expansion of an extremal range, and present a closed-form formula of the average optimal expansion over all extremal ranges. Next, we present new bounds on the worst-case expansion of general classification rules, both in one-dimensional and two-dimensional ranges. Last, we introduce a new TCAM architecture that can leverage these results by providing a guaranteed expansion on the tough rules, while dealing with simpler rules using a regular TCAM. We conclude by verifying our theoretical results in experiments with synthetic and real-life classification databases.},
keywords={computer networks;content-addressable storage;encoding;telecommunication equipment;optimal TCAM encoding scheme;hardware based packet classification;ternary content addressable memory;packet header;optimal encoding;nontrivial rule;generalized extremal rule;real life classification database;closed form formula;average optimal expansion;worst case expansion;general classification rule;Encoding;Indexes;Ports (Computers);Educational institutions;Heuristic algorithms;Reflective binary codes},
doi={10.1109/INFCOM.2013.6567006},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567007,
author={W. Gao and Q. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Wakeup scheduling for energy-efficient communication in opportunistic mobile networks},
year={2013},
volume={},
number={},
pages={2058-2066},
abstract={Opportunistic mobile networks consist of mobile devices which only communicate when they opportunistically contact each other. Periodic contact probing is required to facilitate opportunistic communication, but seriously reduces the limited battery life of mobile devices. Current research efforts on reducing energy consumption of contact probing are restricted to optimize the probing interval, but are insufficient for energy-efficient opportunistic communication. In this paper, we propose novel techniques to adaptively schedule wakeup periods of mobile nodes between their inter-contact times. A node stays asleep during inter-contact times when contact probing is unnecessary, and only wakes up when a contact with another node is likely to happen. Our approach probabilistically predicts node contacts in the future, and analytically balances between energy consumption for contact probing and performance of opportunistic communication. Extensive trace-driven simulations show that our approach significantly improves energy efficiency of opportunistic communication compared to existing schemes.},
keywords={energy consumption;mobile radio;scheduling;mobile nodes;energy-efficient opportunistic communication;contact probing;energy consumption;battery life;mobile devices;opportunistic mobile networks;energy-eficient communication;wakeup scheduling;Schedules;Energy consumption;Peer-to-peer computing;Mobile computing;Mobile nodes},
doi={10.1109/INFCOM.2013.6567007},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567008,
author={Y. Kim and K. Lee and N. B. Shroff and I. Rhee},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Providing probabilistic guarantees on the time of information spread in opportunistic networks},
year={2013},
volume={},
number={},
pages={2067-2075},
abstract={A variety of mathematical tools have been developed for predicting the spreading patterns in a number of varied environments including infectious diseases, computer viruses, and urgent messages broadcast to mobile agents (e.g., humans, vehicles, and mobile devices). These tools have mainly focused on estimating the average time for the spread to reach a fraction (e.g., α) of the agents, i.e., the so-called average completion time E(Tα). We claim that providing probabilistic guarantee on the time for the spread Tαrather than only its average gives a much better understanding of the spread, and hence could be used to design improved methods to prevent epidemics or devise accelerated methods for distributing data. To demonstrate the benefits, we introduce a new metric Gα,βthat denotes the time required to guarantee α completion with probability β, and develop a new framework to characterize the distribution of Tαfor various spread parameters such as number of seeds, level of contact rates, and heterogeneity in contact rates. We apply our technique to an experimental mobility trace of taxies in Shanghai and show that our framework enables us to allocate resources (i.e., to control spread parameters) for acceleration of spread in a far more efficient way than the state-of-the-art.},
keywords={computer network security;mobile agents;pattern recognition;probability;probabilistic guarantees;information spread;opportunistic networks;mathematical tools;pattern spreading;mobile agents;epidemics;data distribution;Measurement;Analytical models;Vectors;Diseases;Acceleration;Sociology;Markov processes},
doi={10.1109/INFCOM.2013.6567008},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567009,
author={C. Lee and J. Kwak and D. Y. Eun},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Characterizing link connectivity for opportunistic mobile networking: Does mobility suffice?},
year={2013},
volume={},
number={},
pages={2076-2084},
abstract={With recent drastic growth in the number of users carrying smart mobile devices, it is not hard to envision opportunistic ad-hoc communications taking place with such devices carried by humans. This leads to, however, a new challenge to the conventional link-level metrics, solely defined based on user mobility, such as inter-contact time, since there are many constraints including limited battery power that prevent the wireless interface of each user from being always `on' for communication. By taking into account the process of each user's availability jointly with mobility-induced contact/inter-contact process, we investigate how each of them affects the link-level connectivity depending on their relative operating time scales. We then identify three distinct regimes in each of which (1) the so-called impact of mobility on network performance prevails; (2) such impact of mobility disappears or its extent is not that significant; (3) the user availability process becomes dominant. Our findings not only caution that mobility alone is not sufficient to characterize the link-level dynamics, which in turn can lead to highly misleading results, but also suggest the presence of many uncharted research territories for further exploration.},
keywords={mobile ad hoc networks;mobility management (mobile radio);smart phones;link connectivity;opportunistic mobile networking;smart mobile devices;opportunistic ad-hoc communications;link-level metrics;intercontact time;limited battery power;wireless interface;user availability;mobility-induced contact-intercontact process;link-level connectivity;user availability process;link-level dynamics;Availability;IEEE 802.11 Standards;Ad hoc networks;Mobile computing;Measurement;Mobile nodes},
doi={10.1109/INFCOM.2013.6567009},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567010,
author={M. Xiao and J. Wu and C. Liu and L. Huang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={TOUR: Time-sensitive Opportunistic Utility-based Routing in delay tolerant networks},
year={2013},
volume={},
number={},
pages={2085-2091},
abstract={In this paper, we propose a time-sensitive utility model for delay tolerant networks (DTNs), in which each message has an attached time-sensitive benefit that decays over time. The utility of a message is the benefit minus the transmission cost incurred by delivering the message. This model is analogous to the postal service in the real world, which inherently provides a good balance between delay and cost. Under this model, we propose a Time-sensitive Opportunistic Utility-based Routing (TOUR) algorithm. TOUR is a single-copy opportunistic routing algorithm, in which a time-sensitive forwarding set is maintained for each node by considering the probabilistic contacts in DTNs. By forwarding messages via nodes in these sets, TOUR can achieve the optimal expected utilities. We show the outstanding performance of TOUR through extensive simulations with several real DTN traces. To the best of our knowledge, TOUR is the first utility-based routing algorithm in DTNs.},
keywords={delay tolerant networks;delays;postal services;radio networks;telecommunication network routing;postal service;forwarding messages;time-sensitive forwarding set;single-copy opportunistic routing algorithm;delay;transmission cost;time-sensitive benefit;DTN;delay tolerant networks;TOUR algorithm;time-sensitive opportunistic utility-based routing algorithm;Routing;Nickel;Delays;Educational institutions;Probabilistic logic;Exponential distribution;Probability density function;Delay tolerant networks;opportunistic routing;utility},
doi={10.1109/INFCOM.2013.6567010},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567011,
author={N. M. Jones and B. Shrader and E. Modiano},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Distributed CSMA with pairwise coding},
year={2013},
volume={},
number={},
pages={2094-2102},
abstract={We consider distributed strategies for joint routing, scheduling, and network coding to maximize throughput in wireless networks. Network coding allows for an increase in network throughput under certain routing conditions. We previously developed a centralized control policy to jointly optimize for routing and scheduling combined with a simple network coding strategy using max-weight scheduling (MWS) [9]. In this work we focus on pairwise network coding and develop a distributed carrier sense multiple access (CSMA) policy that supports all arrival rates allowed by the network subject to the pairwise coding constraint. We extend our scheme to optimize for packet overhearing to increase the number of beneficial coding opportunities. Simulation results show that the CSMA strategy yields the same throughput as the optimal centralized policy of [9], but at the cost of increased delay. Moreover, overhearing provides up to an additional 25% increase in throughput on random topologies.},
keywords={carrier sense multiple access;network coding;radio networks;scheduling;telecommunication network routing;telecommunication network topology;distributed CSMA;joint routing;throughput maximization;wireless network;centralized control policy;network coding strategy;max-weight scheduling;pairwise network coding;distributed carrier sense multiple access;pairwise coding constraint;packet overhearing;random topology;Encoding;Network coding;Routing;Multiaccess communication;Throughput;Standards;Wireless networks},
doi={10.1109/INFCOM.2013.6567011},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567012,
author={J. Liu and C. H. Xia and N. B. Shroff and H. D. Sherali},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Distributed cross-layer optimization in wireless networks: A second-order approach},
year={2013},
volume={},
number={},
pages={2103-2111},
abstract={Due to the rapidly growing scale and heterogeneity of wireless networks, the design of distributed cross-layer optimization algorithms has received significant interest from the networking research community. So far, the standard distributed cross-layer approach in the literature is based on the first-order Lagrangian dual decomposition and the subgradient method, which suffers from a slow convergence rate. In this paper, we make the first known attempt to develop a distributed Newton's method, which is second-order and enjoys a quadratic convergence rate. However, due to the inherent interference in wireless networks, the Hessian matrix of the cross-layer problem has a non-separable structure. As a result, developing a distributed second-order algorithm is far more difficult than its counterpart for wireline networks. Our main contributions in this paper are two-fold: i) For a special network setting where all links mutually interfere, we derive closed-form expressions for the Hessian inverse, which further yield a distributed Newton's method; ii) For general wireless networks where the interference relationships are arbitrary, we propose a double matrix-splitting scheme, which also leads to a distributed Newton's method. Collectively, these results create a new theoretical framework for distributed cross-layer optimization in wireless networks. More importantly, our work contributes to a potential second-order paradigm shift in wireless networks optimization theory.},
keywords={gradient methods;Hessian matrices;Newton method;optimisation;radio networks;distributed cross-layer optimization;first-order Lagrangian dual decomposition;subgradient method;distributed Newton method;quadratic convergence rate;Hessian matrix;cross-layer problem;distributed second-order algorithm;Hessian inverse;double matrix-splitting scheme;wireless network optimization theory;Wireless networks;Vectors;Newton method;Optimization;Convergence;Routing;Interference},
doi={10.1109/INFCOM.2013.6567012},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567013,
author={B. Li and A. Eryilmaz and R. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Wireless scheduling for network utility maximization with optimal convergence speed},
year={2013},
volume={},
number={},
pages={2112-2120},
abstract={In this paper, we study the design of joint flow rate control and scheduling policies in multi-hop wireless networks for achieving maximum network utility with provably optimal convergence speed. Fast convergence is especially important in wireless networks which are dominated by the dynamics of incoming and outgoing flows as well as the time sensitive applications. Yet, the design of fast converging policies in wireless networks is complicated by: (i) the interference-constrained communication capabilities, and (ii) the finite set of transmission rates to select from due to operational and physical-layer constraints. We tackle these challenges by explicitly incorporating such discrete constraints to understand their impact on the convergence speed at which the running average of the received service rates and the network utility converges to their limits. In particular, we establish a fundamental fact that the convergence speed of any feasible policy cannot be faster than Ω (1/T) under both the T rate and utility metrics. Then, we develop an algorithm that achieves this optimal convergence speed in both metrics. We also show that the well-known dual algorithm can achieve the optimal convergence speed in terms of its utility value. These results reveal the interesting fact that the convergence speed of rates and utilities in wireless networks is dominated by the discrete choices of scheduling and transmission rates, which also implies that the use of higher-order flow rate controllers with fast convergence guarantees cannot overcome the aforementioned fundamental limitation.},
keywords={convergence;optimisation;radio networks;radiofrequency interference;scheduling;telecommunication congestion control;wireless scheduling policy;network utility maximization;optimal convergence speed;joint flow rate control desgin;multihop wireless networks;fast converging policy;interference-constrained communication;physical-layer constraints;operational layer constraints;discrete constraints;received service rates;utility metrics;dual algorithm;transmission rates;higher-order flow rate controllers;Convergence;Wireless networks;Vectors;Algorithm design and analysis;Measurement;Upper bound;Joints},
doi={10.1109/INFCOM.2013.6567013},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567014,
author={P. Wan and X. Jia and G. Dai and H. Du and Z. Wan and O. Frieder},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Scalable algorithms for wireless link schedulings in multi-channel multi-radio wireless networks},
year={2013},
volume={},
number={},
pages={2121-2129},
abstract={For wireless link scheduling in multi-channel multi-radio wireless networks aiming at maximizing (concurrent) multi-flow, constant-approximation algorithms have recently been developed in [11]. However, the running time of those algorithms grows quickly with the number of radios per node (at least in the sixth order) and the number of channels (at least in the cubic order). Such poor scalability stems intrinsically from the exploding size of the fine-grained network representation upon which those algorithms are built. In this paper, we introduce a new structure, termed as concise conflict graph, on the node-level links directly. Such structure succinctly captures the essential advantage of multiple radios and multiple channels. By exploring and exploiting the rich structural properties of the concise conflict graphs, we are able to develop fast and scalable link scheduling algorithms for either minimizing the communication latency or maximizing the (concurrent) multi-flow. These algorithms have running time growing linearly in both the number of radios per node and the number of channels, while not sacrificing the approximation bounds.},
keywords={approximation theory;minimisation;network theory (graphs);radio links;radio networks;scheduling;wireless channels;multichannel multiradio wireless network;multiflow maximization;constant approximation algorithm;fine grained network representation;concise conflict graph;node level link;scalable wireless link scheduling algorithm;communication latency minimization;Interference;Wireless networks;Schedules;Approximation methods;Educational institutions;Approximation algorithms;Scheduling;Link scheduling;multi-channel multi-radio;approximation algorithms},
doi={10.1109/INFCOM.2013.6567014},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567015,
author={A. Dixit and P. Prakash and Y. C. Hu and R. R. Kompella},
booktitle={2013 Proceedings IEEE INFOCOM},
title={On the impact of packet spraying in data center networks},
year={2013},
volume={},
number={},
pages={2130-2138},
abstract={Modern data center networks are commonly organized in multi-rooted tree topologies. They typically rely on equal-cost multipath to split flows across multiple paths, which can lead to significant load imbalance. Splitting individual flows can provide better load balance, but is not preferred because of potential packet reordering that conventional wisdom suggests may negatively interact with TCP congestion control. In this paper, we revisit this “myth” in the context of data center networks which have regular topologies such as multi-rooted trees. We argue that due to symmetry, the multiple equal-cost paths between two hosts are composed of links that exhibit similar queuing properties. As a result, TCP is able to tolerate the induced packet reordering and maintain a single estimate of RTT. We validate the efficacy of random packet spraying (RPS) using a data center testbed comprising real hardware switches. We also reveal the adverse impact on the performance of RPS when the symmetry is disturbed (e.g., during link failures) and suggest solutions to mitigate this effect.},
keywords={computer centres;queueing theory;telecommunication congestion control;telecommunication network topology;transport protocols;trees (mathematics);packet spraying impact;data center networks;multirooted tree topologies;equal-cost multipath;load imbalance;potential packet reordering;TCP congestion control;multiple equal-cost paths;similar queuing properties;random packet spraying efficacy;RPS;data center testbed;link failures;Spraying;Throughput;Topology;Network topology;Bandwidth;Servers;Kernel},
doi={10.1109/INFCOM.2013.6567015},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567016,
author={J. Guo and F. Liu and D. Zeng and J. C. S. Lui and H. Jin},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A cooperative game based allocation for sharing data center networks},
year={2013},
volume={},
number={},
pages={2139-2147},
abstract={In current IaaS datacenters, tenants are suffering unfairness since the network bandwidth is shared in a besteffort manner. To achieve predictable network performance for rented virtual machines (VMs), cloud providers should guarantee minimum bandwidth for VMs or allocate the network bandwidth in a fairness fashion at VM-level. At the same time, the network should be efficiently utilized in order to maximize cloud providers' revenue. In this paper, we model the bandwidth sharing problem as a Nash bargaining game, and propose the allocation principles by defining a tunable base bandwidth for each VM. Specifically, we guarantee bandwidth for those VMs with lower network rates than their base bandwidth, while maintaining fairness among other VMs with higher network rates than their base bandwidth. Based on rigorous cooperative game-theoretic approaches, we design a distributed algorithm to achieve efficient and fair bandwidth allocation corresponding to the Nash bargaining solution (NBS). With simulations under typical scenarios, we show that our strategy can meet the two desirable requirements towards predictable performance for tenants as well as high utilization for providers. And by tuning the base bandwidth, our solution can enable cloud providers to flexibly balance the tradeoff between minimum guarantees and fair sharing of datacenter networks.},
keywords={bandwidth allocation;cloud computing;computer centres;game theory;virtual machines;cooperative game based allocation;data center networks;IaaS datacenters;network bandwidth;network performance;rented virtual machines;cloud providers;fairness fashion;VM-level;bandwidth sharing problem;Nash bargaining game;tunable base bandwidth;network rates;cooperative game-theoretic approaches;distributed algorithm;fair bandwidth allocation;Nash bargaining solution;Bandwidth;Servers;Channel allocation;Resource management;Games;Bismuth;NIST},
doi={10.1109/INFCOM.2013.6567016},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567017,
author={Z. Shao and X. Jin and W. Jiang and M. Chen and M. Chiang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Intra-data-center traffic engineering with ensemble routing},
year={2013},
volume={},
number={},
pages={2148-2156},
abstract={Today's data centers are shared among multiple tenants running a wide range of applications. These applications require a network with a scalable and robust layer-2 network management solution that enables load-balancing and QoS provisioning. Ensemble routing was proposed to achieve management scalability and robustness by using Virtual Local Area Networks (VLANs) and operating on the granularity of flow ensembles, i.e. group of flows. The key challenge of intra-data-center traffic engineering with ensemble routing is the combinatorial optimization of VLAN assignment, i.e., optimally assigning flow ensembles to VLANs to achieve load balancing and low network costs. Based on the Markov approximation framework, we solve the VLAN assignment problem with a general objective function and arbitrary network topologies by designing approximation algorithms with close-to-optimal performance guarantees. We study several properties of our algorithms, including performance optimality, perturbation bound, convergence of algorithms and impacts of algorithmic parameter choices. Then we extend these results to variants of VLAN assignment problem, including interaction with TCP congestion and QoS considerations. We validate our analytical results by conducting extensive numerical experiments. The results show that our algorithms can be tuned to meet different temporal constraints, incorporate fine-grained traffic management, overcome traffic measurement limitations, and tolerate imprecise and incomplete traffic matrices.},
keywords={approximation theory;combinatorial mathematics;computer centres;convergence;local area networks;Markov processes;optimisation;quality of service;resource allocation;telecommunication congestion control;telecommunication network management;telecommunication network reliability;telecommunication network routing;telecommunication network topology;telecommunication traffic;transport protocols;traffic matrices;traffic measurement limitation;fine-grained traffic management;temporal constraint;TCP congestion;algorithmic parameter choices;convergence;perturbation bound;performance optimality;close-to-optimal performance guarantees;approximation algorithm;network topologies;VLAN assignment problem;Markov approximation framework;network cost;load balancing;combinatorial optimization;flow ensembles granularity;virtual local area network;management scalability;QoS provisioning;layer-2 network management solution;ensemble routing;intra-data-center traffic engineering;Routing;Markov processes;Switches;Approximation methods;Algorithm design and analysis;Approximation algorithms;Upper bound},
doi={10.1109/INFCOM.2013.6567017},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567018,
author={A. Munir and I. A. Qazi and Z. A. Uzmi and A. Mushtaq and S. N. Ismail and M. S. Iqbal and B. Khan},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Minimizing flow completion times in data centers},
year={2013},
volume={},
number={},
pages={2157-2165},
abstract={For provisioning large-scale online applications such as web search, social networks and advertisement systems, data centers face extreme challenges in providing low latency for short flows (that result from end-user actions) and high throughput for background flows (that are needed to maintain data consistency and structure across massively distributed systems). We propose L<sup>2</sup>DCT, a practical data center transport protocol that targets a reduction in flow completion times for short flows by approximating the Least Attained Service (LAS) scheduling discipline, without requiring any changes in application software or router hardware, and without adversely affecting the long flows. L<sup>2</sup>DCT can co-exist with TCP and works by adapting flow rates to the extent of network congestion inferred via Explicit Congestion Notification (ECN) marking, a feature widely supported by the installed router base. Though L<sup>2</sup>DCT is deadline unaware, our results indicate that, for typical data center traffic patterns and deadlines and over a wide range of traffic load, its deadline miss rate is consistently smaller compared to existing deadline-driven data center transport protocols. L<sup>2</sup>DCT reduces the mean flow completion time by up to 50% over DCTCP and by up to 95% over TCP. In addition, it reduces the completion for 99th percentile flows by 37% over DCTCP. We present the design and analysis of L<sup>2</sup>DCT, evaluate its performance, and discuss an implementation built upon standard Linux protocol stack.},
keywords={computer centres;Linux;protocols;scheduling;flow completion times minimization;data centers;large-scale online applications;Web search;social networks;advertisement systems;L2DCT;least attained service scheduling discipline;LAS;application software;router hardware;explicit congestion notification marking;ECN;DCTCP;Linux protocol stack;Throughput;Transport protocols;Bandwidth;Oscillators;Hardware;Routing protocols},
doi={10.1109/INFCOM.2013.6567018},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567019,
author={X. Ban and M. Goswami and W. Zeng and X. Gu and J. Gao},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Topology dependent space filling curves for sensor networks and applications},
year={2013},
volume={},
number={},
pages={2166-2174},
abstract={In this paper we propose an algorithm to construct a “space filling” curve for a sensor network with holes. Mathematically, for a given multi-hole domain R, we generate a path P that is provably aperiodic (i.e., any point is covered at most a constant number of times) and dense (i.e., any point of R is arbitrarily close to P). In a discrete setting as in a sensor network, the path visits the nodes with progressive density, which can adapt to the budget of the path length. Given a higher budget, the path covers the network with higher density. With a lower budget the path becomes proportional sparser. We show how this density-adaptive space filling curve can be useful for applications such as serial data fusion, motion planning for data mules, sensor node indexing, and double ruling type in-network data storage and retrieval. We show by simulation results the superior performance of using our algorithm vs standard space filling curves and random walks.},
keywords={sensor fusion;wireless sensor networks;topology dependent space filling curves;sensor networks;multihole domain R;discrete setting;progressive density;density-adaptive space filling curve;motion planning;serial data fusion;data mules;sensor node indexing;double ruling type in-network;data storage;data retrieval;standard space filling curves;random walks;Harmonic analysis;Planning;Indexing;Shape;Educational institutions;Data integration;Approximation methods},
doi={10.1109/INFCOM.2013.6567019},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567020,
author={T. Yu and H. Jiang and G. Tan and C. Wang and C. Tian and Y. Wu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={SINUS: A scalable and distributed routing algorithm with guaranteed delivery for WSNs on high genus 3D surfaces},
year={2013},
volume={},
number={},
pages={2175-2183},
abstract={In this paper, we put forward a novel scalable and distributed routing algorithm, called SINUS, for sensor networks deployed on the surface of complex-connected 3D settings such as tunnels, whose topologies are often theoretically modeled as high genus 3D surfaces. SINUS is carried out by first slicing the genus-n surface along a maximum cut set based on Morse theory and Reeb graph, in order to form a genus-0 surface with 2n boundaries. Then, it groups these 2n boundaries into two groups each of which is next connected together. By doing so, a genus-0 surface with exactly two boundaries emerges, which can be flattened into a strip, using the Ricci flow algorithm and next mapped to a planar annulus by Möbius Transform. By assigning nodes virtual coordinates on the planar annulus, SINUS finally realizes a variation of greedy routing to enable individual nodes to make local muting decisions. Our simulation results show that SINUS can achieve low-stretch routing with guaranteed delivery, as well as balanced traffic load.},
keywords={graph theory;set theory;telecommunication network routing;wireless sensor networks;traffic load balancing;local muting decision;greedy routing;virtual coordinate;Mobius transform;planar annulus;Ricci flow algorithm;Reeb graph;Morse theory;maximum cut set;complex connected 3D setting;high genus 3D surface;WSN;guaranteed delivery;distributed routing algorithm;scalable routing algorithm;SINUS;Routing;Wireless sensor networks;Measurement;Topology;Strips;Indexes;Network topology},
doi={10.1109/INFCOM.2013.6567020},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567021,
author={K. Xing and S. Zhang and L. Shi and H. Zhu and Y. Wang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A localized backbone renovating algorithm for wireless ad hoc and sensor networks},
year={2013},
volume={},
number={},
pages={2184-2192},
abstract={In this paper we propose and analyze a localized backbone renovating algorithm (LBR) to renovate a broken backbone in the network. This research is motivated by the problem of virtual backbone maintenance in wireless ad hoc and sensor networks, where the coverage area of nodes are disks with identical radii. According to our theoretical analysis, the proposed algorithm has the ability to renovate the backbone in a purely localized manner with a guaranteed connectivity of the network, while keeping the backbone size within a constant factor from that of the minimum CDS. Both the communication overhead and computation overhead of the LBR algorithm are O(k), where k is the number of nodes broken or added. We also conduct extensive simulation study on connectivity, backbone size, and the communication/computation overhead. The simulation results show that the proposed algorithm can always keep the renovated backbone being connected at low communication/computation overhead with a relatively small backbone, compared with other existing schemes. Furthermore, the LBR algorithm has the ability to deal with arbitrary number of node failures and additions in the network.},
keywords={ad hoc networks;wireless sensor networks;localized backbone renovating algorithm;wireless ad hoc networks;wireless sensor networks;virtual backbone maintenance;identical radii;CDS;communication overhead;computation overhead;LBR algorithm;communication-computation overhead;Ad hoc networks;Algorithm design and analysis;Topology;Wireless sensor networks;Wireless communication;Maintenance engineering;Network topology;maximal independent set;backbone renovating},
doi={10.1109/INFCOM.2013.6567021},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567022,
author={M. Benter and F. Neumann and H. Frey},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Reactive planar spanner construction in wireless ad hoc and sensor networks},
year={2013},
volume={},
number={},
pages={2193-2201},
abstract={Within reactive topology control, a node determines its adjacent edges of a network subgraph without prior knowledge of its neighborhood. The goal is to construct a local view on a topology which provides certain desired properties such as planarity. During algorithm execution, a node, in general, is not allowed to determine all its neighbors of the network graph. There are well-known reactive algorithms for computing planar subgraphs. However, the subgraphs obtained do not have constant Euclidean spanning ratio. This means that routing along these subgraphs may result in potentially long detours. So far, it has been unknown if planar spanners can be constructed reactively. In this work, we show that at least under the unit disk network model, this is indeed possible, by proposing an algorithm for reactive construction of the partial Delaunay triangulation, which recently turned out to be a spanner. Furthermore, we show that our algorithm is message-optimal as a node will only exchange messages with nodes that are also neighbors in the spanner. The algorithm's presentation is complemented by a rigorous proof of correctness.},
keywords={ad hoc networks;graph theory;graphs;mesh generation;telecommunication network topology;wireless sensor networks;reactive planar spanner construction;wireless ad hoc network;wireless sensor network;reactive topology control;network subgraph;planarity;reactive algorithm execution;planar subgraphs;Euclidean spanning ratio;unit disk network model;reactive construction;partial Delaunay triangulation;Topology;Protocols;Routing;Network topology;Bismuth;Computational modeling;Wireless sensor networks;Reactive topology control;Euclidean spanner;partial Delaunay triangulation;localized algorithm},
doi={10.1109/INFCOM.2013.6567022},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567023,
author={T. He and D. Goeckel and R. Raghavendra and D. Towsley},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Endhost-based shortest path routing in dynamic networks: An online learning approach},
year={2013},
volume={},
number={},
pages={2202-2210},
abstract={We consider the problem of endhost-based shortest path routing in a network with unknown, time-varying link qualities. Endhost-based routing is needed when internal nodes of the network do not have the scope or capability to provide globally optimal paths to given source-destination pairs, as can be the case in networks consisting of autonomous subnetworks or those with endhost-based routing restrictions. Assuming the source can probe links along selected paths, we formulate the problem as an online learning problem, where an existing solution achieves a performance loss (called regret) that is logarithmic in time with respect to (wrt) an offline algorithm that knows the link qualities. Current solutions assume coupled probing and routing; in contrast, we give a simple algorithm based on decoupled probing and routing, whose regret is only constant in time. We then extend our solution to support multi-path probing and cooperative learning between multiple sources, where we show an inversely proportional decay in regret wrt the probing rate. We also show that without the decoupling, the regret grows at least logarithmically in time, thus establishing decoupling as critical for obtaining constant regret. Although our analysis assumes certain conditions (i.i.d.) on link qualities, our solution applies with straightforward amendments to much broader scenarios where these conditions are relaxed. The efficacy of the proposed solution is verified by trace-driven simulations.},
keywords={learning (artificial intelligence);telecommunication computing;telecommunication network routing;endhost-based shortest path routing;dynamic network;online learning approach;time-varying link quality;source-destination pair;autonomous subnetwork;endhost-based routing restriction;offline algorithm;decoupled probing;multipath probing;cooperative learning;inversely proportional decay;trace-driven simulation;Routing;Probes;Upper bound;Topology;Time measurement;Weight measurement},
doi={10.1109/INFCOM.2013.6567023},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567024,
author={S. Agarwal and M. Kodialam and T. V. Lakshman},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Traffic engineering in software defined networks},
year={2013},
volume={},
number={},
pages={2211-2219},
abstract={Software Defined Networking is a new networking paradigm that separates the network control plane from the packet forwarding plane and provides applications with an abstracted centralized view of the distributed network state. A logically centralized controller that has a global network view is responsible for all the control decisions and it communicates with the network-wide distributed forwarding elements via standardized interfaces. Google recently announced [5] that it is using a Software Defined Network (SDN) to interconnect its data centers due to the ease, efficiency and flexibility in performing traffic engineering functions. It expects the SDN architecture to result in better network capacity utilization and improved delay and loss performance. The contribution of this paper is on the effective use of SDNs for traffic engineering especially when SDNs are incrementally introduced into an existing network. In particular, we show how to leverage the centralized controller to get significant improvements in network utilization as well as to reduce packet losses and delays. We show that these improvements are possible even in cases where there is only a partial deployment of SDN capability in a network. We formulate the SDN controller's optimization problem for traffic engineering with partial deployment and develop fast Fully Polynomial Time Approximation Schemes (FPTAS) for solving these problems. We show, by both analysis and ns-2 simulations, the performance gains that are achievable using these algorithms even with an incrementally deployed SDN.},
keywords={approximation theory;computer centres;computer networks;network interfaces;telecommunication traffic;traffic engineering;software defined networking;network control plane;packet forwarding plane;distributed network state;logically centralized controller;network-wide distributed forwarding elements;standardized interfaces;Google;data centers;SDN architecture;network utilization;packet loss reduction;packet delay reduction;SDN controller optimization problem;fully polynomial time approximation schemes;FPTAS;Routing;Peer-to-peer computing;Delays;Standards;Optimization;Current measurement;IP networks},
doi={10.1109/INFCOM.2013.6567024},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567025,
author={L. Vanbever and S. Vissicchio and L. Cittadini and O. Bonaventure},
booktitle={2013 Proceedings IEEE INFOCOM},
title={When the cure is worse than the disease: The impact of graceful IGP operations on BGP},
year={2013},
volume={},
number={},
pages={2220-2228},
abstract={Network upgrades, performance optimizations and traffic engineering activities often force network operators to adapt their IGP configuration. Recently, several techniques have been proposed to change an IGP configuration (e.g., link weights) in a disruption-free manner. Unfortunately, none of these techniques considers the impact of IGP changes on BGP correctness. In this paper, we show that known reconfiguration techniques can trigger various kinds of BGP anomalies. First, we illustrate the relevance of the problem by performing simulations on a Tier-1 network. Our simulations highlight that even a few link weight changes can produce long-lasting BGP anomalies affecting a significant part of the BGP routing table. Then, we study the problem of finding a reconfiguration ordering which maintains both IGP and BGP correctness. Unfortunately, we show examples in which such an ordering does not exist. Furthermore, we prove that deciding if such an ordering exists is NP-hard. Finally, we provide sufficient conditions and configuration guidelines that enable graceful operations for both IGP and BGP.},
keywords={network topology;routing protocols;telecommunication traffic;network upgrades;performance optimizations;traffic engineering activities;network operators;reconfiguration techniques;IGP configuration;BGP anomalies;link weight changes;Routing;Topology;Transient analysis;Oscillators;Routing protocols;Maintenance engineering},
doi={10.1109/INFCOM.2013.6567025},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567026,
author={J. T. Chiang and Y. Hu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Did you also hear that? Spectrum sensing using Hermitian inner product},
year={2013},
volume={},
number={},
pages={2229-2237},
abstract={Spectrum sensing is one of the most important enabling techniques on which to build a cognitive radio network. However, previously proposed techniques often have shortcomings in non-ideal environments: 1) An energy detector is simple but cannot perform in face of uncertain noise power; 2) A matched filter is the optimal detector, but performs poorly with clock drifts; 3) Eigenvalue-based blind feature detectors show great promise, but cannot detect signals that are noise-like; and 4) Above protocols all rely on field survey to determine the proper decision thresholds. We propose HIPSS and its extension Δ-HIPSS that are based on the Hermitian-inner-product of two observations acquired by a wireless receiver over multiple radio paths. HIPSS and Δ-HIPSS are lightweight and through extensive analysis and evaluation, we show that 1) HIPSS and Δ-HIPSS are robust in the presence of noise power uncertainties; 2) HIPSS and ΔHIPSS require neither a much longer observation duration nor complex computation compared to an energy detector in ideal setting; 3) HIPSS and Δ-HIPSS can detect noise-like primary signals; and 4) Δ-HIPSS can reliably return sensing decisions without necessitating any field surveys.},
keywords={cognitive radio;eigenvalues and eigenfunctions;Hermitian matrices;matched filters;protocols;radio spectrum management;signal detection;spectrum sensing;Hermitian inner product;cognitive radio network;energy detector;uncertain noise power;matched filter;clock drifts;eigenvalue-based blind feature detectors;signal detection;protocols;Δ-HIPSS;Detectors;Receivers;Signal to noise ratio;Feature extraction;Correlation},
doi={10.1109/INFCOM.2013.6567026},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567027,
author={S. Gong and P. Wang and W. Liu and W. Zhuang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Performance bounds of energy detection with signal uncertainty in cognitive radio networks},
year={2013},
volume={},
number={},
pages={2238-2246},
abstract={The harmonic coexistence of secondary users (SUs) and primary users (PUs) in cognitive radio networks requires SUs to identify the idle spectrum bands. One common approach to achieve spectrum awareness is through spectrum sensing, which usually assumes known distributions of the received signals. However, due to the nature of wireless channels, such an assumption is often too strong to be realistic, and leads to unreliable detection performance in practical networks. In this paper, we study the sensing performance under distribution uncertainty, i.e., the actual distribution functions of the received signals are subject to ambiguity and not fully known. Firstly, we define a series of uncertainty models based on signals' moment statistics in different spectrum conditions. Then we present mathematical formulations to study the detection performance corresponding to these uncertainty models. Moreover, in order to make use of the distribution information embedded in historical data, we extract a reference distribution from past channel observations, and define a new uncertainty model in terms of it. With this uncertainty model, we propose two iterative procedures to study the false alarm probability and detection probability, respectively. Numerical results show that the detection performance with a reference distribution is less conservative compared with that of the uncertainty models merely based on signal statistics.},
keywords={cognitive radio;iterative methods;radio spectrum management;signal detection;statistical distributions;wireless channels;performance bound;energy detection;signal uncertainty distribution;cognitive radio network;harmonic coexistence;secondary user;primary user;idle spectrum band identification;spectrum awareness;spectrum sensing;wireless channels;distribution function;signal moment statistics;spectrum condition;distribution information;uncertainty model detection;iterative procedure;false alarm probability;detection probability;Uncertainty;Sensors;Noise;Distribution functions;Numerical models;Gaussian distribution;Mathematical model;Cognitive radio;spectrum sensing;distribution uncertainty;probabilistic distance measure},
doi={10.1109/INFCOM.2013.6567027},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567028,
author={S. Yoon and L. E. Li and S. C. Liew and R. R. Choudhury and I. Rhee and K. Tan},
booktitle={2013 Proceedings IEEE INFOCOM},
title={QuickSense: Fast and energy-efficient channel sensing for dynamic spectrum access networks},
year={2013},
volume={},
number={},
pages={2247-2255},
abstract={Spectrum sensing, the task of discovering spectrum usage at a given location, is a fundamental problem in dynamic spectrum access networks. While sensing in narrow spectrum bands is well studied in previous work, wideband spectrum sensing is challenging since a wideband radio is generally too expensive and power consuming for mobile devices. Sequential scan, on the other hand, can be very slow if the wide spectrum band contains many narrow channels. In this paper, we propose an analog-filter based spectrum sensing technique, which is much faster than sequential scan and much cheaper than using a wideband radio. The key insight is that, if the sum of energy on a contiguous band is low, we can conclude that all channels in this band are clear with just one measurement. Based on this insight, we design an intelligent search algorithm to minimize the number of total measurements. We prove that the algorithm has the same asymptotic complexity as compressed sensing while our design is much simpler and easily implementable in the real hardware. We show the availability of our technique using hardware devices that include analog filters and analog energy detectors. Our extensive evaluation using real TV “white space” signals shows the effectiveness of our technique.},
keywords={broadband networks;compressed sensing;mobile handsets;radio spectrum management;subscriber loops;wireless channels;energy-efficient channel sensing;dynamic spectrum access networks;spectrum sensing;narrow spectrum bands;wideband spectrum sensing;wideband radio;mobile devices;power consumption;intelligent search algorithm;asymptotic complexity;compressed sensing;hardware devices;analog filters;analog energy detectors;TV white space signal;Bandwidth;Detectors;Noise;Algorithm design and analysis;Noise measurement;Hardware},
doi={10.1109/INFCOM.2013.6567028},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567029,
author={Q. Liu and X. Wang and Y. Cui},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Scheduling of sequential periodic sensing for cognitive radios},
year={2013},
volume={},
number={},
pages={2256-2264},
abstract={Spectrum sensing enables cognitive radios (CRs) to opportunistically access the under-utilized spectrum. Existing efforts on sensing have not adequately addressed sensing scheduling over time for better detection performance. In this work, we consider sequential periodic sensing of an in-band channel. We focus primarily on finding the appropriate sensing frequency during an SU's active data transmission on a licensed channel. Change and outlier detection schemes are designed specifically to facilitate short-term sensing adaptation to the variations in sensed data. Simulation results demonstrate that our design guarantees better conformity to the spectrum access policies by significantly reducing the delay in change detection while ensuring better sensing accuracy.},
keywords={channel allocation;cognitive radio;radio spectrum management;sequential periodic sensing;cognitive radio;spectrum sensing;in-band channel;active data transmission;spectrum access policy;Signal to noise ratio;Data communication;Detectors;Accuracy;Interference;Cognitive radio;sequential periodic spectrum sensing;in-band channel;channel detection time;change and outlier detection},
doi={10.1109/INFCOM.2013.6567029},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567030,
author={I. Safaka and C. Fragouli and K. Argyraki and S. Diggavi},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Exchanging pairwise secrets efficiently},
year={2013},
volume={},
number={},
pages={2265-2273},
abstract={We consider the problem where a group of wireless nodes, connected to the same broadcast domain, want to create pairwise secrets, in the presence of an adversary Eve, who tries to listen in and steal these secrets. Existing solutions assume that Eve cannot perform certain computations (e.g., large-integer factorization) in useful time. We ask the question: can we solve this problem without assuming anything about Eve's computational capabilities? We propose a simple secret-agreement protocol, where the wireless nodes keep exchanging bits until they have agreed on pairwise secrets that Eve cannot reconstruct with very high probability. Our protocol relies on Eve's limited network presence (the fact that she cannot be located at an arbitrary number of points in the network at the same time), but assumes nothing about her computational capabilities. We formally show that, under standard theoretical assumptions, our protocol is information-theoretically secure (it leaks zero information to Eve about the secrets). Using a small wireless testbed of smart-phones, we provide experimental evidence that it is feasible for 5 nodes to create thousands of secret bits per second, with their secrecy being independent from the adversary's capabilities.},
keywords={cryptographic protocols;probability;radio networks;telecommunication security;pairwise secrets;wireless node group;broadcast domain;Eve computational capabilities;secret-agreement protocol;probability;smartphone wireless testbed;Protocols;Communication system security;Wireless communication;Authentication;Reliability;Privacy;Standards},
doi={10.1109/INFCOM.2013.6567030},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567031,
author={C. Hu and X. Cheng and F. Zhang and D. Wu and X. Liao and D. Chen},
booktitle={2013 Proceedings IEEE INFOCOM},
title={OPFKA: Secure and efficient Ordered-Physiological-Feature-based key agreement for wireless Body Area Networks},
year={2013},
volume={},
number={},
pages={2274-2282},
abstract={Body Area Networks (BANs) are expected to play a major role in patient health monitoring in the near future. Providing an efficient key agreement with the prosperities of plug-n-play and transparency to support secure inter-sensor communications is critical especially during the stages of network initialization and reconfiguration. In this paper, we present a novel key agreement scheme termed Ordered-Physiological-Feature-based Key Agreement (OPFKA), which allows two sensors belonging to the same BAN to agree on a symmetric cryptographic key generated from the overlapping physiological signal features, thus avoiding the pre-distribution of keying materials among the sensors embedded in the same human body. The secret features computed from the same physiological signal at different parts of the body by different sensors exhibit some overlap but they are not completely identical. To overcome this challenge, we detail a computationally efficient protocol to securely transfer the secret features of one sensor to another such that two sensors can easily identify the overlapping ones. This protocol possesses many nice features such as the resistance against brute force attacks. Experimental results indicate that OPFKA is secure, efficient, and feasible. Compared with the state-of-the-art PSKA protocol, OPFKA achieves a higher level of security at a lower computational overhead.},
keywords={body area networks;cryptographic protocols;patient monitoring;telecommunication security;OPFKA;wireless body area networks;efficient ordered-physiological-feature-based key agreement;secure ordered-physiological-feature-based key agreement;BAN;patient health monitoring;plug-n-play;transparency;ordered-physiological-feature-based key agreement;symmetric cryptographic key;overlapping physiological signal features;physiological signal;computationally efficient protocol;PSKA protocol;security level;computational overhead;Receivers;Sensor phenomena and characterization;Security;Biomedical monitoring;Vectors;Indexes;Body Area Networks (BANs);secure intersensor communications;Inter-Pulse-Interval (IPI);physiological feature based key agreement},
doi={10.1109/INFCOM.2013.6567031},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567032,
author={X. Zhu and F. Xu and E. Novak and C. C. Tan and Q. Li and G. Chen},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Extracting secret key from wireless link dynamics in vehicular environments},
year={2013},
volume={},
number={},
pages={2283-2291},
abstract={A crucial component of vehicular network security is to establish a secure wireless channel between any two vehicles. In this paper, we propose a scheme to allow two cars to extract a secret key from RSSI (Received Signal Strength Indicator) values in such a way that nearby cars cannot obtain the same secret. Our solution can be executed in noisy, outdoor vehicular environments. We also propose an online parameter learning mechanism to adapt to different channel conditions. We conduct extensive realworld experiments to validate our solution.},
keywords={automobiles;private key cryptography;radio links;telecommunication security;vehicular ad hoc networks;wireless channels;secret key extraction;wireless link dynamics;vehicular network security;wireless channel security;received signal strength indicator values;RSSI values;outdoor vehicular environments;online parameter learning mechanism;real-world experiments;Entropy;Markov processes;Wireless communication;Communication system security;Noise;Correlation;Vehicle dynamics},
doi={10.1109/INFCOM.2013.6567032},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567033,
author={P. Huang and X. Wang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Fast secret key generation in static wireless networks: A virtual channel approach},
year={2013},
volume={},
number={},
pages={2292-2300},
abstract={Physical layer secret key generation exploits the inherent randomness of wireless channels. However, for wireless channels with long coherence time, key generation rate can be extremely low due to lack of randomness in channels. In this paper, a novel key generation scheme is proposed for a general wireless network where channels are static and each transmitter is equipped with two antennas. It integrates opportunistic beamforming and frequency diversity to achieve fast secret key generation. In this scheme, channel fluctuations are first induced by controlling amplitude and phase of each symbol in the training sequence on each antenna. Thus, key generation rate is significantly increased. However, the induced channel fluctuations lead to correlation between the legitimate channel and the eavesdropping channel, which compromises key secrecy. To this end, frequency diversity is then exploited to ensure that key secrecy grows with the key size. The secret key generation scheme is investigated in both narrowband and wideband systems, and its performance is evaluated through both theoretical analysis and simulations. Performance results have validated randomness and secrecy of secret keys and also illustrate that the proposed scheme can generate secret keys at a rate of 2Kb/s for narrowband systems and 20Kb/s for wideband systems.},
keywords={antenna arrays;cryptography;diversity reception;radio networks;radio transmitters;telecommunication security;wireless channels;physical layer secret key generation;static wireless networks;virtual channel approach;wireless channels;long coherence time;lack-of-channel randomness;channel fluctuations;amplitude control;phase control;induced channel fluctuations;legitimate channel;eavesdropping channel;key secrecy;frequency diversity;narrowband systems;wideband systems;performance evaluation;bit rate 20 kbit/s;bit rate 2 kbit/s;Antennas;Quantization (signal);Frequency diversity;Channel estimation;Error probability;Wireless communication;Coherence},
doi={10.1109/INFCOM.2013.6567033},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567034,
author={L. Guo and C. Zhang and H. Yue and Y. Fang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A privacy-preserving social-assisted mobile content dissemination scheme in DTNs},
year={2013},
volume={},
number={},
pages={2301-2309},
abstract={Mobile content dissemination is very useful for many mobile applications in delay tolerant networks (DTNs), like instant messaging, file sharing, and advertisement dissemination, etc. Recently, social-based approaches, which attempt to exploit social behaviors of DTN users to forward time-insensitive data, such as family photos and friends' sightseeing video clips, have attracted intensive attentions in designing routing schemes in DTNs. Most social-based schemes leverage users' contact history and social information (e.g., community and friendship) as metrics to improve the dissemination performance. In these schemes, users need to obtain others' social information to determine their dissemination strategy, which apparently compromises others users' privacy. Moreover, the owner of mobile contents may only want to disclose his/her data to a particular group of users rather than revealing it to the public. In this paper, we propose a privacy-preserving social-assisted mobile content dissemination scheme in DTNs. We apply users' verifiable attributes to establish their potential social relationships in terms of identical attributes in a privacy-preserving way. Besides, to provide the confidentiality of mobile contents, our approach enables users to encrypt contents before the dissemination process, and only allows users who have particular attributes to decrypt them. By trace-driven simulations and experiments, we show the security and efficiency of our proposed scheme.},
keywords={cryptography;delay tolerant networks;mobile computing;privacy-preserving social-assisted mobile content dissemination scheme;DTN;delay tolerant networks;mobile applications;social behaviors;time-insensitive data;routing schemes;dissemination performance;dissemination strategy;threshold attribute-based encryption;Mobile communication;Privacy;Routing;Encryption;Protocols;Threshold attribute-based encryption;witness-indistinguishable proof;privacy-preserving},
doi={10.1109/INFCOM.2013.6567034},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567035,
author={T. Ning and Z. Yang and H. Wu and Z. Han},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Self-Interest-Driven incentives for ad dissemination in autonomous mobile social networks},
year={2013},
volume={},
number={},
pages={2310-2318},
abstract={In this paper, we propose a Self-Interest-Driven (SID) incentive scheme to stimulate cooperation among selfish nodes for ad dissemination in autonomous mobile social networks. As a key innovation of SID, we introduce “virtual checks” to eliminate the needs of accurate knowledge about whom and how many credits ad provider should pay. A virtual check is included in each ad packet. When an intended receiver receives the packet for the first time from an intermediate node, the former authorizes the latter a digitally signed check, which serves as a proof of successful ad delivery. Multiple copies of a virtual check can be created and signed by different receivers. When a node that owns a signed check meets the ad provider, it requests the provider to cash the check. Both ad packets and signed checks can be traded among mobile nodes. We propose the effective mechanisms to define virtual rewards for ad packets and virtual checks, and formulate the nodal interaction as a two-player cooperative game, whose solution is obtained by the Nash Bargaining Theorem. Extensive simulations are carried out to compare SID with other existing incentive algorithms under real world mobility traces.},
keywords={advertising data processing;game theory;mobile computing;social networking (online);self-interest-driven incentives;ad dissemination;autonomous mobile social networks;SID incentive scheme;virtual check;ad packet;digitally signed check;ad delivery;virtual rewards;nodal interaction;two-player cooperative game;Nash bargaining theorem;real world mobility traces;Receivers;Mobile computing;Games;Social network services;Peer-to-peer computing;Mobile nodes},
doi={10.1109/INFCOM.2013.6567035},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567036,
author={J. Wu and M. Xiao and L. Huang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Homing spread: Community home-based multi-copy routing in mobile social networks},
year={2013},
volume={},
number={},
pages={2319-2327},
abstract={A mobile social network (MSN) is a special delay tolerant network (DTN) composed of mobile nodes with social characteristics. Mobile nodes in MSNs generally visit community homes frequently, while other locations are visited less frequently. We propose a novel zero-knowledge MSN routing algorithm, homing spread (HS). The community homes have a higher priority to spread messages into the network. Theoretical analysis shows that the proposed algorithm can spread a given number of message copies in an optimal way when the inter-meeting times between any two nodes and between a node and a community home follow exponential distributions. We also calculate the expected delivery delay of HS. In addition, extensive simulations are conducted. Results show that community homes are important factors in efficient message spreading. By using homes to spread messages faster, HS achieves a better performance than existing zero-knowledge MSN routing algorithms, including Epidemic, with a given number of copies, and Spray&amp;Wait.},
keywords={delay tolerant networks;mobile radio;telecommunication network routing;homing spread;community home based multicopy routing;mobile social networks;delay tolerant network;DTN;mobile nodes;social characteristics;novel zero knowledge MSN routing algorithm;HS;message copies;exponential distributions;message spreading;Delays;Probability density function;Communities;Mobile nodes;Routing;Community home;mobile social networks (M-SNs);routing},
doi={10.1109/INFCOM.2013.6567036},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567037,
author={J. Jeong and Y. Yi and J. Cho and D. Y. Eun and S. Chong},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Wi-Fi sensing: Should mobiles sleep longer as they age?},
year={2013},
volume={},
number={},
pages={2328-2336},
abstract={An essential condition precedent to the success of mobile applications based on Wi-Fi (e.g., iCloud) is an energy-efficient Wi-Fi sensing. From a user's perspective, a good WiFi sensing policy should depend on both inter-AP arrival and contact duration time distributions. Prior work focuses on limited cases of those two distributions (e.g., exponential) or introduces heuristic approaches such as AI (Additive Increase). In this paper, we formulate a functional optimization problem on Wi-Fi sensing under general inter-AP and contact duration distributions, and propose how each user should sense Wi-Fi APs to strike a balance between energy efficiency and performance, depending on the users' mobility pattern. To that end, we derive an optimal condition which sheds insights into the aging property, the key feature required by efficient Wi-Fi sensing polices. Guided by the analytical studies and the implications, we develop a new sensing algorithm, called WiSAG (Wi-Fi Sensing with AGing), which is demonstrated to outperform the existing sensing algorithms up to 34% through extensive trace-driven simulations using the real mobility traces gathered from smartphones.},
keywords={heuristic programming;mobility management (mobile radio);optimisation;smart phones;wireless LAN;energy-efficient Wi-Fi sensing policy;inter-AP arrival;heuristic approaches;additive increase;AI;functional optimization problem;contact duration distributions;user mobility pattern;WiSAG sensing algorithm;Wi-Fi sensing with aging;trace-driven simulations;smart phones;Sensors;IEEE 802.11 Standards;Mobile communication;Aging;Optimized production technology;Artificial intelligence},
doi={10.1109/INFCOM.2013.6567037},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567038,
author={G. S. Paschos and C. Fragiadakis and L. Georgiadis and L. Tassiulas},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Wireless network coding with partial overhearing information},
year={2013},
volume={},
number={},
pages={2337-2345},
abstract={We study an 1-hop broadcast channel with two receivers. Due to overhearing channels, the receivers have side information which can be leveraged by interflow network coding techniques to provide throughput increase. In this setup, we consider two different control mechanisms, the deterministic system, where the contents of the receivers' buffers are announced to the coding node via overhearing reports and the stochastic system, where the coding node makes stochastic control decisions based on statistics and the performance is improved via NACK messages. We study the minimal evacuation times for the two systems and obtain analytical expressions of the throughput region for the deterministic and the code-constrained region for the stochastic. We show that maximum performance is achieved by simple XOR policies. For equal transmission rates r1= r2, the two regions are equal. If r1≠ r2, we showcase the tradeoff between throughput and overhead.},
keywords={broadcast channels;network coding;radio networks;radio receivers;stochastic processes;telecommunication control;wireless channels;wireless network coding;partial overhearing information;1-hop broadcast channel;receivers;interflow network coding technique;control mechanisms;stochastic system;stochastic control decision;statistics;NACK messages;code-constrained region;XOR policyy;Receivers;Encoding;Throughput;Decoding;Stability analysis;Stochastic systems;Wireless networks},
doi={10.1109/INFCOM.2013.6567038},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567039,
author={J. Li and X. Wang and B. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Cooperative pipelined regeneration in distributed storage systems},
year={2013},
volume={},
number={},
pages={2346-2354},
abstract={In distributed storage systems, a substantial volume of data are stored in a distributed fashion, across a large number of storage nodes. To maintain data integrity, when existing storage nodes fail, lost data are regenerated at replacement nodes. Regenerating multiple data losses in batches can reduce the consumption of bandwidth. However, existing schemes are only able to achieve lower bandwidth consumption by utilizing a large number of participating nodes. In this paper, we propose a cooperative pipelined regeneration process that regenerates multiple data losses cooperatively with much fewer participating nodes. We show that cooperative pipelined regeneration is not only able to maintain optimal data integrity, but also able to further reduce the consumption of bandwidth as well.},
keywords={data integrity;information storage;linear codes;storage nodes;cooperative pipelined regeneration process;data loss regeneration;optimal data integrity;bandwidth consumption reduction;distributed storage systems;replacement nodes;Linear code;Bandwidth;Educational institutions;Distributed databases;Redundancy;Resilience},
doi={10.1109/INFCOM.2013.6567039},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567040,
author={Y. Hu and P. P. C. Lee and K. W. Shum},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Analysis and construction of functional regenerating codes with uncoded repair for distributed storage systems},
year={2013},
volume={},
number={},
pages={2355-2363},
abstract={Modern distributed storage systems apply redundancy coding techniques to stored data. One form of redundancy is based on regenerating codes, which can minimize the repair bandwidth, i.e., the amount of data transferred when repairing a failed storage node. Existing regenerating codes mainly require surviving storage nodes encode data during repair. In this paper, we study functional minimum storage regenerating (FMSR) codes, which enable uncoded repair without the encoding requirement in surviving nodes, while preserving the minimum repair bandwidth guarantees and also minimizing disk reads. Under double-fault tolerance settings, we formally prove the existence of FMSR codes, and provide a deterministic FMSR code construction that can significantly speed up the repair process. We further implement and evaluate our deterministic FMSR codes to show the benefits. Our work is built atop a practical cloud storage system that implements FMSR codes, and we provide theoretical validation to justify the practicality of FMSR codes.},
keywords={cloud computing;codes;fault tolerant computing;redundancy;storage management;uncoded repair;distributed storage system;redundancy coding technique;data storage;repair bandwidth minimization;data transfer;storage node;data encoding;functional minimum storage regenerating codes;encoding requirement;minimum repair bandwidth;disk read minimization;double-fault tolerance setting;repair process;deterministic FMSR codes;cloud storage system;Maintenance engineering;Encoding;Bandwidth;Peer-to-peer computing;Fault tolerance;Fault tolerant systems;Reed-Solomon codes},
doi={10.1109/INFCOM.2013.6567040},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567041,
author={X. Yin and Y. Wang and X. Wang and X. Xue and Z. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A graph minor perspective to network coding: Connecting algebraic coding with network topologies},
year={2013},
volume={},
number={},
pages={2364-2372},
abstract={Network Coding encourages information coding across a communication network. While the necessity, benefit and complexity of network coding are sensitive to the underlying graph structure of a network, existing theory on network coding often treats the network topology as a black box, focusing on algebraic or information theoretic aspects of the problem. This work aims at an in-depth examination of the relation between algebraic coding and network topologies. We mathematically establish a series of results along the direction of: if network coding is necessary/beneficial, or if a particular finite field is required for coding, then the network must have a corresponding hidden structure embedded in its underlying topology, and such embedding is computationally efficient to verify. Specifically, we first formulate a meta-conjecture, the NC-Minor Conjecture, that articulates such a connection between graph theory and network coding, in the language of graph minors. We next prove that the NC-Minor Conjecture is almost equivalent to the Hadwiger Conjecture, which connects graph minors with graph coloring. Such equivalence implies the existence of K<sub>4</sub>, K<sub>5</sub>, K<sub>6</sub>, and K<sub>O(q/ log q)</sub> minors, for networks requiring F<sub>3</sub>, F<sub>4</sub>, F<sub>5</sub> and F<sub>q</sub>, respectively. We finally prove that network coding can make a difference from routing only if the network contains a K<sub>4</sub> minor, and this minor containment result is tight. Practical implications of the above results are discussed.},
keywords={algebraic codes;graph colouring;information theory;network coding;telecommunication network topology;graph minor perspective;network coding;algebraic coding;network topologies;information coding;communication network;graph structure;black box;information theoretic aspects;finite field;hidden structure;meta-conjecture;minor conjecture;graph theory;graph minors;Hadwiger conjecture;graph coloring;Encoding;Network coding;Network topology;Routing;Vectors;Receivers;Color},
doi={10.1109/INFCOM.2013.6567041},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567042,
author={B. Jiang and N. Hegde and L. Massoulié and D. Towsley},
booktitle={2013 Proceedings IEEE INFOCOM},
title={How to Optimally allocate your budget of attention in social networks},
year={2013},
volume={},
number={},
pages={2373-2381},
abstract={We consider the performance of information propagation through social networks in a scenario where each user has a budget of attention, that is, a constraint on the frequency with which he pulls content from neighbors. In this context we ask the question “when users make selfish decisions on how to allocate their limited access frequency among neighbors, does information propagate efficiently?” For the metric of average propagation delay, we provide characterizations of the optimal social cost and the social cost under selfish user optimizations for various topologies of interest. Three situations may arise: well-connected topologies where delay is small even under selfish optimization; tree-like topologies where selfish optimization performs poorly while optimal social cost is low; and “stretched” topologies where even optimal social cost is high. We propose a mechanism for incentivizing users to modify their selfish behaviour, and observe its efficiency in the family of tree-like topologies mentioned above.},
keywords={information dissemination;social networking (online);trees (mathematics);social networks;information propagation;access frequency;average propagation delay;optimal social cost;selfish user optimizations;tree-like topologies;stretched topologies;Resource management;Topology;Social network services;Network topology;Delays;Radio spectrum management;Stability analysis},
doi={10.1109/INFCOM.2013.6567042},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567043,
author={J. Cao and H. Gao and L. E. Li and B. Friedman},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Enterprise social network analysis and modeling: A tale of two graphs},
year={2013},
volume={},
number={},
pages={2382-2390},
abstract={Like their public counterpart such as Facebook and Twitter, enterprise social networks are poised to revolutionize how people interact in the workplace. There is a pressing need to understand how people are using these social networks. Unlike the public social networks like Facebook or Twitter which are normally characterized using the social graph or the interaction graph, enterprise social networks are also governed by an organization graph. Based on a six month dataset collected from May through October 2011 of a large enterprise social network, we study the characteristics of activities of its enterprise social network. We observe that the user attributes in the organization graph such as geographic location (eg. country) and his/her rank in the company hierarchy have a significant impact on how the user uses the social network and how user interacts with each other. We then build formal statistical models of user interaction graphs in enterprise social network and quantify effects of user attributes from organization graphs on these interactions. Furthermore, as the enterprise social network medium bring users from diverse locations and social status forming ad-hoc communities, our statistical model can be further enhanced by including these ad-hoc communities.},
keywords={graph theory;social networking (online);statistical analysis;enterprise social network analysis;Facebook;Twitter;public social network;social graph;user interaction graph;organization graph;geographic location;formal statistical model;Communities;Companies;Blogs;Logistics;Twitter},
doi={10.1109/INFCOM.2013.6567043},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567044,
author={D. Wang and H. Park and G. Xie and S. Moon and M. Kaafar and K. Salamatian},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A genealogy of information spreading on microblogs: A Galton-Watson-based explicative model},
year={2013},
volume={},
number={},
pages={2391-2399},
abstract={In this paper, we study the process of information diffusion in a microblog service developing Galton-Watson with Killing (GWK) model. Microblog services offer a unique approach to online information sharing allowing microblog users to forward messages to others. We describe an information propagation as a discrete GWK process based on Galton-Watson model which models the evolution of family names. Our model explains the interaction between the topology of the social graph and the intrinsic interest of the message. We validate our model on dataset collected from Sina Weibo and Twitter microblog. Sina Weibo is a Chinese microblog web service which reached over 100 million users as for January 2011. Our Sina Weibo dataset contains over 261 thousand tweets which have retweets and 2 million retweets from 500 thousand users. Twitter dataset contains over 1.1 million tweets which have retweets and 3.3 million retweets from 4.3 million users. The results of the validation show that our proposed GWK model fits the information diffusion of microblog service very well in terms of the number of message receivers. We show that our model can be used in generating tweets load and also analyze the relationships between parameters of our model and popularity of the diffused information. To the best of our knowledge, this paper is the first to give a systemic and comprehensive analysis for the information diffusion on microblog services, to be used in tweets-like load generators while still guaranteeing popularity distribution characteristics.},
keywords={graph theory;social networking (online);information spreading genealogy;Chinese microblog Web service;Galton-Watson-based explicative model;information diffusion process;Galton-Watson with Killing model;GWK model;online information sharing;information propagation;family name evolution;social graph;message intrinsic interest;Sina Weibo;Twitter microblog;tweets-like load generators;popularity distribution characteristics;Twitter;Integrated circuit modeling;Analytical models;Load modeling;Predictive models;Media},
doi={10.1109/INFCOM.2013.6567044},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567045,
author={J. Xue and Z. Yang and X. Yang and X. Wang and L. Chen and Y. Dai},
booktitle={2013 Proceedings IEEE INFOCOM},
title={VoteTrust: Leveraging friend invitation graph to defend against social network Sybils},
year={2013},
volume={},
number={},
pages={2400-2408},
abstract={Online social networks (OSNs) currently face a significant challenge by the existence and continuous creation of fake user accounts (Sybils), which can undermine the quality of social network service by introducing spam and manipulating online rating. Recently, there has been much excitement in the research community over exploiting social network structure to detect Sybils. However, they rely on the assumption that Sybils form a tight-knit community, which may not hold in real OSNs. In this paper, we present VoteTrust, a Sybil detection system that further leverages user interactions of initiating and accepting links. VoteTrust uses the techniques of trust-based vote assignment and global vote aggregation to evaluate the probability that the user is a Sybil. Using detailed evaluation on real social network (Renren), we show VoteTrust's ability to prevent Sybils gathering victims (e.g., spam audience) by sending a large amount of unsolicited friend requests and befriending many normal users, and demonstrate it can significantly outperform traditional ranking systems (such as TrustRank or BadRank) in Sybil detection.},
keywords={graph theory;social networking (online);trusted computing;VoteTrust;friend invitation graph;social network sybils;online social networks;OSN;fake user accounts;social network service;online rating;sybil detection system;trust-based vote assignment;global vote aggregation;Renren;Communities;Upper bound;Security;Equations;Mathematical model;Facebook},
doi={10.1109/INFCOM.2013.6567045},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567046,
author={S. Li and W. Zeng and D. Zhou and D. X. Gu and J. Gao},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Compact conformal map for greedy routing in wireless mobile sensor networks},
year={2013},
volume={},
number={},
pages={1409-1417},
abstract={Motivated by mobile sensor networks as in participatory sensing applications, we are interested in developing a practical, lightweight solution for routing in a mobile network. While greedy routing is robust to mobility, location errors and link dynamics, it may get stuck in a local minimum, which then requires non-trivial recovery methods. We follow the approach taken by Sarkar et. al. [24] to find an embedding of the network such that greedy routing using the virtual coordinates guarantees delivery, thus eliminating the necessity of any recovery methods. Our new contribution is to replace the in-network computation of the embedding by a preprocessing of the domain before network deployment and encode the map of network domain to virtual coordinate space by using a small number of parameters which can be pre-loaded to all sensor nodes. As a result, the map is only dependent on the network domain and is independent of the network connectivity. Each node can directly compute or update its virtual coordinates by applying the locally stored map on its geographical coordinates. This represents the first practical solution for using virtual coordinates for greedy routing in a sensor network and could be easily extended to the case of a mobile network. Being extremely light-weight, greedy routing on the virtual coordinates is shown to be very robust to mobility, link dynamics and non-unit disk graph connectivity models.},
keywords={graph theory;mobility management (mobile radio);telecommunication network routing;wireless sensor networks;compact conformal map;greedy routing;wireless mobile sensor networks;lightweight solution;mobility;location errors;link dynamics;nontrivial recovery methods;network connectivity;virtual coordinates;disk graph connectivity;Routing;Mobile communication;Mobile computing;Conformal mapping;Sensors;Surface treatment;Educational institutions},
doi={10.1109/INFCOM.2013.6567046},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567047,
author={N. Bartolini and G. Bongiovanni and T. L. Porta and S. Silvestri},
booktitle={2013 Proceedings IEEE INFOCOM},
title={On the security vulnerabilities of the virtual force approach to mobile sensor deployment},
year={2013},
volume={},
number={},
pages={2418-2426},
abstract={In this paper we point out the vulnerabilities of the virtual force approach to mobile sensor deployment, which is at the basis of many deployment algorithms. For the first time in the literature, we show that some attacks significantly hinder the capability of these algorithms to guarantee a satisfactory coverage. An attacker can compromise a few mobile sensors and force them to pursue a malicious purpose by influencing the movement of other legitimate sensors. We make an example of a simple and effective attack, called Opportunistic Movement, and give an analytical study of its efficacy. We also show through simulations that, in a typical scenario, this attack can reduce coverage by more than 50% by compromising a number of nodes as low as the 7%. We propose SecureVF, a virtual force deployment algorithm able to neutralize the above mentioned attack. We show that under SecureVF malicious sensors are detected and then ignored whenever their movement is not compliant with the moving strategy provided by SecureVF. We also investigate the performance of SecureVF through simulations, and compare it to one of the most acknowledged algorithms based on virtual forces. We show that SecureVF enables a remarkably improved coverage of the area of interest, at the expense of a low additional energy consumption.},
keywords={energy consumption;mobile radio;sensor placement;telecommunication security;wireless sensor networks;virtual force security vulnerabilities;mobile sensor;legitimate sensors;energy consumption;virtual forces;malicious sensors;virtual force deployment algorithm;SecureVF;Radio frequency;Decision support systems;Mobile sensors;self-deployment;virtual force approach;security},
doi={10.1109/INFCOM.2013.6567047},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567048,
author={D. Yu and Q. Hua and Y. Wang and J. Yu and F. C. M. Lau},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Efficient distributed multiple-message broadcasting in unstructured wireless networks},
year={2013},
volume={},
number={},
pages={2427-2435},
abstract={Multiple-message broadcast is a generalization of the traditional broadcast problem. It is to disseminate k distinct (1 ≤ k ≤ n) messages stored at k arbitrary nodes to the entire network with the fewest timeslots. In this paper, we study this basic communication primitive in unstructured wireless networks under the physical interference model (also known as the SINR model). The unstructured wireless network assumes unknown network topology, no collision detection and asynchronous communications. Our proposed randomized distributed algorithm can accomplish multiple-message broadcast in O((D + k) log n + log2n) timeslots with high probability, where D is the network diameter and n is the number of nodes in the network. To our best knowledge, this work is the first one to consider distributively implementing multiple-message broadcasting in unstructured wireless networks under a global interference model, which may shed some light on how to efficiently solve in general a “global” problem in a “local” fashion with “global” interference constraints in asynchronous wireless ad hoc networks. Apart from the algorithm, we also show an Ω(D+k+log n) lower bound for randomized distributed multiple message broadcast algorithms under the assumed network model.},
keywords={ad hoc networks;probability;radio broadcasting;radiofrequency interference;telecommunication network topology;unstructured wireless networks;distributed multiple-message broadcasting;basic communication primitive;physical interference model;network topology;collision detection;asynchronous communications;randomized distributed algorithm;probability;global interference model;global interference constraints;asynchronous wireless ad hoc networks;randomized distributed multiple message broadcast algorithms;Interference;Signal to noise ratio;Wireless networks;Receivers;Synchronization;Algorithm design and analysis;TV},
doi={10.1109/INFCOM.2013.6567048},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567049,
author={Z. Zheng and A. Liu and L. X. Cai and Z. Chen and X. Sherman Shen},
booktitle={2013 Proceedings IEEE INFOCOM},
title={ERCD: An energy-efficient clone detection protocol in WSNs},
year={2013},
volume={},
number={},
pages={2436-2444},
abstract={Wireless sensor networks (WSNs) play an increasing role in a wide variety of applications ranging from hostile environment monitoring to telemedicine services. The hardware and cost constraints of sensor nodes, however, make sensors prone to clone attacks and pose great challenges in the design and deployment of an energy-efficient WSN. In this paper, we propose a location-aware clone detection protocol, which guarantees successful clone attack detection and has little negative impact on the network lifetime. Specifically, we utilize the location information of sensors and randomly select witness nodes located in a ring area to verify the privacy of sensors and to detect clone attacks. The ring structure facilitates energy efficient data forwarding along the path towards the witnesses and the sink, and the traffic load is distributed across the network, which improves the network lifetime significantly. Theoretical analysis and simulation results demonstrate that the proposed protocol can approach 100% clone detection probability with trustful witnesses. We further extend the work by studying the clone detection performance with untrustful witnesses and show that the clone detection probability still approaches 98% when 10% of witnesses are compromised. Moreover, our proposed protocol can significantly improve the network lifetime, compared with the existing approach.},
keywords={data privacy;probability;telecommunication security;telecommunication traffic;wireless sensor networks;ERCD;energy-efficient clone detection protocol;WSN;wireless sensor network;environment monitoring;telemedicine service;location-aware clone detection protocol;clone attack detection;sensor privacy;ring structure;data forwarding;traffic load;clone detection probability;network lifetime improvement;Cloning;Protocols;Wireless sensor networks;Privacy;Security;Indexes;Broadcasting},
doi={10.1109/INFCOM.2013.6567049},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567050,
author={Y. Ghiassi-Farrokhfal and J. Liebeherr},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Capacity provisioning for schedulers with tiny buffers},
year={2013},
volume={},
number={},
pages={2445-2453},
abstract={Capacity and buffer sizes are critical design parameters in schedulers which multiplex many flows. Previous studies show that in an asymptotic regime, when the number of traffic flows N goes to infinity, the choice of scheduling algorithm does not have a big impact on performance. We raise the question whether or not the choice of scheduling algorithm impacts the capacity and buffer sizing for moderate values of N (e.g., few hundred). For Markov-modulated On-Off sources and for finite N, we show that the choice of scheduling is influential on (1) buffer overflow probability, (2) capacity provisioning, and (3) the viability of network decomposition in a non-asymptotic regime. This conclusion is drawn based on numerical examples and by a comparison of the scaling properties of different scheduling algorithms. In particular, we show that the per-flow capacity converges to the per-flow long-term average rate of the arrivals with convergence speeds ranging from O (√log N/N) to O(1/N) depending on the scheduling algorithm. This speed of convergences of the required capacities for different schedulers (to meet a target buffer overflow probability) is perceptible even for moderate values of N in our numerical examples.},
keywords={buffer storage;computational complexity;convergence of numerical methods;Markov processes;probability;scheduling;capacity provisioning;schedulers;tiny buffers;capacity sizes;buffer sizing;scheduling algorithm;traffic flows;buffer overflow probability;network decomposition viability;nonasymptotic regime;convergence speeds;design parameters;Markov-modulated on-off sources;Capacity planning;Scheduling algorithms;Convergence;Optical buffering;Aggregates;Multiplexing;Probabilistic logic},
doi={10.1109/INFCOM.2013.6567050},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567051,
author={S. Kadloor and N. Kiyavash},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Delay optimal policies offer very little privacy},
year={2013},
volume={},
number={},
pages={2454-2462},
abstract={Traditionally, scheduling policies have been optimized to perform well on metrics such as throughput, delay and fairness. In the context of shared event schedulers, where a common processor is shared among multiple users, one also has to consider the privacy offered by the scheduling policy. The privacy offered by a scheduling policy measures how much information about the usage pattern of one user of the system can be learnt by another as a consequence of sharing the scheduler. In [1], we introduced an estimation error based metric to quantify this privacy. We showed that the most commonly deployed scheduling policy, the first-come-first-served (FCFS) offers very little privacy to its users. We also proposed a parametric non-work-conserving policy which traded off delay for improved privacy. In this work, we ask the question, is a trade-off between delay and privacy fundamental to the design to scheduling policies? In particular, is there a work-conserving, possibly randomized, scheduling policy that scores high on the privacy metric? Answering the first question, we show that there does exist a fundamental limit on the privacy performance of a work-conserving scheduling policy. We quantify this limit. Furthermore, answering the second question, we demonstrate that the round-robin scheduling policy (a deterministic policy) is privacy optimal within the class of work-conserving policies.},
keywords={data privacy;estimation theory;processor scheduling;round-robin scheduling policy;work-conserving scheduling policy;privacy metric;privacy fundamental;nonwork-conserving policy;FCFS;first-come-first-served;estimation error-based metric;shared event schedulers;delay optimal policies;Privacy;Optimal scheduling;Delays;Estimation error;Time division multiple access;Processor scheduling},
doi={10.1109/INFCOM.2013.6567051},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567052,
author={Z. Mao and C. E. Koksal and N. B. Shroff},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Online packet scheduling with hard deadlines in multihop communication networks},
year={2013},
volume={},
number={},
pages={2463-2471},
abstract={The problem of online job or packet scheduling with hard deadlines has been studied extensively in the single hop setting, whereas it is notoriously difficult in the multihop setting. This difficulty stems from the fact that packet scheduling decisions at each hop influences and are influenced by decisions on other hops and only a few provably efficient online scheduling algorithms exist in the multihop setting. We consider a general multihop network topology in which packets with various deadlines and weights arrive at and are destined to different nodes through given routes. We study the problem of joint admission control and packet scheduling in order to maximize the cumulative weights of the packets that reach their destinations within their deadlines. We first focus on uplink transmissions in the tree topology and show that the well known earliest deadline first algorithm achieves the same performance as the optimal off-line algorithm for any feasible arrival pattern. We then address the general topology with multiple source-destination pairs, develop a simple online algorithm and show that it is O(PM log PM)-competitive where PM is the maximum route length among all packets. Our algorithm only requires information along the route of each packet and our result is valid for general arrival samples. Via numerical results, we show that our algorithm achieves performance that is comparable to the non-causal optimal off-line algorithm. To the best of our knowledge, this is the first algorithm with a provable (based on a sample-path construction) competitive ratio, subject to hard deadline constraints for general network topologies.},
keywords={telecommunication congestion control;telecommunication network topology;online packet scheduling;hard deadline constraint;multihop communication network;multihop network topology;admission control;uplink transmission;earliest deadline first algorithm;multiple source-destination pairs;competitive ratio;Scheduling algorithms;Network topology;Uplink;Spread spectrum communication;Admission control;Topology;Delays},
doi={10.1109/INFCOM.2013.6567052},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567053,
author={L. Golubchik and S. Khuller and K. Mukherjee and Y. Yao},
booktitle={2013 Proceedings IEEE INFOCOM},
title={To send or not to send: Reducing the cost of data transmission},
year={2013},
volume={},
number={},
pages={2472-2478},
abstract={Frequently, ISPs charge for Internet use not based on peak bandwidth usage, but according to a percentile (often the 95th percentile) cost model. In other words, the time slots with the top 5 percent (in the case of 95th percentile) of data transmission volume do not affect the cost of transmission. Instead, we are charged based on the volume of traffic sent in the 95th percentile slot. In such an environment, by allowing a short delay in transmission of some data, we may be able to reduce our cost considerably. We provide an optimal solution to the offline version of this problem (in which the job arrivals are known), for any delay D > 0. The algorithm works for any choice of percentile. We also show that there is no efficient deterministic online algorithm for this problem. However, for a slightly different problem, where the maximum amount of data transmitted is used for cost accounting, we provide an online algorithm with a competitive ratio of 2D+1/D+1. Furthermore, we prove that no online algorithm can achieve a competitive ratio better than 2D+1/D+F(D) where F(D) = Σi=1D+1i/D+i for any D > 0 in an adversarial setting. We also provide a heuristic that can be used in an online setting where the network traffic has a strong correlation over consecutive accounting cycles, based on the solution to the offline percentile problem. Experimental results are used to illustrate the performance of the algorithms proposed in this work.},
keywords={cost reduction;data communication;heuristic programming;Internet;telecommunication traffic;cost reduction;ISP;Internet service provider;percentile cost model;data transmission volume;cost accounting;online algorithm;heuristic programming;network traffic;consecutive accounting cycles;offline percentile problem;Delays;Vectors;Servers;Schedules;Optimized production technology;Bandwidth;Heuristic algorithms},
doi={10.1109/INFCOM.2013.6567053},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567054,
author={S. Yang and P. Dessai and M. Verma and M. Gerla},
booktitle={2013 Proceedings IEEE INFOCOM},
title={FreeLoc: Calibration-free crowdsourced indoor localization},
year={2013},
volume={},
number={},
pages={2481-2489},
abstract={Many indoor localization techniques that rely on RF signals from wireless Access Points have been proposed in the last decade. In recent years, research on crowdsourced (also known as “Organic”) Wi-Fi fingerprint positioning systems has been attracting much attention. This participatory approach introduces new challenges that no previously proposed techniques have taken into account. This paper proposes “FreeLoc”, an efficient localization method addressing three major technical issues posed in crowdsourcing based systems. Our novel solution facilitates 1) extracting accurate fingerprint values from short RSS measurement times 2) calibration-free positioning across different devices and 3) maintaining a single fingerprint for each location in a radio map, irrespective of any number of uploaded data sets for a given location. Through experiments using four different smartphones, we evaluate our new indoor positioning method. The experimental results confirm that the proposed scheme provides consistent localization accuracy in an environment where the device heterogeneity and the multiple surveyor problems exist.},
keywords={indoor radio;wireless LAN;calibration-free crowdsourced indoor localization method;Wi-Fi fingerprint positioning systems;wireless access points;RF signal;uploaded data sets;indoor positioning method;localization accuracy;device heterogeneity;multiple surveyor problem;crowdsourcing based systems;radio map;IEEE 802.11 Standards;Accuracy;Buildings;Servers;Databases;Antenna measurements;Calibration},
doi={10.1109/INFCOM.2013.6567054},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567055,
author={Y. Chen and Z. Liu and X. Fu and B. Liu and W. Zhao},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Theory underlying measurement of AOA with a rotating directional antenna},
year={2013},
volume={},
number={},
pages={2490-2498},
abstract={In many wireless localization applications, we rotate a directional antenna to derive the angle of arrival (AOA) of wireless signals transmitted from a target mobile device. The AOA corresponds to the direction in which the maximum received signal strength (RSS) is sensed. However, an unanswered question is how to make sure the directional antenna picks up packets producing the maximum RSS while rotating. We propose a set of novel RSS sampling theory to answer this question. We recognize the process that a directional antenna measures RSS of wireless packets while rotating as the process that the radiation pattern of the directional antenna is sampled. Therefore, if RSS samples can reconstruct the antenna's radiation pattern, the direction corresponding to the peak of the radiation pattern is the AOA of the target. We derive mathematical models to determine the RSS sampling rate given the target's packet transmission rate. Our RSS sampling theory is applicable to various types of directional antennas. To validate our RSS sampling theory, we developed BotLoc, which is a programmable and self-coordinated robot armed with a wireless sniffer. We conducted extensive real-world experiments and the experimental results match the theory very well. A video of BotLoc is at www.youtube.com/watch?v=WtUt0IqhXRU&amp;feature=youtu.be.},
keywords={antenna radiation patterns;direction-of-arrival estimation;directive antennas;mobile antennas;mobile radio;mobile robots;signal sampling;AOA;rotating directional antenna;wireless localization application;angle of arrival;wireless signal;target mobile device;received signal strength;RSS sampling theory;wireless packet;antenna radiation pattern;packet transmission rate;BotLoc robot;programmable robot;self-coordinated robot;wireless sniffer;Antenna measurements;Directional antennas;Directive antennas;Wireless communication;Antenna radiation patterns;Angular velocity},
doi={10.1109/INFCOM.2013.6567055},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567056,
author={S. Ji and K. Sze and Z. Zhou and A. M. So and Y. Ye},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Beyond convex relaxation: A polynomial-time non-convex optimization approach to network localization},
year={2013},
volume={},
number={},
pages={2499-2507},
abstract={The successful deployment and operation of location-aware networks, which have recently found many applications, depends crucially on the accurate localization of the nodes. Currently, a powerful approach to localization is that of convex relaxation. In a typical application of this approach, the localization problem is first formulated as a rank-constrained semidefinite program (SDP), where the rank corresponds to the target dimension in which the nodes should be localized. Then, the non-convex rank constraint is either dropped or replaced by a convex surrogate, thus resulting in a convex optimization problem. In this paper, we explore the use of a non-convex surrogate of the rank function, namely the so-called Schatten quasi- norm, in network localization. Although the resulting optimization problem is non-convex, we show, for the first time, that a first- order critical point can be approximated to arbitrary accuracy in polynomial time by an interior-point algorithm. Moreover, we show that such a first-order point is already sufficient for recovering the node locations in the target dimension if the input instance satisfies certain established uniqueness properties in the literature. Finally, our simulation results show that in many cases, the proposed algorithm can achieve more accurate localization results than standard SDP relaxations of the problem.},
keywords={convex programming;mathematical programming;polynomials;radio networks;convex relaxation;polynomial-time nonconvex optimization approach;network localization;location-aware networks;rank-constrained semidefinite program;rank-constrained SDP;nonconvex rank constraint;convex surrogate;Schatten quasinorm;polynomial time;interior-point algorithm;first-order point;standard SDP relaxations;Distance measurement;Approximation algorithms;Vectors;Polynomials;Symmetric matrices;Optimization;Standards},
doi={10.1109/INFCOM.2013.6567056},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567057,
author={X. Zhu and Q. Li and G. Chen},
booktitle={2013 Proceedings IEEE INFOCOM},
title={APT: Accurate outdoor pedestrian tracking with smartphones},
year={2013},
volume={},
number={},
pages={2508-2516},
abstract={This paper presents APT, a localization system for outdoor pedestrians with smartphones. APT performs better than the built-in GPS module of the smartphone in terms of accuracy. This is achieved by introducing a robust dead reckoning algorithm and an error-tolerant algorithm for map matching. When the user is walking with the smartphone, the dead reckoning algorithm monitors steps and walking direction in real time. It then reports new steps and turns to the map-matching algorithm. Based on updated information, this algorithm adjusts the user's location on a map in an error-tolerant manner. If location ambiguity among several routes occurs after adjustments, the GPS module is queried to help eliminate this ambiguity. Evaluations in practice show that the error of our system is less than 1/2 that of GPS.},
keywords={Global Positioning System;handicapped aids;pedestrians;smart phones;target tracking;APT;accurate outdoor pedestrian tracking;smart phones;localization system;GPS module;dead reckoning;error-tolerant algorithm;map matching;Global Positioning System;Dead reckoning;Smart phones;Acceleration;Accuracy;Gyroscopes;Legged locomotion},
doi={10.1109/INFCOM.2013.6567057},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567058,
author={X. Oscar Wang and W. Cheng and P. Mohapatra and T. Abdelzaher},
booktitle={2013 Proceedings IEEE INFOCOM},
title={ARTSense: Anonymous reputation and trust in participatory sensing},
year={2013},
volume={},
number={},
pages={2517-2525},
abstract={With the proliferation of sensor-embedded mobile computing devices, participatory sensing is becoming popular to collect information from and outsource tasks to participating users. These applications deal with a lot of personal information, e.g., users' identities and locations at a specific time. Therefore, we need to pay a deeper attention to privacy and anonymity. However, from a data consumer's point of view, we want to know the source of the sensing data, i.e., the identity of the sender, in order to evaluate how much the data can be trusted. “Anonymity” and “trust” are two conflicting objectives in participatory sensing networks, and there are no existing research efforts which investigated the possibility of achieving both of them at the same time. In this paper, we propose ARTSense, a framework to solve the problem of “trust without identity” in participatory sensing networks. Our solution consists of a privacy-preserving provenance model, a data trust assessment scheme and an anonymous reputation management protocol. We have shown that ARTSense achieves the anonymity and security requirements. Validations are done to show that we can capture the trust of information and reputation of participants accurately.},
keywords={data communication;mobile computing;wireless sensor networks;ARTSense;participatory sensing;proliferation;sensor-embedded mobile computing devices;anonymity;trust without identity;data trust assessment scheme;anonymous reputation management protocol;Sensors;Servers;Privacy;Security;Videos;Registers;Protocols},
doi={10.1109/INFCOM.2013.6567058},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567059,
author={R. Zhang and J. Zhang and Y. Zhang and C. Zhang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Secure crowdsourcing-based cooperative pectrum sensing},
year={2013},
volume={},
number={},
pages={2526-2534},
abstract={Cooperative (spectrum) sensing is a key function for dynamic spectrum access and is essential for avoiding interference with licensed primary users and identifying spectrum holes. A promising approach for effective cooperative sensing over a large geographic region is to rely on special spectrum-sensing providers (SSPs), which outsource spectrum-sensing tasks to distributed mobile users. Its feasibility is deeply rooted in the ubiquitous penetration of mobile devices into everyday life. Crowdsourcing-based cooperative spectrum sensing is, however, vulnerable to malicious sensing data injection attack, in which a malicious CR users submit false sensing reports containing power measurements much larger (or smaller) than the true value to inflate (or deflate) the final average, in which case the SSP may falsely determine that the channel is busy (or vacant). In this paper, we propose a novel scheme to enable secure crowdsourcing-based cooperative spectrum sensing by jointly considering the instantaneous trustworthiness of mobile detectors in combination with their reputation scores during data fusion. Our scheme can enable robust cooperative sensing even if the malicious CR users are the majority. The efficacy and efficiency of our scheme have been confirmed by extensive simulation studies.},
keywords={mobile radio;signal detection;telecommunication security;secure crowdsourcing;cooperative spectrum sensing;dynamic spectrum access;large geographic region;spectrum sensing provider;distributed mobile use;malicious sensing data injection attack;malicious CR;instantaneous trustworthiness;mobile detector;Detectors;Mobile communication;Silicon;Cascading style sheets;Robustness;Fading},
doi={10.1109/INFCOM.2013.6567059},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567060,
author={S. Li and H. Zhu and Z. Gao and X. Guan and K. Xing},
booktitle={2013 Proceedings IEEE INFOCOM},
title={YouSense: Mitigating entropy selfishness in distributed collaborative spectrum sensing},
year={2013},
volume={},
number={},
pages={2535-2543},
abstract={Collaborative spectrum sensing has been recognized as a promising approach to improve the sensing performance via exploiting the spatial diversity of the secondary users. In this study, a new selfishness issue is identified, that selfish users sense no spectrum in collaborative sensing. For easier presentation, it's denoted as entropy selfishness. This selfish behavior is difficult to distinguish, making existing detection based incentive schemes fail to work. To thwart entropy selfishness in distributed collaborative sensing, we propose YouSense, a One-Time Pad (OTP) based incentive design that could naturally isolate entropy selfish users from the honest users without selfish node detection. The basic idea of YouSense is to construct a trapdoor onetime pad for each sensing report by combining the original report and a random key. Such a one-time pad based encryption could prevent entropy selfish users from accessing the original sensing report while enabling the honest users to recover the report. Different from traditional cryptography based OTP which requires the key delivery, YouSense allows an honest user to recover the pad (or key) by exploiting a unique characteristic of collaborative sensing that different secondary users share some common observations on the same radio spectrum. We further extend YouSense to improve the recovery successful rate by reducing the cardinality of set of the possible pads. By extensive USRP based experiments, we show that YouSense can successfully thwart entropy selfishness with low system overhead.},
keywords={cognitive radio;entropy;radio spectrum management;secondary users;selfish behavior;incentive schemes;one-time pad based incentive design;entropy selfish users;selfish node detection;trapdoor onetime pad;one-time pad based encryption;sensing report;cryptography based OTP;key delivery;radio spectrum;system overhead;spatial diversity;sensing performance;distributed collaborative spectrum sensing;entropy selfishness;YouSense;Sensors;Entropy;Collaboration;Encryption;Incentive schemes;Silicon;Cognitive Radio Security;Collaborative Spectrum Sensing;Incentive Mechanism Design},
doi={10.1109/INFCOM.2013.6567060},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567061,
author={Z. Jiang and J. Zhao and X. Li and J. Han and W. Xi},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Rejecting the attack: Source authentication for Wi-Fi management frames using CSI Information},
year={2013},
volume={},
number={},
pages={2544-2552},
abstract={Comparing to well protected data frames, Wi-Fi management frames (MFs) are extremely vulnerable to various attacks. Since MFs are transmitted without encryption or authentication, attackers can easily launch various attacks by forging the MFs. In a collaborative environment with many Wi-Fi sniffers, such attacks can be easily detected by sensing the anomaly RSS changes. However, it is quite difficult to identify these spoofing attacks without assistance from other nodes. By exploiting some unique characteristics (e.g., rapid spatial decorrelation, independence of Txpower, and much richer dimensions) of 802.11n Channel State Information (CSI), we design and implement CSITE, a prototype system to authenticate the Wi-Fi management frames on PHY layer merely by one station. Our system CSITE, built upon off-the-shelf hardware, achieves precise spoofing detection without collaboration and in-advance fingerprint. Several novel techniques are designed to address the challenges caused by user mobility and channel dynamics. To verify the performances of our solution, we conduct extensive evaluations in various scenarios. Our test results show that our design significantly outperforms the RSS-based method. We observe about 8 times improvement by CSITE over RSS-based method on the falsely accepted attacking frames.},
keywords={telecommunication channels;telecommunication network management;telecommunication security;wireless LAN;source authentication;Wi-Fi management frames;CSI information;Wi-Fi sniffers;rapid spatial decorrelation;Txpower;channel state information;802.11n;CSITE;PHY layer;CSITE;channel dynamics;user mobility;RSS-based method;Authentication;Decorrelation;IEEE 802.11n Standard;Cryptography;OFDM},
doi={10.1109/INFCOM.2013.6567061},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567062,
author={Y. Li and W. Wang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Horizon on the move: Geocast in intermittently connected vehicular ad hoc networks},
year={2013},
volume={},
number={},
pages={2553-2561},
abstract={Vehicular ad hoc network (VANET) is one of the most promising large-scale applications of mobile ad hoc networks. VANET applications are rooted in advanced understanding of communication networks because both control messages and data information need to be disseminated in geographic regions (i.e., Geocast). The challenges come from highly dynamic environments in VANET. Destination nodes in geocast are dynamic over time due to vehicle mobility, which undermines existing results on dissemination latency and information propagation speed with pre-determined destinations. Moreover, the affected area by the dissemination, which is referred to as horizon of message (HOM), is critical in geocast as it determines the latency for the message reaching nodes inside the area of interest (AOI), in which the message is relevant to drivers. Therefore, we characterize the HOM in geocast by how far the message can reach within time t (referred as dissemination distance) and how long the message takes to inform nodes at certain locations (referred as hitting time). Analytic bounds of dissemination distance and hitting time are derived under four types of dissemination mechanisms, which provide insights into the spatial and temporal limits of HOM as well as how the numbers of disseminators and geographic information exchanges affect them. Applying analytic and simulation results to two real applications, we observe that geocast with AOI near the source or high reliability requirement should recruit multiple disseminators while geocast with AOI far from the source need to utilize geographic information for fast message propagation.},
keywords={information dissemination;telecommunication network reliability;vehicular ad hoc networks;geocast;intermittently connected vehicular ad hoc networks;mobile ad hoc networks;VANET applications;communication networks;control messages;data information;geographic regions;destination nodes;vehicle mobility;dissemination latency;information propagation speed;horizon of message;HOM;message reaching nodes;area of interest;AOI;dissemination distance;analytic bounds;hitting time;dissemination mechanisms;geographic information exchanges;fast message propagation;Vehicles;Vectors;Vehicular ad hoc networks;Upper bound;Vehicle dynamics;Simulation},
doi={10.1109/INFCOM.2013.6567062},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567063,
author={T. H. Luan and X. Sherman Shen and F. Bai},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Integrity-oriented content transmission in highway vehicular ad hoc networks},
year={2013},
volume={},
number={},
pages={2562-2570},
abstract={The effective inter-vehicle transmission of content files, e.g., images, music and video clips, is the basis of media communications in vehicular networks, such as social communications and video sharing. However, due to the presence of diverse node velocities, severe channel fadings and intensive mutual interferences among vehicles, the inter-vehicle or vehicle-to-vehicle (V2V) communications tend to be transient and highly dynamic. Content transmissions among vehicles over the volatile and spotty V2V channels are thus susceptible to frequent interruptions and failures, resulting in many fragment content transmissions which are unable to finish during the connection time and unusable by on-top media applications. The interruptions of content transmissions not only lead to the failure of media presentations to users, but the transmission of the invalid fragment contents would also result in the significant waste of precious vehicular bandwidth. On addressing this issue, in this work we target on provisioning the integrity-oriented inter-vehicle content transmissions. Given the initial distance and mobility statistics of vehicles, we develop an analytical framework to evaluate the data volume that can be transmitted upon the short-lived and spotty V2V connection from the source to the destination vehicle. Provided the content file size, we are able to evaluate the likelihood of successful content transmissions through the model. Based upon this analysis, we propose an admission control scheme at the transmitters, that filters the suspicious content transmission requests which are unlikely to be accomplished over the transient inter-vehicle links. Using extensive simulations, we demonstrate the accuracy of the developed analytical model, and the effectiveness of the proposed admission control scheme. In the simulated scenario, with the proposed admission control scheme applied, it is observed that about 30% of the network bandwidth can be saved for effective content transmissions.},
keywords={data communication;telecommunication channels;telecommunication network reliability;telecommunication security;vehicular ad hoc networks;vehicular ad hoc networks;diverse node velocities;severe channel fadings;intensive mutual interferences;vehicle to vehicle;V2V communications;integrity oriented intervehicle content transmissions;data volume;content file size;successful content transmissions;admission control scheme;suspicious content transmission requests;transient intervehicle links;Vehicles;Road transportation;Media;Fading;Analytical models;Throughput;Physical layer},
doi={10.1109/INFCOM.2013.6567063},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567064,
author={X. Li and C. Qiao and Y. Hou and Y. Zhao and A. Wagh and A. Sadek and L. Huang and H. Xu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={On-road ads delivery scheduling and bandwidth allocation in vehicular CPS},
year={2013},
volume={},
number={},
pages={2571-2579},
abstract={We consider a promising application in Vehicular Cyber-Physical Systems (VCPS) called On-road Ad Delivery (OAD), where targeted advertisements are delivered via roadside APs to attract commuters to nearby shops. Different from most existing works on VANETs which only focused on a single technical area, this work on OAD involves technical elements from human factors, cyber systems and transportation systems since a commuter's shopping decision depends on e.g. the attractiveness of the ads, the induced detour, and traffic conditions on different routes. In this paper, we address a new optimization problem in OAD whose goal is to schedule ad messages and allocate a limited amount of AP bandwidth so as to maximize the system-wide performance in terms of total realized utilities (TRU) of the delivered ads. A number of efficient heuristics are proposed to deal with ad message scheduling and AP bandwidth allocation. Besides largescale simulations, we also present a case study in a more realistic scenario utilizing real traces collected from taxis in the city of Shanghai. In addition, we use a commercial traffic simulator (PARAMICS) to show that our proposed solutions are also useful for traffic management in terms of balancing vehicular traffic and alleviating congestion.},
keywords={bandwidth allocation;optimisation;telecommunication traffic;vehicular ad hoc networks;on-road ads delivery scheduling;bandwidth allocation;vehicular CPS;vehicular cyber-physical system;VCPS;OAD;VANET;human factor;cyber system;transportation system;optimization problem;total realized utilities;commercial traffic simulator;PARAMICS;traffic management;vehicular traffic;Bandwidth;Unicast;Channel allocation;Wireless communication;Roads;Protocols;On-road Ad Delivery;Human-in-the-Loop;Daily Commuters;Shopping Activities;Vehicular CPS},
doi={10.1109/INFCOM.2013.6567064},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567065,
author={Y. Wu and Y. Zhu and H. Zhu and B. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={CCR: Capacity-constrained replication for data delivery in vehicular networks},
year={2013},
volume={},
number={},
pages={2580-2588},
abstract={Given the unique characteristics of vehicular networks, specifically, frequent communication unavailability and short encounter time, packet replication has been commonly used to facilitate data delivery. Replication enables multiple copies of the same packet to be forwarded towards the destination, which increases the chance of delivery to a target destination. However, this is achieved at the expense of consuming extra already scarce bandwidth resource in vehicular networks. Therefore, it is crucial to investigate the fundamental problem of exploiting constrained network capacity with packet replication. We make the first attempt in this work to address this challenging problem. We first conduct extensive empirical analysis using three large datasets of real vehicle GPS traces. We show that a replication scheme that either underestimates or overestimates the network capacity results in poor delivery performance. Based on the observation, we propose a Capacity-Constrained Replication scheme or CCR for data delivery in vehicular networks. The key idea is to explore the residual capacity for packet replication. We introduce an analytical model for characterizing the relationship among the number of replicated copies of a packet, replication limit and queue length. Based on this insight, we derive the rule for adaptive adjustment towards the optimal replication strategy. We then design a distributed algorithm to dictate how each vehicle can adaptively determine its replication strategy subject to the current network capacity. Extensive simulations based on real vehicle GPS traces show that our proposed CCR can significantly improve delivery ratio comparing with the state-of-the-art algorithms.},
keywords={Global Positioning System;mobile radio;queueing theory;capacity-constrained replication;CCR;data delivery;vehicular networks;communication unavailability;packet replication;constrained network capacity;extensive empirical analysis;vehicle GPS traces;residual capacity;replication limit;delivery ratio;optimal replication strategy;queue length;Vehicles;Barium;Bandwidth;Global Positioning System;Analytical models;Queueing analysis;Performance gain;vehicular networks;network capacity;packet replication;data delivery;analytical model;trace-driven simulations},
doi={10.1109/INFCOM.2013.6567065},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567066,
author={B. Ji and G. R. Gupta and X. Lin and N. B. Shroff},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Performance of low-complexity greedy scheduling policies in multi-channel wireless networks: Optimal throughput and near-optimal delay},
year={2013},
volume={},
number={},
pages={2589-2597},
abstract={In this paper, we focus on the scheduling problem in multi-channel wireless networks, e.g., the downlink of a single cell in fourth generation (4G) OFDM-based cellular networks. Our goal is to design efficient scheduling policies that can achieve provably good performance in terms of both throughput and delay, at a low complexity. While a recently developed scheduling policy, called Delay Weighted Matching (DWM), has been shown to be both rate-function delay-optimal (in the many-channel many-user asymptotic regime) and throughput-optimal (in general non-asymptotic setting), it has a high complexity O(n<sup>5</sup>), which makes it impractical for modern OFDM systems. To address this issue, we first develop a simple greedy policy called Delay-based Queue-Side-Greedy (D-QSG) with a lower complexity O(n3), and rigorously prove that D-QSG not only achieves throughput optimality, but also guarantees near-optimal rate-function-based delay performance. Specifically, the rate-function attained by DQSG for any fixed integer threshold b &gt; 0, is no smaller than the maximum achievable rate-function by any scheduling policy for threshold b-1. Further, we develop another simple greedy policy called Delay-based Server-Side-Greedy (D-SSG) with an even lower complexity O(n<sup>2</sup>), and show that D-SSG achieves the same performance as D-QSG. Thus, we are able to achieve a dramatic reduction in complexity (from O(n<sup>5</sup>) of DWM to O(n<sup>2</sup>)) with a minimal drop in the delay performance. Finally, we conduct numerical simulations to validate our theoretical results in various scenarios. The simulation results show that our proposed greedy policies not only guarantee a near-optimal rate-function, but also empirically are virtually indistinguishable from the delay-optimal policy DWM.},
keywords={4G mobile communication;cellular radio;greedy algorithms;numerical analysis;queueing theory;scheduling;wireless channels;low-complexity greedy scheduling policies;multichannel wireless networks;fourth generation OFDM-based cellular networks;4G OFDM-based cellular networks;delay weighted matching;rate-function delay-optimal;delay-based queue-side-greedy;D-QSG;throughput optimality;near-optimal rate-function-based delay performance;fixed integer;delay-based server-side-greedy;D-SSG;numerical simulations;delay-optimal policy DWM;Delays;Servers;Throughput;Complexity theory;Scheduling;Indexes;Markov processes},
doi={10.1109/INFCOM.2013.6567066},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567067,
author={P. Huang and X. Lin},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Improving the delay performance of CSMA algorithms: A Virtual Multi-Channel approach},
year={2013},
volume={},
number={},
pages={2598-2606},
abstract={CSMA algorithms have recently received a significant amount of interest in the literature for designing efficient wireless control algorithms. CSMA algorithms are attractive because they incur low computation complexity and communication overhead, and can be shown to achieve the optimal capacity under certain assumptions. However, it has also been observed that CSMA algorithms suffer the starvation problem and incur large delay that may grow exponentially with the network size. In this paper, we propose a new algorithm, called Virtual-Multi-Channel (VMC-) CSMA, that can dramatically reduce delay without sacrificing the high capacity and low complexity of CSMA. The key idea of VMC-CSMA to avoid the starvation problem is to use multiple virtual channels to emulate a multi-channel system and compute a good set of feasible schedules simultaneously (without constantly switching/re-computing schedules). Under the protocol interference model and a single-hop utility-maximization setting, our proposed VMC-CSMA algorithm can approach arbitrarily close to the optimal total system utility, with both the number of virtual channels and the computation complexity increasing logarithmically with the network size. The VMC-CSMA algorithm inherits the distributed nature of CSMA algorithms. Further, once our algorithm converges to the steady-state, the expected packet delay for each link equals to the inverse of its long-term average rate, and the distribution of its head-of-line (HOL) waiting time can also be asymptotically bounded. Our simulation results confirm that the proposed VMC-CSMA algorithm indeed achieves both high throughput and low delay. Further, it can quickly adapt to network traffic changes.},
keywords={carrier sense multiple access;computational complexity;telecommunication traffic;delay performance;CSMA algorithms;virtual multichannel approach;wireless control algorithms;computation complexity;communication overhead;optimal capacity;starvation problem;head-of-line;network traffic;Schedules;Delays;Multiaccess communication;Algorithm design and analysis;Complexity theory;Standards;Throughput},
doi={10.1109/INFCOM.2013.6567067},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567068,
author={R. Urgaonkar and R. Ramanathan and J. Redi and W. N. Tetteh},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Channel assignment in dense MC-MR wireless networks: Scaling laws and algorithms},
year={2013},
volume={},
number={},
pages={2607-2615},
abstract={We investigate optimal channel assignment algorithms that maximize per node throughput in dense multi-channel multi-radio (MC-MR) wireless networks. Specifically, we consider an MC-MR network where all nodes are within the transmission range of each other. This situation is encountered in many real-life settings such as students in a lecture hall, delegates attending a conference, or soldiers in a battlefield. In this scenario, we show that intelligent assignment of the available channels results in a significantly higher per node throughput. We first propose a class of channel assignment algorithms, parameterized by T (the number of transceivers per node), that can achieve Θ(1/N1/T) per node throughput using Θ(TN1-1/T) channels. In view of practical constraints on T, we then propose another algorithm that can achieve Θ((1/log2n)2) per node throughput using only two transceivers per node. Finally, we identify a fundamental relationship between the achievable per node throughput, the total number of channels used, and the network size under any strategy. Using analysis and simulations, we show that our algorithms achieve close to optimal performance at different operating points on this curve. Our work has several interesting implications on the optimal network design for dense MC-MR wireless networks.},
keywords={channel allocation;radio networks;radio transceivers;channel assignment;dense multi-channel multi-radio wireless networks;intelligent assignment;transceivers;optimal network design;Transceivers;Throughput;Routing;Level set;Indexes;Wireless networks;Algorithm design and analysis},
doi={10.1109/INFCOM.2013.6567068},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567069,
author={R. Li and A. Eryilmaz and B. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Throughput-optimal wireless scheduling with regulated inter-service times},
year={2013},
volume={},
number={},
pages={2616-2624},
abstract={Motivated by the low-jitter requirements of streaming multi-media traffic, we focus on the development of scheduling strategies under fading conditions that not only maximize throughput performance but also provide regular inter-service times to users. Since the service regularity of the traffic is related to the higher-order statistics of the arrival process and the policy operation, it is highly challenging to characterize and analyze directly. We overcome this obstacle by introducing a new quantity, namely the time-since-last-service, which has a unique evolution different from a tradition queue. By combining it with the queue-length in the weight, we propose a novel maximum-weight type scheduling policy that is proven to be throughput-optimal and also provides provable service regularity guarantees. In particular, our algorithm can achieve a degree of service regularity within a constant factor of a fundamental lower bound we derive. This constant is independent of the higher-order statistics of the arrival process and can be as low as two. Our results, both analytical and numerical, exhibit significant service regularity improvements over the traditional throughput-optimal policies, which reveals the importance of incorporating the metric of time-since-last-service into the scheduling policy for providing regulated service.},
keywords={multimedia communication;scheduling;statistical analysis;telecommunication traffic;throughput-optimal wireless scheduling;regulated inter-service times;multimedia traffic;scheduling strategies;throughput performance;inter-service times;higher-order statistics;arrival process;time-since-last-service;unique evolution;queue-length;maximum-weight type scheduling policy;degree of service regularity;service regularity;throughput-optimal policies;Throughput;Steady-state;Vectors;Fading;Delays;Stability analysis},
doi={10.1109/INFCOM.2013.6567069},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567070,
author={T. Jung and X. Li and Z. Wan and M. Wan},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Privacy preserving cloud data access with multi-authorities},
year={2013},
volume={},
number={},
pages={2625-2633},
abstract={Cloud computing is a revolutionary computing paradigm which enables flexible, on-demand and low-cost usage of computing resources. Those advantages, ironically, are the causes of security and privacy problems, which emerge because the data owned by different users are stored in some cloud servers instead of under their own control. To deal with security problems, various schemes based on the Attribute-Based Encryption have been proposed recently. However, the privacy problem of cloud computing is yet to be solved. This paper presents an anonymous privilege control scheme AnonyControl to address not only the data privacy problem in a cloud storage, but also the user identity privacy issues in existing access control schemes. By using multiple authorities in cloud computing system, our proposed scheme achieves anonymous cloud data access and fine-grained privilege control. Our security proof and performance analysis shows that AnonyControl is both secure and efficient for cloud computing environment.},
keywords={authorisation;cloud computing;cryptography;data privacy;privacy preserving cloud data access;multiauthorities;cloud computing privacy problem;cloud servers;attribute-based encryption;anonymous privilege control scheme AnonyControl;cloud storage;user identity privacy issues;access control schemes;anonymous cloud data access;fine-grained privilege control;Encryption;Servers;Gold;Cloud computing;Public key;Generators},
doi={10.1109/INFCOM.2013.6567070},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567071,
author={T. Jung and X. Mao and X. Li and S. Tang and W. Gong and L. Zhang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Privacy-preserving data aggregation without secure channel: Multivariate polynomial evaluation},
year={2013},
volume={},
number={},
pages={2634-2642},
abstract={Much research has been conducted to securely outsource multiple parties' data aggregation to an untrusted aggregator without disclosing each individual's privately owned data, or to enable multiple parties to jointly aggregate their data while preserving privacy. However, those works either require secure pair-wise communication channels or suffer from high complexity. In this paper, we consider how an external aggregator or multiple parties can learn some algebraic statistics (e.g., sum, product) over participants' privately owned data while preserving the data privacy. We assume all channels are subject to eavesdropping attacks, and all the communications throughout the aggregation are open to others. We propose several protocols that successfully guarantee data privacy under this weak assumption while limiting both the communication and computation complexity of each participant to a small constant.},
keywords={computational complexity;data privacy;telecommunication channels;privacy-preserving data aggregation;secure channel;multivariate polynomial evaluation;pairwise communication channels;computation complexity;Protocols;Computational modeling;Polynomials;Communication channels;Complexity theory;Cryptography;Privacy;aggregation;secure channels;SMC;homomorphic},
doi={10.1109/INFCOM.2013.6567071},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567072,
author={Z. Zhou and H. Zhang and X. Du and P. Li and X. Yu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Prometheus: Privacy-aware data retrieval on hybrid cloud},
year={2013},
volume={},
number={},
pages={2643-2651},
abstract={With the advent of cloud computing, data owner is motivated to outsource their data to the cloud platform for great flexibility and economic savings. However, the development is hampered by data privacy concerns: Data owner may have privacy data and the data cannot be outsourced to cloud directly. Previous solutions mainly use encryption. However, encryption causes a lot of inconveniences and large overheads for other data operations, such as search and query. To address the challenge, we adopt hybrid cloud. In this paper, we present a suit of novel techniques for efficient privacy-aware data retrieval. The basic idea is to split data, keeping sensitive data in trusted private cloud while moving insensitive data to public cloud. However, privacy-aware data retrieval on hybrid cloud is not supported by current frameworks. Data owners have to split data manually. Our system, called Prometheus, adopts the popular MapReduce framework, and uses data partition strategy independent to specific applications. Prometheus can automatically separate sensitive information from public data. We formally prove the privacy-preserving feature of Prometheus. We also show that our scheme can defend against the malicious cloud model, in addition to the semi-honest cloud model. We implement Prometheus on Hadoop and evaluate its performance using real data set on a large-scale cloud test-bed. Our extensive experiments demonstrate the validity and practicality of the proposed scheme.},
keywords={cloud computing;cryptography;data privacy;outsourcing;query processing;trusted computing;Prometheus;privacy-aware data retrieval;hybrid cloud computing;data owner;data outsourcing;cloud platform;economic savings;data privacy concerns;encryption;data operations;privacy-aware data retrieval;trusted private cloud;MapReduce framework;data partition strategy;sensitive information;public data;privacy-preserving features;malicious cloud model;semihonest cloud model;Hadoop;large-scale cloud test-bed;Cloud computing;Data privacy;Privacy;Partitioning algorithms;Algorithm design and analysis;Encryption;data retrieval;hybrid cloud;MapReduce;privacy-aware;data partition},
doi={10.1109/INFCOM.2013.6567072},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567073,
author={J. Yuan and S. Yu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Efficient privacy-preserving biometric identification in cloud computing},
year={2013},
volume={},
number={},
pages={2652-2660},
abstract={Biometric identification is a reliable and convenient way of identifying individuals. The widespread adoption of biometric identification requires solid privacy protection against possible misuse, loss, or theft of biometric data. Existing techniques for privacy-preserving biometric identification primarily rely on conventional cryptographic primitives such as homomorphic encryption and oblivious transfer, which inevitably introduce tremendous cost to the system and are not applicable to practical large-scale applications. In this paper, we propose a novel privacy-preserving biometric identification scheme which achieves efficiency by exploiting the power of cloud computing. In our proposed scheme, the biometric database is encrypted and outsourced to the cloud servers. To perform a biometric identification, the database owner generates a credential for the candidate biometric trait and submits it to the cloud. The cloud servers perform identification over the encrypted database using the credential and return the result to the owner. During the identification, cloud learns nothing about the original private biometric data. Because the identification operations are securely outsourced to the cloud, the realtime computational/communication costs at the owner side are minimal. Thorough analysis shows that our proposed scheme is secure and offers a higher level of privacy protection than related solutions such as kNN search in encrypted databases. Real experiments on Amazon cloud, over databases of different sizes, show that our computational/communication costs at the owner side are several magnitudes lower than the existing biometric identification schemes.},
keywords={biometrics (access control);cloud computing;data privacy;privacy-preserving biometric identification;cloud computing;privacy protection;encrypted biometric database;biometric trait;cloud servers;private biometric data;realtime computational-communication costs;Amazon cloud;Cryptography;Servers;Vectors;Euclidean distance;Biological system modeling;Indexes},
doi={10.1109/INFCOM.2013.6567073},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567074,
author={H. Huang and J. Yun and Z. Zhong and S. Kim and T. He},
booktitle={2013 Proceedings IEEE INFOCOM},
title={PSR: Practical synchronous rendezvous in low-duty-cycle wireless networks},
year={2013},
volume={},
number={},
pages={2661-2669},
abstract={Low-duty-cycle radio operations have been proposed for wireless networks facing severe energy constraints. Despite energy savings, duty-cycling the radio creates transient-available wireless links, making communication rendezvous a challenging task under the practical issue of clock drift. To overcome limitations of prior work, this paper presents PSR, a practical design for synchronous rendezvous in low-duty-cycle wireless networks. The key idea behind PSR is to extract timing information naturally embedded in the pattern of radio duty-cycling, so that normal traffic in the network can be utilized as a “free” input for drift detection, which helps reduce (or even eliminate) the overhead of traditional time-stamp exchange with dedicated packets or bits. To prevent an overuse of such free information, leading to energy waste, an energy-driven adaptive mechanism is developed for clock calibration to balance between energy efficiency and rendezvous accuracy. PSR is evaluated with both test-bed experiments and extensive simulations, by augmenting and comparing with four different MAC protocols. Results show that PSR is practical and effective under different levels of traffic load, and can be fused with those MAC protocols to improve their energy efficiency without major change of the original designs.},
keywords={access protocols;radio links;radio networks;telecommunication traffic;practical synchronous rendezvous;PSR;low-duty-cycle wireless networks;energy constraints;transient-available wireless links;clock drift;timing information extraction;radio duty-cycling;time-stamp exchange;energy waste;energy-driven adaptive mechanism;clock calibration;energy efficiency;MAC protocol;traffic load;energy savings;Synchronization;Calibration;Clocks;Schedules;Wireless networks;Estimation;Media Access Protocol},
doi={10.1109/INFCOM.2013.6567074},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567075,
author={Y. Peng and Z. Li and D. Qiao and W. Zhang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={I lt;sup gt;2 lt;/sup gt;C: A holistic approach to prolong the sensor network lifetime},
year={2013},
volume={},
number={},
pages={2670-2678},
abstract={We present a novel holistic approach (called I<sup>2</sup>C - Intra-route and Inter-route Coordination) to prolong the sensor network lifetime under the end-to-end delivery delay constraint. I<sup>2</sup>C is composed of two lifetime balancing modules: (i) the IntraRoute Coordination module that allows the nodes on the same route to balance their nodal lifetimes through adjusting the MAC behaviors collaboratively; (ii) the Inter-Route Coordination module that balances the nodal lifetimes across different routes via adjusting the communication routes. Different from existing works which conduct either intra-route or inter-route lifetime balancing, or a simple combination of the two, I<sup>2</sup>C leverages the advantages of both techniques with a sophisticated design that emphasizes the awareness and collaboration between two modules. Thus, I<sup>2</sup>C is able to prolong the network lifetime much more effectively than the state-of-the-art solutions, while guaranteeing the desired delay bound and maintaining a similar level of network power consumption. This has been demonstrated with extensive ns-2 simulation and TinyOS experiment results.},
keywords={power consumption;telecommunication network routing;wireless sensor networks;I2C;sensor network lifetime;intraroute and interroute coordination;end-to-end delivery delay constraint;lifetime balancing modules;nodal lifetimes;MAC behaviors;communication routes;network power consumption;ns-2 simulation;TinyOS experiment;Delays;Routing;Equations;Switches;Media Access Protocol;Data collection;Energy consumption},
doi={10.1109/INFCOM.2013.6567075},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567076,
author={W. Dong and Y. Liu and Y. He and T. Zhu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Measurement and analysis on the packet delivery performance in a large scale sensor network},
year={2013},
volume={},
number={},
pages={2679-2687},
abstract={Understanding the packet delivery performance of a wireless sensor network (WSN) is critical for improving system performance and exploring further development and applications of WSN techniques. In spite of many empirical measurements in the literature, we still lack in-depth understanding on how and to what extent different factors contribute to the overall packet losses with respect to a complete stack of protocols at large scales. Specifically, very little is known about (1) When, where, and under what kind of circumstances packet losses occur. (2) Why packets are lost. As a step towards addressing those issues, we deploy a large-scale WSN and design a measurement system for retrieving important system metrics. We propose MAP, a step-by-step methodology to identify the losses, extract system events, and perform spatial-temporal correlation analysis by employing a carefully examined casual graph. MAP enables us to get a closer look at the root causes of packet losses in a low-power adhoc network. This study validates some earlier conjectures on WSNs and reveals some new findings. The quantitative results also shed lights for future large-scale WSN deployments.},
keywords={sensor placement;wireless sensor networks;packet delivery performance;large scale sensor network;wireless sensor network;empirical measurement;packet loss;complete protocol stack;large scale WSN deployment;WSN design;measurement system;loss identification;system event extraction;spatial-temporal correlation analysis;casual graph;MAP;performance measurement;performance analysis;Wireless sensor networks;Packet loss;Protocols;Routing;Radiation detectors;Correlation},
doi={10.1109/INFCOM.2013.6567076},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567077,
author={Q. Ma and K. Liu and X. Xiao and Z. Cao and Y. Liu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Link Scanner: Faulty link detection for wireless sensor networks},
year={2013},
volume={},
number={},
pages={2688-2696},
abstract={In large-scale wireless sensor networks, it proves very difficult to dynamically monitor system degradation and detect bad links. Faulty link detection plays a critical role in network diagnosis. Indeed, a destructive node impacts its links' performances including transmitting and receiving. Similarly, other potential network bottlenecks such as network partition and routing errors can be detected by link scan. Since sequentially checking all potential links incurs high transmission and storage cost, existing approaches often focus on links currently in use, while overlook those unused yet ones, thus fail to offer more insights to guide following operations. We propose a novel scheme Link Scanner (LS) for monitoring wireless links at real time. LS issues one probe message in the network and collects hop counts of the received probe messages at sensor nodes. Based on the observation that faulty links can result in mismatch between the received hop counts and the network topology, we are able to deduce all links' status with a probabilistic model. We evaluate our scheme by carrying out experiments on a testbed with 60 TelosB motes and conducting extensive simulation tests. A real outdoor system is also deployed to verify that LS can be reliably applied to surveillance networks.},
keywords={fault diagnosis;probability;radio links;telecommunication network topology;wireless sensor networks;link scanner;faulty link detection;wireless sensor network;system degradation monitoring;bad link detection;network diagnosis;destructive node;network partition error detection;routing error detection;wireless link monitoring;network topology;probabilistic model;outdoor system;surveillance network;Probes;Wireless sensor networks;Topology;Routing;Monitoring;Network topology;Educational institutions},
doi={10.1109/INFCOM.2013.6567077},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567078,
author={C. Zhao and J. Zhao and X. Lin and C. Wu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Capacity of P2P on-demand streaming with simple, robust and decentralized control},
year={2013},
volume={},
number={},
pages={2697-2705},
abstract={The performance of large-scaled peer-to-peer (P2P) video-on-demand (VoD) streaming systems can be very challenging to analyze. In practical P2P VoD systems, each peer only interacts with a small number of other peers/neighbors. Further, its upload capacity may vary randomly, and both its downloading position and content availability change dynamically. In this paper, we rigorously study the achievable streaming capacity of large-scale P2P VoD systems with sparse connectivity among peers, and investigate simple and decentralized P2P control strategies that can provably achieve close-to-optimal streaming capacity. We first focus on a single streaming channel. We show that a close-to-optimal streaming rate can be asymptotically achieved for all peers with high probability as the number of peers N increases, by assigning each peer a random set of Θ(log N) neighbors and using a uniform rate-allocation algorithm. Further, the tracker does not need to obtain detailed knowledge of which chunks each peer caches, and hence incurs low overhead. We then study multiple streaming channels where peers watching one channel may help in another channel with insufficient upload bandwidth. We propose a simple random cache-placement strategy, and show that a close-to-optimal streaming capacity region for all channels can be attained with high probability, again with only Θ(log N) per-peer neighbors. These results provide important insights into the dynamics of large-scale P2P VoD systems, which will be useful for guiding the design of improved P2P control protocols.},
keywords={cache storage;computational complexity;decentralised control;peer-to-peer computing;video on demand;video streaming;P2P On-demand streaming;decentralized control;large-scaled peer-to-peer video-on-demand streaming systems;P2P VoD systems;decentralized P2P control strategies;close-to-optimal streaming capacity;streaming channel;Θ(log N) neighbors;uniform rate-allocation algorithm;peer caches;multiple streaming channels;upload bandwidth;random cache-placement strategy;close-to-optimal streaming capacity region;large-scale P2P VoD systems;Streaming media;Peer-to-peer computing;Availability;Algorithm design and analysis;Robustness;Decentralized control;Servers},
doi={10.1109/INFCOM.2013.6567078},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567079,
author={Z. Lu and G. de Veciana},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Optimizing stored video delivery for mobile networks: The value of knowing the future},
year={2013},
volume={},
number={},
pages={2706-2714},
abstract={This paper considers the design of cross-layer opportunistic transport for stored video over wireless networks with a slow varying (average) capacity. We focus on two key ideas: (1) scheduling data transmissions when capacity is high; and (2), exploiting knowledge of future capacity variations. The latter is possible when users' mobility is known or predictable, e.g., users riding on public transportation or using navigation systems. We consider the design of cross-layer transmission schedules which minimize system utilization (and thus possibly transmit/receive energy) while avoiding, if at all possible, rebuffering/delays, in several scenarios. For the single-user anticipative case where all future capacity variations are known beforehand; we establish the optimal transmission schedule is a Generalized Piecewise Constant Thresholding (GPCT) scheme. For the single-user partially anticipative case where only a finite window of future capacity variations is known, we propose an online Greedy Fixed Horizon Control (GFHC). An upper bound on the competitive ratio of GFHC and GPCT is established showing how performance loss depends on the window size, receiver playback buffer, and capacity variability. Finally we consider the multiuser case where we can exploit both future temporal and multiuser diversity. Our simulations and evaluation based on a measured wireless capacity trace exhibit robust potential gains for our proposed transmission schemes.},
keywords={mobile radio;video signal processing;stored video delivery;mobile networks;cross-layer opportunistic transport;wireless networks;data transmission scheduling;public transportation;generalized piecewise constant thresholding scheme;GPCT scheme;online greedy fixed horizon control;GFHC;multiuser case;receiver playback buffer;window size;capacity variability;wireless capacity;Streaming media;Schedules;Wireless communication;Servers;Delays;Optimization;Mobile communication},
doi={10.1109/INFCOM.2013.6567079},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567080,
author={Y. Xu and S. E. Elayoubi and E. Altman and R. El-Azouzi},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Impact of flow-level dynamics on QoE of video streaming in wireless networks},
year={2013},
volume={},
number={},
pages={2715-2723},
abstract={The Quality of Experience (QoE) of streaming service is often degraded by frequent playback interruptions. To mitigate the interruptions, the media player prefetches streaming contents before starting playback, at a cost of delay. We study the QoE of streaming from the perspective of flow dynamics. First, a framework is developed for QoE when streaming users join the network randomly and leave after downloading completion. We compute the distribution of prefetching delay using partial differential equations (PDEs), and the probability generating function of playout buffer starvations using ordinary differential equations (ODEs). Second, we extend our framework to characterize the throughput variation caused by opportunistic scheduling at the base station in the presence of fast fading. Our study reveals that the flow dynamics is the fundamental reason of playback starvation. The QoE of streaming service is dominated by the average throughput of opportunistic scheduling, while the variance of throughput has very limited impact on starvation behavior.},
keywords={mobile radio;partial differential equations;probability;quality of experience;radio networks;video streaming;flow-level dynamics impact;QoE;quality-of-experience;video streaming service;wireless networks;frequent playback interruptions;interruption mitigation;media player;streaming contents;prefetching delay distribution;partial differential equations;PDE;probability generating function;playout buffer starvations;ordinary differential equations;ODE;opportunistic scheduling;base station;starvation behavior;mobile networks;fast fading presence;Streaming media;Prefetching;Delays;Throughput;Markov processes;Mathematical model},
doi={10.1109/INFCOM.2013.6567080},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567081,
author={D. Ciullo and V. Martina and M. Garetto and E. Leonardi},
booktitle={2013 Proceedings IEEE INFOCOM},
title={How much can large-scale Video-on-Demand benefit from users' cooperation?},
year={2013},
volume={},
number={},
pages={2724-2732},
abstract={We propose an analytical framework to tightly characterize the scaling laws for the additional bandwidth that servers must supply to guarantee perfect service in peer-assisted Video-on-Demand systems, taking into account essential aspects such as peer churn, bandwidth heterogeneity, and Zipf-like video popularity. Our results reveal that the catalog size and the content popularity distribution have a huge effect on the system performance. We show that users' cooperation can effectively reduce the servers' burden for a wide range of system parameters, confirming to be an attractive solution to limit the costs incurred by content providers as the system scales to large populations of users.},
keywords={network servers;peer-to-peer computing;video on demand;servers;peer-assisted video-on-demand systems;peer churn;bandwidth heterogeneity;Zipf-like video popularity;content popularity distribution;Bandwidth;Streaming media;Servers;Catalogs;Upper bound;Aggregates;Watches},
doi={10.1109/INFCOM.2013.6567081},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567082,
author={H. Wang and Z. Wang and G. Shen and F. Li and S. Han and F. Zhao},
booktitle={2013 Proceedings IEEE INFOCOM},
title={WheelLoc: Enabling continuous location service on mobile phone for outdoor scenarios},
year={2013},
volume={},
number={},
pages={2733-2741},
abstract={The proliferation of location-based services and applications calls for provisioning of location service as a first class system component that can return accurate location fix in short response time and is energy efficient. In this paper, we present the design, implementation and evaluation of WheelLoc - a continuous system location service for outdoor scenarios. Unlike previous localization efforts that try to directly obtain a point location fix, WheelLoc adopts an indirect approach: it seeks to capture a user mobility trace first and to obtain any point location by time- and speed-aware interpolation or extrapolation. WheelLoc avoids energy-expensive sensors completely and relies solely on commonly available cheap sensors such as accelerometer and magnetometer. With a set of novel techniques and the leverage of publicly available road maps and cell tower information, WheelLoc is able to meet those requirements of a first class component. Experimental results confirmed the effectiveness of WheelLoc. It can return a location estimate within 40ms with an accuracy about 40 meters, consumes only 240mW energy, and effectively strikes a better energy-accuracy tradeoff than GPS duty-cycling.},
keywords={accelerometers;extrapolation;interpolation;magnetometers;mobile handsets;mobility management (mobile radio);telecommunication services;mobile phone;outdoor scenarios;location based services;user mobility trace;point location;interpolation;extrapolation;accelerometer;magnetometer;road maps;cell tower information;GPS duty cycling;power 240 mW;Vectors;Acceleration;Estimation;Roads;Accelerometers;Poles and towers;Magnetometers},
doi={10.1109/INFCOM.2013.6567082},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567083,
author={C. Cheng and P. Hsiu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Extend your journey: Introducing signal strength into location-based applications},
year={2013},
volume={},
number={},
pages={2742-2750},
abstract={Reducing the communication energy is essential to facilitate the growth of emerging mobile applications. In this paper, we introduce signal strength into location-based applications to reduce the energy consumption of mobile devices for data reception. First, we model the problem of data fetch scheduling, with the objective of minimizing the energy required to fetch location-based information without adversely impacting user experience. Then, we propose a dynamic-programming algorithm to solve the fundamental problem and prove its optimality in terms of energy savings. We also provide an optimality condition with respect to signal strength fluctuations. Finally, based on the algorithm, we consider implementation issues. We have also developed a virtual tour system integrated with existing web applications to validate the practicability of the proposed concept. The results of experiments conducted based on real-world case studies are very encouraging.},
keywords={data handling;dynamic programming;mobile computing;scheduling;signal processing;signal strength;location-based applications;communication energy;mobile applications;data reception;data fetch scheduling;dynamic programming;Schedules;Energy consumption;Heuristic algorithms;Delays;IEEE 802.11 Standards;Mobile communication;Mobile handsets;Energy-efficient optimization;signal strength;cellular data fetch scheduling;location-based applications},
doi={10.1109/INFCOM.2013.6567083},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567084,
author={Z. Gao and H. Zhu and Y. Liu and M. Li and Z. Cao},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Location privacy in database-driven Cognitive Radio Networks: Attacks and countermeasures},
year={2013},
volume={},
number={},
pages={2751-2759},
abstract={Cognitive Radio Network (CRN) is regarded as a promising way to address the increasing demand for wireless channel resources. It solves the channel resource shortage problem by allowing a Secondary User (SU) to access the channel of a Primary User (PU) when the channel is not occupied by the PU. The latest FCC's rule in May 2012 enforces database-driven CRNs, in which an SU queries a database to obtain spectrum availability information by submitting a location based query. However, one concern about database-driven CRNs is that the queries sent by SUs will inevitably leak the location information. In this study, we identify a new kind of attack against location privacy of database-drive CRNs. Instead of directly learning the SUs' locations from their queries, our discovered attacks can infer an SU's location through his used channels. We propose Spectrum Utilization based Location Inferring Algorithm that enables the attacker to geo-locate an SU. To thwart location privacy leaking from query process, we propose a novel Private Spectrum Availability Information Retrieval scheme that utilizes a blind factor to hide the location of the SU. To defend against the discovered attack, we propose a novel prediction based Private Channel Utilization protocol that reduces the possibilities of location privacy leaking by choosing the most stable channels. We implement our discovered attack and proposed scheme on the data extracted from Google Earth Coverage Maps released by FCC. Experiment results show that the proposed protocols can significantly improve the location privacy.},
keywords={cognitive radio;data privacy;protocols;query processing;radio spectrum management;telecommunication security;wireless channels;database-driven cognitive radio networks;wireless channel resources;channel resource shortage problem;secondary user;primary user;database-driven CRN;database queries;spectrum availability information;location-based query submission;attack identification;spectrum utilization;location inferring algorithm;private spectrum availability information retrieval scheme;blind factor;SU location;PU channel access;prediction-based private channel utilization protocol;location privacy leakage possibility reduction;Google Earth Coverage Maps;FCC;location privacy improvement;Databases;Data privacy;Vectors;Protocols;Privacy;Wireless communication;Availability;Location Privacy;database-driven Cognitive Radio Network;Private Information Retrieval},
doi={10.1109/INFCOM.2013.6567084},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567085,
author={X. Li and T. Jung},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Search me if you can: Privacy-preserving location query service},
year={2013},
volume={},
number={},
pages={2760-2768},
abstract={Location-Based Service (LBS) becomes increasingly popular with the dramatic growth of smartphones and social network services (SNS), and its context-rich functionalities attract considerable users. Many LBS providers use users' location information to offer them convenience and useful functions. However, the LBS could greatly breach personal privacy because location itself contains much information. Hence, preserving location privacy while achieving utility from it is still an challenging question now. This paper tackles this non-trivial challenge by designing a suite of novel fine-grained Privacy-preserving Location Query Protocol (PLQP). Our protocol allows different levels of location query on encrypted location information for different users, and it is efficient enough to be applied in mobile platforms.},
keywords={protocols;smart phones;privacy-preserving location query service;location-based service;smartphones;social network services;context-rich functionalities;LBS providers;user location information;personal privacy;non-trivial challenge;privacy-preserving location query protocol;PLQP;encrypted location information;mobile platforms;Encryption;Protocols;Privacy;Access control;Iron;Smart phones},
doi={10.1109/INFCOM.2013.6567085},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567086,
author={N. Cheng and X. Oscar Wang and W. Cheng and P. Mohapatra and A. Seneviratne},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Characterizing privacy leakage of public WiFi networks for users on travel},
year={2013},
volume={},
number={},
pages={2769-2777},
abstract={Deployment of public wireless access points (also known as public hotspots) and the prevalence of portable computing devices has made it more convenient for people on travel to access the Internet. On the other hand, it also generates large privacy concerns due to the open environment. However, most users are neglecting the privacy threats because currently there is no way for them to know to what extent their privacy is revealed. In this paper, we examine the privacy leakage in public hotspots from activities such as domain name querying, web browsing, search engine querying and online advertising. We discover that, from these activities multiple categories of user privacy can be leaked, such as identity privacy, location privacy, financial privacy, social privacy and personal privacy. We have collected real data from 20 airport datasets in four countries and discover that the privacy leakage can be up to 68%, which means two thirds of users on travel leak their private information while accessing the Internet at airports. Our results indicate that users are not fully aware of the privacy leakage they can encounter in the wireless environment, especially in public WiFi networks. This fact can urge network service providers and website designers to improve their service by developing better privacy preserving mechanisms.},
keywords={data privacy;Internet;online front-ends;radio access networks;wireless LAN;privacy leakage;public Wi-Fi networks;public wireless access points;portable computing device;Internet;public hotspots;domain name querying;Web browsing;search engine querying;online advertising;identity privacy;location privacy;financial privacy;social privacy;personal privacy;airports;wireless environment;network service provider;privacy preserving mechanism;Privacy;IEEE 802.11 Standards;Protocols;Cognition;Electronic mail;Data privacy;Engines},
doi={10.1109/INFCOM.2013.6567086},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567087,
author={T. Wang and Y. Yang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Analysis on perfect location spoofing attacks using beamforming},
year={2013},
volume={},
number={},
pages={2778-2786},
abstract={Location spoofing attacks pose serious threats to the location based wireless network mechanisms. Most existing literature focuses on detecting location spoofing attacks or design of robust localization algorithms. However, our study shows that, in many circumstances, perfect location spoofing (PLS) can stay undetected even if robust localization algorithms or detection mechanisms are used. In this paper, we present theoretical analysis on the feasibility of beamforming-based PLS attacks and how it is affected by the anchor deployment. We formulate PLS as a nonlinear feasibility problem based on smart antenna array pattern synthesis. Due to the intractable nature of this feasibility problem, we solve it using semidefinite relaxation (SDR) in conjunction with a heuristic local search algorithm. Simulation results show the effectiveness of our analytical approach and provide insightful advices for defence against PLS attacks.},
keywords={adaptive antenna arrays;array signal processing;radio networks;search problems;telecommunication security;perfect location spoofing attack;location based wireless network;beamforming-based PLS attack;anchor deployment;nonlinear feasibility problem;smart antenna array pattern synthesis;semidefinite relaxation;SDR;heuristic local search algorithm;Array signal processing;Robustness;Vectors;Search problems;Partitioning algorithms;Algorithm design and analysis;Wireless communication},
doi={10.1109/INFCOM.2013.6567087},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567088,
author={Y. Zhang and L. Lazos},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Countering selfish misbehavior in multi-channel MAC protocols},
year={2013},
volume={},
number={},
pages={2787-2795},
abstract={We address the problem of MAC-layer misbehavior in distributed multi-channel MAC protocols. We show that selfish users can manipulate the protocol parameters to gain an unfair share of the available bandwidth, while remaining undetected. We identify optimal misbehavior strategies that can lead to the isolation of a subset of frequency bands for exclusive use by the misbehaving nodes and evaluate their impact on performance and fairness. We develop corresponding detection and mitigation strategies that practically eliminate the misbehavior gains. To the best of our knowledge, this is the first attempt in characterizing the impact of misbehavior on multi-channel MAC protocols.},
keywords={access protocols;wireless channels;selfish misbehavior;MAC-layer misbehavior;distributed multichannel MAC protocol;selfish user;protocol parameter;bandwidth;optimal misbehavior strategies;frequency band;misbehaving node;detection strategies;mitigation strategies;misbehavior gains;Media Access Protocol;Monitoring;Radiation detectors;Throughput;Receivers;Bandwidth},
doi={10.1109/INFCOM.2013.6567088},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567089,
author={B. Han and J. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Secrecy capacity maximization for secure cooperative ad-hoc networks},
year={2013},
volume={},
number={},
pages={2796-2804},
abstract={This paper investigates secure cooperative communication with the involvement of multiple malicious eavesdroppers. By characterizing the security performance of the system by secrecy capacity, we study the secrecy capacity maximization problem in cooperative communication aware ad hoc networks. Specifically, we propose a system model where secrecy capacity enhancement is achieved by the assignment of cooperative relays. We theoretically present a corresponding formulation for the problem and discuss the security gain brought by the relay assignment process. Then, we develop an optimal relay assignment algorithm to solve the secrecy capacity maximization problem in polynomial time. The basic idea behind our proposed algorithm is to boost the capacity of the primary channel by simultaneously decreasing the capacity of the eavesdropping channel. To further increase the system secrecy capacity, we exploit the jamming technique and propose a smart jamming algorithm to interfere the eavesdropping channels. Analysis and experimental results reveal that our proposed algorithms significantly increase the system secrecy capacity under various network settings.},
keywords={ad hoc networks;communication complexity;cooperative communication;jamming;telecommunication security;secrecy capacity maximization;secure cooperative ad hoc networks;secure cooperative communication;multiple malicious eavesdroppers;polynomial time;primary channel;jamming technique;Relays;Ad hoc networks;Security;Jamming;Channel capacity;Wireless communication;Polynomials},
doi={10.1109/INFCOM.2013.6567089},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567090,
author={T. V. Nguyen and F. Baccelli and K. Zhu and S. Subramanian and X. Wu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A performance analysis of CSMA based broadcast protocol in VANETs},
year={2013},
volume={},
number={},
pages={2805-2813},
abstract={The broadcast of periodic messages is a key functionality in vehicular ad hoc networks. In the emerging vehicular networks, IEEE 802.11p is the standard of choice to support the PHY and MAC layer functionalities. The broadcast process in IEEE 802.11p is based on the CSMA mechanism where a device transmitting a packet senses the channel for ongoing transmissions and performs a random back-off before accessing the channel. Without RTS/CTS mechanisms, carrier sensing is expected to provide a protection region around the transmitter where no other transmitters are allowed. The point process characterizing the concurrent transmitters is expected to enforce a minimum separation between concurrent transmitters. However, at increasing densities, the CSMA behavior breaks down to an ALOHA-like transmission pattern where concurrent transmitters are distributed as a Poisson point process, indicating the lack of protection around transmitters. In this paper, we model the CSMA mechanism as a slotted system and analytically characterize the critical node/packet arrival density where the CSMA mechanism approaches an ALOHAlike behavior. Further, we use tools from stochastic geometry to establish closed-form expressions for the performance metrics of the broadcast mechanism in the ALOHA regime. Finally, using ns2 (an unslotted asynchronous simulator), we compare the theoretical results with simulations.},
keywords={carrier sense multiple access;radio transmitters;stochastic processes;vehicular ad hoc networks;wireless LAN;CSMA performance analysis;broadcast protocol;VANET;vehicular ad hoc network;IEEE 802.11p standard;MAC layer functionality;PHY layer functionality;RTS-CTS mechanism;carrier sensing;concurrent transmitter;ALOHA-like transmission pattern;Poisson point process;slotted system;critical node-packet arrival density;stochastic geometry;closed-form expression;ALOHA;Radiation detectors;Multiaccess communication;Transmitters;Vehicles;Protocols;Safety},
doi={10.1109/INFCOM.2013.6567090},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567091,
author={L. Zhang and L. Cai and J. Pan},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Connectivity in two-dimensional lattice networks},
year={2013},
volume={},
number={},
pages={2814-2822},
abstract={Connectivity has been extensively studied in ad hoc networks, most recently with the application of percolation theory in two-dimensional square lattices. Given a message source and the bond probability to connect neighbor vertexes on the lattice, percolation theory tries to determine the critical bond probability above which there exists an infinite connected giant component with high probability. This paper studies a related but different problem: what is the connectivity from the source to any vertex on the square lattice following certain directions? The original directed percolation problem has been studied in statistical physics for more than half a century, with only simulation results available. In this paper, by using a recursive decomposition approach, we have obtained the analytical expressions for directed connectivity. The results can be widely used in wireless and mobile ad hoc networks, including vehicular ad hoc networks.},
keywords={lattice theory;mobile ad hoc networks;percolation;probability;2D lattice network;percolation theory;2D square lattice;critical bond probability;infinite connected giant component;directed percolation problem;recursive decomposition;analytical expression;directed connectivity;wireless ad hoc network;mobile ad hoc network;vehicular ad hoc network;Lattices;Poles and towers;Silicon;Vehicular ad hoc networks;Complexity theory;Probability;Connectivity;square lattice;directed percolation},
doi={10.1109/INFCOM.2013.6567091},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567092,
author={H. Wang and Y. Zhu and Q. Zhang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Compressive sensing based monitoring with vehicular networks},
year={2013},
volume={},
number={},
pages={2823-2831},
abstract={Vehicles are becoming powerful mobile sensors, and vehicular networks provide a promising platform to support a wide range of existing large-scale monitoring applications such as road surface monitoring, and etc. In vehicular networks, inter-vehicle contacts are scarce resources for data delivery. This presents a major challenge for monitoring applications with vehicular networks. By analyzing a large dataset of taxi traces collected from around 2,600 taxis in Shanghai, China, we reveal that there is strong correlation with data readings on vehicles. Motivated by this important observation, we propose a compressive sensing based approach called CSM to monitor with vehicular networks. Two key issues must be addressed. First, there is an intrinsic tradeoff between communication cost and estimation accuracy. Second, guaranteed estimation accuracy should be provided over the highly dynamic network. To address the above issues, we first characterize the relationship between estimation error (12 error) and sparsity property of a dataset. Then, we determine two critical parameters: the minimum number of seeds and the minimum transmission hop length for compressive measurements in the network. The selection of the two parameters can reduce the communication cost while guaranteeing the required estimation accuracy. Extensive simulations based on real vehicular GPS traces collected in Shanghai, China have been performed and results demonstrate that CSM achieves much higher estimation accuracy at the same communication cost compared with other alternative schemes.},
keywords={compressed sensing;vehicular ad hoc networks;wireless sensor networks;compressive sensing;vehicular networks;mobile sensors;monitoring applications;dataset;taxi;estimation error;real vehicular GPS traces;Vehicles;Monitoring;Compressed sensing;Entropy;Accuracy;Estimation error;Vehicular networks;monitoring;compressive Sensing;routing;seed selection},
doi={10.1109/INFCOM.2013.6567092},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567093,
author={H. Zhu and M. Dong and S. Chang and Y. Zhu and M. Li and X. Sherman Shen},
booktitle={2013 Proceedings IEEE INFOCOM},
title={ZOOM: Scaling the mobility for fast opportunistic forwarding in vehicular networks},
year={2013},
volume={},
number={},
pages={2832-2840},
abstract={Vehicular networks consist of highly mobile vehicles communications, where connectivity is intermittent. Due to the distributed and highly dynamic nature of vehicular network, to minimize the end-to-end delay and the network traffic at the same time in data forwarding is very hard. Heuristic algorithms utilizing either contact-level or social-level scale of vehicular mobility have only one-sided view of the network and therefore are not optimal. In this paper, by analyzing three large sets of Global Positioning System (GPS) trace of more than ten thousand public vehicles, we find that pairwise contacts have strong temporal correlation. Furthermore, the contact graph of vehicles presents complex structure when aggregating the underlying contacts. In understanding the impact of both levels of mobility to the data forwarding, we propose an innovative scheme, named ZOOM, for fast opportunistic forwarding in vehicular networks, which automatically choose the most appropriate mobility information when deciding next data-relays in order to minimize the end-to-end delay while reducing the network traffic. Extensive trace-driven simulations demonstrate the efficacy of ZOOM design. On average, ZOOM can improve 30% performance gain comparing to the state-of-art algorithms.},
keywords={data communication;Global Positioning System;mobile radio;telecommunication traffic;mobility;fast opportunistic forwarding;vehicular networks;mobile vehicles communications;end-to-end delay;network traffic;contact-level scale;social-level scale;vehicular mobility;Global Positioning System;GPS trace;public vehicles;temporal correlation;data forwarding;opportunistic forwarding;data-relays;ZOOM design;Vehicles;Delays;Routing;Algorithm design and analysis;Communities;Global Positioning System;Entropy;mobility scale;vehicular networks;opportunistic forwarding;social network analysis;inter-contact time},
doi={10.1109/INFCOM.2013.6567093},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567094,
author={R. Combes and Z. Altman and E. Altman},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Interference coordination in wireless networks: A flow-level perspective},
year={2013},
volume={},
number={},
pages={2841-2849},
abstract={In dense wireless networks, inter-cell interference highly limits the capacity and quality of service perceived by users. Previous work has shown that approaches based on frequency reuse provide important capacity gains. We model a wireless network with Inter-Cell Interference Coordination (ICIC) at the flow level where users arrive and depart dynamically, in order to optimize quality of service indicators perceivable by users such as file transfer time for elastic traffic. We propose an algorithm to tune the parameters of ICIC schemes automatically based on measurements. The convergence of the algorithm to a local optimum is proven, and a heuristic to improve its convergence speed is given. Numerical experiments show that the distance between local optima and the global optimum is very small, and that the algorithm is fast enough to track changes in traffic on the time scale of hours. The proposed algorithm can be implemented in a distributed way with very small signaling load.},
keywords={convergence;numerical analysis;quality of service;radio networks;radiofrequency interference;telecommunication traffic;interference coordination;wireless networks;flow-level perspective;intercell interference coordination;quality of service;file transfer time;elastic traffic;ICIC scheme;numerical analysis;local optima;global optimum;convergence speed;small signaling load;Interference;Fading;Wireless networks;Load modeling;Convergence;Optimization;Niobium;Wireless Networks;Queuing Theory;Traffic Engineering;Self-Organizing Networks;Stochastic Approximation;Stability;OFDMA;Load Balancing;Self configuration;Self Optimization},
doi={10.1109/INFCOM.2013.6567094},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567095,
author={A. Badr and A. Khisti and W. Tan and J. Apostolopoulos},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Streaming codes for channels with burst and isolated erasures},
year={2013},
volume={},
number={},
pages={2850-2858},
abstract={We study low-delay error correction codes for streaming recovery over a class of packet-erasure channels that introduce both burst-erasures and isolated erasures. We propose a simple, yet effective class of codes whose parameters can be tuned to obtain a tradeoff between the capability to correct burst and isolated erasures. Our construction generalizes previously proposed low-delay codes which are effective only against burst erasures. We establish an information theoretic upper bound on the capability of any code to simultaneously correct burst and isolated erasures and show that our proposed constructions meet the upper bound in some special cases. We discuss the operational significance of column-distance and column-span metrics and establish that the rate 1/2 codes discovered by Martinian and Sundberg [IT Trans. 2004] through a computer search indeed attain the optimal column-distance and column-span tradeoff. Numerical simulations over a Gilbert-Elliott channel model and a Fritchman model show significant performance gains over previously proposed low-delay codes and random linear codes for certain range of channel parameters.},
keywords={channel coding;error correction codes;video coding;video streaming;low delay error correction codes;streaming recovery;packet erasure channels;burst erasures;isolated erasures;information theoretic upper bound;column distance metrics;column span metrics;rate 1/2 codes;Gilbert Elliott channel model;Fritchman model;random linear codes;Parity check codes;Delays;Linear code;Decoding;Upper bound;Error correction codes;Convolutional codes},
doi={10.1109/INFCOM.2013.6567095},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567096,
author={C. Qiu and H. Shen and S. Soltani and K. Sapra and H. Jiang and J. Hallstrom},
booktitle={2013 Proceedings IEEE INFOCOM},
title={CEDAR: An optimal and distributed strategy for packet recovery in wireless networks},
year={2013},
volume={},
number={},
pages={2859-2867},
abstract={Underlying link-layer protocols of wireless networks use the conventional “store and forward” design paradigm cannot provide highly sustainable reliability and stability in wireless communication, which introduce significant barriers and setbacks in scalability and deployments of wireless networks. In this paper, we propose a Code Embedded Distributed Adaptive and Reliable (CEDAR) link-layer framework that targets low latency and high throughput. CEDAR is the first comprehensive theoretical framework for analyzing and designing distributed and adaptive error recovery for wireless networks. It employs a theoretically-sound framework for embedding channel codes in each packet and performs the error correcting process in selected intermediate nodes in packet's route. To identify the intermediate nodes for the en/decoding for minimizing average packet latency, we mathematically analyze the average packet delay, using Finite State Markovian Channel model and priority queuing model, and then formalize the problem as a non-linear integer programming problem. Also, we propose a scalable and distributed scheme to solve this problem. The results from real-world testbed “NESTbed” and simulation with Matlab prove that CEDAR is superior to the schemes using hop-by-hop decoding and destination-decoding not only in packet delay but also in throughput. In addition, the simulation results show that CEDAR can achieve the optimal performance in most cases.},
keywords={channel coding;decoding;error correction;error correction codes;packet radio networks;destination decoding;hop by hop decoding;Matlab;nonlinear integer programming problem;queuing model;finite state Markovian channel model;average packet delay;average packet latency;error correcting process;channel codes;adaptive error recovery;reliable link layer framework;code embedded distributed adaptive link layer framework;wireless communication stability;sustainable reliability;link layer protocols;wireless networks;packet recovery;distributed strategy;CEDAR;Delays;Decoding;Mathematical model;Wireless communication;Bit error rate;Stability analysis},
doi={10.1109/INFCOM.2013.6567096},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567097,
author={P. Wan and Z. Wan and Z. Wang and X. Xu and S. Tang and X. Jia},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Stability analyses of static greedy link schedulings in MC-MR wireless networks},
year={2013},
volume={},
number={},
pages={2868-2876},
abstract={Static greedy link schedulings have much simpler implementation than dynamic greedy link schedulings such as Longest-queue-first (LQF) link scheduling. However, its stability performance in multi-channel multi-radio (MC-MR) wireless networks is largely under-explored. In this paper, we present a stability subregion with closed form of a static greedy link scheduling in MC-MR wireless networks under the 802.11 interference model. By adopting some special static link orderings, the stability subregion is within a constant factor of the stable capacity region of the network. We also obtain constant lower bounds on the throughput efficiency ratios of the static greedy link schedulings in some special static link orderings.},
keywords={radio links;radiofrequency interference;scheduling;wireless channels;wireless LAN;static greedy link scheduling;stability analysis;MC-MR wireless networks;stability performance;multichannel multiradio wireless networks;stability subregion;802.11 interference model;static link orderings;stable network capacity region;throughput efficiency ratios;constant lower bounds;Wireless networks;Throughput;Interference;Stability analysis;IEEE 802.11 Standards;Vectors;Stability;multi-channel multi-radio;link scheduling},
doi={10.1109/INFCOM.2013.6567097},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567098,
author={F. Hao and M. Kodialam and T. V. Lakshman and K. P. N. Puttaswamy},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Protecting cloud data using dynamic inline fingerprint checks},
year={2013},
volume={},
number={},
pages={2877-2885},
abstract={Preventing flow of confidential data out of a network is a fundamental problem faced by network operators. This problem gets even more complex in the context of Cloud Computing, where multiple distrusting customers share the same underlying infrastructure, and data is often replicated and moved across regions. Despite the significance of this problem, existing solutions are based on generic search for keywords in outgoing data, and hence severely lack the ability to control data flow at a fine granularity with low false positives. In this paper, we advocate a fine-grained approach to prevent confidential data from leaking out of the cloud. We propose a solution using document-level fingerprint checks. We show via analysis and experiments that our algorithm for checking the fingerprints on-the-fly scale to a large amount of documents at very low cost. For example, for one TB of documents, our solution only requires 340 MB memory to achieve worst case expected detection lag (i.e. leakage length) of 1000 bytes.},
keywords={cloud computing;document handling;security of data;cloud data protection;dynamic inline fingerprint checks;confidential data flow;cloud computing;keyword generic search;document-level fingerprint checks;fingerprint on-the-fly scale checking;worst case expected detection lag;Databases;Probabilistic logic;Memory management;Heuristic algorithms;Protocols;Equations;Algorithm design and analysis},
doi={10.1109/INFCOM.2013.6567098},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567099,
author={G. Wang and Q. Liu and F. Li and S. Yang and J. Wu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Outsourcing privacy-preserving social networks to a cloud},
year={2013},
volume={},
number={},
pages={2886-2894},
abstract={In the real world, companies would publish social networks to a third party, e.g., a cloud service provider, for marketing reasons. Preserving privacy when publishing social network data becomes an important issue. In this paper, we identify a novel type of privacy attack, termed 1*-neighborhood attack. We assume that an attacker has knowledge about the degrees of a target's one-hop neighbors, in addition to the target's 1-neighborhood graph, which consists of the one-hop neighbors of the target and the relationships among these neighbors. With this information, an attacker may re-identify the target from a k-anonymity social network with a probability higher than 1/k, where any node's 1-neighborhood graph is isomorphic with k - 1 other nodes' graphs. To resist the 1*-neighborhood attack, we define a key privacy property, probability indistinguishability, for an outsourced social network, and propose a heuristic indistinguishable group anonymization (HIGA) scheme to generate an anonymized social network with this privacy property. The empirical study indicates that the anonymized social networks can still be used to answer aggregate queries with high accuracy.},
keywords={cloud computing;data privacy;graph theory;outsourcing;query processing;social networking (online);privacy-preserving social network outsourcing;cloud service provider;marketing reasons;privacy attack;1*-neighborhood attack;one-hop neighbors;1-neighborhood graph;k-anonymity social network;privacy property;heuristic indistinguishable group anonymization scheme;HIGA;anonymized social network;privacy property;aggregate queries;Social network services;Privacy;Measurement;Probabilistic logic;Outsourcing;Educational institutions;Aggregates;Cloud computing;social networks;privacy;probability indistinguishability},
doi={10.1109/INFCOM.2013.6567099},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567100,
author={K. Yang and X. Jia and K. Ren and B. Zhang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={DAC-MACS: Effective data access control for multi-authority cloud storage systems},
year={2013},
volume={},
number={},
pages={2895-2903},
abstract={Data access control is an effective way to ensure the data security in the cloud. However, due to data outsourcing and untrusted cloud servers, the data access control becomes a challenging issue in cloud storage systems. Existing access control schemes are no longer applicable to cloud storage systems, because they either produce multiple encrypted copies of the same data or require a fully trusted cloud server. Ciphertext-Policy Attribute-based Encryption (CP-ABE) is a promising technique for access control of encrypted data. It requires a trusted authority manages all the attributes and distributes keys in the system. In cloud storage systems, there are multiple authorities co-exist and each authority is able to issue attributes independently. However, existing CP-ABE schemes cannot be directly applied to data access control for multi-authority cloud storage systems, due to the inefficiency of decryption and revocation. In this paper, we propose DAC-MACS (Data Access Control for Multi-Authority Cloud Storage), an effective and secure data access control scheme with efficient decryption and revocation. Specifically, we construct a new multi-authority CP-ABE scheme with efficient decryption and also design an efficient attribute revocation method that can achieve both forward security and backward security. The analysis and the simulation results show that our DAC-MACS is highly efficient and provably secure under the security model.},
keywords={authorisation;cloud computing;cryptography;network servers;outsourcing;storage management;trusted computing;cloud data security;data outsourcing;untrusted cloud servers;encrypted copies;trusted cloud server;ciphertext-policy attribute-based encryption;data access control for multiauthority cloud storage;DAC-MACS;secure data access control scheme;multiauthority CP-ABE scheme;Cloud computing;Servers;Access control;Public key;Encryption;Access Control;CP-ABE;Decryption Outsourcing;Attribute Revocation;Multi-authority Cloud},
doi={10.1109/INFCOM.2013.6567100},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567101,
author={B. Wang and B. Li and H. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Public auditing for shared data with efficient user revocation in the cloud},
year={2013},
volume={},
number={},
pages={2904-2912},
abstract={With data services in the cloud, users can easily modify and share data as a group. To ensure data integrity can be audited publicly, users need to compute signatures on all the blocks in shared data. Different blocks are signed by different users due to data modifications performed by different users. For security reasons, once a user is revoked from the group, the blocks, which were previously signed by this revoked user must be re-signed by an existing user. The straightforward method, which allows an existing user to download the corresponding part of shared data and re-sign it during user revocation, is inefficient due to the large size of shared data in the cloud. In this paper, we propose a novel public auditing mechanism for the integrity of shared data with efficient user revocation in mind. By utilizing proxy re-signatures, we allow the cloud to re-sign blocks on behalf of existing users during user revocation, so that existing users do not need to download and re-sign blocks by themselves. In addition, a public verifier is always able to audit the integrity of shared data without retrieving the entire data from the cloud, even if some part of shared data has been re-signed by the cloud. Experimental results show that our mechanism can significantly improve the efficiency of user revocation.},
keywords={cloud computing;data integrity;digital signatures;public auditing mechanism;shared data integrity;user revocation;cloud;data services;data modifications;straightforward method;proxy resignatures;public verifier;Forgery;Public key;Indexes;Manganese;Games;Writing},
doi={10.1109/INFCOM.2013.6567101},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567102,
author={S. K. A. Imon and A. Khan and M. Di Francesco and S. K. Das},
booktitle={2013 Proceedings IEEE INFOCOM},
title={RaSMaLai: A Randomized Switching algorithm for Maximizing Lifetime in tree-based wireless sensor networks},
year={2013},
volume={},
number={},
pages={2913-2921},
abstract={In most wireless sensor network (WSN) applications, data are typically gathered by the sensor nodes and reported to a data collection point, called the sink. In order to support such data collection, a tree structure rooted at the sink is usually defined. Based on different aspects, including the actual WSN topology and the available energy budget, the energy consumption of nodes belonging to different paths in the data collection tree may vary significantly. This affects the overall network lifetime, defined in terms of when the first node in the network runs out of energy. In this paper, we address the problem of lifetime maximization of WSNs in the context of data collection trees. In particular, we propose a novel and efficient algorithm, called Randomized Switching for Maximizing Lifetime (RaSMaLai) that aims at maximizing the lifetime of WSNs through load balancing with a low time complexity. We further design a distributed version of our algorithm, called D-RaSMaLai. Simulation results show that both the proposed algorithms outperform several existing approaches in terms of network lifetime. Moreover, RaSMaLai offers lower time complexity while the distributed version, D-RaSMaLai, is very efficient in terms of energy expenditure.},
keywords={telecommunication network topology;telecommunication switching;trees (mathematics);wireless sensor networks;randomized switching for maximizing lifetime;D-RaSMaLai;network lifetime;tree-based wireless sensor networks;WSN topology;sensor nodes;data collection point;sink nodes;energy budget;lifetime maximization;time complexity;Switches;Data collection;Wireless sensor networks;Load management;Time complexity;Oscillators;Educational institutions;Sensor Networks;Load Balancing;Data Collection Tree;Network Lifetime;Randomized Algorithm},
doi={10.1109/INFCOM.2013.6567102},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567103,
author={L. Fu and P. Cheng and Y. Gu and J. Chen and T. He},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Minimizing charging delay in wireless rechargeable sensor networks},
year={2013},
volume={},
number={},
pages={2922-2930},
abstract={As a pioneering experimental platform of wireless rechargeable sensor networks, the Wireless Identification and Sensing Platform (WISP) is an open-source platform that integrates sensing and computation capabilities to the traditional RFID tags. Different from traditional tags, a RFID-based wireless rechargeable sensor node needs to charge its onboard energy storage above a threshold in order to power its sensing, computation and communication components. Consequently, such charging delay imposes a unique design challenge for deploying wireless rechargeable sensor networks. In this paper, we tackle this problem by planning the optimal movement strategy of the RFID reader, such that the time to charge all nodes in the network above their energy threshold is minimized. We first propose an optimal solution using the linear programming method. To further reduce the computational complexity, we then introduce a heuristic solution with a provable approximation ratio of (1 + θ)/(1 - ε) by discretizing the charging power on a two-dimensional space. Through extensive evaluations, we demonstrate that our design outperforms the set-cover-based design by an average of 24.7% while the computational complexity is O((N/ε)2).},
keywords={approximation theory;communication complexity;energy storage;linear programming;radiofrequency identification;wireless sensor networks;charging delay;wireless rechargeable sensor network;wireless identification;wireless sensing platform;WISP;open-source platform;sensing capability;computation capability;RFID tag;onboard energy storage;optimal movement strategy;linear programming method;computational complexity;heuristic solution;approximation ratio;two-dimensional space;Delays;Wireless sensor networks;Wireless communication;Robot sensing systems;Merging;Radiofrequency identification},
doi={10.1109/INFCOM.2013.6567103},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567104,
author={M. I. V. Gallego and F. Rousseau},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Energy efficient neighborhood maintenance and medium access with Wake on Idle},
year={2013},
volume={},
number={},
pages={2931-2939},
abstract={The main idea of this paper is to rely only on analog channel sensing to provide integrated neighborhood maintenance and medium access control in a single mechanism for energy constrained low-power wireless sensor nodes. We propose Wake on Idle, a solution that can be implemented in the radio chip and provides these services without relying on the processor when the stable state is reached. No digital decoding is needed as we use pseudo random sequences to identify analog beacon positions in time and assess neighbors presence. Additionally, we use a code violation principle to provide medium access, ensuring that when a node wants to transmit data to a neighbor, the latter will be listening. We analyze this proposition, give some simulation results, and also provide extensive experimentation results based on an implementation running on two different hardware platforms and showing the concept to be valid, although some adaptation was needed to run on off-the-shelf radios. Even with this far from optimal initial implementation we can reach below 0.1% duty cycles for signaling periods on the order of minutes.},
keywords={access protocols;analogue circuits;energy conservation;low-power electronics;microprocessor chips;random sequences;telecommunication power management;telecommunication signalling;wireless sensor networks;energy efficient neighborhood maintenance;analog channel sensing;integrated neighborhood maintenance;medium access control;energy constrained low-power wireless sensor node;wake on idle;radio chip;pseudorandom sequence;analog beacon position;code violation principle;hardware platform;off-the-shelf radio;duty cycle;signaling period;Synchronization;Schedules;Maintenance engineering;Media Access Protocol;Sensors;Decoding},
doi={10.1109/INFCOM.2013.6567104},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567105,
author={Y. Gao and J. Niu and R. Zhou and G. Xing},
booktitle={2013 Proceedings IEEE INFOCOM},
title={ZiFind: Exploiting cross-technology interference signatures for energy-efficient indoor localization},
year={2013},
volume={},
number={},
pages={2940-2948},
abstract={Indoor localization becomes increasingly important as context-aware applications gain popularity in mobile users. A promising approach for indoor localization is to leverage the pervasive WiFi infrastructure via fingerprinting-based inference. However, a WiFi device must frequently scan for WiFi signals during localization, leading to high power consumption. Moreover, switching to the scanning mode introduces inevitable disruptions to data communication of WiFi interface. This paper presents a new indoor localization system called ZiFind that exploits the cross-technology interference in the unlicensed 2.4 GHz frequency spectrum. ZiFind utilizes low-power ZigBee interface to collect WiFi interference signals and adopts digital signal processing techniques to extract unique signatures as fingerprints for localization. To deal with the noise in the fingerprints, we design a new learning algorithm called R-KNN that can improve the accuracy of localization by assigning different weights to fingerprint features according to their importance. We implement ZiFind on TelosB motes and evaluate its performance through extensive experiments in a 16,000 ft<sup>2</sup> office building floor consisting of 28 rooms. Our results show that ZiFind leads to significant power saving compared with existing approaches based on WiFi interface, and yields satisfactory localization accuracy in a range of realistic settings.},
keywords={indoor communication;ubiquitous computing;wireless LAN;ZiFind;cross-technology interference signatures;energy-efficient indoor localization;context-aware applications;mobile users;pervasive WiFi infrastructure;fingerprinting;data communication;IEEE 802.11 Standards;Zigbee;Servers;Accuracy;Interference;Mobile communication;Timing},
doi={10.1109/INFCOM.2013.6567105},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567106,
author={C. Li and E. Modiano},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Receiver-based flow control for networks in overload},
year={2013},
volume={},
number={},
pages={2949-2957},
abstract={We consider utility maximization in networks where the sources do not employ flow control and may consequently overload the network. In the absence of flow control at the sources, some packets will inevitably have to be dropped when the network is in overload. To that end, we first develop a distributed, threshold-based packet dropping policy that maximizes the weighted sum throughput. Next, we consider utility maximization and develop a receiver-based flow control scheme that, when combined with threshold-based packet dropping, achieves the optimal utility. The flow control scheme creates virtual queues at the receivers as a push-back mechanism to optimize the amount of data delivered to the destinations via back-pressure routing. A novel feature of our scheme is that a utility function can be assigned to a collection of flows, generalizing the traditional approach of optimizing per-flow utilities. Our control policies use finite-buffer queues and are independent of arrival statistics. Their near-optimal performance is proved and further supported by simulation results.},
keywords={optimisation;queueing theory;statistical analysis;telecommunication control;telecommunication network routing;receiver-based flow control scheme;utility maximization;overload;distributed dropping policy;threshold-based packet dropping policy;weighted sum throughput maximization;optimal utility;virtual queues;push-back mechanism;back-pressure routing;flow collection;finite-buffer queues;arrival statistics;data networks;Throughput;Receivers;Routing;Vectors;Resource management;Optimization;Aggregates},
doi={10.1109/INFCOM.2013.6567106},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567107,
author={C. Lai and K. Leung and V. O. K. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Does it hurt when others prosper?: Exploring the impact of heterogeneous reordering robustness of TCP},
year={2013},
volume={},
number={},
pages={2958-2966},
abstract={The congestion control mechanisms in the standardized Transmission Control Protocol (TCP) may misinterpret packet reordering as congestive loss, leading to spurious congestion response and under-utilization of network capacity. Therefore, many TCP enhancements have been proposed to better differentiate between packet reordering and congestive loss, in order to enhance the reordering robustness (RR) of TCP. Since such enhancements are incrementally deployed, it is important to study the interactions of TCP flows with heterogeneous RR. This paper presents the first systematic study of such interactions by exploring how changing RR of TCP flows influences the bandwidth sharing among these flows. We define the quantified RR (QRR) of a TCP flow as the probability that packet reordering causes congestion response. We analyze the variation of bandwidth sharing as QRR changes. This leads to the discovery of several interesting properties. Most notably, we discover the counter-intuitive result that changing one flow's QRR does not affect its competing flows in certain network topologies. We further characterize the deviation, from the ideal case of bandwidth sharing, as RR changes. We find that enhancing RR of a flow may increase, rather than decrease, the deviation in some typical network scenarios.},
keywords={bandwidth allocation;telecommunication congestion control;telecommunication network topology;transport protocols;TCP;congestion control mechanism;standardized transmission control protocol;packet reordering;spurious congestion response;congestive loss;reordering robustness;quantified RR;bandwidth sharing;network topology;Bandwidth;Robustness;Routing;Systematics;Network topology;Internet;Aggregates},
doi={10.1109/INFCOM.2013.6567107},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567108,
author={K. Inoue and D. Pasetto and K. Lynch and M. Meneghin and K. Muller and J. Sheehan},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Low-latency and high bandwidth TCP/IP protocol processing through an integrated HW/SW approach},
year={2013},
volume={},
number={},
pages={2967-2975},
abstract={Ultra low-latency networking is critical in many domains, such as high frequency trading and high performance computing (HPC), and highly desirable in many others such as VoIP and on-line gaming. In closed systems - such as those found in HPC - Infiniband, iWARP or RoCE are common choices as system architects have the opportunity to choose the best host configurations and networking fabric. However, the vast majority of networks are built upon Ethernet with nodes exchanging data using the standard TCP/IP stack. On such networks, achieving ultra low-latency while maintaining compatibility with a standard TCP/IP stack is crucial. To date, most efforts for low-latency packet transfers have focused on three main areas: (i) avoiding context switches, (ii) avoiding buffer copies, and (iii) off-loading protocol processing. This paper describes IBM PowerENTM and its networking stack, showing that an integrated system design which treats Ethernet adapters as first class citizens that share the system bus with CPUs and memory, rather than as peripheral PCI Express attached devices, is a winning solution for achieving minimal latency. The work presents outstanding performance figures, including 1.30μs from wire to wire for UDP, usually the chosen protocol for latency sensitive applications, and excellent latency and bandwidth figures for the more complex TCP.},
keywords={hardware-software codesign;local area networks;system buses;transport protocols;low-latency TCP/IP protocol;high bandwidth TCP/IP protocol;HW-SW approach;ultra low-latency networking;high frequency trading;high performance computing;HPC;Infiniband;iWARP;RoCE;packet transfer;IBM PowerENTM;Ethernet adapter;system bus;Kernel;IP networks;Protocols;Hardware;Sockets;Ports (Computers);Payloads},
doi={10.1109/INFCOM.2013.6567108},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567109,
author={J. Nair and K. Jagannathan and A. Wierman},
booktitle={2013 Proceedings IEEE INFOCOM},
title={When heavy-tailed and light-tailed flows compete: The response time tail under generalized max-weight scheduling},
year={2013},
volume={},
number={},
pages={2976-2984},
abstract={This paper focuses on the design and analysis of scheduling policies for multi-class queues, such as those found in wireless networks and high-speed switches. In this context, we study the response time tail under generalized max-weight policies in settings where the traffic flows are highly asymmetric. Specifically, we study an extreme setting with two traffic flows, one heavy-tailed, and one light-tailed. In this setting, we prove that classical max-weight scheduling, which is known to be throughput optimal, results in the light-tailed flow having heavy-tailed response times. However, we show that via a careful design of inter-queue scheduling policy (from the class of generalized max-weight policies) and intra-queue scheduling policies, it is possible to maintain throughput optimality, and guarantee light-tailed delays for the light-tailed flow, without affecting the response time tail for the heavy-tailed flow.},
keywords={queueing theory;radio networks;scheduling;telecommunication traffic;heavy-tailed flow;light-tailed flow;generalized max-weight scheduling;multiclass queue;scheduling policy;high-speed switch;wireless networks;traffic flow;heavy-tailed response time;interqueue scheduling policy;intraqueue scheduling policy;light-tailed delay;Time factors;Indexes;Queueing analysis;Servers;Throughput;Stability analysis;Optimal scheduling},
doi={10.1109/INFCOM.2013.6567109},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567110,
author={X. Liu and K. Liu and L. Guo and X. Li and Y. Fang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A game-theoretic approach for achieving k-anonymity in Location Based Services},
year={2013},
volume={},
number={},
pages={2985-2993},
abstract={Location Based Service (LBS), although it greatly benefits the daily life of mobile device users, has introduced significant threats to privacy. In an LBS system, even under the protection of pseudonyms, users may become victims of inference attacks, where an adversary reveals a user's real identity and complete moving trajectory with the aid of side information, e.g., accidental identity disclosure through personal encounters. To enhance privacy protection for LBS users, a common approach is to include extra fake location information associated with different pseudonyms, known as dummy users, in normal location reports. Due to the high cost of dummy generation using resource constrained mobile devices, self-interested users may free-ride on others' efforts. The presence of such selfish behaviors may have an adverse effect on privacy protection. In this paper, we study the behaviors of self-interested users in the LBS system from a game-theoretic perspective. We model the distributed dummy user generation as Bayesian games in both static and timing-aware contexts, and analyze the existence and properties of the Bayesian Nash Equilibria for both models. Based on the analysis, we propose a strategy selection algorithm to help users achieve optimized payoffs. Leveraging a beta distribution generalized from real-world location privacy data traces, we perform simulations to assess the privacy protection effectiveness of our approach. The simulation results validate our theoretical analysis for the dummy user generation game models.},
keywords={Bayes methods;data privacy;game theory;mobile computing;game theory;K-anonymity;location based service;moving trajectory;extra fake location information;dummy user;location report;selfish behavior;self-interested user;Bayesian game;static aware context;timing aware context;Bayesian Nash equilibria;location privacy data trace;privacy protection effectiveness;Privacy;Games;Trajectory;Bayes methods;Servers;Analytical models;Correlation},
doi={10.1109/INFCOM.2013.6567110},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567111,
author={D. Yang and X. Fang and G. Xue},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Truthful incentive mechanisms for k-anonymity location privacy},
year={2013},
volume={},
number={},
pages={2994-3002},
abstract={Tremendous efforts have been made to protect the location privacy of mobile users. Some of them, e.g., k-anonymity, require the participation of multiple mobile users to impede the adversary from tracing. These participating mobile users constitute an anonymity set. However, not all mobile users are seriously concerned about their location privacy. Therefore, to achieve k-anonymity, we need to provide incentives for mobile users to participate in the anonymity set. In this paper, we study the problem of incentive mechanism design for k-anonymity location privacy. We first consider the case where all mobile users have the same privacy degree requirement. We then study the case where the requirements are different. Finally, we consider a more challenging case where mobile users can cheat about not only their valuations but also their requirements. We design an auction-based incentive mechanism for each of these cases and prove that all the auctions are computational efficient, individually rational, budget-balanced, and truthful. We evaluate the performance of different auctions through extensive simulations.},
keywords={data privacy;game theory;mobile computing;truthful incentive mechanism;K-anonymity location privacy;multiple mobile user;privacy degree requirement;auction based incentive mechanism;budget balancing;Mobile communication;Privacy;Cost accounting;Data privacy;Algorithm design and analysis;Mechanical factors;Sorting},
doi={10.1109/INFCOM.2013.6567111},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567112,
author={X. Zhao and L. Li and G. Xue},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Checking in without worries: Location privacy in location based social networks},
year={2013},
volume={},
number={},
pages={3003-3011},
abstract={In current location based social networks (LBSNs), users expose their location when they check in at a venue or search a place. The release of location privacy could lead to a severe breach of other privacy, such as identity or health condition. In this paper, we propose a framework to safeguard users' location information as well as the check-in records. Considering the special demands in LBSNs, we design a novel index structure to provide a fast search for users when they check in at the same venue frequently. At the same time, our framework outsources the heavy cryptographic computations to the server to reduce the computational overhead for mobile clients. Due to the dynamic feature of LBSNs, our framework uses a lightweight approach to handle a user's revoked friends and new friends. We prove the security of our framework in the random oracle model and demonstrate its efficiency on a Motorola Droid phone.},
keywords={cryptography;data privacy;mobile radio;social networking (online);location privacy;location based social networks;LBSN;user location information;check-in records;index structure design;heavy cryptographic computations;mobile clients;computational overhead;random oracle model;Motorola Droid phone;Indexes;Servers;Encryption;Privacy;Protocols},
doi={10.1109/INFCOM.2013.6567112},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567113,
author={M. Li and S. Salinas and A. Thapa and P. Li},
booktitle={2013 Proceedings IEEE INFOCOM},
title={n-CD: A geometric approach to preserving location privacy in location-based services},
year={2013},
volume={},
number={},
pages={3012-3020},
abstract={With great advances in mobile devices, e.g., smart phones and tablets, location-based services (LBSs) have recently emerged as a very popular application in mobile networks. However, since LBS service providers require users to report their location information, how to preserve users' location privacy is one of the most challenging problems in LBSs. Most existing approaches either cannot fully protect users' location privacy, or cannot provide accurate LBSs. Many of them also need the help of a trusted third-party, which may not always be available. In this paper, we propose a geometric approach, called n-CD, to provide realtime accurate LBSs while preserving users' location privacy without involving any third-party. Specifically, we first divide a user's region of interest (ROI), which is a disk centered at the user's location, into n equal sectors. Then, we generate n concealing disks (CDs), one for each sector, one by one to collaboratively and fully cover each of the n sectors. We call the area covered by the n CDs the concealing space, which fully contains the user's ROI. After rotating the concealing space with respect to the user's location, we send the rotated centers of the n CDs along with their radii to the service provider, instead of the user's real location and his/her ROI. To investigate the performance of n-CD, we theoretically analyze its privacy level and concealing cost. Extensive simulations are finally conducted to evaluate the efficacy and efficiency of the proposed schemes.},
keywords={data privacy;geometry;mobile computing;n-CD;geometric approach;preserving location privacy;location based services;mobile devices;smart phones;tablets;mobile networks;LBS service providers;location information;region of interest;ROI;equal sectors;concealing disks;concealing space;Privacy;Servers;Mobile radio mobility management;Engines;Accuracy;Security},
doi={10.1109/INFCOM.2013.6567113},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567114,
author={R. Shi and M. Goswami and J. Gao and X. Gu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Is random walk truly memoryless — Traffic analysis and source location privacy under random walks},
year={2013},
volume={},
number={},
pages={3021-3029},
abstract={Random walk on a graph is a Markov chain and thus is `memoryless' as the next node to visit depends only on the current node and not on the sequence of events that preceded it. With these properties, random walk and its many variations have been used in network routing to `randomize' the traffic pattern and hide the location of the data sources. In this paper we examine a myth in common understanding of the memoryless property of a random walk applied for protecting source location privacy in a wireless sensor network. In particular, if one monitors only the network boundary and records the first boundary node hit by a random walk, this distribution can be related to the location of the source node. For the scenario of a single data source, a very simple algorithm by integrating along the network boundary would reveal the location of the source. We also develop a generic algorithm to reconstruct the source locations for various sources that have simple descriptions (e.g., k source locations, sources on a line segment, sources in a disk). This represents a new type of traffic analysis attack for invading sensor data location privacy and essentially re-opens the problem for further examination.},
keywords={graph theory;Markov processes;memoryless systems;telecommunication network routing;telecommunication security;telecommunication traffic;wireless sensor networks;random walk;traffic analysis;source location privacy;graph;Markov chain;network routing;traffic pattern;data sources;memoryless property;wireless sensor network;sensor data location privacy;Harmonic analysis;Position measurement;Monitoring;Routing;Privacy;Brownian motion;Wireless sensor networks},
doi={10.1109/INFCOM.2013.6567114},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567115,
author={X. Hei and X. Du and S. Lin and I. Lee},
booktitle={2013 Proceedings IEEE INFOCOM},
title={PIPAC: Patient infusion pattern based access control scheme for wireless insulin pump system},
year={2013},
volume={},
number={},
pages={3030-3038},
abstract={Wireless insulin pumps have been widely deployed in hospitals and home healthcare systems. Most of these insulin pump systems have limited security mechanisms embedded to protect them from malicious attacks. In this paper, two attacks against insulin pump systems via wireless links are investigated: a single acute overdose with a significant amount of medication, and chronic overdose with an insignificant amount of extra medication over a long time period, e.g., several months. These attacks can be launched unobtrusively and may jeopardize patients' lives. It is very important and urgent to protect patients from these attacks. To address this issue, we propose a novel patient infusion pattern based access control scheme (PIPAC) for wireless insulin pumps. This scheme employs a supervised learning approach to learn normal patient infusions pattern with the dosage amount, rate, and time of infusion, which are automatically recorded in insulin pump logs. The generated regression models are used to dynamically configure a safety infusion range for abnormal infusion identification. The proposed algorithm is evaluated with real insulin pump logs used by several patients for up to 6 months. The evaluation results demonstrate that our scheme can reliably detect the single overdose attack with a success rate up to 98% and defend against the chronic overdose attack with a very high success rate.},
keywords={access control;biomedical equipment;health care;learning (artificial intelligence);patient care;radio links;regression analysis;PIPAC;patient infusion pattern based access control scheme;wireless insulin pump system;hospital;healthcare system;wireless link;medication;chronic overdose;patient infusion pattern;insulin pump log;regression model;safety infusion range;abnormal infusion identification;Insulin;Wireless communication;Wireless sensor networks;Communication system security;Safety;Diabetes;Access control;wireless insulin pump;implantable medical devices;access control;infusion pattern;patient safety},
doi={10.1109/INFCOM.2013.6567115},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567116,
author={K. Xing and Z. Wan and P. Hu and H. Zhu and Y. Wang and X. Chen and Y. Wang and L. Huang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Mutual privacy-preserving regression modeling in participatory sensing},
year={2013},
volume={},
number={},
pages={3039-3047},
abstract={As the advancement of sensing and networking technologies, participatory sensing has raised more and more attention as it provides a promising way enabling public and professional users to gather and analyze private data to understand the world. However, in these participatory sensing applications both data at the individuals and analysis results obtained at the users are usually private and sensitive to be disclosed, e.g., locations, salaries, utility usage, consumptions, behaviors, etc. A natural question, also an important but challenging problem is how to keep both participants and users data privacy while still producing the best analysis to explain a phenomenon. In this paper, we have addressed this issue and proposed M-PERM, a mutual privacy preserving regression modeling approach. Particularly, we launch a series of data transformation and aggregation operations at the participatory nodes, the clusters, and the user. During regression model fitting, we provide a new way for model fitting without any need of the original private data or the exact knowledge of the model expression. To evaluate our approach, we conduct both theoretical analysis and simulation study. The evaluation results show that the proposed approach produces exactly the same best model as if the original private data were used without leakage of the fitted model to any participatory nodes, which is a significant advance compared with the existing approaches [1-5]. It is also shown that the data gathering design is able to reach maximum privacy protection under certain conditions and be robust against collusion attack. Furthermore, compared with existing works under the same context (e.g., [1-5]), to our best knowledge it is the first work showing that not only the model coefficients estimation but also a series of regression analysis and model selection methods are reachable in mutual privacy preserving data analysis scenarios such as participatory sensing.},
keywords={data acquisition;data analysis;data privacy;regression analysis;wireless sensor networks;mutual privacy preserving regression model fitting;participatory sensing;networking technology;users data privacy protection;M-PERM;data transformation series;aggregation operation;participatory node;data cluster;data gathering design;collusion attack;model selection method;mutual privacy preserving data analysis;Data models;Sensors;Analytical models;Data privacy;Computational modeling;Privacy;Regression analysis},
doi={10.1109/INFCOM.2013.6567116},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567117,
author={H. Liu and Y. Wang and J. Yang and Y. Chen},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Fast and practical secret key extraction by exploiting channel response},
year={2013},
volume={},
number={},
pages={3048-3056},
abstract={Securing wireless communication remains challenging in dynamic mobile environments due to the shared nature of wireless medium and lacking of fixed key management infrastructures. Generating secret keys using physical layer information thus has drawn much attention to complement traditional cryptographic-based methods. Although recent work has demonstrated that Received Signal Strength (RSS) based secret key extraction is practical, existing RSS-based key generation techniques are largely limited in the rate they generate secret bits and are mainly applicable to mobile wireless networks. In this paper, we show that exploiting the channel response from multiple Orthogonal Frequency-Division Multiplexing (OFDM) subcarriers can provide fine-grained channel information and achieve higher bit generation rate for both static and mobile cases in real-world scenarios. We further develop a Channel Gain Complement (CGC) assisted secret key extraction scheme to cope with channel non-reciprocity encountered in practice. Our extensive experiments using WiFi networks in both indoor as well as outdoor environments demonstrate that our approach can achieve significantly faster secret bit generation rate at 60 ~ 90bit/packet, and is resilient to malicious attacks identified to be harmful to RSS-based techniques including predictable channel attack and stalking attack.},
keywords={cryptography;mobile communication;radiocommunication;telecommunication security;channel response;wireless communication;dynamic mobile environment;wireless medium;fixed key management infrastructure;physical layer information;complement traditional cryptographic based method;received signal strength;RSS based secret key extraction;RSS based key generation;secret bits;mobile wireless networks;multiple orthogonal frequency division multiplexing;OFDM subcarriers;fine grained channel information;channel gain complement assisted secret key extraction;channel nonreciprocity;WiFi networks;secret bit generation rate;malicious attack;predictable channel attack;stalking attack;Wireless communication;OFDM;Quantization (signal);Wireless sensor networks;Probes;Data mining;Time measurement},
doi={10.1109/INFCOM.2013.6567117},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567118,
author={Z. Zhou and Z. Yang and C. Wu and L. Shangguan and Y. Liu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Towards omnidirectional passive human detection},
year={2013},
volume={},
number={},
pages={3057-3065},
abstract={Passive human detection and localization serve as key enablers for various pervasive applications such as smart space, human-computer interaction and asset security. The primary concern in devising scenario-tailored detecting systems is the coverage of their monitoring units. In conventional radio-based schemes, the basic unit tends to demonstrate a directional coverage, even if the underlying devices are all equipped with omnidirectional antennas. Such an inconsistency stems from the link-centric architecture, creating an anisotropic wireless propagating environment. To achieve an omnidirectional coverage while retaining the link-centric architecture, we propose the concept of Omnidirectional Passive Human Detection, and investigate to harness the PHY layer features to virtually tune the shape of the unit coverage by fingerprinting approaches, which is previously prohibited with mere MAC layer RSSI. We design the scheme with ubiquitously deployed WiFi infrastructure and evaluate it in typical multipath-rich indoor scenarios. Experimental results show that our scheme achieves an average false positive of 8% and an average false negative of 7% in detecting human presence in 4 directions.},
keywords={human computer interaction;omnidirectional antennas;passive human detection;omnidirectional antennas;smart space;human-computer interaction;asset security;scenario-tailored detecting systems;monitoring units;directional coverage;link-centric architecture;Computer architecture;Histograms;Feature extraction;Monitoring;Microprocessors;Azimuth;Wireless communication},
doi={10.1109/INFCOM.2013.6567118},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567119,
author={L. Shangguan and Z. Li and Z. Yang and M. Li and Y. Liu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={OTrack: Order tracking for luggage in mobile RFID systems},
year={2013},
volume={},
number={},
pages={3066-3074},
abstract={In many logistics applications of RFID technology, goods attached with tags are placed on moving conveyor belts for processing. It is important to figure out the order of goods on the belts so that further actions like sorting can be accurately taken on proper goods. Due to arbitrary goods placement or the irregularity of wireless signal propagation, neither of the order of tag identification nor the received signal strength provides sufficient evidence on their relative positions on the belts. In this study, we observe, from experiments, a critical region of reading rate when a tag gets close enough to a reader. This phenomenon, as well as other signal attributes, yields the stable indication of tag order. We establish a probabilistic model for recognizing the transient critical region and propose the OTrack protocol to continuously monitor the order of tags. To validate the protocol, we evaluate the accuracy and effectiveness through a one-month experiment conducted through a working conveyor at Beijing Capital International Airport.},
keywords={mobile radio;probability;radiofrequency identification;radiowave propagation;order tracking;luggage;mobile RFID systems;RFID technology;logistics applications;conveyor belts;wireless signal propagation;tag identification;received signal strength;signal attributes;stable indication;tag order;transient critical region;OTrack protocol;Beijing Capital International Airport;Belts;Market research;Protocols;Radiofrequency identification;Accuracy;Airports;Correlation},
doi={10.1109/INFCOM.2013.6567119},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567120,
author={W. Cheng and K. Tan and V. Omwando and J. Zhu and P. Mohapatra},
booktitle={2013 Proceedings IEEE INFOCOM},
title={RSS-Ratio for enhancing performance of RSS-based applications},
year={2013},
volume={},
number={},
pages={3075-3083},
abstract={RSS (Received Signal Strength) has been widely utilized in wireless applications. It is, however, susceptible to environmental unknowns from both temporal and spatial domains. As a result, the fluctuation of RSS may degrade performance of RSS based applications. In this work, we propose a novel RSS processing method at the receiver for three antenna based systems. The output of our approach is `RSS-Ratio', which eliminates the environmental unknowns and thus is a more stable variable compared to RSS itself. To validate the efficacy of the proposed method, we conduct a series of experiments in a range of wireless scenarios, including indoor laptop based measurement, indoor software defined radio - WARP based measurement, and outdoor wireless measurement. In addition, we also give an analysis to the relationship between the location of transmitter and the value of RSS-Ratio, and examine the accuracy of the estimated RSS-Ratio value via both simulations and experiments. All the experimental, analytical, and simulated results demonstrate that RSS-Ratio will be a better replacement for RSS to improve the performance of RSS based applications.},
keywords={indoor radio;receiving antennas;software radio;received signal strength;RSS-based applications;performance enhancement;wireless applications;RSS processing method;antenna based systems;receiver antenna;indoor laptop based measurement;indoor software defined radio;WARP based measurement;outdoor wireless measurement;transmitter;estimated RSS-Ratio value;Decision support systems},
doi={10.1109/INFCOM.2013.6567120},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567121,
author={S. Cheng and J. Li and Z. Cai},
booktitle={2013 Proceedings IEEE INFOCOM},
title={O(ε)-Approximation to physical world by sensor networks},
year={2013},
volume={},
number={},
pages={3084-3092},
abstract={To observe the complicate physical world by a WSN, the sensors in the WSN senses and samples the data from the physical world. Currently, most of the existing work use equi-frequency sampling methods (EFS) or EFS based sampling methods for data acquisition in sensor networks. However, the accuracies of EFS and EFS based sampling methods cannot be guaranteed in practice since the physical world usually varies continuously, and these methods does not support reconstructing of the monitored physical world. To overcome the shortages of EFS and EFS based sampling methods, this paper focuses on designing physical-world-aware data acquisition algorithms to support O(ϵ)-approximation to the physical world for any ϵ ≥ 0. Two physical-world-aware data acquisition algorithms based on Hermit and Spline interpolation are proposed in the paper. Both algorithms can adjust the sensing frequency automatically based on the changing trend of the physical world and given c. The thorough analysis on the performance of the algorithms are also provided, including the accuracies, the smooth of the outputted curves, the error bounds for computing first and second derivatives, the number of the sampling times and complexities of the algorithms. It is proven that the error bounds of the algorithms are O(ϵ) and the complexities of the algorithms are O(1/ϵ1/4). Based on the new data acquisition algorithms, an algorithm for reconstructing physical world is also proposed and analyzed. The theoretical analysis and experimental results show that all the proposed algorithms have high performance in items of accuracy and energy consumption.},
keywords={approximation theory;data acquisition;interpolation;splines (mathematics);wireless sensor networks;wireless sensor networks;WSN;equi-frequency sampling methods;EFS based sampling methods;physical-world-aware data acquisition algorithms;Hermit interpolation;spline interpolation;error bounds;Interpolation;Algorithm design and analysis;Data acquisition;Wireless sensor networks;Monitoring;Accuracy;Splines (mathematics)},
doi={10.1109/INFCOM.2013.6567121},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567122,
author={X. Zhang and K. G. Shin},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Gap Sense: Lightweight coordination of heterogeneous wireless devices},
year={2013},
volume={},
number={},
pages={3094-3101},
abstract={Coordination of co-located wireless devices is a fundamental function/requirement for reducing interference. However, different devices cannot directly coordinate with one another as they often use incompatible modulation schemes. Even for the same type (e.g., WiFi) of devices, their coordination is infeasible when neighboring transmitters adopt different spectrum widths. Such an incompatibility between heterogeneous devices may severely degrade the network performance. In this paper, we introduce Gap Sense (GSense), a novel mechanism that can coordinate heterogeneous devices without modifying their PHYlayer modulation schemes or spectrum widths. GSense prepends legacy packets with a customized preamble, which piggy-backs information to enhance inter-device coordination. The preamble leverages the quiet period between signal pulses to convey such information, and can be detected by neighboring nodes even when they have incompatible PHY layers. We have implemented and evaluated GSense on a software radio platform, demonstrating its significance and utility in three popular protocols. GSense is shown to deliver coordination information with close to 100% accuracy within practical SNR regions. It can also reduce the energy consumption by around 44%, and the collision rate by more than 88% in networks of heterogeneous transmitters and receivers.},
keywords={mobile radio;modulation;protocols;radio receivers;radio transmitters;radiofrequency interference;software radio;gap sense;heterogeneous wireless devices;colocated wireless device coordination;interference reduction;incompatible modulation schemes;neighboring transmitters;spectrum widths;WiFi;network performance degradation;GSense;legacy packets;piggy-backs information;interdevice coordination enhancement;signal pulses;neighboring nodes;software radio platform;energy consumption reduction;SNR regions;collision rate;heterogeneous transmitters;heterogeneous receivers;mobile networks;wireless networks;PHY-layer modulation schemes;Receivers;Signal to noise ratio;IEEE 802.11 Standards;Zigbee;Radio transmitters;Protocols;Clocks},
doi={10.1109/INFCOM.2013.6567122},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567123,
author={J. Manweiler and N. Santhapuri and R. R. Choudhury and S. Nelakuditi},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Predicting length of stay at WiFi hotspots},
year={2013},
volume={},
number={},
pages={3102-3110},
abstract={Today's smartphones provide a variety of sensors, enabling high-resolution measurements of user behavior. We envision that many services can benefit from short-term predictions of complex human behavioral patterns. While enablement of behavior awareness through sensing is a broad research theme, one possibility is in predicting how quickly a person will move through a space. Such a prediction service could have numerous applications. For one example, we imagine shop owners predicting how long a particular customer is likely to browse merchandise, and issue targeted mobile coupons accordingly - customers in a hurry can be encouraged to stay and consider discounts. Within a space of moderate size, WiFi access points are uniquely positioned to track a statistical framework for user length of stay, passively recording metrics such as WiFI signal strength (RSSI) and potentially receiving client-uploaded sensor data. In this work, we attempt to quantity this opportunity, and show that human dwell time can be predicted with reasonable accuracy, even when restricted to passively observed WiFi RSSI.},
keywords={smart phones;wireless LAN;WiFi hotspots;smartphones;sensors;high-resolution measurements;access points;RSSI;client-uploaded sensor data;IEEE 802.11 Standards;Accuracy;Support vector machines;Compass;Sensor phenomena and characterization;Feature extraction},
doi={10.1109/INFCOM.2013.6567123},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567124,
author={R. Zheng and T. Le and Z. Han},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Approximate online learning for passive monitoring of multi-channel wireless networks},
year={2013},
volume={},
number={},
pages={3111-3119},
abstract={We consider the problem of optimally assigning p sniffers to K channels to monitor the transmission activities in a multi-channel wireless network. The activity of users is initially unknown to the sniffers and is to be learned along with channel assignment decisions. Previously proposed online learning algorithms face high computational costs due to the NPhardness of the decision problem. In this paper, we propose two approximate online learning algorithms, ϵ-GREEDY-APPROX and EXP3-APPROX, which are shown to have better scalability, and achieve sub-linear regret bounds over time compared to a greedy offline algorithm with complete information. We demonstrate both analytically and empirically the trade-offs between the computation cost and rate of learning.},
keywords={computational complexity;radio networks;approximate online learning;passive monitoring;multichannel wireless networks;K channels;transmission activities;face high computational costs;NP hardness;decision problem;online learning algorithms;sublinear regret bounds;greedy offline algorithm;computation cost;Monitoring;Approximation algorithms;Approximation methods;Complexity theory;Greedy algorithms;Joints;Wireless networks},
doi={10.1109/INFCOM.2013.6567124},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567125,
author={W. Cheng and X. Zhang and H. Zhang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Optimal dynamic power control for full-duplex bidirectional-channel based wireless networks},
year={2013},
volume={},
number={},
pages={3120-3128},
abstract={We consider the full-duplex transmission over bidirectional channels with imperfect self-interference cancelation in wireless networks. In particular, together using propagation-domain interference suppression, analog-domain interference cancellation, and digital-domain interference cancellation, we develop the optimal dynamic power allocation schemes for the wireless full-duplex sum-rate optimization problem which aims at maximizing the sum-rate of wireless full-duplex bidirectional transmissions. In the high signal-to-interference-plus-noise ratio (SINR) region, the full-duplex sum-rate maximization problem is a convex optimization problem. For interference-dominated wireless full-duplex transmission in the high SINR region, we derive the closed-form expression for the optimal dynamic power allocation scheme. For non-interference-dominated wireless full-duplex transmission in the high SINR region, we obtain the optimal dynamic power allocation scheme by numerically solving the corresponding Karush-Kuhn-Tucker (KKT) conditions. While the full-duplex sum-rate maximization problem is usually not a convex optimization problem, by developing the tightest lower-bound function and using the logarithmic change of variables technique, we convert the full-duplex sum-rate maximization problem to a convex optimization problem. Then, using our proposed iteration algorithm, we can numerically derive the optimal dynamic power allocation scheme for the more generic scenario. Also presented are the numerical results which validate our developed optimal dynamic power allocation schemes.},
keywords={convex programming;interference suppression;optimal control;power control;radio networks;telecommunication control;wireless channels;optimal dynamic power control;full-duplex bidirectional channel;imperfect self-interference cancelation;wireless networks;propagation-domain interference suppression;analog-domain interference cancellation;digital-domain interference cancellation;optimal dynamic power allocation scheme;wireless full-duplex sum-rate optimization problem;wireless full-duplex bidirectional transmission;signal-to-interference-plus-noise ratio;SINR region;full-duplex sum-rate maximization problem;convex optimization problem;interference-dominated wireless full-duplex transmission;closed-form expression;noninterference-dominated wireless full-duplex transmission;Karush-Kuhn-Tucker conditions;KKT conditions;lower bound function;iteration algorithm;Wireless communication;Interference;Resource management;Dynamic scheduling;Lead;Transmitters;Signal to noise ratio;Full-duplex;bidirectional transmission;power control;self-interference;sum-rate},
doi={10.1109/INFCOM.2013.6567125},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567126,
author={G. S. Thakur and P. Hui and A. Helmy},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Modeling and characterization of vehicular density at scale},
year={2013},
volume={},
number={},
pages={3129-3134},
abstract={Future vehicular networks shall enable new classes of services and applications for car-to-car and car-to-roadside communication. The underlying vehicular mobility patterns significantly impact the operation and effectiveness of these services, and hence it is essential to model and characterize such patterns. In this paper, we examine the mobility of vehicles as a function of traffic density of more than 800 locations from six major metropolitan regions around the world. The traffic densities are generated from more than 25 million images and processed using background subtraction algorithm. The resulting vehicular density time series and distributions are then analyzed. It is found using the goodness-of-fit test that the vehicular density distribution follows heavy-tail distributions such as Log-gamma, Log-logistic, and Weibull in over 90% of these locations. Moreover, a heavy-tail gives rise to long-range dependence and self-similarity, which we studied by estimating the Hurst exponent (H). Our analysis based on seven different Hurst estimators signifies that the traffic patterns are stochastically self-similar (0.5 ≤ H ≤ 1.0). We believe this is an important finding, which will influence the design and deployment of the next generation vehicular network and also aid in the development of opportunistic communication services and applications for the vehicles. In addition, it shall provide a much needed input for the development of smart cities.},
keywords={gamma distribution;mobility management (mobile radio);next generation networks;telecommunication traffic;time series;smart city;opportunistic communication service;next generation vehicular network;traffic pattern;Hurst exponent estimation;Weibull distribution;log-logistic distribution;log-gamma distribution;heavy-tail distribution;vehicular density distribution;vehicular density time series;background subtraction algorithm;vehicular mobility pattern;car-to-roadside communication;car-to-car communication;vehicular density modeling;Data models;Cameras;Time series analysis;Vehicles;Kernel;Analytical models;Internet},
doi={10.1109/INFCOM.2013.6567126},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567127,
author={Y. Wang and M. Faloutsos and H. Zang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={On the usage patterns of multimodal communication: Countries and evolution},
year={2013},
volume={},
number={},
pages={3135-3140},
abstract={How do people use phone calls and text messages for their communication needs? Most studies so far study each mode of communication in isolation. Here, we study the interplay of multi-modal communications. We analyze more than a billion call and text records from a Chinese city and San Francisco Area between 2007 and 2011. First, we provide some definitions towards a framework for analyzing multi-modal communications. Then,we study the relationship of the two communication modes and quantify several aspects of correlation and inference. For a communicating pair, we find that the existence of texting during the weekend is the strongest indicator that the pair will communicate at other times with texts or calls. We compare the behavior between China and the U.S. and we find several similarities and differences. For example, we find evidence of an after-lunch siesta among Chinese users. Finally, we study the evolution of the two modes over time. We find that texting has taken over in sheer number of ”events” by flipping the number of calls over that of texts from 2: 1 in 2007 to 1:2 in 2011.},
keywords={cellular radio;electronic messaging;pattern recognition;text messages;phone calls;multimodal communication;usage patterns;Correlation;Internet;Telecommunications;Measurement;Cultural differences;Pricing;Electronic mail},
doi={10.1109/INFCOM.2013.6567127},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567128,
author={M. Varvello and M. Steiner},
booktitle={2013 Proceedings IEEE INFOCOM},
title={DHT-based traffic localization in the wild},
year={2013},
volume={},
number={},
pages={3141-3146},
abstract={BitTorrent is both the dominant Peer-to-Peer (P2P) protocol for file-sharing and a nightmare for ISPs due to its network agnostic nature. Many solutions exist to localize BitTorrent traffic relying on cooperation between ISPs and the trackers. Recently, BitTorrent users have been abandoning the trackers in favor of Distributed Hash Tables (DHTs). Despite DHTs are complex heterogeneous systems, DHT-based traffic localization is also possible; however, it is unclear how it performs. The goal of this work is to measure DHT-based traffic localization in the wild. We run multiple experiments involving up to five commercial ISPs and a maximum duration of one month, collecting about 400 GB of BitTorrent traffic. Then, we perform an extensive analysis with the following goals: understand the impact of system parameters, verify accuracy of the measurements, estimate the localization benefits.},
keywords={cryptography;peer-to-peer computing;protocols;telecommunication traffic;localization benefit estimation;DHT-based traffic localization;heterogeneous system;distributed hash table;BitTorrent traffic localization;network agnostic nature;ISP;file-sharing;P2P protocol;peer-to-peer protocol;Peer-to-peer computing;Protocols;Linux;Internet;Instruments;Accuracy;Radio access networks},
doi={10.1109/INFCOM.2013.6567128},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567129,
author={Y. Yu and D. Wessels and M. Larson and L. Zhang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Check-Repeat: A new method of measuring DNSSEC validating resolvers},
year={2013},
volume={},
number={},
pages={3147-3152},
abstract={As more and more authority DNS servers turn on DNS security extensions (DNSSEC), it becomes increasingly important to understand whether, and how many, DNS resolvers perform DNSSEC validation. In this paper we present a query-based measurement method, called Check-Repeat, to gauge the presence of DNSSEC validating resolvers. Utilizing the fact that most validating resolver implementations retry DNS queries with a different authority server if they receive a bad DNS response, Check-Repeat can identify validating resolvers by removing the signatures from regular DNS responses and observing whether a resolver retries DNS queries. We tested Check-Repeat in different scenarios and our results showed that Check-Repeat can identify validating resolvers with a low error rate. We also cross-checked our measurement results with DNS query logs from .COM and .NET domains, and confirmed that the resolvers measured in our study can account for more than 60% of DNS queries in the Internet.},
keywords={computer network security;Internet;query processing;Internet;.NET domain;.COM domain;DNS query log;DNS response signature;validating resolver identification;authority server;Check-Repeat;query-based measurement method;DNS security extension;authority DNS server;DNSSEC validating resolver measurement;Servers;IP networks;Public key;Probes;Monitoring;Browsers;Conferences},
doi={10.1109/INFCOM.2013.6567129},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567130,
author={A. Berger and W. N. Gansterer},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Modeling DNS agility with DNSMap},
year={2013},
volume={},
number={},
pages={3153-3158},
abstract={More and more Internet services are hosted by Content Distribution Networks or Cloud operators. Often, IP addresses are reused for several services, and the mapping between domain names and IPs has become highly agile. This complicates the analysis of monitoring data, as it is not clear anymore which IP address represents which service at which time. We propose a system that continuously monitors this activity using captured DNS packets in a large network. Thereby we are able to (i) understand the allocation strategies inside a hosting provider, and (ii) report significant changes that are not due the normal agility of a particular service. We evaluate our system using a 2-weeks data set from a large network operator, and demonstrate how it can be used to find malicious sites.},
keywords={cloud computing;security of data;Web sites;malicious sites;network operator;DNS packets;IP addresses;cloud operators;content distribution networks;Internet services;DNSMap;DNS agility modeling;IP networks;Monitoring;Conferences;Facebook;Merging;Clustering algorithms;Quality of service},
doi={10.1109/INFCOM.2013.6567130},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567131,
author={A. Bär and A. Paciello and P. Romirer-Maierhofer},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Trapping botnets by DNS failure graphs: Validation, extension and application to a 3G network},
year={2013},
volume={},
number={},
pages={3159-3164},
abstract={In the last years, botnets have become one of the major sources of cyber-crime activities carried out via the public Internet. Typically, they may serve a number of different malicious activities such as Distributed Denial of Service (DDoS) attacks, email spam and phishing attacks. In this paper we validate the Domain Name System (DNS) failure graph approach presented earlier in [1]. In our work we apply this approach in an operational 3G mobile network serving a significantly larger user population.Based on the introduction of stable host identifiers we implement a novel approach to the tracking of botnets over a period of several weeks. Our results reveal the presence of several groups of hosts that are members of botnets. We analyze the host groups exhibiting the most suspicious behavior and elaborate on how these participate in botnets and other malicious activities. In the last part of this work we discuss how the accuracy of our detection approach could be improved in the future by correlating the knowledge obtained from applying our method in different networks.},
keywords={3G mobile communication;computer crime;computer network security;graph theory;Internet;botnet trapping;DNS failure graphs;operational 3G mobile network;cyber-crime activities;public Internet;malicious activities;distributed denial of service attacks;DDoS attacks;email spam;phishing attacks;domain name system failure graph approach;host identifiers;botnet tracking;Servers;IP networks;Superluminescent diodes;Monitoring;Electronic mail;Clustering algorithms;Algorithm design and analysis},
doi={10.1109/INFCOM.2013.6567131},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567132,
author={A. Tizghadam and A. Shariat and A. Leon-Garcia and H. Naser},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Robust QoS-guaranteed network engineering in interference-aware wireless networks},
year={2013},
volume={},
number={},
pages={3165-3170},
abstract={Due to the time-varying nature of wireless networks, it is required to find robust optimal methods to control the behavior and performance of such networks; however, this is a challenging task since robustness metrics and QoS-based (Quality of service) constraints in a wireless environment are typically highly non-linear and non-convex. This paper explores the possibility of using graph theoretic metrics to provide robustness in a wireless network at the presence of a set of QoS constraints. In particular, we are interested in robust planning of a wireless network for a given demand matrix while preserving end-to-end delay for input demands below a given threshold set. To this end, we show that the upper bound of end-to-end round trip time between two nodes of a network can be approximated by point-to-point network criticality (or resistance distance) of the network. We construct a convex optimization problem to provide a delay-guaranteed jointly optimal allocation of transmit powers and link flows. We show that the solution provides a robust behavior, i.e. it is insensitive to the environmental changes such as wireless link disruption, this is expected because network criticality is a robustness metric. Our framework can be applied to a wide range of SINR (Signal to Interference plus Noise Ratio) values.},
keywords={convex programming;delays;graph theory;interference;quality of service;radio networks;telecommunication network planning;telecommunication network topology;quality of service;QoS-guaranteed network engineering;interference aware wireless networks;wireless environment;graph theoretic metrics;robust network planning;demand matrix;end-to-end delay;round trip time;point-to-point network criticality;resistance distance;convex optimization problem;joint optimal allocation;link flows;wireless link disruption;signal to interference plus noise ratio;SINR;Robustness;Optimization;Delays;Wireless networks;Interference;Quality of service},
doi={10.1109/INFCOM.2013.6567132},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567133,
author={S. Mainardi and E. Gregori and L. Lenzini},
booktitle={2013 Proceedings IEEE INFOCOM},
title={The twofold nature of autonomous systems: Evidence combining stock market data with topological properties},
year={2013},
volume={},
number={},
pages={3171-3176},
abstract={Autonomous Systems (AS) exist and co-exist in two parallel dimensions. In one dimension they are physical networks, whose interconnections are necessary to ensure global Internet reachabilty. In the other dimension, ASes are large well-known companies competing in the same industry. In this paper we bridge together these dimensions by investigating synchronous cross correlations of stock market data and AS-level topological properties. We find that geographically close companies offering similar services are driven by common economic factors. We also provide evidence on the existence and nature of factors governing AS global as well as local topological properties.},
keywords={correlation methods;economics;Internet;stock markets;topology;economic factors;geographically close companies;AS-level topological properties;synchronous cross correlations;global Internet reachabilty;physical networks;parallel dimensions;stock market data;autonomous systems;Companies;Correlation;Internet;Topology;Stock markets},
doi={10.1109/INFCOM.2013.6567133},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567134,
author={S. N. Akshay Uttama Nambi and T. G. Papaioannou and D. Chakraborty and K. Aberer},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Sustainable energy consumption monitoring in residential settings},
year={2013},
volume={},
number={},
pages={3177-3182},
abstract={The continuous growth of energy needs and the fact that unpredictable energy demand is mostly served by unsustainable (i.e. fossil-fuel) power generators have given rise to the development of Demand Response (DR) mechanisms for flattening energy demand. Building effective DR mechanisms and user awareness on power consumption can significantly benefit from fine-grained monitoring of user consumption at the appliance level. However, installing and maintaining such a monitoring infrastructure in residential settings can be quite expensive. In this paper, we study the problem of fine-grained appliance power-consumption monitoring based on one house-level meter and few plug-level meters. We explore the trade-off between monitoring accuracy and cost, and exhaustively find the minimum subset of plug-level meters that maximize accuracy. As exhaustive search is time- and resource-consuming, we define a heuristic approach that finds the optimal set of plug-level meters without utilizing any other sets of plug-level meters. Based on experiments with real data, we found that few plug-level meters - when appropriately placed - can very accurately disaggregate the total real power consumption of a residential setting and verified the effectiveness of our heuristic approach.},
keywords={demand side management;heuristic programming;power consumption;sustainable development;heuristic approach;plug-level meters;house-level meter;fine-grained appliance;demand response mechanisms;energy demand;energy needs;residential settings;sustainable energy consumption monitoring;Home appliances;Hidden Markov models;Monitoring;Accuracy;Power demand;Energy consumption;Heuristic algorithms;Energy disaggregation;Hidden Markov Models;FHMM;NILM;plug-level meter},
doi={10.1109/INFCOM.2013.6567134},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567135,
author={U. Montanari and A. T. Siwe},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Real time market models and prosumer profiling},
year={2013},
volume={},
number={},
pages={3183-3188},
abstract={Decentralized power management systems will play a key role in reducing greenhouse gas emissions and increasing electricity production through alternative energy sources. In this paper, we focus on power market models in which prosumers interact in a distributed environment during the purchase or sale of electric power. We have chosen to follow the distributed power market model DEZENT. Our contribution is the planning phase of the consumption of prosumers based on the negotiation mechanism of DEZENT. We propose a controller for the planning of the consumption which aims at minimizing the electricity cost achieved at the end of a day. In the paper we discuss the assumptions on which the controller design is based.},
keywords={marketing;multi-agent systems;power engineering computing;power markets;software agents;electricity cost minimization;consumption planning;negotiation mechanism;DEZENT;distributed power market model;electric power sale;electric power purchase;distributed environment;decentralized power management systems;prosumer profiling;real time market model;Electricity;Power markets;Learning (artificial intelligence);Planning;Power generation;Heuristic algorithms;Conferences},
doi={10.1109/INFCOM.2013.6567135},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567136,
author={J. Tadrous and A. Eryilmaz and H. E. Gamal},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Pricing for demand shaping and proactive download in smart data networks},
year={2013},
volume={},
number={},
pages={3189-3194},
abstract={We address the question of optimal proactive service and demand shaping for content distribution in data networks through smart pricing. We develop a proactive download scheme that utilizes the probabilistic predictability of the human demand by proactively serving potential users' future requests during the off-peak times. Thus, it smooths-out the network traffic and minimizes the time average cost of service. Moreover, we incorporate the varying economic responsiveness and demand flexibilities of users into our model to develop a demand shaping mechanism that further improves the gains of proactive downloads. To that end, we propose a model that captures the uncertainty about the users' demand as well as their responsiveness to the pricing employed by the service providers. We propose a joint proactive resource allocation and demand shaping scheme based on nonconvex optimization algorithms, and show that it always leads to strictly better performance over its proactive counterpart without demand shaping.},
keywords={computer networks;concave programming;packet radio networks;pricing;probability;resource allocation;telecommunication services;telecommunication traffic;nonconvex optimization algorithm;joint proactive resource allocation;demand shaping mechanism;demand flexibility;economic responsiveness;time average cost of service minimization;network traffic;proactive potential user future request service;human demand;probabilistic predictability;proactive download scheme;smart pricing;content distribution;optimal proactive service;smart data network;Pricing;Joints;Resource management;Entropy;Wireless communication;Linear programming;Approximation methods},
doi={10.1109/INFCOM.2013.6567136},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567137,
author={Y. R. Chen and R. Jana},
booktitle={2013 Proceedings IEEE INFOCOM},
title={SpeedGate: A smart data pricing testbed based on speed tiers},
year={2013},
volume={},
number={},
pages={3195-3200},
abstract={The explosive growth of cellular traffic and its highly dynamic nature often make it increasingly expensive or even infeasible for a cellular service provider to provision enough cellular resources to support the peak traffic demands. Some service providers have started exploring various economic incentives, including smart data pricing, to manage network congestion. We present SpeedGate, a smart mobile data pricing testbed that allows a service provider to experiment with different dynamic pricing strategies. SpeedGate maintains persistent VPN connections to smartphones as users roam between different wireless networks (3G, 4G/LTE, WiFi). The maximum available bandwidth per user session can be adjusted according to various data pricing strategies. We report preliminary results on two trials with a total of 29 users for assessing their willingness to pay (WTP) for various speed tiers. Preliminary observations suggest the challenges of QoS guarantees through speed tiers in the field, the limited dynamic range of WTP values from individual users for different speed tiers, and potential opportunities for auction-based dynamic pricing.},
keywords={3G mobile communication;4G mobile communication;cellular radio;data communication;Long Term Evolution;pricing;smart phones;telecommunication traffic;wireless LAN;auction-based dynamic pricing;WTP values;dynamic range;QoS;willingness to pay;data pricing strategies;WiFi;4G-LTE;3G;wireless networks;service provider;smart mobile data pricing testbed;SpeedGate;network congestion;economic incentives;smart data pricing;peak traffic demands;cellular service provider;cellular traffic;speed tiers;Pricing;Virtual private networks;Smart phones;Bandwidth;Quality of service;Wireless communication;IEEE 802.11 Standards},
doi={10.1109/INFCOM.2013.6567137},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567138,
author={H. Lee and H. Jang and Y. Yi and J. Cho},
booktitle={2013 Proceedings IEEE INFOCOM},
title={On the interaction between content-oriented traffic scheduling and revenue sharing among providers},
year={2013},
volume={},
number={},
pages={3201-3206},
abstract={The Internet consists of economically selfish players in terms of access/transit connection, content distribution, and users. Such selfish behaviors often lead to techno-economic inefficiencies such as unstable peering and revenue imbalance. Recent research results suggest that cooperation in revenue sharing (thus multi-level ISP settlements) can be a candidate solution for the problem of unfair revenue share. However, it is unclear whether providers are willing to behave cooperatively. In this paper, we study the interaction between how content-oriented traffic scheduling at the edge is and how stable the intended cooperation is. We consider three traffic scheduling policies having various degrees of content-value preference, compare them in terms of implementation complexity, network neutrality, and stability of cooperation, and present interesting trade-offs among them.},
keywords={Internet;scheduling;telecommunication traffic;cooperation stability;network neutrality;implementation complexity;content-value preference;traffic scheduling policies;multilevel ISP settlements;unstable peering;revenue imbalance;techno-economic inefficiencies;content distribution;access-transit connection;Internet;revenue sharing;content-oriented traffic scheduling;Stability analysis;Games;Internet;Conferences;Economics;Sociology;Statistics},
doi={10.1109/INFCOM.2013.6567138},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567139,
author={Y. Song and A. Venkataramani and L. Gao},
booktitle={2013 Proceedings IEEE INFOCOM},
title={On the CDN pricing game},
year={2013},
volume={},
number={},
pages={3207-3212},
abstract={Content Delivery Networks (CDNs) serve a large fraction of Internet traffic today improving user-perceived response time and availability of content. With tens of CDNs competing for content producers, it is important to understand the game played by these CDNs and whether the game is sustainable in the long term. In this paper, we formulate a game-theoretic model to analyze price competition among CDNs. Under this model, we propose an optimal strategy employed by two-CDN games. The strategy is incentive-compatible since any CDN that deviates from the strategy ends up with a lower utility. The strategy is also efficient since it produces a total utility that is at least two thirds of the social optimal utility. We formally derive the sufficient conditions for such a strategy to exist, and empirically show that there exists an optimal strategy for the games with more than two CDNs.},
keywords={game theory;Internet;pricing;content producers;incentive-compatible strategy;sufficient conditions;social optimal utility;two-CDN games;price competition analysis;game-theoretic model;content availability improvement;user-perceived response time improvement;Internet traffic;content delivery networks;CDN pricing game;Games;Markov processes;Equations;Pricing;Internet;Conferences;Servers},
doi={10.1109/INFCOM.2013.6567139},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567140,
author={M. Andrews and U. Özen and M. I. Reiman and Q. Wang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Economic models of sponsored content in wireless networks with uncertain demand},
year={2013},
volume={},
number={},
pages={3213-3218},
abstract={The interaction of a content provider with end users on an infrastructure platform built and maintained by a service provider can be viewed as a two-sided market. Content sponsoring, i.e., charging the content provider instead of viewers for resources consumed in viewing the content, can benefit all parties involved. Without being charged directly or having it counted against their monthly data quotas, end users will view more content, allowing the content provider to generate more advertising revenue, extracted by the service provider to subsidize its investment and operation of the network infrastructure. However, realizing such gains requires a proper contractual relationship between the service provider and content provider. We consider the determination of this contract through a Stackelberg game. The service provider sets a pricing schedule for sponsoring and the content provider responds by deciding how much content to sponsor. We analyze the best strategies for the content provider and service provider in the event that the underlying demand for the content is uncertain. Two separate settings are defined. In the first, end users can be charged for non-sponsored views on a per-byte basis. In the second we extend the model to the more common case in which end users purchase data quotas on a periodic basis. Our main conclusion is that a coordinating contract can be designed that maximizes total system profit. Moreover, the additional profit due to sponsoring can be split between the content provider and service provider in an arbitrary manner.},
keywords={contracts;game theory;investment;pricing;telecommunication industry;total system profit;data quotas;pricing schedule;Stackelberg game;contractual relationship;advertising revenue;content sponsoring;service provider;content provider;wireless networks;economic models;Contracts;Bandwidth;Advertising;Random variables;Conferences;Pricing;Standards},
doi={10.1109/INFCOM.2013.6567140},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567141,
author={S. Sun and M. Dong and B. Liang},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Real-time welfare-maximizing regulation allocation in aggregator-EVs systems},
year={2013},
volume={},
number={},
pages={3219-3224},
abstract={The concept of vehicle-to-grid (V2G) has gained recent interest as more and more electric vehicles (EVs) are put to use. In this paper, we consider a dynamic aggregator-EVs system, where an aggregator centrally coordinates a large number of EVs to perform regulation service. We propose a Welfare-Maximizing Regulation Allocation (WMRA) algorithm for the aggregator to fairly allocate the regulation amount among the EVs. The algorithm operates in real time and does not require any prior knowledge on the statistical information of the system. Compared with previous works, WMRA accommodates a wide spectrum of vital system characteristics, including limited EV battery size, EV self charging/discharging, EV battery degradation cost, and the cost of using external energy sources. Furthermore, our simulation results indicate that WMRA can substantially outperform a suboptimal greedy algorithm.},
keywords={battery powered vehicles;greedy algorithms;statistical analysis;suboptimal greedy algorithm;EV battery degradation cost;EV self charging-discharging;vital system characteristics;statistical information;WMRA algorithm;welfare-maximizing regulation allocation algorithm;dynamic aggregator-EV system;electric vehicles;V2G concept;vehicle-to-grid concept;aggregator-EV systems;real-time welfare-maximizing regulation allocation;Batteries;Resource management;Degradation;Energy states;Real-time systems;Power grids;Conferences},
doi={10.1109/INFCOM.2013.6567141},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567142,
author={T. Zhang and W. Chen and Z. Han and Z. Cao},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A cross-layer perspective on energy harvesting aided green communications over fading channels},
year={2013},
volume={},
number={},
pages={3235-3230},
abstract={In this paper, we consider the power allocation of the physical layer and the buffer delay of the upper application layer in energy harvesting green networks. We analyze the delay-optimal power allocation problem over fading channels. The total power required for reliable transmission includes the transmission power as well as the circuit power. The harvested power (which is stored in a battery) and the grid power constitute the power resource. The objective is to find a policy to minimize the buffer delay under the constraint on the average grid power. The policy is a two-dimensional vector with the transmission rate and the power allocation of the battery as its elements. In each transmission, the transmitter decides the transmission rate and the allocated power from the battery (the rest of the required power will be supplied by the power grid). A constrained Markov decision process (MDP) problem is formulated when the data arrival process, the harvested energy arrival process, and the channel process are Markov processes. We prove that the optimal policy can be obtained as follows. First, we solve the optimal rate through a reduced MDP problem that is only related to the average harvested energy but not the harvested energy arrival process. Second, the battery's power allocation can be given based on the optimal rate. By analyzing the reduced MDP problem through the transformations to the average cost MDP and discount optimal MDP, we derive some structural properties of the optimal policy. Moreover, the closed-form expression is obtained for the independent and identically distributed (i.i.d.) cases.},
keywords={environmental factors;fading channels;Markov processes;telecommunication power supplies;vectors;MDP;Markov decision process;two-dimensional vector;delay-optimal power allocation;green networks;buffer delay;physical layer;fading channels;green communications;energy harvesting;cross-layer perspective;Batteries;Resource management;Delays;Energy harvesting;Transmitters;Markov processes;Green products},
doi={10.1109/INFCOM.2013.6567142},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567143,
author={G. Betti and E. Amaldi and A. Capone and G. Ercolani},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Cost-aware optimization models for communication networks with renewable energy sources},
year={2013},
volume={},
number={},
pages={3231-3236},
abstract={We address a traffic engineering problem where, given a communication network and a set of origin-destination demands, we have to select a single-path routing for each demand and decide which communication interfaces to switch off or run at partial load so as to minimize the total operational costs. We account for the presence of renewable energy plants at some nodes of the network, as well as feed-in-tariffs, rebates and variable energy prices. We also consider the related problem of deciding where renewable energy sources (photovoltaic modules in this case) have to be installed so as to maximize the profit, while respecting a maximum investment budget constraint. We propose mixed integer optimization models for these two problems and we report results for two different network topologies.},
keywords={computer networks;integer programming;photovoltaic power systems;telecommunication network routing;telecommunication network topology;telecommunication power management;telecommunication power supplies;network topology;mixed integer optimization model;maximum investment budget constraint;photovoltaic module;renewable energy plant;single path routing;origin-destination demand;traffic engineering problem;renewable energy source;communication network;cost aware optimization model;Routing;Switches;Optimization;Electricity;Photovoltaic systems;Power demand},
doi={10.1109/INFCOM.2013.6567143},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567144,
author={P. Marchetta and A. Pescapé},
booktitle={2013 Proceedings IEEE INFOCOM},
title={DRAGO: Detecting, quantifying and locating hidden routers in Traceroute IP paths},
year={2013},
volume={},
number={},
pages={3237-3242},
abstract={Traceroute is probably the most famous networking tool widely adopted in both industry and research. Despite its long life, however, measurements based on Traceroute are potentially inaccurate, misleading or incomplete due to several unresolved issues. In this paper, we face the limitation represented by hidden routers-devices that do not decrement the TTL, being thus totally invisible to Traceroute. We present, evaluate and release DRAGO, a novel active probing technique composed of three main steps. First, a novel Traceroute enhanced by the IP Timestamp option is launched toward a destination. Second, a procedure is applied to quantify the hidden routers contained in the path, if any. Third, a last procedure is performed to identify the exact position in the path of the detected hidden routers. Experimental results demonstrate that the phenomenon is not uncommon: DRAGO detects the presence of hidden routers in at least 6% of the considered Traceroute IP paths and limits the affected area to one fifth of the trace containing these devices.},
keywords={computer network performance evaluation;IP networks;system monitoring;time-to-live field;hidden router location;hidden router quantification;IP Timestamp option;active probing technique;TTL;Traceroute;hidden router detection;networking tool;DRAGO;IP networks;Internet;Probes;Binary trees;Uncertainty;Payloads;Vectors},
doi={10.1109/INFCOM.2013.6567144},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567145,
author={A. Lutu and M. Bagnulo and O. Maennel},
booktitle={2013 Proceedings IEEE INFOCOM},
title={The BGP visibility scanner},
year={2013},
volume={},
number={},
pages={3243-3248},
abstract={By tweaking the BGP configurations, the network operators are able to express their interdomain routing preferences, designed to accommodate a myriad goals. Given the complex interactions between policies in the Internet, the origin AS by itself cannot ensure that only by configuring a routing policy it can also achieve the anticipated results. Moreover, the definition of routing policies is a complicated process, involving a number of subtle tuning operations prone to errors. In this paper, we propose the BGP Visibility Scanner which allows network operators to validate the correct implementation of their routing policies, by corroborating the BGP routing information from approximatively 130 independent observation points in the Internet. We exemplify the use of the proposed methodology and also perform an initial validation for the BGP Visibility Scanner capabilities through various real operational use cases.},
keywords={Internet;internetworking;protocols;telecommunication network routing;Border Gateway Protocol;autonomous system;BGP routing information;routing policy;AS;Internet;interdomain routing preferences;BGP configurations;BGP visibility scanner;Routing;Monitoring;Feeds;Internet;Topology;Labeling;Educational institutions},
doi={10.1109/INFCOM.2013.6567145},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567146,
author={V. Giotsas and S. Zhou},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Improving the discovery of IXP peering links through passive BGP measurements},
year={2013},
volume={},
number={},
pages={3249-3254},
abstract={The Internet Autonomous System (AS) topology has important implications on end-to-end routing, network economics and security. Despite the significance of the AS topology research, it has not been possible to collect a complete map of the AS interconnections due to the difficulties involved in discovering peering links. The problem of topology incompleteness is amplified by the increasing popularity of Internet eXchange Points (IXPs) and the “flattening” AS hierarchy. A recent study discovered that the number of missing peering links at a single IXP is larger than the total number of the observable peering links. As a result a large body of research focuses on measurement techniques that can alleviate the incompleteness problem. Most of these proposals require the deployment of additional BGP vantage points and traceroute monitors. In this paper we propose a new measurement methodology for improving the discovery of missing peering links through the publicly available BGP data. Our approach utilizes the traffic engineering BGP Communities used by IXPs' Route Servers to implement multi-lateral peering agreements. We are able to discover 36K additional p2p links from 11 large IXPs. The discovered links are not only invisible in previous BGP-based AS topology collections, but also 97% of those links are invisible to traceroute data from CAIDA's Ark and DIMES projects for June 2012. The advantages of the proposed technique are threefold. First, it provides a new source of previously invisible p2p links. Second, it does not require changes in the existing measurement infrastructure. Finally, it offers a new source of policy data regarding multilateral peering links at IXPs.},
keywords={computer network security;Internet;internetworking;peer-to-peer computing;protocols;telecommunication links;telecommunication network routing;telecommunication network topology;telecommunication traffic;passive BGP measurement;IXP peering link discovery;Internet autonomous system topology;end-to-end routing;network economics;network security;Internet exchange points;flattening AS hierarchy;publicly available BGP data;traffic engineering BGP Communities;IXP route servers;multilateral peering agreements;P2P links;BGP-based AS topology collections;CAIDA's Ark;DIMES projects;policy data;multilateral peering links;border gateway protocol;Servers;Communities;Topology;Routing;Internet;Monitoring;Network topology;BGP;Internet;Autonomous Systems;BGP;measurement;inter-domain;routing;IXP;topology;missing links},
doi={10.1109/INFCOM.2013.6567146},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567147,
author={E. Altman and F. De Pellegrini and R. El-Azouzi and D. Miorandi and T. Jimenez},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Emergence of equilibria from individual strategies in online content diffusion},
year={2013},
volume={},
number={},
pages={3255-3260},
abstract={Social scientists have observed that human behavior in society can often be modeled as corresponding to a threshold type policy. A new behavior would propagate by a procedure in which an individual adopts the new behavior if the fraction of his neighbors or friends having adopted such behavior exceeds some threshold. In this paper we study the question of whether the emergence of threshold policies may be modeled as a result of some rational process which would describe the behavior of non-cooperative rational members of some social network. We focus on situations in which individuals take the decision whether to access or not some content, based on the number of views that the content has. Our analysis aims at understanding not only the behavior of individuals, but also the way in which information about the quality of a given content can be deduced from view counts when only part of the viewers that access the content are informed about its quality. In this paper we present a game formulation for the behavior of individuals using a meanfield model: the number of individuals is approximated by a continuum of atomless players and for which the Wardrop equilibrium is the solution concept. We derive conditions on the problem's parameters that result indeed in the emergence of threshold equilibria policies. But we also identify some parameters in which other structures are obtained for the equilibrium behavior of individuals.},
keywords={content management;game theory;social networking (online);equilibrium behavior;threshold equilibrium policy;Wardrop equilibrium;atomless player;meanfield model;game formulation;content quality;content access;social network;noncooperative rational member;rational process;threshold policy;behavior adoption;threshold type policy;society;human behavior;social science;online content diffusion;YouTube;Games;Conferences;Communication networks;Measurement;Market research;User-generated content;Complex Systems;Video popularity;Game theory;Wardrop equilibria},
doi={10.1109/INFCOM.2013.6567147},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567148,
author={P. Sermpezis and T. Spyropoulos},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Information diffusion in heterogeneous networks: The configuration model approach},
year={2013},
volume={},
number={},
pages={3261-3266},
abstract={In technological or social networks, diffusion processes (e.g. information dissemination, rumour/virus spreading) strongly depend on the structure of the network. In this paper, we focus on epidemic processes over one such class of networks, Opportunistic Networks, where mobile nodes within range can communicate with each other directly. As the node degree distribution is a salient property for process dynamics on complex networks, we use the well known Configuration Model, that captures generic degree distributions, for modeling and analysis. We also assume that information spreading between two neighboring nodes can only occur during random contact times. Using this model, we proceed to derive closed-form approximative formulas for the information spreading delay that only require the first and second moments of the node degree distribution. Despite the simplicity of our model, simulations based on both synthetic and real traces suggest a considerable accuracy for a large range of heterogeneous contact networks arising in this context, validating its usefulness for performance prediction.},
keywords={complex networks;delays;graph theory;mobile radio;telecommunication network topology;information diffusion;heterogeneous network;configuration model approach;technological network;social network;diffusion process;information dissemination;rumour spreading;virus spreading;network structure;epidemic process;opportunistic networks;mobile nodes;node degree distribution;process dynamics;complex network;random contact time;closed-form approximative formula;information spreading delay;heterogeneous contact networks;performance prediction;Delays;Approximation methods;Peer-to-peer computing;Accuracy;Social network services;Complex networks;Random variables},
doi={10.1109/INFCOM.2013.6567148},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567149,
author={Y. Jin and J. Ok and Y. Yi and J. Shin},
booktitle={2013 Proceedings IEEE INFOCOM},
title={On the impact of global information on diffusion of innovations over social networks},
year={2013},
volume={},
number={},
pages={3267-3272},
abstract={This paper studies how global information affects the diffusion of innovations on a network. The diffusion of innovation is modeled by the logit dynamics of a weighted N-person coordination game among (bounded) rational users where innovations spread through users' strategic choices. We find a critical asymptotic threshold for the weight on global information where the diffusion of innovations undergoes a transition in the rate of convergence regardless of any network structure. In particular, it is found that the convergence to the pervasive adoption is slowed down by global information.},
keywords={convergence;game theory;innovation management;social networking (online);global information;innovation diffusion;social networks;logit dynamics;weighted N-person coordination game;rational users;user strategic choices;critical asymptotic threshold;convergence rate;network structure;Games;Convergence;Social network services;Technological innovation;Nickel;Conferences;Communication networks},
doi={10.1109/INFCOM.2013.6567149},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567150,
author={K. Poularakis and L. Tassiulas},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Surviving in a competitive market of information providers},
year={2013},
volume={},
number={},
pages={3273-3278},
abstract={As the processing and transport capacity of the information and communication technologies (ICT) infrastructure increased vastly the last few years, the bottleneck of the information exchange process moved to the end points of the process, i.e. the consumers and the producers of information. On one hand there is the limited time that a consumer has to access the information and on the other hand there is the minimum utility level that a provider needs to provide to the society of consumers to cover it's investment cost. In this paper we present a novel decision model for a set of competing providers that wish to enter a market. It may happen that due to the competition, some competitors will not be able to cover their investment cost and therefore will disappear. We analyze the optimum way of forming the market, in order to maximize the aggregate utility of it. We show that this problem is NP-complete and present a linear programming rounding heuristic algorithm to solve it. Besides, we study a game where every player (provider) is to choose whether to join the market or not. We compute the price of anarchy of the game and present a heuristic algorithm that belongs to the family of best response dynamic algorithms. Systematic experiments on a real world data set have demonstrated the effectiveness of our proposed approach.},
keywords={computational complexity;decision making;electronic data interchange;linear programming;competitive market;information providers;ICT;information and communication technologies infrastructure;information exchange process;investment cost;decision model;NP-complete;linear programming rounding heuristic algorithm;heuristic algorithm;best response dynamic algorithms;transport capacity;Heuristic algorithms;Games;Nash equilibrium;Aggregates;Vectors;Linear programming;Conferences},
doi={10.1109/INFCOM.2013.6567150},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567151,
author={A. Botta and A. Pescapé},
booktitle={2013 Proceedings IEEE INFOCOM},
title={New generation satellite broadband Internet services: Should ADSL and 3G worry?},
year={2013},
volume={},
number={},
pages={3279-3284},
abstract={In the context of Internet access technologies, satellite networks have traditionally been considered for specific purposes or as a backup technology for users not reached by traditional access networks, such as 3G, cable or ADSL. In recent years, however, new satellite technologies have been introduced in the market, reopening the debate on the possibilities of having high-performance satellite access networks. In this paper, we describe the testbed we set up - in collaboration with one of the main satellite operators in Europe - and the experiments we performed to evaluate and analyze the performance of both Tooway and Tooway on KA-SAT (or KASAT for short), two satellite broadband Internet access services. Also, we build a simulator to study the behavior of the traffic shaping mechanism used by the satellite operator. In terms of performance, our results show how new generation Internet satellite services are a promising way to provide broadband Internet connection to users. In terms of traffic shaping, our results shed light on the mechanisms employed by the operator for shaping user traffic and the possibilities left for the users.},
keywords={broadband networks;computer network performance evaluation;IP networks;radio access networks;satellite communication;telecommunication traffic;broadband Internet connection;traffic shaping mechanism;satellite broadband Internet access services;KA-SAT performance analysis;KA-SAT performance evaluation;Tooway performance analysis;Tooway performance evaluation;Europe;satellite operators;high-performance satellite access networks;Satellites;Throughput;Uplink;Internet;Downlink;Delays;Jitter},
doi={10.1109/INFCOM.2013.6567151},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567152,
author={H. Cui and E. Biersack},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Troubleshooting slow webpage downloads},
year={2013},
volume={},
number={},
pages={3285-3290},
abstract={One common way to search and access information available in the Internet is via a Web browser. When clicking on a Web page, the user expects that the page gets rendered quickly, otherwise he will lose interest and may abort the page load. The causes for a Webpage to load slowly are multiple and not easy to comprehend for an end-user. In this paper, we present FireLog, a plugin for the Firefox Web browser that relies on passive measurements during users' browsing, and helps identify why a web page loads slowly. We present details of our methodology and illustrate it in a case study with real users.},
keywords={information retrieval;Internet;online front-ends;search problems;Web sites;slow Web page download troubleshooting;information search;information access;Internet;Web browser;FireLog;Firefox Web browser;Web pages;Servers;Delays;Browsers;Internet;Degradation},
doi={10.1109/INFCOM.2013.6567152},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567153,
author={Y. Gong and D. Rossi and C. Testa and S. Valenti and M. D. Täht},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Fighting the bufferbloat: On the coexistence of AQM and low priority congestion control},
year={2013},
volume={},
number={},
pages={3291-3296},
abstract={Nowadays, due to excessive queuing, delays on the Internet can grow longer than several round trips between the Moon and the Earth - for which the “bufferbloat” term was recently coined. Some point to active queue management (AQM) as the solution. Others propose end-to-end low-priority congestion control techniques (LPCC). Under both approaches, promising advances have been made in recent times: notable examples are CoDel for AQM, and LEDBAT for LPCC. In this paper, we warn of a potentially fateful interaction when AQM and LPCC techniques are combined: namely (i) AQM resets the relative level of priority between best effort and low-priority congestion control protocols; (ii) while reprioritization generally equalizes the priority of LPCC and TCP, we also find that some AQM settings may actually lead best effort TCP to starvation. By an extended set of experiments conducted on both controlled testbeds and on the Internet, we show the problem to hold in the real world for all tested combination of AQM policies and LPCC protocols. To further validate the generality of our findings, we complement our experiments with packet-level simulation, to cover cases of other popular AQM and LPCC that are not available in the Linux kernel. To promote cross-comparison, we make our scripts and dataset available to the research community.},
keywords={computer network management;Internet;queueing theory;telecommunication congestion control;transport protocols;bufferbloat;Internet;active queue management;end-to-end low-priority congestion control techniques;potentially fateful interaction;AQM policies;testbed control;Internet;LPCC protocols;packet-level simulation;research community;Delays;Protocols;Linux;Internet;Kernel;Electric breakdown;Monitoring;Bufferbloat;AQM;Scavenger protocol;Experiments;Simulation},
doi={10.1109/INFCOM.2013.6567153},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567154,
author={C. Chirichella and D. Rossi},
booktitle={2013 Proceedings IEEE INFOCOM},
title={To the Moon and back: Are Internet bufferbloat delays really that large?},
year={2013},
volume={},
number={},
pages={3297-3302},
abstract={Recently, the “bufferbloat” term has been coined to describe very large queuing delays (up to several seconds) experienced by Internet users. This problem has pushed protocol designer to deploy alternative (delay-based) models to the standard (lossbased) TCP best effort congestion control. In this work, we exploit timestamp information carried in the LEDBAT header, a protocol proposed by BitTorrent as replacement for TCP data transfer, to infer the queuing delay suffered by remote hosts. We conduct a thorough measurement campaign, that let us conclude that (i) LEDBAT delay-based congestion control is effective in keeping the queuing delay low for the bulk of the peers, (ii) yet about 1% of peers often experience queuing delay in excess of 1s, and (iii) not only the network access type, but also the BitTorrent client and the operating system concurr in determining the bufferbloat magnitude.},
keywords={Internet;operating systems (computers);packet switching;peer-to-peer computing;protocols;queueing theory;telecommunication congestion control;Internet bufferbloat delays;Internet users;timestamp information;LEDBAT header;remote hosts;LEDBAT delay-based congestion control;queuing delay;network access type;BitTorrent client;operating system;Delays;Internet;Probes;Protocols;Operating systems;Monitoring;Conferences},
doi={10.1109/INFCOM.2013.6567154},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567155,
author={L. Gao and G. Iosifidis and J. Huang and L. Tassiulas},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Economics of mobile data offloading},
year={2013},
volume={},
number={},
pages={3303-3308},
abstract={Mobile data offloading is a promising approach to alleviate network congestion and enhance quality of service (QoS) in mobile cellular networks. In this paper, we investigate the economics of mobile data offloading through third-party WiFi or femtocell access points (APs). Specifically, we consider a market-based data offloading solution, where macrocellular base stations (BSs) pay APs for offloading traffic. The key questions arising in such a marketplace are following: (i) how much traffic should each AP offload for each BS? and (ii) what is the corresponding payment of each BS to each AP? We answer these questions by using the non-cooperative game theory. In particular, we define a multi-leader multi-follower data offloading game (DOFF), where BSs (leaders) propose market prices, and accordingly APs (followers) determine the traffic volumes they are willing to offload. We characterize the subgame perfect equilibrium (SPE) of this game, and further compare the SPE with two other classic market outcomes: (i) the market balance (MB) in a perfect competition market (i.e., without price participation), and (ii) the monopoly outcome (MO) in a monopoly market (i.e., without price competition). Our results analytically show that (i) the price participation (of BSs) will drive market prices down, compared to those under the MB outcome, and (ii) the price competition (among BSs) will drive market prices up, compared to those under the MO outcome.},
keywords={economics;femtocellular radio;game theory;monopoly;quality of service;telecommunication traffic;wireless LAN;economics;mobile data offloading;network congestion;quality of service;QoS;mobile cellular network;third-party WiFi;femtocell access point;market-based data offloading;macrocellular base station;offloading traffic;noncooperative game theory;multileader multifollower data offloading game;DOFF;market price;traffic volume;subgame perfect equilibrium;market outcome;market balance;perfect competition market;price participation;monopoly outcome;monopoly market;price competition;Manganese;Games;IEEE 802.11 Standards;Mobile communication;Monopoly;Data models;Conferences},
doi={10.1109/INFCOM.2013.6567155},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567156,
author={J. Lee and Y. Yi and S. Chong and Y. Jin},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Economics of WiFi offloading: Trading delay for cellular capacity},
year={2013},
volume={},
number={},
pages={3309-3314},
abstract={Cellular networks are facing severe traffic overloads due to the proliferation of smart handheld devices and traffichungry applications. A cost-effective and practical solution is to offload cellular data through WiFi. Recent theoretical and experimental studies show that a scheme, referred to as delayed WiFi offloading, can significantly save the cellular capacity by delaying users' data and exploiting mobility and thus increasing chance of meeting WiFi APs (Access Points). Despite a huge potential of WiFi offloading in alleviating mobile data explosion, its success largely depends on the economic incentives provided to users and network providers to deploy and use delayed offloading. In this paper, we study how much economic benefits can be generated due to delayed WiFi offloading, by modeling the interaction between a single provider and users based on a two-stage sequential game. We first analytically prove that WiFi offloading is economically beneficial for both the provider and users. Also, we conduct trace-driven numerical analysis to quantify the practical gain, where the increase ranges from 21 to 152% in the provider's revenue, and from 73 to 319% in the users' surplus.},
keywords={cellular radio;wireless LAN;WiFi offloading economics;trading delay;cellular capacity;cellular networks;smart handheld device;traffic hungry application;cost effective;cellular data;mobility;WiFi AP;mobile data explosion;economic incentives;network providers;trace driven numerical analysis;IEEE 802.11 Standards;Pricing;Delays;Economics;Numerical models;Mobile communication;Conferences},
doi={10.1109/INFCOM.2013.6567156},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567157,
author={B. Kim and S. Ren and M. van der Schaar and J. Lee},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Tiered billing scheme for residential load scheduling with bidirectional energy trading},
year={2013},
volume={},
number={},
pages={3315-3320},
abstract={Future generation smart grids will allow customers to trade energy bidirectionally. Specifically, each customer will be able to not only buy energy from the aggregator during its peak hours but also sell its surplus energy during its off-peak hours. In these emerging energy trading markets, a key component will be the deployment of effective energy billing schemes which consider the customers residential load scheduling. In this paper, we consider a residential load scheduling problem with bidirectional energy trading. Compared with the previous work, in which customers are assumed to be obedient and agree to maximize the social welfare of the smart grid system, in this paper, we consider a non-collaborative approach, where consumers are self-interested. We model the energy scheduling problem as a non-cooperative game, where each customer determines its load scheduling and energy trading to maximize its own profit. In order to resolve the unfairness between heavy and light customers, we propose a novel tiered billing scheme that can control the electricity rates for customers according to their different energy consumption levels. We also propose a distributed energy scheduling algorithm that converges to the unique Nash equilibrium of the studied non-cooperative game. Through the numerical results, we study the impact of the proposed tiered billing scheme on the selfish customers' behavior and on their incentives to participate in the energy trading market.},
keywords={energy consumption;game theory;power markets;smart power grids;tiered billing scheme;residential load scheduling;bidirectional energy trading;smart grids;surplus energy;off-peak hours;energy trading markets;energy billing schemes;noncooperative game;electricity rates;energy consumption levels;distributed energy scheduling algorithm;unique Nash equilibrium;Games;Energy consumption;Electricity;Batteries;Collaboration;Home appliances;Nash equilibrium},
doi={10.1109/INFCOM.2013.6567157},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567158,
author={O. Dalkilic and O. Candogan and A. Eryilmaz},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Pricing algorithms for the day-ahead electricity market with flexible consumer participation},
year={2013},
volume={},
number={},
pages={3321-3326},
abstract={In this paper, we consider the design of the day-ahead market for the smart electrical grid. Consumers with flexible demand and generator companies participate in the market to settle on their load and supply schedules, respectively. The market is operated by an Independent System Operator (ISO) whose purpose is to maximize social welfare while keeping load and supply balanced in the electricity network. We develop two distributed pricing algorithms that achieve optimum welfare. The first algorithm yields time-dependent market prices under convexity assumptions on utility and cost functions and the second algorithm yields bundle prices for arbitrary utility and cost functions. In both algorithms, flexible consumers and generator companies simply determine their own schedules based on the prices updated by the ISO at each iteration. We show that the participation of flexible demand in the day-ahead market reduces supply volatility, which would be present when flexible demand does not take part in price setting procedure.},
keywords={iterative methods;power markets;pricing;smart power grids;supply volatility;cost functions;time-dependent market prices;distributed pricing algorithms;electricity network;social welfare;ISO;independent system operator;load and supply schedules;generator companies;smart electrical grid;flexible consumer participation;day-ahead electricity market;Schedules;Pricing;Companies;Generators;Cost function;ISO;Electricity supply industry},
doi={10.1109/INFCOM.2013.6567158},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567159,
author={M. Sheng and C. Joe-Wong and S. Ha and F. M. F. Wong and S. Sen},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Smart data pricing: Lessons from trial planning},
year={2013},
volume={},
number={},
pages={3327-3332},
abstract={Rapid increases in the demand for broadband data are increasingly causing a growth in costs for communication service providers (CSPs). Yet under the current pricing plans, CSPs' revenue has not kept pace with these costs. Thus, many CSPs are considering Smart Data Pricing (SDP) as a way to reduce cost or increase revenue. Before offering such novel data plans, however, CSPs must conduct trials of the specific data plans proposed. Due to the complexity of necessary changes in network equipment and a need to carefully design the trial in order to understand customer behavior, planning such trials is not only a critical precursor to SDP deployment, but also a nontrivial undertaking in itself. This paper discusses general principles of trial design and proposes two methods for estimating their effectiveness. We first give an introduction to the goals of SDP research and review three possible SDP approaches. We then discuss the importance of pre-trial participant surveys and some technical considerations of implementing the trial infrastructure for a particular SDP algorithm. Finally, we show how the CSP may extrapolate from the trial results to estimate the SDP trial's benefits, in terms of changes in traffic patterns and a reduction in spectrum requirements. We conclude with some remarks about future work.},
keywords={broadband networks;cost reduction;pricing;telecommunication network planning;smart data pricing;trial planning;broadband data;communication service provider;cost reduction;network equipment;customer behavior;effectiveness estimation;traffic pattern;spectrum requirement reduction;Pricing;Planning;Mobile communication;Monitoring;Protocols;Conferences;Internet;Smart Data Pricing;Spectrum Requirements},
doi={10.1109/INFCOM.2013.6567159},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567160,
author={L. Spinelli and M. Crovella and B. Eriksson},
booktitle={2013 Proceedings IEEE INFOCOM},
title={AliasCluster: A lightweight approach to interface disambiguation},
year={2013},
volume={},
number={},
pages={3333-3338},
abstract={Internet topologies discovered by standard traceroute-based probing schemes are limited by many factors. One of the main factors is the ambiguity of the returned interfaces, where multiple unique interface IP addresses belong to the same physical router. The unknown assignment of interface IPs to physical routers can result in grossly inflated estimated topologies compared with the true underlying physical infrastructure of the network. The ability to determine which interfaces belong to which router would aid in the ability to accurately reconstruct the underlying topology of the Internet. In this paper, we present ALIASCLUSTER, a lightweight learning-based methodology that disambiguates router aliases using only observed traceroute measurements and requires no additional load on the network. Compared with existing techniques, we find that ALIASCLUSTER can resolve the same number of true router alias pairs with 50% fewer false alarms.},
keywords={Internet;IP networks;telecommunication network routing;telecommunication network topology;traceroute measurement;lightweight learning-based methodology;ALIASCLUSTER;physical infrastructure;interface IP assignment;physical router;traceroute-based probing scheme;Internet topology;interface disambiguation;Feature extraction;IP networks;Internet;Bayes methods;Topology;Probes;Data mining},
doi={10.1109/INFCOM.2013.6567160},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567161,
author={G. Hampel and M. Steiner and T. Bu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Applying software-defined networking to the telecom domain},
year={2013},
volume={},
number={},
pages={3339-3344},
abstract={The concept of Software-Defined Networking (SDN) has been successfully applied to data centers and campus networks but it has had little impact in the fixed wireline and mobile telecom domain. Although telecom networks demand fine-granular flow definition, which is one of SDN's principal strengths, the scale of these networks and their legacy infrastructure constraints considerably limit the applicability of SDN principles. Instead, telecom networks resort to tunneling solutions using a plethora of specialized gateway nodes, which create high operation cost and single points of failure. We propose extending the concept of SDN so that it can tackle the challenges of the telecom domain. We see vertical forwarding, i.e. programmable en- and decapsulation operations on top of IF, as one of the fundamental features to be integrated into SDN. We discuss how vertical forwarding enables flow-based policy enforcement, mobility and security by replacing specialized gateways with virtualized controllers and commoditized forwarding elements, which reduces cost while adding robustness and flexibility.},
keywords={cellular radio;computer network security;IP networks;mobility management (mobile radio);flexibility;robustness;cost reduction;commoditized forwarding elements;virtualized controllers;security;mobility;flow-based policy enforcement;IP;programmable decapsulation operations;programmable encapsulation operations;vertical forwarding;SDN;telecom networks;software-defined networking;Decision support systems;Internet;Software-defined networking;telecom;cellular network;fixed wireline network;tunneling;gateway},
doi={10.1109/INFCOM.2013.6567161},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567162,
author={Y. Shvartzshnaider and M. Ott and O. Mehani and G. Jourjon and T. Rakotoarivelo and D. Levy},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Into the Moana1— Hypergraph-based network layer indirection},
year={2013},
volume={},
number={},
pages={3345-3350},
abstract={In this paper, we introduce the Moana network infrastructure. It draws on well-adopted practices from the database and software engineering communities to provide a robust and expressive information-sharing service using hypergraph-based network indirection. Our proposal is twofold. First, we argue for the need for additional layers of indirection used in modern information systems to bring the network layer abstraction closer to the developer's world, allowing for expressiveness and flexibility in the creation of future services. Second, we present a modular and extensible design of the network fabric to support incremental architectural evolution and innovation, as well as its initial evaluation.},
keywords={graph theory;information systems;Internet;hypergraph-based network layer indirection;Moana network infrastructure;software engineering communities;robust expressive information-sharing service;modern information systems;network layer abstraction;network fabric design;incremental architectural evolution;Iron;Ports (Computers);Internet;Fabrics;Engines;Databases;Communities},
doi={10.1109/INFCOM.2013.6567162},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567163,
author={G. S. Thakur and A. Helmy},
booktitle={2013 Proceedings IEEE INFOCOM},
title={COBRA: A framework for the analysis of realistic mobility models},
year={2013},
volume={},
number={},
pages={3351-3356},
abstract={The future global Internet is going to have to cater to users that will be largely mobile. Mobility is one of the main factors affecting the design and performance of wireless networks. Mobility modeling has been an active field for the past decade, mostly focusing on matching a specific mobility or encounter metric with little focus on matching protocol performance. This study investigates the adequacy of existing mobility models in capturing various aspects of human mobility behavior (including communal behavior), as well as network protocol performance. This is achieved systematically through the introduction of a framework that includes a multi-dimensional mobility metric space. We then introduce COBRA, a new mobility model capable of spanning the mobility metric space to match realistic traces. A methodical analysis using a range of protocol (epidemic, spraywait, Prophet, and Bubble Rap) dependent and independent metrics (modularity) of various mobility models (SMOOTH and TVC) and traces (university campuses, and theme parks) is done. Our results indicate significant gaps in several metric dimensions between real traces and existing mobility models. Our findings show that COBRA matches communal aspect and realistic protocol performance, reducing the overhead gap (w.r.t existing models) from 80% to less than 12%, showing the efficacy of our framework.},
keywords={mobile computing;protocols;COBRA;realistic mobility models;future global Internet;wireless networks;human mobility behavior;network protocol performance;multidimensional mobility metric space;realistic traces;overhead gap;Protocols;Analytical models;Mobile communication;Communities;Extraterrestrial measurements;Accuracy},
doi={10.1109/INFCOM.2013.6567163},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567164,
author={Y. Li and M. Steiner and L. Wang and Z. Zhang and J. Bao},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Exploring venue popularity in Foursquare},
year={2013},
volume={},
number={},
pages={3357-3362},
abstract={In this paper, we provide a detailed analysis on the venue popularity in Foursquare, a leading location-based social network. By collecting 2.4 million venues from 14 geographic regions all over the world, we study the common characteristics of popular venues, and make the following observations. First, venues with more complete profile information are more likely to be popular. Second, venues in the Food category attract the most (43%) public tips (comments) by users, and the Travel & Transport category is the most popular category with the highest per venue check-ins, i.e., each venue in this category attracts on average 376 check-ins. Moreover, the stickiness of users checking in venues in the residence, office, and school categories is higher than in other categories. Last but not least, in general, old venues created at the early stage of Foursquare are more popular than new venues. Our results help to understand the factors that cause venues to become popular, and have applications in venue recommendations and advertisement in location based social networks.},
keywords={mobile computing;social networking (online);Foursquare;venue popularity;location-based social network;geographic regions;profile information;food category;public tips;travel & transport category;venue check-ins;school category;office category;residence category;Educational institutions;Cities and towns;Social network services;Art;Conferences;Communication networks;Data collection},
doi={10.1109/INFCOM.2013.6567164},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567165,
author={Y. Wang and H. Zang and M. Faloutsos},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Inferring cellular user demographic information using homophily on call graphs},
year={2013},
volume={},
number={},
pages={3363-3368},
abstract={Homophily refers to the phenomenon where people who are socially-connected share many characteristics including demographic and behavioral properties. The goal of this paper is to see whether homophily exists in call networks and if so, to what degree we can infer a cellphone user's demographic properties by knowing the demographic information of the people that s/he talks to. We focus on three types of demographic information: a) home location, b) age group, and c) income level. The novelty is two-folds. First, we use both communication metrics and structural properties of call graphs to identify those “important” friends for each user with whom (s)he is most likely to be in homophily. Second, we assess the importance of different time slices such as weekdays, or nights and weekends for capturing different user relationships. We conduct our study on a real data trace with 20M subscribers during one month from a nationwide cellular carrier. Our first contribution is that we quantify the extent of homophily on the call graph and identify the correlations between homophily and communication and structural features. As a second contribution, we develop effective methods to infer demographic information for a cellular user using linear regression to select the most homophily-like friend of her/him. We find that we can predict home location within 20km radius with 80% accuracy, and age group and income level with 78% and 72% accuracy, respectively.},
keywords={cellular radio;demography;graph theory;regression analysis;homophily-like friend;linear regression;structural feature;nationwide cellular carrier;user relationship;structural property;communication metrics;income level;age group;home location;cellphone user demographic properties;call network;behavioral properties;socially-connected people;call graph;cellular user demographic information;Accuracy;Correlation;Prediction algorithms;Conferences;Communication networks;Linear regression;Social network services},
doi={10.1109/INFCOM.2013.6567165},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567166,
author={T. Phe-Neau and M. Dias de Amorim and V. Conan},
booktitle={2013 Proceedings IEEE INFOCOM},
title={The strength of vicinity annexation in opportunistic networking},
year={2013},
volume={},
number={},
pages={3369-3374},
abstract={Most disruption-tolerant networking protocols focus on mere contact and intercontact characteristics to make forwarding decisions. We propose to relax such a simplistic approach and include multi-hop opportunities by annexing a node's vicinity to its network vision. We investigate how the vicinity of a node evolves through time and whether such knowledge is useful when routing data. By analyzing a modified version of the pure WAIT forwarding strategy, we observe a clear tradeoff between routing performance and cost for monitoring the neighborhood. By observing a vicinity-aware WAIT strategy, we emphasize how the pure WAIT misses interesting end-to-end transmission opportunities through nearby nodes. For the datasets we consider, our analyses also suggest that limiting a node's neighborhood view to four hops is enough to improve forwarding efficiency while keeping control overhead low.},
keywords={computer networks;protocols;telecommunication network routing;vicinity annexation;opportunistic networking;disruption-tolerant networking protocols;forwarding decisions;multihop opportunities;network vision;WAIT forwarding strategy;routing performance;vicinity-aware WAIT strategy;end-to-end transmission opportunities;control overhead;Protocols;Monitoring;Delays;Communities;Routing;Conferences;Communication networks;Opportunistic networks;disruption-tolerant networks;contact;intercontact;vicinity},
doi={10.1109/INFCOM.2013.6567166},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567167,
author={S. Trajanovski and F. A. Kuipers and P. Van Mieghem},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Finding critical regions in a network},
year={2013},
volume={},
number={},
pages={3375-3380},
abstract={It is important that our vital networks (e.g., infrastructures) are robust to more than single-link failures. Failures might for instance affect a part of the network that resides in a certain geographical region. In this paper, considering networks embedded in a two-dimensional plane, we study the problem of finding a critical region - that is, a part of the network that can be enclosed by a given elementary figure (a circle, ellipse, rectangle, square, or equilateral triangle) with a predetermined size - whose removal would lead to the highest network disruption. We determine that there is a polynomial number of non-trivial positions for such a figure that need to be considered and, subsequently, we propose a polynomial-time algorithm for the problem. Simulations on realistic networks illustrate that different figures with equal area result in different critical regions in a network.},
keywords={computational complexity;computational geometry;geography;network theory (graphs);vital network;infrastructure;single-link failure;geographical region;two-dimensional plane;critical region;elementary figure;circle;ellipse;rectangle;square;equilateral triangle;network disruption;polynomial number;nontrivial position;polynomial-time algorithm;realistic network;computational geometry;geographical failure;Measurement;Robustness;Conferences;Communication networks;Complexity theory;Polynomials;Shape;geographical failures;critical regions;network robustness;computational geometry},
doi={10.1109/INFCOM.2013.6567167},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567168,
author={L. Grimaudo and M. Mellia and E. Baralis and R. Keralapura},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Self-learning classifier for Internet traffic},
year={2013},
volume={},
number={},
pages={3381-3386},
abstract={Network visibility is a critical part of traffic engineering, network management, and security. Recently, unsupervised algorithms have been envisioned as a viable alternative to automatically identify classes of traffic. However, the accuracy achieved so far does not allow to use them for traffic classification in practical scenario. In this paper, we propose SeLeCT, a Self-Learning Classifier for Internet traffic. It uses unsupervised algorithms along with an adaptive learning approach to automatically let classes of traffic emerge, being identified and (easily) labeled. SeLeCT automatically groups flows into pure (or homogeneous) clusters using alternating simple clustering and filtering phases to remove outliers. SeLeCT uses an adaptive learning approach to boost its ability to spot new protocols and applications. Finally, SeLeCT also simplifies label assignment (which is still based on some manual intervention) so that proper class labels can be easily discovered. We evaluate the performance of SeLeCT using traffic traces collected in different years from various ISPs located in 3 different continents. Our experiments show that SeLeCT achieves overall accuracy close to 98%. Unlike state-of-art classifiers, the biggest advantage of SeLeCT is its ability to help discovering new protocols and applications in an almost automated fashion.},
keywords={information filtering;Internet;pattern classification;pattern clustering;protocols;telecommunication traffic;unsupervised learning;Internet service provider;ISP;traffic traces;label assignment;protocols;outlier removal;adaptive learning approach;SeLeCT;traffic classification;unsupervised learning algorithms;network security;network management;traffic engineering;network visibility;Internet traffic;self-learning classifier;Protocols;Accuracy;Clustering algorithms;Servers;Ports (Computers);Labeling;Algorithm design and analysis},
doi={10.1109/INFCOM.2013.6567168},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567169,
author={D. Malone and D. F. Kavanagh and N. R. Murphy},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Rogue femtocell owners: How Mallory can monitor my devices},
year={2013},
volume={},
number={},
pages={3387-3392},
abstract={Femtocells are small cellular telecommunication base stations that provide improved cellular coverage. These devices provide important improvements in coverage, battery life and throughput, they also present security challenges. We identify a problem which has not been identified in previous studies of femtocell security: rogue owners of femtocells can secretly monitor third-party mobile devices by using the femtocell's access control features. We present traffic analysis of real femtocell traces and demonstrate the ability to monitor mobile devices through classification of the femtocell's encrypted backhaul traffic. We also consider the femtocell's power usage and status LEDs as other side channels that provide information on the femtocell's operation. We conclude by presenting suitable solutions to overcome this problem.},
keywords={access control;femtocellular radio;telecommunication security;telecommunication traffic;femtocell owners;small cellular telecommunication base stations;cellular coverage;battery life;third-party mobile devices;access control;traffic analysis;backhaul traffic;LED;Monitoring;Light emitting diodes;Cryptography;History;Algorithm design and analysis;Conferences;Femtocell;security;traffic analysis;cellular devices;rogue owners},
doi={10.1109/INFCOM.2013.6567169},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567170,
author={S. Ruehrup and P. Urbano and A. Berger and A. D'Alconzo},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Botnet detection revisited: Theory and practice of finding malicious P2P networks via Internet connection graphs},
year={2013},
volume={},
number={},
pages={3393-3398},
abstract={In this paper we review state-of-the-art botnet detection algorithms that reveal the control traffic of malicious peer-topeer (P2P) networks by targeting topological properties of their interconnectivity graph. This class of detection methods does not rely on the exchanged content and therefore is also applicable to encrypted control traffic. However, in practice, an ISP monitoring customer traffic over an edge router will usually see only a fraction of the overall botnet, thus restricting the available bot connectivity information and limiting the applicability of general community detection approaches. In this paper we critically review graph based detection methods suitable for edge router monitoring using two types of real network traces. We show experimentally that using meta-graphs of mutual contacts proposed by Coskun et al. 2010 has the highest potential on result quality. We improve this approach by presenting a computationally less complex algorithm with similar result quality. Furthermore we explain ways to alleviate the cost of dealing with false positives in the result set.},
keywords={computer network security;graph theory;peer-to-peer computing;botnet detection algorithm;malicious P2P networks;Internet connection graphs;malicious peer-topeer networks;interconnectivity graph;customer traffic;ISP;bot connectivity information;graph based detection methods;edge router monitoring;Monitoring;DSL;Communities;Clustering algorithms;Peer-to-peer computing;Topology;Dispersion},
doi={10.1109/INFCOM.2013.6567170},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567171,
author={L. Deri and A. Cardigliano and F. Fusco},
booktitle={2013 Proceedings IEEE INFOCOM},
title={10 Gbit line rate packet-to-disk using n2disk},
year={2013},
volume={},
number={},
pages={3399-3404},
abstract={Capturing packets to disk at line rate and with high precision packet timestamping is required whenever an evidence of network communications has to be provided. Typical applications of long-term network traffic repositories are network troubleshooting, analysis of security violations, and analysis of high-frequency trading communications. Appliances for 10 Gbit packet capture to disk are often based on dedicated network adapters, and therefore very expensive, making them usable only in specific domains. This paper covers the design and implementation of n2disk, a packet capture to disk application, capable of dumping 10 Gbit traffic to disk using commodity hardware and open-source software. In addition to packet capture, n2disk is able to index the traffic at line-rate during capture, enabling users to efficiently search specific packets in network traffic dump files.},
keywords={computer network reliability;computer network security;public domain software;storage area networks;storage management;telecommunication traffic;10 Gbit line rate packet-to-disk;network traffic dump files;packet searching;open-source software;commodity hardware;n2disk;network adapters;high-frequency trading communications;security violation analysis;network troubleshooting;long-term network traffic repositories;network communications;high precision packet timestamping;Instruction sets;Matched filters;Indexing;Band-pass filters;Monitoring;Traffic Dump to Disk;Packet Capture;10 Gbit Traffic Monitoring},
doi={10.1109/INFCOM.2013.6567171},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567172,
author={V. Kounev and D. Tipper},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Advanced metering and demand response communication performance in Zigbee based HANs},
year={2013},
volume={},
number={},
pages={3405-3410},
abstract={Using IEEE 802.15.4 and Zigbee for home area networks (HANs) in the Smart Grid is becoming an increasingly prominent topic in the research area. As the standard designed for low data rate and low cost wireless personal area networks, IEEE 802.15.4 is widely employed in the construction of home sensor networks to assist with real-time environment information. For the purposes of Smart Grid the Zigbee Alliance has defined new Smart Energy Profile Protocol that leverages the existing TCP and HTTP protocols. In this paper, we provide an overview of the Smart Grid's Advanced Metering Infrastructure (AMI) and Demand Response (DR) functionalities, and the communication requirement they pose for the new SEP protocol. The discussion is followed by an evaluation of the theoretical performance bounds of the new architecture based on a analytical model. We conclude, by extending the model to account for WiFi interference which is expected to be present in home and office environments.},
keywords={demand side management;home networks;radiofrequency interference;real-time systems;smart meters;smart power grids;transport protocols;wireless LAN;wireless sensor networks;Zigbee;demand response communication performance;Zigbee-based HAN;IEEE 802.15.4 networks;home area networks;smart grid;low data rate;low cost wireless personal area networks;home sensor networks;real-time environment information;smart energy profile protocol;TCP protocols;HTTP protocols;advanced metering infrastructure;AMI;DR functionalities;SEP protocol;theoretical performance bounds;analytical model;WiFi interference;office environments;home environments;Zigbee;Smart grids;IEEE 802.11 Standards;Protocols;Delays;Home appliances;Load management},
doi={10.1109/INFCOM.2013.6567172},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567173,
author={S. Finster and I. Baumgart},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Elderberry: A peer-to-peer, privacy-aware smart metering protocol},
year={2013},
volume={},
number={},
pages={3411-3416},
abstract={The deployment of smart metering provides an immense amount of data for power grid operators and energy providers. By using this data, a more efficient and flexible power grid can be realized. However, this data also raises privacy concerns since it contains very sensitive information about customers. In this paper, we present Elderberry, a peer-to-peer protocol that enables near real-time smart metering while preserving the customer's privacy. By forming small groups of cooperating smart meters, their consumption traces are anonymized before being aggregated and sent to the grid operator. Through aggregation, Elderberry realizes efficient monitoring of large numbers of smart meters. It reaches this goal without computationally complex cryptography and adds only little communication overhead.},
keywords={data privacy;peer-to-peer computing;power engineering computing;power meters;smart power grids;Elderberry;peer-to-peer protocol;privacy-aware smart metering protocol;flexible power grid;real-time smart metering;Peer-to-peer computing;Vegetation;Protocols;Aggregates;Time measurement;Cryptography;Meter reading},
doi={10.1109/INFCOM.2013.6567173},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567174,
author={H. Georg and N. Dorsch and M. Putzke and C. Wietfeld},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Performance evaluation of time-critical communication networks for smart grids based on IEC 61850},
year={2013},
volume={},
number={},
pages={3417-3422},
abstract={Driven by the increasing application of Smart Grid technologies in today's power systems, communication networks are becoming more and more important for exchanging monitoring, control and protection information on local and wide area level. For communication the IEC 61850 standard is a candidate for the Smart Grid and has been introduced for Substation Automation Systems (SAS) some years ago. IEC 61850 provides interoperability among various manufactures and enables systemwide communication between intelligent components of future power systems. However, as IEC 61850 addresses Ethernet (ISO/IEC 8802-3 family) as network technology, especially high performance aspects of Ethernet have become increasingly important for time-critical communication within substation automation systems. In this paper we introduce the generic architecture of IEC 61850 and present our modelling approach for evaluating high performance and real-time capability of communication technologies for future smart grid application. First, we give a short overview of the IEC 61850 protocol and present communication flows in substation automation systems according to the standard. Here we focus on substation automation at bay level, located inside an exemplary substation node taken from the IEEE 39-bus power system network. Afterwards we demonstrate our modeling approach for communication networks based on IEC 61850. For performance evaluation we developed a simulation model along with an analytical approach on basis of Network Calculus, enabling to identify worst case boundaries for intra-substation communication. Finally results for simulative and analytical modelling are provided and cross validated for two bay level scenarios, showing the applicability of Network Calculus for real-time constrained smart grid communication.},
keywords={IEC standards;open systems;smart power grids;substation automation;bay level scenarios;intra-substation communication;worst case boundaries;network calculus;IEEE 39-bus power system network;substation node;communication flows;communication technologies;generic architecture;network technology;ISO/IEC 8802-3 family;Ethernet;system-wide communication;interoperability;substation automation systems;IEC 61850 standard;wide area level;protection information;power systems;smart grids;time-critical communication networks;IEC standards;Substations;Delays;Calculus;Switches;Merging;Smart Grid;Power System Simulation;Communication Network Simulation;IEC 61850;Network Calculus},
doi={10.1109/INFCOM.2013.6567174},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567175,
author={T. Liu and Y. Gu and D. Wang and Y. Gui and X. Guan},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A novel method to detect bad data injection attack in smart grid},
year={2013},
volume={},
number={},
pages={3423-3428},
abstract={Bad data injection is one of most dangerous attacks in smart grid, as it might lead to energy theft on the end users and device breakdown on the power generation. The attackers can construct the bad data evading the bad data detection mechanisms in power system. In this paper, a novel method, named as Adaptive Partitioning State Estimation (APSE), is proposed to detect bad data injection attack. The basic ideas are: 1) the large system is divided into several subsystems to improve the sensitivity of bad data detection; 2) the detection results are applied to guide the subsystem updating and re-partitioning to locate the bad data. Two attack cases are constructed to inject bad data into an IEEE 39-bus system, evading the traditional bad data detection mechanism. The experiments demonstrate that all bad data can be detected and located within a small area using APSE.},
keywords={electric power generation;IEEE standards;power engineering computing;power system security;power system state estimation;security of data;smart power grids;bad data injection attack detection mechanism;smart grid;power generation;adaptive partitioning state estimation;IEEE 39-bus system;APSE;power system;sensitivity;testing result;Chi-squares method;subsystem-extension;device breakdown;Decision support systems;Conferences;smart grid;security;detection;bad data injection;adaptive partitioning state estimation},
doi={10.1109/INFCOM.2013.6567175},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567176,
author={H. Nicanfar and S. Hosseininezhad and P. TalebiFard and V. C. M. Leung},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Robust privacy-preserving authentication scheme for communication between electric vehicle as power energy storage and power stations},
year={2013},
volume={},
number={},
pages={3429-3434},
abstract={The concept of Electric Vehicle as Power Energy Storage has gained much attention from the research community and market recently. The increasing capacity of the power storages in the electric vehicles (EV) motivates this concept and makes it more feasible. However, the privacy of the customers can be compromised by tracing the stations that an EV has been connected to during a period of the time. The stations that are providing power charging as well as purchasing the power back from EVs can be owned by third party businesses. EVs should be authenticated through these stations in order to give or receive appropriate credit for the power. In this paper, we identify potential privacy issues and propose a robust privacy-preserving authentication scheme for communication of the EV and the station to prevent customer information leakage. In our approach, the EV and the station communication utilizes pseudonym of the EV, in which only the smart grid server (a trusted entity) can map the pseudonym to the real vehicle identity and provide the identity management. The pseudonym of an EV changes when the EV moves from one station to another, which prevents the adversary from tracing foot prints of the EV. Our analysis shows that our model is robust enough to make sure the privacy of the customers is fully preserved, and at the same time, it is efficient by consuming very limited resources.},
keywords={battery powered vehicles;computer network security;data privacy;energy storage;power engineering computing;secondary cells;smart power grids;robust privacy preserving authentication scheme;electric vehicle;power energy storage;power station;power charging;potential privacy issue;customer information leakage;station communication;identity management;foot print tracing;Vehicles;Public key;Privacy;Servers;Authentication;Smart grids;Pseudonymity;Untraceability;Identity Management;Privacy;Security;Electric Vehicle;Smart Grid},
doi={10.1109/INFCOM.2013.6567176},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567177,
author={L. Tang and Y. Rui and Q. Wang and H. Li and Z. Bu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={A reverse transmission mechanism for surveillance network in smart grid},
year={2013},
volume={},
number={},
pages={3435-3440},
abstract={A reverse transmission mechanism for smart grid is introduced, in which the transmission robustness of ad-hoc net-segment is guaranteed. The goal is to address the deadlock issue caused by the broken down of the targeted tower , and to reduce transmission power consumption. Performance analysis of the new mechanism is presented, in which the configuration of different number of electrical towers is discussed. Upon examination of the simulation results, we conclude that the proposed mechanism provides lower latency and power consumption compared to the traditional transmission mechanism, and guarantees the transmission robustness for the smart grid.},
keywords={power consumption;smart power grids;electrical towers;transmission power consumption;ad-hoc net-segment;smart grid;surveillance network;reverse transmission;Poles and towers;Monitoring;Logic gates;Smart grids;Power demand;Equations;Silicon},
doi={10.1109/INFCOM.2013.6567177},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567178,
author={R. Beuran and S. Miwa and Y. Shinoda},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Network emulation testbed for DTN applications and protocols},
year={2013},
volume={},
number={},
pages={3441-3446},
abstract={Wireless devices are widely used today to access the Internet, despite the intermittent network connectivity they often provide, especially in mobile circumstances. The paradigm of Delay/Disruption Tolerant Networks (DTN) can be applied in such cases to improve the user experience. In this paper we present a network testbed for DTN applications and protocols that we developed based on the generic-purpose wireless network emulation testbed named QOMB. Our testbed is intended for quantitative performance assessments of DTN application and protocol implementations in realistic scenarios. We illustrate the practicality of our emulation testbed through a series of experiments with the DTN2 and IBR-DTN implementations, focusing on mobility in urban environments. The scalability issues that we have identified for DTN2 emphasize the need to perform large-scale repeatable evaluations of DTN applications and protocols for functionality validation and performance optimization.},
keywords={delay tolerant networks;mobile radio;protocols;radio networks;telecommunication network reliability;performance optimization;large-scale repeatable evaluation;scalability issue;IBR-DTN;DTN2;quantitative performance assessment;QOMB;generic-purpose wireless network emulation testbed;delay-disruption tolerant network;intermittent network connectivity;Internet;wireless device;protocol;Emulation;Protocols;Libraries;Internet;Mobile communication;Wireless networks},
doi={10.1109/INFCOM.2013.6567178},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567179,
author={S. Cevher and M. Ulutas and I. Hokelek},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Trade-off analysis of multi topology routing based IP fast reroute mechanisms},
year={2013},
volume={},
number={},
pages={3447-3452},
abstract={To seamlessly support real-time services such as voice and video over next generation IP networks, routers must continue their forwarding tasks in case of link/node failures by limiting the service disruption time to sub-100 ms. IETF Routing Area Working Group (RTGWG) has been working on standardizing IP Fast Reroute (IPFRR) methods with a complete alternate path coverage. In this paper, a trade-off analysis of Multi Topology Routing (MTR) based IPFRR technologies targeting full coverage, namely Multiple Routing Configurations (MRC) and Maximally Redundant Trees (MRT), are presented. We implemented a comprehensive analysis tool to evaluate the performance of MRC and MRT mechanisms on various synthetic network topologies. The performance results show that MRT's alternative path lengths are not scalable with respect to the network size and density while the alternative path lengths of MRC only slightly change as the network size and density vary. We believe that this is an important scalability result providing a guidance in the selection of MTR-based IPFRR mechanism for improving the availability in ISP networks.},
keywords={computer network reliability;IP networks;next generation networks;quality of service;redundancy;telecommunication network routing;telecommunication network topology;trees (mathematics);multitopology routing;IP fast reroute mechanism;Routing Area Working Group;alternate path coverage;IPFRR;multiple routing configuration;maximally redundant tree;MRC;MRT;comprehensive analysis tool;synthetic network topology;network size;network density;ISP network;next generation service;Topology;Network topology;Routing;IP networks;Routing protocols;Nickel;Internet;IP fast reroute;multi topology routing;redundant tree},
doi={10.1109/INFCOM.2013.6567179},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567180,
author={H. Zhang and C. Papadopoulos and D. Massey},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Detecting encrypted botnet traffic},
year={2013},
volume={},
number={},
pages={3453-1358},
abstract={Bot detection methods that rely on deep packet inspection (DPI) can be foiled by encryption. Encryption, however, increases entropy. This paper investigates whether adding highentropy detectors to an existing bot detection tool that uses DPI can restore some of the bot visibility. We present two high-entropy classifiers, and use one of them to enhance BotHunter. Our results show that while BotHunter misses about 50% of the bots when they employ encryption, our high-entropy classifier restores most of its ability to detect bots, even when they use encryption.},
keywords={cryptography;entropy;BotHunter;bot visibility;bot detection tool;entropy;encryption;deep packet inspection;encrypted botnet traffic detection;Entropy;Encryption;Payloads;Detectors;IP networks;Malware},
doi={10.1109/INFCOM.2013.6567180},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567181,
author={V. Arnaboldi and M. Conti and A. Passarella and F. Pezzoni},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Ego networks in Twitter: An experimental analysis},
year={2013},
volume={},
number={},
pages={3459-3464},
abstract={Online Social Networks are amongst the most important platforms for maintaining social relationships online, supporting content generation and exchange between users. They are therefore natural candidate to be the basis of future humancentric networks and data exchange systems, in addition to novel forms of Internet services exploiting the properties of human social relationships. Understanding the structural properties of OSN and how they are influenced by human behaviour is thus fundamental to design such human-centred systems. In this paper we analyse a real Twitter data set to investigate whether well known structures of human social networks identified in “offline” environments can also be identified in the social networks maintained by users on Twitter. According to the well known model proposed by Dunbar, offline social networks are formed of circles of relationships having different social characteristics (e.g., intimacy, contact frequency and size). These circles can be directly ascribed to cognitive constraints of human brain, that impose limits on the number of social relationships maintainable at different levels of emotional closeness. Our results indicate that a similar structure can also be found in the Twitter users' social networks. This suggests that the structure of social networks also in online environments are controlled by the same cognitive properties of human brain that operate offline.},
keywords={Internet;social networking (online);social sciences computing;ego network;Twitter;experimental analysis;online social network;social relationship maintenance;content generation;humancentric network;data exchange system;Internet service;human social relationship;structural property;human-centred system;human social network;offline environment;offline social network;social characteristics;cognitive constraint;human brain;Twitter;Facebook;Indexes;Accuracy;Conferences;Communication networks},
doi={10.1109/INFCOM.2013.6567181},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567182,
author={S. Rallapalli and W. Dong and G. M. Lee and Y. Chen and L. Qiu},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Analysis and applications of smartphone user mobility},
year={2013},
volume={},
number={},
pages={3465-3470},
abstract={Users around the world have embraced new generation of mobile devices such as the smartphones at a remarkable rate. These devices are equipped with powerful communication and computation capabilities and they enable a wide range of exciting location-based services, e.g., location based ads, content prefetching etc. Many of these services can benefit from a better understanding of the smartphone user mobility, which may differ significantly from the general user mobility. Hence, previous works on understanding user mobility models and predicting user mobility may not directly apply to smartphone users. To overcome this, in this paper we analyze data from two popular location based social networks, where the users are real smartphone users and the places they check-in represent the typical locations where they use their smartphone applications. Specifically, we analyze how individual users move across different locations. We identify several factors that affect user mobility and their relative significance. We then leverage these factors to perform individual mobility prediction. We further show that our mobility prediction yields significant benefit to two important location based applications: content prefetching and shared ride recommendation.},
keywords={mobility management (mobile radio);smart phones;social networking (online);storage management;smartphone user mobility;mobile devices;location-based services;user mobility prediction;social networks-based popular location;real smartphone users;individual mobility prediction;content prefetching;shared ride recommendation;Prefetching;Accuracy;Cities and towns;Predictive models;Measurement;Wireless communication;Markov processes},
doi={10.1109/INFCOM.2013.6567182},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567183,
author={K. Benson and A. Dainotti and K. C. Claffy and E. Aben},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Gaining insight into AS-level outages through analysis of Internet background radiation},
year={2013},
volume={},
number={},
pages={3471-3476},
abstract={Internet Background Radiation (IBR) is unsolicited network traffic mostly generated by malicious software, e.g., worms, scans. In previous work, we extracted a signal from IBR traffic arriving at a large (/8) segment of unassigned IPv4 address space to identify large-scale disruptions of connectivity at an Autonomous System (AS) granularity, and used our technique to study episodes of government censorship and natural disasters [1]. Here we explore other IBR-derived metrics that may provide insights into the causes of macroscopic connectivity disruptions. We propose metrics indicating packet loss (e.g., due to link congestion) along a path from a specific AS to our observation point. We use three case studies to illustrate how our metrics can help identify packet loss characteristics of an outage. These metrics could be used in the diagnostic component of a semiautomated system for detecting and characterizing large-scale outages.},
keywords={computer network security;Internet;invasive software;telecommunication traffic;AS-level outages;Internet background radiation analysis;IBR;network traffic;malicious software;autonomous system;IBR-derived metrics;macroscopic connectivity disruptions;packet loss;Packet loss;IP networks;Internet;Telescopes;Routing},
doi={10.1109/INFCOM.2013.6567183},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567184,
author={P. Vervier and O. Thonnard},
booktitle={2013 Proceedings IEEE INFOCOM},
title={SpamTracer: How stealthy are spammers?},
year={2013},
volume={},
number={},
pages={3477-3482},
abstract={The Internet routing infrastructure is vulnerable to the injection of erroneous routing information resulting in BGP hijacking. Some spammers, also known as fly-by spammers, have been reported using this attack to steal blocks of IP addresses and use them for spamming. Using stolen IP addresses may allow spammers to elude spam filters based on sender IP address reputation and remain stealthy. This remains a open conjecture despite some anecdotal evidences published several years ago. In order to confirm the first observations and reproduce the experiments at large scale, a system called SpamTracer has been developed to monitor the routing behavior of spamming networks using BGP data and IP/AS traceroutes. We then propose a set of specifically tailored heuristics for detecting possible BGP hijacks. Through an extensive experimentation on a six months dataset, we did find a limited number of cases of spamming networks likely hijacked. In one case, the network owner confirmed the hijack. However, from the experiments performed so far, we can conclude that the fly-by spammers phenomenon does not seem to currently be a significant threat.},
keywords={Internet;protocols;security of data;telecommunication network routing;unsolicited e-mail;SpamTracer;Internet routing infrastructure;BGP hijacking;fly-by spammers;stolen IP addresses;spam filters;sender IP address reputation;stealthy spammer;spamming networks routing behavior;BGP data;IP/AS traceroutes;Routing;IP networks;Monitoring;Unsolicited electronic mail;Internet;Feeds;Conferences},
doi={10.1109/INFCOM.2013.6567184},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{6567185,
author={L. Neudorfer and Y. Shavitt and N. Zilberman},
booktitle={2013 Proceedings IEEE INFOCOM},
title={Improving AS relationship inference using PoPs},
year={2013},
volume={},
number={},
pages={3483-3488},
abstract={The Internet is a complex network, comprised of thousands of interconnected Autonomous Systems. Considerable research is done in order to infer the undisclosed commercial relationships between ASes. These relationships, which have been commonly classified to four distinct Type of Relationships (ToRs), dictate the routing policies between ASes. These policies are a crucial part in understanding the Internet's traffic and behavior patterns. This work leverages Internet Point of Presence (PoP) level maps to improve AS ToR inference. We propose a method which uses PoP level maps to find complex AS relationships and detect anomalies on the AS relationship level. We present experimental results of using the method on ToR reported by CAIDA and report several types of anomalies and errors. The results demonstrate the benefits of using PoP level maps for ToR inference, requiring considerable less resources than other methods theoretically capable of detecting similar phenomena.},
keywords={Internet;telecommunication network routing;telecommunication traffic;ToR inference;CAIDA;PoP level maps;internet point-of-presence;Internet traffic;routing policy;interconnected autonomous system;PoPs;AS relationship inference;IP networks;Internet;Monitoring;Conferences;Routing;Databases;Educational institutions},
doi={10.1109/INFCOM.2013.6567185},
ISSN={0743-166X},
month={April},}


