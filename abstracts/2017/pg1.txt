@INPROCEEDINGS{8056944,
author={C. Tian and A. Munir and A. X. Liu and Y. Liu and Y. Li and J. Sun and F. Zhang and G. Zhang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Multi-tenant multi-objective bandwidth allocation in datacenters using stacked congestion control},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In datacenter networks, flows can have different performance objectives. We use a tenant-objective division to denote all flows of a tenant that share the same objective. Bandwidth allocation in datacenters should support not only performance isolation among divisions but also objective-oriented scheduling among flows within the same division. This paper studies the Multi-Tenant Multi-Objective (MT-MO) bandwidth allocation problem. To our best knowledge, no existing practical work support performance isolation and objective scheduling simultaneously. We propose Stacked Congestion Control (SCC), a distributed host-based bandwidth allocation design, where an underlay congestion control (UCC) layer handles contention among divisions, and a private congestion control (PCC) layer for each division optimizes its performance objective. Via the tenant-objective tunnel abstraction, SCC achieves weighted bandwidth sharing for each division in a distributed and transparent way. By adding a rate-limiting send queue in the ingress of each tunnel, mechanisms between performance isolation and objective scheduling are completely decoupled. We evaluate SCC both on a small-scale testbed and with large-scale NS-2 simulations. Compared to the direct coexistence cases, SCC reduces latency by up to 40% for Latency-Sensitive flows, deadline miss ratio by up to 3.2× for Deadline-Sensitive flows, and average flow-completion-time by up to 53% for Completion-Sensitive flows.},
keywords={bandwidth allocation;computer centres;computer networks;control engineering computing;queueing theory;telecommunication congestion control;telecommunication scheduling;telecommunication traffic;SCC;Latency-Sensitive flows;Deadline-Sensitive flows;Completion-Sensitive flows;Multitenant multiobjective bandwidth allocation;datacenter networks;objective-oriented scheduling;private congestion control layer;tenant-objective tunnel abstraction;MT-MO bandwidth allocation problem;stacked congestion control;distributed host-based bandwidth allocation design;underlay congestion control;UCC;private congestion control;PCC;large-scale NS-2 simulation;Channel allocation;Bandwidth;Switches;Interference;Optimal scheduling;Conferences;Servers},
doi={10.1109/INFOCOM.2017.8056944},
ISSN={},
month={May},}
@INPROCEEDINGS{8056945,
author={R. Yu and G. Xue and X. Zhang and D. Li},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Survivable and bandwidth-guaranteed embedding of virtual clusters in cloud data centers},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Cloud computing has emerged as a powerful and elastic platform for internet service hosting, yet it also draws concerns of the unpredictable performance of cloud-based services due to network congestion. To offer predictable performance, the virtual cluster abstraction of cloud services has been proposed, which enables allocation and performance isolation regarding both computing resources and network bandwidth in a simplified virtual network model. One issue arisen in virtual cluster allocation is the survivability of tenant services against physical failures. Existing works have studied virtual cluster backup provisioning with fixed primary embeddings, but have not considered the impact of primary embeddings on backup resource consumption. To address this issue, in this paper we study how to embed virtual clusters survivably in the cloud data center, by jointly optimizing primary and backup embeddings of the virtual clusters. We formally define the survivable virtual cluster embedding problem. We then propose a novel algorithm, which computes the most resource-efficient embedding given a tenant request. Since the optimal algorithm has high time complexity, we further propose a faster heuristic algorithm, which is several orders faster than the optimal solution, yet able to achieve similar performance. Besides theoretical analysis, we evaluate our algorithms via extensive simulations.},
keywords={cloud computing;computer centres;embedded systems;Internet;optimisation;resource allocation;telecommunication network reliability;telecommunication traffic;virtualisation;cloud data center;cloud computing;internet service hosting;cloud-based services;network congestion;virtual cluster abstraction;computing resources;network bandwidth;virtual cluster allocation;tenant services;virtual cluster backup;backup resource consumption;survivable virtual cluster embedding problem;virtual network model;Bandwidth;Cloud computing;Algorithm design and analysis;Heuristic algorithms;Clustering algorithms;Resource management;Virtual cluster;survivability;bandwidth guarantee},
doi={10.1109/INFOCOM.2017.8056945},
ISSN={},
month={May},}
@INPROCEEDINGS{8056946,
author={Y. Lu and G. Chen and L. Luo and K. Tan and Y. Xiong and X. Wang and E. Chen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={One more queue is enough: Minimizing flow completion time with explicit priority notification},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Ideally, minimizing the flow completion time (FCT) requires millions of priorities supported by the underlying network so that each flow has its unique priority. However, in production datacenters, the available switch priority queues for flow scheduling are very limited (merely 2 or 3). This practical constraint seriously degrades the performance of previous approaches. In this paper, we introduce Explicit Priority Notification (EPN), a novel scheduling mechanism which emulates fine-grained priorities (i.e., desired priorities or DP) using only two switch priority queues. EPN can support various flow scheduling disciplines with or without flow size information. We have implemented EPN on commodity switches and evaluated its performance with both testbed experiments and extensive simulations. Our results show that, with flow size information, EPN achieves comparable FCT as pFabric that requires clean-slate switch hardware. And EPN also outperforms TCP by up to 60.5% if it bins the traffic into two priority queues according to flow size. In information-agnostic setting, EPN outperforms PIAS with two priority queues by up to 37.7%. To the best of our knowledge, EPN is the first system that provides millions of priorities for flow scheduling with commodity switches.},
keywords={computer centres;packet switching;queueing theory;scheduling;telecommunication traffic;transport protocols;scheduling mechanism;switch priority queues;flow scheduling;TCP;clean-slate switch hardware;commodity switches;flow size information;fine-grained priorities;EPN;flow completion time;queue;Switches;Hardware;Processor scheduling;Dynamic scheduling;Conferences;Bandwidth},
doi={10.1109/INFOCOM.2017.8056946},
ISSN={},
month={May},}
@INPROCEEDINGS{8056947,
author={J. E and Y. Cui and P. Wang and Z. Li and C. Zhang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={CoCloud: Enabling efficient cross-cloud file collaboration based on inefficient web APIs},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Cloud storage services such as Dropbox have been widely used for file collaboration among multiple users. However, this desirable functionality is yet restricted to the “walled-garden” of each service. At present, the only effective approach to cross-cloud file collaboration seems to be using web APIs, whose performance is known to be highly unstable and unpredictable. Now that using inefficient web APIs is inevitable, in this paper we attempt to achieve sound user-perceived performance for cross-cloud file collaboration. This attempt is enabled by two key observations from real-world measurements. First, for each cloud, we are always able to deploy one or several nearby (client) proxies which can efficiently access the web APIs. Second, during file collaboration, significant similarity exists among different versions of a file. This can be exploited to substantially reduce inter-proxy traffic and thus shorten the data sync time. Guided by the observations, we design and implement an open-source prototype system called CoCloud. Currently, it supports file collaboration among four popular cloud storage services in the US and China. Its performance is well acceptable to users under representative workloads, even approaching or exceeding intra-cloud performance in many cases.},
keywords={application program interfaces;cloud computing;public domain software;storage management;Web API;cloud storage services;cross-cloud file collaboration;user-perceived performance;interproxy traffic reduction;data sync time reduction;open-source prototype system;CoCloud;US;China;intracloud performance;Collaboration;Cloud computing;Synchronization;Servers;Protocols;Google;Metadata},
doi={10.1109/INFOCOM.2017.8056947},
ISSN={},
month={May},}
@INPROCEEDINGS{8056948,
author={C. Chen and W. Wang and S. Zhang and B. Li},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Cluster fair queueing: Speeding up data-parallel jobs with delay guarantees},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Cluster scheduler serves as a critical component to data-parallel systems in datacenters. Ideally, a scheduler should provide predictable performance with guarantees on the maximal job completion delay, while at the same time ensuring the minimal mean response time. Practically however, performance predictability and optimality are often conflicting with each other. The results often are a plethora of scheduling policies that either achieve predictable performance at the expense of long response times (e.g., max-min fairness), or run the risk of starving some jobs to obtain the minimal mean response time (e.g., Shortest Remaining Processing Time First). To address these problems, we develop a new scheduler, Cluster Fair Queueing (CFQ), which preferentially offers resources to jobs that complete the earliest under a fair sharing policy. We show that CFQ is able to minimize the mean response time while at the same time ensuring jobs to finish within a constant time after their completion under fair sharing. Our Spark deployment on a 100-node EC2 cluster demonstrates that compared to the built-in fair scheduler, CFQ can decrease the mean response time by 40%, which speeds up more than 40% of jobs by over 75% on average.},
keywords={queueing theory;scheduling;CFQ;fair sharing policy;constant time;fair scheduler;Cluster fair queueing;data-parallel jobs;delay guarantees;Cluster scheduler;data-parallel systems;performance predictability;scheduling policies;max-min fairness;Shortest Remaining Processing Time First;Cluster Fair Queueing;mean response time;job completion delay;Spark deployment;EC2 cluster;Time factors;Sparks;Resource management;Delays;Global Positioning System;Prediction algorithms},
doi={10.1109/INFOCOM.2017.8056948},
ISSN={},
month={May},}
@INPROCEEDINGS{8056949,
author={L. Chen and S. Liu and B. Li and B. Li},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Scheduling jobs across geo-distributed datacenters with max-min fairness},
year={2017},
volume={},
number={},
pages={1-9},
abstract={It has become routine for large volumes of data to be generated, stored, and processed across geographically distributed datacenters. To run a single data analytic job on such geo-distributed data, recent research proposed to distribute its tasks across datacenters, considering both data locality and network bandwidth across datacenters. Yet, it remains an open problem in the more general case, where multiple analytic jobs need to fairly share the resources at these geo-distributed data-centers. In this paper, we focus on the problem of assigning tasks belonging to multiple jobs across datacenters, with the specific objective of achieving max-min fairness across jobs sharing these datacenters, in terms of their job completion times. We formulate this problem as a lexicographical minimization problem, which is challenging to solve in practice due to its inherent multi-objective and discrete nature. To address these challenges, we iteratively solve its single-objective subproblems, which can be transformed to equivalent linear programming (LP) problems to be efficiently solved, thanks to their favorable properties. As a highlight of this paper, we have designed and implemented our proposed solution as a fair job scheduler based on Apache Spark, a modern data processing framework. With extensive evaluations of our real-world implementation on Amazon EC2, we have shown convincing evidence that max-min fairness has been achieved using our new job scheduler.},
keywords={cloud computing;computer centres;data analysis;linear programming;minimax techniques;scheduling;max-min fairness;geographically distributed datacenters;data locality;network bandwidth;job completion times;lexicographical minimization problem;single-objective subproblems;fair job scheduler;data centers;linear programming problems;data processing framework;geo-distributed datacenters;data analytic job scheduling;Apache Spark;Amazon EC2;Bandwidth;Data analysis;Sparks;Distributed databases;Minimization;Conferences},
doi={10.1109/INFOCOM.2017.8056949},
ISSN={},
month={May},}
@INPROCEEDINGS{8056950,
author={C. Li and D. Feng and Y. Hua and W. Xia and L. Qin and Y. Huang and Y. Zhou},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={BAC: Bandwidth-aware compression for efficient live migration of virtual machines},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Live migration of virtual machines (VM) is one of the key characteristics of virtualization for load balancing, system maintenance, power management, etc., in data centers or clusters. In order to reduce the data transferred and shorten the migration time, the compression techniques have been widely used to accelerate VM migration. However, different compression approaches have different compression ratios and speeds. Because there is a trade-off between compression and transmission, the migration performance improvements obtained from different compression approaches are differentiated, and the improvements vary with the network bandwidth. Besides, the compression window sizes used in most compression algorithms are typically much larger than a single page size, so the traditional single page compression loses some potential compression benefits. In this paper, we design and implement a Bandwidth-Aware Compression (BAC) scheme for VM migration. BAC chooses suitable compression approach according to the network bandwidth available for the migration process, and employs multi-page compression. These features make BAC obtain more migration performance improvements from compression. Experiments under various network scenarios demonstrate that, compared with conventional compression approaches, BAC shortens the total migration time while achieving comparable performance for the total data transferred and the downtime.},
keywords={computer centres;data compression;resource allocation;virtual machines;workstation clusters;BAC;efficient live migration;virtual machines;data centers;clusters;VM migration;migration performance improvements;network bandwidth;compression window;single page size;traditional single page compression;Bandwidth-Aware Compression scheme;migration process;multipage compression;total migration time;compression ratios;Bandwidth;Acceleration;Kernel;Virtualization;Load management;Maintenance engineering},
doi={10.1109/INFOCOM.2017.8056950},
ISSN={},
month={May},}
@INPROCEEDINGS{8056951,
author={A. O. F. Atya and Z. Qian and S. V. Krishnamurthy and T. L. Porta and P. McDaniel and L. Marvel},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Malicious co-residency on the cloud: Attacks and defense},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Attacker VMs try to co-reside with victim VMs on the same physical infrastructure as a precursor to launching attacks that target information leakage. VM migration is an effective countermeasure against attempts at malicious co-residency. In this paper, we first undertake an experimental study on Amazon EC2 to obtain an in-depth understanding of the side-channels an attacker can use to ascertain co-residency with a victim. Here, we identify a new set of stealthy side-channel attacks which, we show to be more effective than currently available attacks towards verifying co-residency. Based on the study, we develop a set of guidelines to determine under what conditions victim VM migrations should be triggered given performance costs in terms of bandwidth and downtime, that a user is willing to bear. Via extensive experiments on our private in-house cloud, we show that migrations using our guidelines can limit the fraction of the time that an attacker VM co-resides with a victim VM to about 1 % of the time with bandwidth costs of a few MB and downtimes of a few seconds, per day per VM migrated.},
keywords={cloud computing;security of data;virtual machines;malicious co-residency;attacker VM;physical infrastructure;launching attacks;VM migration;Amazon EC2;stealthy side-channel attacks;in-house cloud;information leakage;victim VM;bandwidth costs;Side-channel attacks;Timing;Guidelines;Bandwidth;Cloud computing;Conferences;Virtual machine monitors},
doi={10.1109/INFOCOM.2017.8056951},
ISSN={},
month={May},}
@INPROCEEDINGS{8056952,
author={W. Sun and N. Zhang and W. Lou and Y. T. Hou},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={When gene meets cloud: Enabling scalable and efficient range query on encrypted genomic data},
year={2017},
volume={},
number={},
pages={1-9},
abstract={As the cost of human full genome sequencing continues to fall, we will soon witness a prodigious amount of human genomic data in the public cloud. To protect the confidentiality of the genetic information of individuals, the data has to be encrypted at rest. On the other hand, encryption severely hinders the use of this valuable information, such as Genome-wide Range Query (GRQ), in medical/genomic research. While the problem of secure range query on outsourced encrypted data has been extensively studied, the current schemes are far from practical deployment in terms of efficiency and scalability due to the data volume in human genome sequencing. In this paper, we investigate the problem of secure GRQ over human raw aligned genomic data in a third-party outsourcing model. Our solution contains a novel secure range query scheme based on multi-keyword symmetric searchable encryption (MSSE). The proposed scheme incurs minimal ciphertext expansion and computation overhead. We also present a hierarchical GRQ-oriented secure index structure tailored for efficient and large-scale genomic data lookup in the cloud while preserving the query privacy. Our experiment on real human genomic data shows that a secure GRQ request with range size 100,000 over more than 300 million encrypted short reads takes less than 3 minutes, which is orders of magnitude faster than existing solutions.},
keywords={biology computing;cloud computing;cryptography;data privacy;genetics;genomics;outsourcing;query processing;GRQ;MSSE;scalable range query;GRQ request;human genomic data;ciphertext expansion;secure index structure;hierarchical GRQ;multikeyword symmetric searchable encryption;secure range query scheme;third-party outsourcing model;human raw aligned genomic data;human genome sequencing;data volume;outsourced encrypted data;medical/genomic research;Genome-wide Range Query;public cloud;human full genome sequencing;encrypted genomic data;query privacy;large-scale genomic data lookup;Genomics;Bioinformatics;Encryption;Sequential analysis;Cloud computing},
doi={10.1109/INFOCOM.2017.8056952},
ISSN={},
month={May},}
@INPROCEEDINGS{8056953,
author={X. Li and Q. Xue and M. C. Chuah},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={CASHEIRS: Cloud assisted scalable hierarchical encrypted based image retrieval system},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Image retrieval has become an important function in many emerging computer vision applications e.g. online shopping via images, medical health care systems. More and more images are being generated and stored in public clouds. However, recent photo leakage events raise concerns about privacy leaks for images stored in public clouds. In this paper, we present an efficient scalable hierarchical image retrieval system (CASHEIRS) which provides privacy-aware image retrieval feature. CASHEIRS employs transformed Convolutional Neural Network features to improve image retrieval accuracy and an encrypted hierarchical index tree to speed up the query process. Extensive evaluations using Caltech256 and INRIA Holiday datasets show that CASHEIRS is more effective than three existing schemes. We also demonstrate its practicality on a mobile device.},
keywords={cloud computing;computer vision;content-based retrieval;data privacy;health care;image retrieval;neural nets;privacy leaks;public clouds;privacy-aware image retrieval feature;Convolutional Neural Network features;image retrieval accuracy;encrypted hierarchical index tree;medical health care systems;computer vision applications;photo leakage events;scalable hierarchical image retrieval system;CASHEIRS system;hierarchical index tree;Caltech256 dataset;INRIA Holiday dataset;Cryptography;Image retrieval;Indexes;Servers;Feature extraction;Cloud computing;Binary codes;Hierarchical image retrieval;Convolutional Neural Network;image privacy},
doi={10.1109/INFOCOM.2017.8056953},
ISSN={},
month={May},}
@INPROCEEDINGS{8056954,
author={L. Yang and Q. Zheng and X. Fan},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={RSPP: A reliable, searchable and privacy-preserving e-healthcare system for cloud-assisted body area networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The integration of cloud computing and Internet of Things (loT) is quickly becoming the key enabler for the digital transformation of the healthcare industry by offering comprehensive improvements in patient engagements, productivity and risk mitigation. This paradigm shift, while bringing numerous benefits and new opportunities to healthcare organizations, has raised a lot of security and privacy concerns. In this paper, we present a reliable, searchable and privacy-preserving e-healthcare system, which takes advantage of emerging cloud storage and IoT infrastructure and enables healthcare service providers (HSPs) to realize remote patient monitoring in a secure and regulatory compliant manner. Our system is built upon a novel dynamic searchable symmetric encryption scheme with forward privacy and delegated verifiability for periodically generated healthcare data. While the forward privacy is achieved by maintaining an increasing counter for each keyword at an IoT gateway, the data owner delegated verifiability comes from the combination of the Bloom filter and aggregate message authentication code. Moreover, our system is able to support multiple HSPs through either data owner assistance or delegation. The detailed security analysis as well as the extensive simulations on a large data set with millions of records demonstrate the practical efficiency of the proposed system for real world healthcare applications.},
keywords={body area networks;cloud computing;cryptography;data privacy;health care;medical computing;message authentication;patient monitoring;cloud-assisted body area networks;cloud computing;healthcare industry;privacy concerns;cloud storage;healthcare service providers;dynamic searchable symmetric encryption scheme;forward privacy;periodically generated healthcare data;data owner assistance;world healthcare applications;security concerns;privacy-preserving e-healthcare system;security analysis;HSP;Bloom filter;message authentication code;Cloud computing;Servers;Medical services;Encryption;Reliability},
doi={10.1109/INFOCOM.2017.8056954},
ISSN={},
month={May},}
@INPROCEEDINGS{8056955,
author={Q. Wang and S. Hu and M. Du and J. Wang and K. Ren},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Learning privately: Privacy-preserving canonical correlation analysis for cross-media retrieval},
year={2017},
volume={},
number={},
pages={1-9},
abstract={A massive explosion of various types of data has been triggered in the “Big Data” era. In big data systems, machine learning plays an important role due to its effectiveness in discovering hidden information and valuable knowledge. Data privacy, however, becomes an unavoidable concern since big data usually involve multiple organizations, e.g., different healthcare systems and hospitals, who are not in the same trust domain and may be reluctant to share their data publicly. Applying traditional cryptographic tools is a straightforward approach to protect sensitive information, but it often renders learning algorithms useless inevitably. In this work, we, for the first time, propose a novel privacy-preserving scheme for canonical correlation analysis (CCA), which is a well-known learning technique and has been widely used in cross-media retrieval system. We first develop a library of building blocks to support various arithmetics over encrypted real numbers by leveraging additively homomorphic encryption and garbled circuits. Then we encrypt private data by randomly splitting the numerical data, formalize CCA problem and reduce it to a symmetric eigenvalue problem by designing new protocols for privacy-preserving QR decomposition. Finally, we solve all the eigenvalues and the corresponding eigenvectors by running Newton-Raphson method and inverse power method over the ciphertext domain. We carefully analyze the security and extensively evaluate the effectiveness of our design. The results show that our scheme is practically secure, incurs negligible errors compared with performing CCA in the clear and performs comparably in cross-media retrieval systems.},
keywords={Big Data;cryptography;data privacy;eigenvalues and eigenfunctions;information retrieval;learning (artificial intelligence);Newton-Raphson method;Privacy-preserving canonical correlation analysis;big data systems;machine learning;hidden information;Data privacy;hospitals;trust domain;cross-media retrieval system;encrypted real numbers;garbled circuits;private data;numerical data;CCA problem;symmetric eigenvalue problem;privacy-preserving QR decomposition;inverse power method;health care systems;cryptographic tools;homomorphic encryption;privacy-preserving scheme;Cryptography;Correlation;Servers;Wires;Eigenvalues and eigenfunctions;Protocols;Big Data},
doi={10.1109/INFOCOM.2017.8056955},
ISSN={},
month={May},}
@INPROCEEDINGS{8056956,
author={P. Li and H. Dau and G. Puleo and O. Milenkovic},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Motif clustering and overlapping clustering for social network analysis},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Motivated by applications in social network community analysis, we introduce a new clustering paradigm termed motif clustering. Unlike classical clustering, motif clustering aims to minimize the number of clustering errors associated with both edges and certain higher order graph structures (motifs) that represent “atomic units” of social organizations. Our contributions are two-fold: We first introduce motif correlation clustering, in which the goal is to agnostically partition the vertices of a weighted complete graph so that certain predetermined “important” social subgraphs mostly lie within the same cluster, while “less relevant” social subgraphs are allowed to lie across clusters. We then proceed to introduce the notion of motif covers, in which the goal is to cover the vertices of motifs via the smallest number of (near) cliques in the graph. Motif cover algorithms provide a natural solution for overlapping clustering and they also play an important role in latent feature inference of networks. For both motif correlation clustering and its extension introduced via the covering problem, we provide hardness results, algorithmic solutions and community detection results for two well-studied social networks.},
keywords={graph theory;network theory (graphs);pattern clustering;motif cover algorithms;motif correlation clustering;social networks;motif clustering;social network analysis;social network community analysis;clustering paradigm;clustering errors;social organizations;motif covers;social subgraphs;overlapping clustering;higher order graph structures;weighted complete graph;Correlation;Clustering algorithms;Social network services;Conferences;Clustering methods;Approximation algorithms;Linear programming},
doi={10.1109/INFOCOM.2017.8056956},
ISSN={},
month={May},}
@INPROCEEDINGS{8056957,
author={G. A. Tong and W. Wu and L. Guo and D. Li and C. Liu and B. Liu and D. Du},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={An efficient randomized algorithm for rumor blocking in online social networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Social networks allow rapid spread of ideas and innovations while the negative information can also propagate widely. When the cascades with different opinions reaching the same user, the cascade arriving first is the most likely to be taken by the user. Therefore, once misinformation or rumor is detected, a natural containment method is to introduce a positive cascade competing against the rumor. Given a budget k, the rumor blocking problem asks for k seed users to trigger the spread of the positive cascade such that the number of the users who are not influenced by rumor can be maximized. The prior works have shown that the rumor blocking problem can be approximated within a factor of (1 - 1/e- δ) by a classic greedy algorithm combined with Monte Carlo simulation with the running time of O(k3mn ln n/δ2), where n and m are the number of users and edges, respectively. Unfortunately, the Monte-Carlo-simulation-based methods are extremely time consuming and the existing algorithms either trade performance guarantees for practical efficiency or vice versa. In this paper, we present a randomized algorithm which runs in O(km ln n/δ2) expected time and provides a (1 - 1/e - δ)-approximation with a high probability. The experimentally results on both the real-world and synthetic social networks have shown that the proposed randomized rumor blocking algorithm is much more efficient than the state-of-the-art method and it is able to find the seed nodes which are effective in limiting the spread of rumor.},
keywords={greedy algorithms;Monte Carlo methods;randomised algorithms;social networking (online);online social networks;natural containment method;positive cascade;rumor blocking problem;classic greedy algorithm;Monte Carlo simulation;synthetic social networks;randomized rumor blocking algorithm;randomized algorithm;Social network services;Integrated circuit modeling;Approximation algorithms;Diffusion processes;Monte Carlo methods;Sampling methods},
doi={10.1109/INFOCOM.2017.8056957},
ISSN={},
month={May},}
@INPROCEEDINGS{8056958,
author={J. Ok and J. Shin and Y. Yi},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Incentivizing strategic users for social diffusion: Quantity or quality?},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We consider a problem of how to effectively diffuse a new product over social networks by incentivizing selfish users. Traditionally, this problem has been studied in the form of influence maximization via seeding, where most prior work assumes that seeded users unconditionally and immediately start by adopting the new product and they stay at the new product throughout their lifetime. However, in practice, seeded users often adjust the degree of their willingness to diffuse, depending on how much incentive is given. To address such diffusion willingness, we propose a new incentive model and characterize the speed of diffusion as the value of a combinatorial optimization. Then, we apply the characterization to popular network graph topologies (Erdos-Renyi, planted partition and power law graphs) as well as general ones, for asymptotically computing the diffusion time for those graphs. Our analysis shows that the diffusion time undergoes two levels of order-wise reduction, where the first and second one are solely contributed by the number of seeded users, i.e., quantity, and the amount of incentives, i.e., quality, respectively. In other words, it implies that the best strategy given budget is (a) first identify the minimum seed set depending on the underlying graph topology, and (b) then assign largest possible incentives to users in the set. We believe that our theoretical results provide useful implications and guidelines for designing successful advertising strategies in various practical applications.},
keywords={complex networks;graph theory;network theory (graphs);social sciences;social networks;selfish users;seeded users;incentive model;power law graphs;diffusion time;minimum seed set;strategic users;social diffusion;graph topology;network graph topologies;Erdos-Renyi graph;planted partition graph;Games;Social network services;Topology;Noise measurement;Algorithm design and analysis;Conferences;Network topology},
doi={10.1109/INFOCOM.2017.8056958},
ISSN={},
month={May},}
@INPROCEEDINGS{8056959,
author={M. T. A. Amin and C. Aggarwal and S. Yao and T. Abdelzaher and L. Kaplan},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Unveiling polarization in social networks: A matrix factorization approach},
year={2017},
volume={},
number={},
pages={1-9},
abstract={This paper presents unsupervised algorithms to uncover polarization in social networks (namely, Twitter) and identify polarized groups. The approach is language-agnostic and thus broadly applicable to global and multilingual media. In cases of conflict, dispute, or situations involving multiple parties with contrasting interests, opinions get divided into different camps. Previous manual inspection of tweets has shown that such situations produce distinguishable signatures on Twitter, as people take sides leading to clusters that preferentially propagate information confirming their individual cluster-specific bias. We propose a model for polarized social networks, and show that approaches based on factorizing the matrix of sources and their claims can automate the discovery of polarized clusters with no need for prior training or natural language processing. In turn, identifying such clusters offers insights into prevalent social conflicts and helps automate the generation of less biased descriptions of ongoing events. We evaluate our factorization algorithms and their results on multiple Twitter datasets involving polarization of opinions, demonstrating the efficacy of our approach. Experiments show that our method is almost always correct in identifying the polarized information from real-world twitter traces, and outperforms the baseline mechanisms by a large margin.},
keywords={matrix decomposition;social networking (online);unveiling polarization;matrix factorization approach;unsupervised algorithms;polarized groups;global media;multilingual media;distinguishable signatures;individual cluster-specific bias;polarized social networks;polarized clusters;prevalent social conflicts;biased descriptions;factorization algorithms;multiple Twitter datasets;polarized information;real-world twitter traces;social networks;language-agnostic;Twitter;Algorithm design and analysis;Media;Natural language processing;Conferences;Clustering algorithms},
doi={10.1109/INFOCOM.2017.8056959},
ISSN={},
month={May},}
@INPROCEEDINGS{8056960,
author={Q. Chen and H. Gao and S. Cheng and J. Li and Z. Cai},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Distributed non-structure based data aggregation for duty-cycle wireless sensor networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Data aggregation is an essential operation for the sink to obtain summary information in a Wireless Sensor Network (WSN). The problem of Minimum Latency Aggregation Schedule (MLAS) which seeks a fastest and collision-free aggregation schedule has been well studied when nodes are always awake. However, in duty-cycle WSNs, nodes can only receive data in active state. In such networks, it is of great importance to exploit the limited active time slots to reduce aggregation latency. Unfortunately, few studies have addressed this issue and most previous aggregation methods rely on fixed structures which greatly limit the exploitation of the active time slots from other neighbors. In this paper, we investigate the MLAS problem in duty-cycle WSNs without considering structures. We propose the first distributed aggregation algorithm for duty-cycle WSNs, in which the aggregation tree and a conflict-free schedule are generated simultaneously. Compared with the previous centralized and distributed methods, the aggregation latency and the utilization ratio of available time slots are greatly improved. The theoretical analysis and simulation results verify that the proposed algorithm has high performance in terms of latency and communication cost.},
keywords={data aggregation;telecommunication scheduling;trees (mathematics);wireless sensor networks;Wireless Sensor Network;Minimum Latency Aggregation Schedule;duty-cycle WSNs;active state;active time slots;fixed structures;MLAS problem;distributed aggregation algorithm;aggregation tree;conflict-free schedule;distributed nonstructure based data aggregation;duty-cycle wireless sensor networks;aggregation methods;centralized distributed methods;time slots;collision-free aggregation schedule;Schedules;Wireless sensor networks;Data aggregation;Interference;Scheduling algorithms;Approximation algorithms;Conferences},
doi={10.1109/INFOCOM.2017.8056960},
ISSN={},
month={May},}
@INPROCEEDINGS{8056961,
author={K. S. Liu and T. Mayer and H. T. Yang and E. Arkin and J. Gao and M. Goswami and M. P. Johnson and N. Kumar and S. Lin},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Joint sensing duty cycle scheduling for heterogeneous coverage guarantee},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In this paper we study the following problem: given a set of m sensors that collectively cover a set of n target points with heterogeneous coverage requirements (target j needs to be covered every fjslots), how to schedule the sensor duty cycles such that all coverage requirements are satisfied and the maximum number of sensors turned on at any time slot is minimized. The problem models varied real-world applications in which sensing tasks exhibit high discrepancy in coverage requirements - critical locations often need to be covered much more frequently. We provide multiple algorithms with best approximation ratio of O (log n + log m) for the maximum number of sensors to turn on, and bi-criteria algorithm with (α, β)-approximation factors with high probability, where the number of sensors turned on is an α = O(δ(log (n) + log(m))/β)-approximation of the optimal (satisfying all requirements) and the coverage requirement is a β-approximation; δ is the approximation ratio achievable in an appropriate instance of set multi-cover. When the sensor coverage exhibits extra geometric properties, the approximation ratios can be further improved. We also evaluated our algorithms via simulations and experiments on a camera testbed. The performance improvement (energy saving) is substantial compared to turning on all sensors all the time, or a random scheduling baseline.},
keywords={approximation theory;probability;sensor placement;telecommunication power management;telecommunication scheduling;bicriteria algorithm;sensor duty cycles;heterogeneous coverage requirements;joint sensing duty cycle scheduling;Sensors;Schedules;Approximation algorithms;Optimal scheduling;Monitoring;Computer science;Cameras},
doi={10.1109/INFOCOM.2017.8056961},
ISSN={},
month={May},}
@INPROCEEDINGS{8056962,
author={K. Fu and W. Ren and W. Dong},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Multihop calibration for mobile sensing: K-hop Calibratability and reference sensor deployment},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Mobile vehicles, equipped with low-cost sensors, can provide unprecedented opportunities for urban monitoring with a wide coverage. The quality of collected data is very important for many applications. The low-cost mobile sensors can, however, suffer from limited accuracy, high instability, and sensor drift. Therefore, they need to be frequently calibrated to preserve a good data quality. Frequent sensor calibration can be achieved by deploying static high-precision sensors (called reference sensors) and exploiting meeting points (i.e., rendezvous) between the reference sensors and mobile sensors. A calibrated mobile sensor can also be used to calibrate an uncalibrated mobile sensor when they meet, referred to as multihop calibration. In this paper, we introduce a novel concept, k-hop calibratability, in the context of multihop calibration: a sensor is k-hop calibratable if the calibration path is no larger than k. We consider the problem of how to deploy the reference sensors to ensure that all sensors in the network are k-hop calibratable. To address this problem, we formally define the rendezvous connection graph to precisely describe the relationship between mobile sensors and (virtual) reference sensors. Based on this definition, we formulate the reference sensor deployment problem as a set cover problem. We also extend our problem to consider practical deployment requirements. We propose efficient algorithms and conduct an extensive evaluation in the context of mobile air quality monitoring. We present a detailed prototype implementation of the mobile sensors and reference sensors. Evaluations using real-world datasets show the effectiveness of the algorithms.},
keywords={calibration;mobile radio;sensor placement;wireless sensor networks;low-cost mobile sensors;sensor drift;frequent sensor calibration;high-precision sensors;multihop calibration;k-hop calibratability;reference sensor deployment problem;mobile air quality monitoring;mobile sensing;rendezvous connection graph;mobile vehicles;urban monitoring;Mobile communication;Calibration;Robot sensing systems;Monitoring;Air quality;Urban areas},
doi={10.1109/INFOCOM.2017.8056962},
ISSN={},
month={May},}
@INPROCEEDINGS{8056963,
author={Y. Wu and Y. Wang and G. Cao},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Photo crowdsourcing for area coverage in resource constrained environments},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Photos crowdsourced from mobile devices can be used in many applications such as disaster recovery to obtain information about a target area. However, such applications often have resource constraints in terms of bandwidth, storage, and processing capability, which limit the number of photos that can be crowdsourced. Thus, it is a challenge to use the limited resources to crowdsource photos that best cover the target area. In this paper, we leverage various geographical and geometrical information about photos, called metadata, to address this challenge. Metadata includes the location, orientation, field of view, and range of a camera. Based on metadata, we define photo utility to measure how well a target area is covered by a set of photos. We propose various techniques to analyze such coverage and calculate photo utility accurately and efficiently. We also study the problem of selecting photos with the largest utility under a resource budget, and propose an efficient algorithm that achieves constant approximation ratio. With our design, the crowdsourcing server can select photos based on metadata instead of real images, and thus use the limited resources to crowdsource the most useful photos. Both simulation and experimental results demonstrate the effectiveness of our design.},
keywords={approximation theory;computational complexity;emergency management;geophysical image processing;meta data;mobile computing;photo crowdsourcing;area coverage;resource constrained environments;metadata;mobile devices;disaster recovery;geographical information;geometrical information;resource budget;constant approximation ratio;Metadata;Cameras;Crowdsourcing;Bandwidth;Servers;Mobile handsets;Conferences},
doi={10.1109/INFOCOM.2017.8056963},
ISSN={},
month={May},}
@INPROCEEDINGS{8056964,
author={Y. Zhuo and H. Zhu and H. Xue and S. Chang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Perceiving accurate CSI phases with commodity WiFi devices},
year={2017},
volume={},
number={},
pages={1-9},
abstract={WiFi technology has gained a wide prevalence for not only wireless communication but also pervasive sensing. A wide variety of emerging applications leverage accurate measurements of the Channel State Information (CSI) information obtained from commodity WiFi devices. Due to hardware imperfection of commodity WiFi devices, the frequency response of internal signal processing circuit is mixed with the real channel frequency response in passband, which makes deriving accurate channel frequency response from CSI measurements a challenging task. In this paper, we identify non-negligible non-linear CSI phase errors and report that IQ imbalance is the root source of non-linear CSI phase errors. We conduct intensive analysis on the characteristics of such non-linear errors and find that such errors are prevalent among various WiFi devices. Furthermore, they are rather stable along time and the received signal strength indication (RSSI) but sensitive to frequency bands used between a transmission pair. Based on these key observations, we propose new calibration methods to compensate both non-linear and linear CSI phase errors. We demonstrate the efficacy of the proposed methods by applying them in CSI splicing. Results of extensive real-world experiments indicate that accurate CSI phase measurements can significantly improve the performance of splicing and the stability of the derived power delay profiles (PDPs).},
keywords={calibration;frequency response;phase measurement;RSSI;signal processing;wireless channels;wireless LAN;nonlinear CSI phase errors;CSI splicing;perceiving accurate CSI phases;commodity WiFi devices;WiFi technology;internal signal processing circuit;Channel State Information;channel frequency response;CSI phase measurements;IQ imbalance;received signal strength indication;RSSI;power delay profiles;calibration methods;Frequency measurement;Phase measurement;Wireless fidelity;Receivers;Measurement uncertainty;Frequency response;Delays;Channel State Information (CSI);non-linear phase errors;rotation phase error;empirical study;CSI splicing},
doi={10.1109/INFOCOM.2017.8056964},
ISSN={},
month={May},}
@INPROCEEDINGS{8056965,
author={S. Byeon and K. Yoon and C. Yang and S. Choi},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={STRALE: Mobility-aware PHY rate and frame aggregation length adaptation in WLANs},
year={2017},
volume={},
number={},
pages={1-9},
abstract={IEEE 802.11n/ac wireless local area network (WLAN) supports frame aggregation, called aggregate medium access control (MAC) protocol data unit (A-MPDU), to enhance MAC efficiency by reducing protocol overhead. However, the current channel estimation process conducted only once during the preamble reception is known to be insufficient to ensure robust delivery of long A-MPDU frames in mobile environments. To cope with this problem, we first build a model which represents the impact of mobility with a noise vector in the I-Q plane, and then analyze how the mobility affects the A-MPDU reception performance. Based on our analysis, we develop STRALE, a standard-compliant and mobility-aware PHY rate and A-MPDU length adaptation scheme with ease of implementation. Through extensive simulations with 802.11ac using ns-3 and prototype implementation with commercial 802.11n devices, we demonstrate that STRALE achieves up to 2.9x higher throughput, compared to a fixed duration setting according to IEEE 802.11 standard.},
keywords={access protocols;channel estimation;mobility management (mobile radio);wireless channels;wireless LAN;STRALE;channel estimation;mobility-aware PHY rate aggregation length adaptation;mobility-aware PHY frame aggregation length adaptation;IEEE 802.11n/ac wireless local area network;aggregate medium access control protocol data unit;A-MPDU length adaptation scheme;Wireless LAN;Channel estimation;OFDM;Signal to noise ratio;Protocols;Receivers;MIMO},
doi={10.1109/INFOCOM.2017.8056965},
ISSN={},
month={May},}
@INPROCEEDINGS{8056966,
author={S. Han and K. G. Shin},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Enhancing wireless performance using reflectors},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Signal decay is the fundamental problem of wireless communications, especially in an indoor environment where line-of-sight (LOS) paths for signal propagation are often blocked and various indoor objects exacerbate signal fading. There are three reasons for signal decay: long transmission distance, signal penetration, and reflection. In this paper, we propose OptRe which optimally places metallic reflectors - providing a highly reflective surface that can reflect impinging signals almost 100% - in indoor environments to reduce the reflection loss and enhance wireless transmissions. It enhances both WiFi signal and low-power IoT devices without changing their configurations or network protocols. To enable OptRe, we first develop an empirical signal propagation model that can accurately estimate the signal strength and adapt itself to the reflectors' location. Using micro-benchmarks, our empirical signal propagation model is shown to be more accurate than the other existing path loss models. We also optimally place reflectors to maximize the worst-case signal coverage within the target indoor areas. Our extensive experimental evaluation results have shown OptRe to enhance signal strength for different types of wireless signals by almost 2x.},
keywords={indoor radio;Internet of Things;protocols;wireless LAN;wireless communications;indoor environment;line-of-sight paths;indoor objects exacerbate signal fading;signal decay;long transmission distance;signal penetration;OptRe;metallic reflectors;highly reflective surface;impinging signals;reflection loss;wireless transmissions;empirical signal propagation model;worst-case signal coverage;target indoor areas;wireless signals;path loss models;signal strength estimation;wireless performance enhancement;WiFi signal;low-power IoT devices;Wireless communication;Wireless fidelity;Adaptation models;Conferences;Indoor environments;Fading channels;Propagation losses},
doi={10.1109/INFOCOM.2017.8056966},
ISSN={},
month={May},}
@INPROCEEDINGS{8056967,
author={A. A. Renani and J. Huang and G. Xing and A. Esfahanian},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Harnessing hardware defects for improving wireless link performance: Measurements and applications},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The design trade-offs of transceiver hardware are crucial to the performance of wireless systems. In this paper, we present an in-depth study to characterize the surprisingly notable systemic impacts of low-pass filter (LPF) design, which is a small yet indispensable component used for shaping spectrum and rejecting interference. Using a bottom-up approach, we examine how signal-level distortions caused by the trade-off of LPF design propagate to the upper-layers of wireless communication, reshaping bit error patterns and degrading link performance of today's 802.11 systems. Moreover, we propose a novel algorithm that harnesses LPF defects for improving video streaming, which substantially enhances video quality in mobile environments.},
keywords={low-pass filters;radio links;radio transceivers;video streaming;wireless LAN;hardware defects;video streaming;802.11 systems;LPF defects;bit error patterns;wireless communication;LPF design propagate;signal-level distortions;rejecting interference;shaping spectrum;low-pass filter design;in-depth study;wireless systems;transceiver hardware;design trade-offs;wireless link performance;Distortion;IEEE 802.11 Standard;Distortion measurement;Wireless communication;Frequency measurement;Semiconductor device measurement;Hardware},
doi={10.1109/INFOCOM.2017.8056967},
ISSN={},
month={May},}
@INPROCEEDINGS{8056968,
author={G. C. Sankaran and K. M. Sivalingam},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Combinatorial approach for network switch design in data center networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={This paper deals with the efficient design of network switch/routers for an optical data center network. Each switch has multiple components such as ingress/egress interfaces, optical and/or electronic buffers, interconnection switching fabric and so on. There are several possible choices available for each of these components. This paper presents a systematic approach to designing the switch architecture using a combination of these component choices, while meeting specified design criteria. It requires formally defining the structure of a switch and enforcing semantics across components. This is formulated as a constraint optimization problem with formal language grammar guiding its search process. This problem formulation is used to identify the best-possible architecture for a hierarchical DCN. Two of the three solutions identified were new and were not reported in literature. These solutions were also validated experimentally.},
keywords={computer centres;formal languages;optical communication equipment;optical switches;telecommunication network routing;telecommunication traffic;network routers;hierarchical DCN;multiple components;optical data center network;network switch design;combinatorial approach;best-possible architecture;formal language grammar;constraint optimization problem;design criteria;switch architecture;systematic approach;Optical switches;Servers;Computer architecture;Grammar;Formal languages},
doi={10.1109/INFOCOM.2017.8056968},
ISSN={},
month={May},}
@INPROCEEDINGS{8056969,
author={S. Jia and X. Jin and G. Ghasemiesfeh and J. Ding and J. Gao},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Competitive analysis for online scheduling in software-defined optical WAN},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Modern planetary-scale online services have massive data to transfer over the wide area network (WAN). Due to the tremendous cost of building WANs and the stringent timing requirement of distributed applications, it is critical for network operators to make efficient use of network resources to optimize data transfers. By leveraging software-defined networking (SDN) and reconfigurable optical devices, recent solutions design centralized systems to jointly control the network layer and the optical layer. While these solutions show it is promising to significantly reduce data transfer times by centralized cross-layer control, they do not have any theoretical guarantees on the proposed algorithms. This paper presents approximation algorithms and theoretical analysis for the online transfer scheduling problem over optical WANs. The goal of the scheduling problem is to minimize the makespan (the time to finish all transfers) or the total sum of completion times. We design and analyze various greedy, online scheduling algorithms that can achieve 3-competitive ratio for makespan, 2-competitive ratio for minimum sum completion time for jobs of unit size, and 3α-competitive ratio for jobs of arbitrary transfer size and each node having degree constraint d, where α = 1 when d = 1 and α = 1.86 when d ≥ 2. We also evaluated the performance of these algorithms and compared the performance with prior heuristics.},
keywords={approximation theory;computational complexity;greedy algorithms;optical fibre networks;software defined networking;telecommunication scheduling;wide area networks;planetary-scale online services;wide area network;stringent timing requirement;distributed applications;network operators;network resources;data transfers;reconfigurable optical devices;network layer;optical layer;centralized cross-layer control;approximation algorithms;online transfer scheduling problem;greedy scheduling algorithms;online scheduling algorithms;3-competitive ratio;2-competitive ratio;minimum sum completion time;3α-competitive ratio;arbitrary transfer size;software-defined networking;software-defined optical WAN;centralized system design;Optical devices;Wide area networks;Optical fiber networks;Data transfer;Algorithm design and analysis;Schedules},
doi={10.1109/INFOCOM.2017.8056969},
ISSN={},
month={May},}
@INPROCEEDINGS{8056970,
author={Y. Nakayama and T. Tsutsumi and K. Maruta and K. Sezaki},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={ABSORB: Autonomous base station with optical reflex backhaul to adapt to fluctuating demand},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Metropolitan areas witness significant fluctuations in mobile traffic due to patterns of human mobility. This fluctuation drastically deteriorates the efficiency and financial viability of conventional maximum-based network design. If networks are deployed to deal with the peak traffic rate at each site, their capacities are underutilized for most of time. To improve the efficiency of deploying base stations (BSs), this paper proposes a concept of an Autonomous Base Station with Optical Reflex Backhaul (ABSORB) architecture that can adapt to fluctuations in mobile traffic. In the ABSORB architecture, traffic at demand nodes is forwarded to and from an ABS with an arbitrary radio access technology (RAT). An ABS is connected to a gateway node through ORB, which consists of fiber optic networks. ABSs move to new locations following the demand movement, according to a relocation schedule that is periodically rearranged by an ABSORB controller. The network is flexibly reconstructed according to the demand distribution. The ABSORB architecture can be employed in various networks, and can coexist with traditional static architectures. It will drastically reduce the number of BSs, total deployment cost, and power consumption in comparison with the traditional design.},
keywords={metropolitan area networks;mobile computing;mobility management (mobile radio);optical fibre networks;radio access networks;telecommunication network planning;telecommunication traffic;Autonomous base station;mobile traffic;human mobility;Autonomous Base Station;Optical Reflex Backhaul architecture;demand nodes;arbitrary radio access technology;fiber optic networks;ABSORB controller;demand distribution;traditional static architectures;metropolitan areas;deployment cost;maximum-based network design;gateway node;Computer architecture;Mobile communication;Topology;Optical fiber communication;Network topology;Adaptation models;Ad hoc networks},
doi={10.1109/INFOCOM.2017.8056970},
ISSN={},
month={May},}
@INPROCEEDINGS{8056971,
author={S. Gay and R. Hartert and S. Vissicchio},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Expect the unexpected: Sub-second optimization for segment routing},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In this paper, we study how to perform traffic engineering at an extremely-small time scale with segment routing, addressing a critical need for modern wide area networks. Prior work has shown that segment routing enables to better engineer traffic, thanks to its ability to program detours in forwarding paths, at scale. Two main approaches have been explored for traffic engineering with segment routing, respectively based on integer linear programming and constraint programming. However, no previous work deeply investigated how quickly those approaches can react to unexpected traffic changes and failures. We highlight limitations of existing algorithms, both in terms of required execution time and amount of path changes to be applied. Thus, we propose a new approach, based on local search and focused on the quick re-arrangement of (few) forwarding paths. We describe heuristics for sub-second recomputation of segment-routing paths that comply with requirements on the maximum link load (e.g., for congestion avoidance). Our heuristics enable a prompt answer to sudden criticalities affecting network services and business agreements. Through extensive simulations, we indeed experimentally show that our proposal significantly outperforms previous algorithms in the context of time-constrained optimization, supporting radical traffic changes in few tens of milliseconds for realistic networks.},
keywords={constraint handling;integer programming;linear programming;telecommunication network routing;telecommunication traffic;wide area networks;segment routing;traffic engineering;engineer traffic;forwarding paths;integer linear programming;constraint programming;segment-routing paths;radical traffic changes;wide area networks;Routing;Optimization;Routing protocols;Computational modeling;Conferences;Proposals;Heuristic algorithms},
doi={10.1109/INFOCOM.2017.8056971},
ISSN={},
month={May},}
@INPROCEEDINGS{8056972,
author={H. Xu and G. de Veciana and W. C. Lau},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Addressing job processing variability through redundant execution and opportunistic checkpointing: A competitive analysis},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The completion times of jobs in a computing cluster may be influenced by a variety of factors including job size and machine processing variability. In this paper, we explore online resource allocation policies which combine size-dependent scheduling with redundant execution and opportunistic checkpointing to minimize the overall job flowtime. We introduce a simplified model for the job service capacity of a computing cluster while leveraging redundant execution/checkpointing. In this setting, we propose two resource allocation algorithms, SRPT+R and LAPS+R(β) subject to checkpointing overhead not exceeding the number of jobs which are processed. We provide new theoretical performance bounds for these algorithms: SRPT+R is shown to be O(1/ϵ) competitive under (1 + ϵ)-speed resource augmentation, while LAPS+R(β) is shown to be O(1/βϵ) competitive under (2+ 2β + 2ϵ)-speed resource augmentation.},
keywords={checkpointing;job shop scheduling;resource allocation;scheduling;job processing variability;opportunistic checkpointing;computing cluster;job size;machine processing variability;online resource allocation policies;combine size-dependent scheduling;job flowtime;job service capacity;resource allocation algorithms;checkpointing overhead;resource augmentation;redundant execution;SRPT+R algorithm;LAPS+R(β) algorithm;Redundancy;Algorithm design and analysis;Multitasking;Checkpointing;Servers;Scheduling algorithms;Job Scheduling;Redundancy;Optimization;Competitive Analysis;Dual-Fitting},
doi={10.1109/INFOCOM.2017.8056972},
ISSN={},
month={May},}
@INPROCEEDINGS{8056973,
author={Y. Niu and F. Liu and X. Fei and B. Li},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Handling flash deals with soft guarantee in hybrid cloud},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Flash deal applications, which offer significant benefits (e.g., discount) to subscribers within a short period of time, are becoming increasingly prevalent. Motivated by such transient profit, flash crowds of subscribers request services simultaneously. Considering the unique business logic, a hybrid cloud with soft guarantee, i.e., bounding the response time of delay-tolerant requests, has great potential to handle flash crowds. In this paper, to cost-effectively withstand flash crowds with soft guarantee, we propose a solution that makes smart decisions on scheduling requests in the hybrid cloud and adjusting the capacity of the public cloud. In respect of scheduling requests, we apply Sequential Quadratic Programming (SQP) to achieve soft guarantee. Furthermore, for adjusting capacity, we design an online algorithm to tune the scale of the public cloud towards jointly minimizing cost and response time, yet without a priori knowledge of request arrival rate. We prove that the online algorithm can obtain a competitive ratio of 1-6ε against the optimal solution, where ε can be tuned close to 0. By conducting extensive trace-driven experiments in a website prototype deployed on OpenStack Mitaka and Amazon Web Service, our solution reduces response time by 15% compared with previous work under given budget.},
keywords={bandwidth allocation;cloud computing;peer-to-peer computing;quadratic programming;resource allocation;scheduling;Web services;flash deals;soft guarantee;hybrid cloud;flash crowds;response time;delay-tolerant requests;scheduling requests;public cloud;online algorithm;request arrival rate;business logic;sequential quadratic programming;SQP;cost minimization;response time minimization;competitive ratio;OpenStack Mitaka;Amazon Web service;Cloud computing;Time factors;Algorithm design and analysis;Degradation;Outsourcing;Computational modeling;Web servers},
doi={10.1109/INFOCOM.2017.8056973},
ISSN={},
month={May},}
@INPROCEEDINGS{8056974,
author={R. Birke and J. F. Pérez and Z. Qiu and M. Björkqvist and L. Y. Chen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Power of redundancy: Designing partial replication for multi-tier applications},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Replicating redundant requests has been shown to be an effective mechanism to defend application performance from high capacity variability - the common pitfall in the cloud. While the prior art centers on single-tier systems, it still remains an open question how to design replication strategies for distributed multi-tier systems, where interference from neighboring workloads is entangled with complex tier interdependency. In this paper, we design a first of its kind PArtial REplication system, sPARE, that replicates and dispatches read-only workloads for multi-tier web applications, determining replication factors per tier. The two key components of sPARE are (i) the variability-aware replicator that coordinates the replication levels on all tiers via an iterative searching algorithm, and (ii) the replication-aware arbiter that uses a novel token-based arbitration algorithm (TAD) to dispatch requests in each tier. We evaluate sPARE on web serving and web searching applications, i.e., MediaWiki and Solr, deployed on our private cloud testbed. Our results based on various interference patterns and traffic loads show that sPARE is able to improve the tail latency of MediaWiki and Solr by a factor of almost 2.7x and 2.9x, respectively.},
keywords={Internet;web searching applications;multitier applications;single-tier systems;replication strategies;distributed multitier systems;complex tier interdependency;multitier web applications;replication factors;variability-aware replicator;replication levels;iterative searching algorithm;token-based arbitration algorithm;sPARE system;partial replication system;replication-aware arbiter;Web serving applications;MediaWiki;Solr;Servers;Interference;Cloud computing;Protocols;Redundancy;Dispatching;Measurement},
doi={10.1109/INFOCOM.2017.8056974},
ISSN={},
month={May},}
@INPROCEEDINGS{8056975,
author={Z. Huang and S. M. Weinberg and L. Zheng and C. Joe-Wong and M. Chiang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Discovering valuations and enforcing truthfulness in a deadline-aware scheduler},
year={2017},
volume={},
number={},
pages={1-9},
abstract={A cloud computing cluster equipped with a deadline-aware job scheduler faces fairness and efficiency challenges when greedy users falsely advertise the urgency of their jobs. Penalizing such untruthfulness without demotivating users from using the cloud service calls for advanced mechanism design techniques that work together with deadline-aware job scheduling. We propose a Bayesian incentive compatible pricing mechanism based on matching by replica-surrogate valuation functions. User valuations can be discovered by the mechanism, even when the users themselves do not fully understand their own valuations. Furthermore, users who are charged a Bayesian incentive compatible price have no reason to lie about the urgency of their jobs. The proposed mechanism achieves multiple desired truthful properties such as Bayesian incentive compatibility and ex-post individual rationality. We implement the proposed pricing mechanism. Through experiments in a Hadoop cluster with real-world datasets, we show that our prototype is capable of suppressing untruthful behavior from users.},
keywords={Bayes methods;cloud computing;incentive schemes;pricing;scheduling;valuation discovery;truthfulness enforcement;cloud computing;Hadoop cluster;ex-post individual rationality;user valuations;replica-surrogate valuation functions;Bayesian incentive compatible pricing mechanism;advanced mechanism design techniques;cloud service;deadline-aware job scheduler;Cost accounting;Pricing;Resource management;Quality of service;Cloud computing;Bayes methods;Processor scheduling},
doi={10.1109/INFOCOM.2017.8056975},
ISSN={},
month={May},}
@INPROCEEDINGS{8056976,
author={X. Zheng and Z. Cai and J. Li and H. Gao},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Location-privacy-aware review publication mechanism for local business service systems},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Local business service systems (LBSS), such as Yelp and Dianping, play an essential role in making decisions like choosing a restaurant for our daily life. These systems heavily rely on individuals' voluntarily submitted reviews to build the reputation for nearby businesses. Unfortunately, the reviews expose users' private information such as visited places to the public and adversaries. Even worse, such location information is always public as it is the basic information of businesses, and adversaries could be anyone ranging from advertisement spammer to physical stalker. This paper formalizes the privacy preserving problem in local business service systems and propose a novel location privacy preserving framework. The framework can preserve users' location privacy in arbitrary local area and can maintain a good utility for both the system and every user. We evaluate our framework thoroughly towards real-world data traces. The results validate that the framework can achieve a good performance.},
keywords={catering industry;data protection;electronic commerce;mobile computing;local business service systems;location-privacy-aware review publication mechanism;LBSS;Yelp;Dianping;user private information;location privacy preserving framework;arbitrary local area;Privacy;Business;Publishing;Conferences;Computer science;Electronic mail;Mobile radio mobility management},
doi={10.1109/INFOCOM.2017.8056976},
ISSN={},
month={May},}
@INPROCEEDINGS{8056977,
author={S. Wang and Y. Nie and P. Wang and H. Xu and W. Yang and L. Huang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Local private ordinal data distribution estimation},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The categorical data that have natural ordering between categories are termed ordinal data, which are pervasive in numerous areas, including discrete sensor readings, metering data or preference options. Though aggregating such ordinal data from the population is facilitating plenty of crowdsourcing applications, contributing such data is privacy risky and may reveal sensitive information (e.g. locations, identities) about individuals. This work studies ordinal data aggregation for distribution estimation meanwhile locally preserving individuals' data privacy (such as on their mobile devices). Under ε-geo-indistinguishable constraints, which capture intrinsic dissimilarity between ordinal categories in the framework of differential privacy, we provide an efficient and effective locally private mechanism: Subset Exponential Mechanism (SEM) for ordinal data distribution estimation. The mechanism randomly responds with a fixed-size subset of the categories with calibrated probability assignment. Specially for uniform ordinal data, we propose a circling technique to symmetrically randomizing categories and estimating frequencies of categories, hence the computational/space costs and estimation performance of SEM are further optimized. Besides contributing theoretical error bounds of SEM, we also evaluate the mechanism on extensive scenarios, the evaluation results show that SEM reduces distribution estimation error on average by exp(ϵ/2) factor over existing private mechanisms.},
keywords={data privacy;probability;security of data;SEM;distribution estimation error;private mechanisms;local private ordinal data distribution estimation;categorical data;discrete sensor readings;privacy risky;ordinal categories;effective locally private mechanism;Subset Exponential Mechanism;uniform ordinal data;frequency estimation;natural ordering;metering data;preference options;ordinal data aggregation;ε-geo-indistinguishable constraints;symmetrical categories randomization;circling technique;calibrated probability assignment;Data privacy;Estimation;Privacy;Data models;Crowdsourcing;Data aggregation;Databases},
doi={10.1109/INFOCOM.2017.8056977},
ISSN={},
month={May},}
@INPROCEEDINGS{8056978,
author={H. Liu and X. Li and H. Li and J. Ma and X. Ma},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Spatiotemporal correlation-aware dummy-based privacy protection scheme for location-based services},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Since the dummy-based method can provide precise query results without any requirement for a third party or key sharing, it has been widely used to protect the user's location privacy in location-based services. However, the neighboring location sets submitted in consecutive requests always include a close spatiotemporal correlation, which enables the adversary to identify some dummies. Therefore, the existing dummy-based schemes cannot protect the user's location privacy completely. To solve this problem, based on the dummies generated by the existing schemes, this paper filters out the dummies that can be identified by taking into account of the spatiotemporal correlation from three aspects, namely time reachability, direction similarity and in-degree/out-degree. In this way, the rest dummies can satisfy the user's personalized privacy protection requirement. Security analysis shows that the proposed scheme successfully perturbs the spatiotemporal correlation between neighboring location sets, therefore, it is infeasible for the adversary to distinguish the user's real location from the dummies. Furthermore, extensive experiments indicate that the proposal is able to protect the user's location privacy effectively and efficiently.},
keywords={data protection;mobile computing;query processing;spatiotemporal correlation;location-based services;dummy-based method;neighboring location sets;time reachability;direction similarity;in-degree/out-degree;security analysis;user location privacy protection;Privacy;Spatiotemporal phenomena;Correlation;Mobile radio mobility management;Conferences;Computer security},
doi={10.1109/INFOCOM.2017.8056978},
ISSN={},
month={May},}
@INPROCEEDINGS{8056979,
author={M. Gramaglia and M. Fiore and A. Tarable and A. Banchs},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Preserving mobile subscriber privacy in open datasets of spatiotemporal trajectories},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Mobile network operators can track subscribers via passive or active monitoring of device locations. The recorded trajectories offer an unprecedented outlook on the activities of large user populations, which enables developing new networking solutions and services, and scaling up studies across research disciplines. Yet, the disclosure of individual trajectories raises significant privacy concerns: thus, these data are often protected by restrictive non-disclosure agreements that limit their availability and impede potential usages. In this paper, we contribute to the development of technical solutions to the problem of privacy-preserving publishing of spatiotemporal trajectories of mobile subscribers. We propose an algorithm that generalizes the data so that they satisfy-anonymity, an original privacy criterion that thwarts attacks on trajectories. Evaluations with real-world datasets demonstrate that our algorithm attains its objective while retaining a substantial level of accuracy in the data. Our work is a step forward in the direction of open, privacy-preserving datasets of spatiotemporal trajectories.},
keywords={data protection;mobile computing;privacy-preserving publishing;spatiotemporal trajectories;real-world datasets;open privacy-preserving datasets;mobile network operators;passive monitoring;active monitoring;device locations;mobile subscriber privacy preservation;data protection;restrictive nondisclosure agreements;Trajectory;Spatiotemporal phenomena;Privacy;Mobile communication;Data privacy;Databases;Couplings},
doi={10.1109/INFOCOM.2017.8056979},
ISSN={},
month={May},}
@INPROCEEDINGS{8056980,
author={A. Sinha and E. Modiano},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Optimal control for generalized network-flow problems},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We consider the problem of throughput-optimal packet dissemination, in the presence of an arbitrary mix of unicast, broadcast, multicast and anycast traffic, in a general wireless network. We propose an online dynamic policy, called Universal Max-Weight (UMW), which solves the above problem efficiently. To the best of our knowledge, UMW is the first throughput-optimal algorithm of such versatility in the context of generalized network flow problems. Conceptually, the UMW policy is derived by relaxing the precedence constraints associated with multi-hop routing, and then solving a min-cost routing and max-weight scheduling problem on a virtual network of queues. When specialized to the unicast setting, the UMW policy yields a throughput-optimal cycle-free routing and link scheduling policy. This is in contrast to the well-known throughput-optimal BackPressure (BP) policy which allows for packet cycling, resulting in excessive delay. Extensive simulation results show that the proposed policy incurs a substantially lower delay as compared to the BP policy. The proof of throughput-optimality of the UMW policy combines techniques from stochastic Lyapunov theory with a sample path argument from adversarial queueing theory and may be of independent theoretical interest.},
keywords={Lyapunov methods;multicast communication;optimal control;optimisation;queueing theory;radio networks;telecommunication control;telecommunication network routing;telecommunication scheduling;telecommunication traffic;generalized network-flow problems;throughput-optimal packet dissemination;anycast traffic;online dynamic policy;throughput-optimal algorithm;UMW policy;multihop routing;min-cost routing;max-weight scheduling problem;link scheduling policy;throughput-optimal BackPressure policy;BP policy;throughput-optimality;optimal control;multicast traffic;wireless network;Universal Max-Weight;Routing;Unicast;Delays;Wireless networks;Heuristic algorithms;Spread spectrum communication;Throughput},
doi={10.1109/INFOCOM.2017.8056980},
ISSN={},
month={May},}
@INPROCEEDINGS{8056981,
author={K. Lampka and S. Bondorf and J. B. Schmitt and N. Guan and W. Yi},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Generalized finitary real-time calculus},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Real-time Calculus (RTC) is a non-stochastic queuing theory to the worst-case performance analysis of distributed real-time systems. Workload as well as resources are modelled as piece-wise linear, pseudo-periodic curves and the system under investigation is modelled as a sequence of algebraic operations over these curves. The memory footprint of computed curves increases exponentially with the sequence of operations and RTC may become computationally infeasible fast. Recently, Finitary RTC has been proposed to counteract this problem. Finitary RTC restricts curves to finite input domains and thereby counteracts the memory demand explosion seen with pseudo periodic curves of common RTC implementations. However, the proof to the correctness of Finitary RTC specifically exploits the operational semantic of the greed processing component (GPC) model and is tied to the maximum busy window size. This is an inherent limitation, which prevents a straight-forward generalization. In this paper, we provide a generalized Finitary RTC that abstracts from the operational semantic of a specific component model and reduces the finite input domains of curves even further. The novel approach allows for faster computations and the extension of the Finitary RTC idea to a much wider range of RTC models.},
keywords={calculus;queueing theory;nonstochastic queuing theory;worst-case performance analysis;distributed real-time systems;pseudoperiodic curves;algebraic operations;memory demand explosion;operational semantic;greed processing component model;generalized Finitary RTC;generalized finitary realtime calculus;piecewise linear curves;RTC implementations;Mathematical model;Computational modeling;Real-time systems;Analytical models;Delays;Semantics;Calculus},
doi={10.1109/INFOCOM.2017.8056981},
ISSN={},
month={May},}
@INPROCEEDINGS{8056982,
author={P. Wan and H. Yuan and X. Jia and J. Wang and Z. Wang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Maximum-weighted subset of communication requests schedulable without spectral splitting},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Consider a set of point-to-point communication requests in a multi-channel multihop wireless network, each of which is associated with a traffic demand of at most one unit of transmission time, and a weight representing the utility if its demand is fully met. A subset of requests is said to be schedulable without spectral splitting if they can be scheduled within one unit of time subject to the constraint each request is assigned with a unique channel throughout its transmission. This paper develops efficient and provably good approximation algorithms for finding a maximum-weighted subset of communication requests schedulable without spectral splitting.},
keywords={approximation theory;radio networks;telecommunication scheduling;telecommunication traffic;wireless channels;maximum-weighted subset;spectral splitting;multichannel multihop wireless network;traffic demand;transmission time;time subject;constraint each request;point-to-point communication;approximation algorithms;Schedules;Approximation algorithms;Channel allocation;Interference;Computer science;Receivers;Conferences},
doi={10.1109/INFOCOM.2017.8056982},
ISSN={},
month={May},}
@INPROCEEDINGS{8056983,
author={S. Krishnasamy and P. T. Akhil and A. Arapostathis and S. Shakkottai and R. Sundaresan},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Augmenting max-weight with explicit learning for wireless scheduling with switching costs},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In small-cell wireless networks where users are connected to multiple base stations (BSs), it is often advantageous to opportunistically switch off a subset of BSs to minimize energy costs. We consider two types of energy cost: (i) the cost of maintaining a BS in the active state, and (ii) the cost of switching a BS from the active state to inactive state. The problem is to operate the network at the lowest possible energy cost (sum of activation and switching costs) subject to queue stability. In this setting, the traditional approach - a Max-Weight algorithm along with a Lyapunov-based stability argument - does not suffice to show queue stability, essentially due to the temporal co-evolution between channel scheduling and the BS activation decisions induced by the switching cost. Instead, we develop a learning and BS activation algorithm with slow temporal dynamics, and a Max-Weight based channel scheduler that has fast temporal dynamics. We show using convergence of time-inhomogeneous Markov chains, that the co-evolving dynamics of learning, BS activation and queue lengths lead to near optimal average energy costs along with queue stability.},
keywords={cellular radio;cost reduction;learning (artificial intelligence);Markov processes;minimisation;queueing theory;radio networks;telecommunication computing;telecommunication scheduling;wireless scheduling;small-cell wireless networks;multiple base stations;active state;inactive state;queue stability;Max-Weight algorithm;BS activation decisions;Max-Weight based channel scheduler;queue lengths;optimal average energy costs;switching costs;max-weight augmentation;energy costs minimization;time-inhomogeneous Markov chains;Switches;Heuristic algorithms;Resource management;Energy consumption;Markov processes;Stability criteria;Convergence;wireless scheduling;base-station activation;energy minimization},
doi={10.1109/INFOCOM.2017.8056983},
ISSN={},
month={May},}
@INPROCEEDINGS{8056984,
author={Y. Hou and Y. Zheng},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={PHY assisted tree-based RFID identification},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Tree-based RFID identification adopts a binary-tree structure to collect IDs of an unknown set. Tag IDs locate at the leaf nodes and the reader queries through intermediate tree nodes and converges to these IDs using feedbacks from tag responses. Existing works cannot function well under random ID distribution as they ignore the distribution information hidden in the physical-layer signal of colliding tags. Different from them, we introduce PHY-Tree, a novel tree-based scheme that collects two types of distribution information from every encountered colliding signal. First, we detect if all colliding tags send the same bit content at each bit index by looking into inherent temporal features of the tag modulation schemes. If such resonant states are detected, either left or right branch of a certain subtree can be trimmed horizontally. Second, we estimate the number of colliding tags in a slot by computing a related metric defined over the signal's constellation map, based on which nodes in the same layers of a certain subtree can be skipped vertically. Evaluations from both experiments and simulations demonstrate that PHY-Tree outperforms state-of-the-art schemes by at least 1.79×.},
keywords={radiofrequency identification;trees (mathematics);leaf nodes;intermediate tree nodes;tag responses;random ID distribution;distribution information;physical-layer signal;colliding tags;PHY-Tree;tag modulation schemes;RFID identification;binary-tree structure;tag IDs;colliding signal;Indexes;Physical layer;Radiofrequency identification;Encoding;Conferences;Feature extraction;Modulation},
doi={10.1109/INFOCOM.2017.8056984},
ISSN={},
month={May},}
@INPROCEEDINGS{8056985,
author={J. Liu and F. Zhu and Y. Wang and X. Wang and Q. Pan and L. Chen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={RF-scanner: Shelf scanning with robot-assisted RFID systems},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Shelf scanning is one of the most important processes for inventory management in a library. It helps the librarians and library users discover the miss-shelved books and pinpoint where they are, improving the quality of service. By traditional means, however, manually checking each bookshelf suffers from extremely intensive labor and long scanning delay. Although some existing RFID-enabled approaches have been proposed, they suffer from either high-cost infrastructure or complicated system deployment, forming a great barrier to commercial adoption. In light of this, we in this paper propose a smart system called RF-Scanner that can perform the shelf scanning automatically by combining the robot technology and the RFID technology. The former is used for replacing the librarians and liberating them from intensively manual labor. The later is installed on the robot and moves with the robot to scan the on-the-shelf books. We formulate two important issues concerned by librarians, the book localization and the lying-down book detection, and give the sophisticated solutions to them. Besides, we implement RF-Scanner and put it into practical use in our school library. Long-term experiments and studies show that RF-Scanner provides fine-grained book localization with a mean error of just 1.3 cm and accurate detection accuracy of lying-down books with a mean error of 6%.},
keywords={inventory management;library automation;radiofrequency identification;service robots;book localization;lying-down book detection;RF-Scanner;school library;lying-down books;shelf scanning;robot-assisted RFID systems;inventory management;miss-shelved books;quality of service;high-cost infrastructure;complicated system deployment;smart system;robot technology;RFID technology;intensively manual labor;on-the-shelf books;size 1.3 cm;Libraries;Radiofrequency identification;Robot kinematics;Radio frequency;Manuals;Systems architecture},
doi={10.1109/INFOCOM.2017.8056985},
ISSN={},
month={May},}
@INPROCEEDINGS{8056986,
author={M. Chen and J. Liu and S. Chen and Y. Qiao and Y. Zheng},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={DBF: A general framework for anomaly detection in RFID systems},
year={2017},
volume={},
number={},
pages={1-9},
abstract={RFID technologies are making their way into numerous applications, including inventory management, supply chain, product tracking, transportation, logistics, etc. One important application is to automatically detect anomalies in RFID systems, such as missing tags, unknown tags, or cloned tags due to theft, management error, or targeted attacks. Existing solutions are all designed to detect a certain type of RFID anomalies, but lack a general functionality for detecting different types of anomalies. This paper attempts to propose a general framework for anomaly detection in RFID systems, thereby reducing the complexity for readers and tags to implement different anomaly-detection protocols. We introduce a new concept of differential Bloom filter (DBF), which turns physical-layer signal data into a segmented Bloom filter that encodes the IDs of abnormal tags. As a case study, we propose a protocol that builds DBF for identifying all missing tags in an efficient way. We implement a prototype for missing-tag identification using USRP and WISP tags to verify the effectiveness our protocol, and use large-scale simulations for performance evaluation. The results show that our solution can significantly improve time efficiency, when comparing with the best existing work.},
keywords={data structures;protocols;radiofrequency identification;telecommunication security;anomaly detection;RFID systems;anomaly-detection protocols;differential Bloom filter;DBF;segmented Bloom filter;abnormal tags;missing-tag identification;WISP tags;RFID technologies;RFID anomalies;Radiofrequency identification;Anomaly detection;Nickel;Protocols;Wireless communication;Aggregates;Databases},
doi={10.1109/INFOCOM.2017.8056986},
ISSN={},
month={May},}
@INPROCEEDINGS{8056987,
author={H. Aantjes and A. Y. Majid and P. Pawełczak and J. Tan and A. Parks and J. R. Smith},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Fast downstream to many (computational) RFIDs},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We present Stork - an extension of the EPC C1G2 protocol allowing streaming of data to multiple Computational Radio Frequency IDentification tags (CRFIDs) simultaneously at up to 20 times faster than the prior state of the art. Stork introduces downstream attributes never before seen in (C)RFIDs: (i) fast feedback for CRFID downstream verification based on the internal EPC C1G2 memory check command - which we analytically and experimentally show to be the best possible downstream verification process based on EPC C1G2; (ii) ability to perform multi-CRFID transfer - which in our experiments speeds up downstream by more than two times compared to sequential transmission; and (iii) the use of compressed data streams - which improves firmware reprogramming times by up to 10% at large reader-to-CRFID distances.},
keywords={cryptographic protocols;data compression;firmware;program verification;radiofrequency identification;fast downstream;Stork;EPC C1G2 protocol allowing;multiple Computational Radio Frequency IDentification tags;fast feedback;CRFID downstream verification;internal EPC C1G2 memory check command;multiCRFID transfer;compressed data streams;firmware reprogramming times;Protocols;Radiofrequency identification;Robustness;Software;Payloads},
doi={10.1109/INFOCOM.2017.8056987},
ISSN={},
month={May},}
@INPROCEEDINGS{8056988,
author={Z. Chen and X. Zhang and S. Wang and Y. Xu and J. Xiong and X. Wang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={BUSH: Empowering large-scale MU-MIMO in WLANs with hybrid beamforming},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Large-scale MU-MIMO is a promising technology to scale network capacity and the capacity gain grows linearly with the numbers of antennas and users in theory. However, its practical deployment faces three critical challenges in the state-of-the-art WLANs: i) the demand of a large number of expensive RF chains; ii) the linear growth of feedback overheads with the number of antennas; iii) the lack of scalable user selection scheme for a large user population. In this paper, we design BUSH, a large-scale MU-MIMO prototype that performs scalable beam user selection with hybrid beamforming for phased-array antennas in legacy WLANs. The architecture of BUSH consists of three components. Firstly, a low complexity algorithm assigns each pair of RF chain and analog beam to the users to effectively reduce channel correlation and cross-talk interference without instantaneous CSI feedbacks. Secondly, as a prerequisite of user selection, BUSH presents a low-overhead probing scheme in multi-carrier WLANs, and designs a highly accurate blind Power Azimuth Spectrum (PAS) estimation algorithm using a single RF chain. Thirdly, the phased-array antennas use analog beamforming to steer spatial beams toward each selected downlink user, and the finite number of RF chains use beamforming to further mitigate the interference among users. We implement BUSH on the WARPv3 boards and evaluate its performance in more than 30 indoor scenarios. The experimental results show that in terms of total throughput BUSH outperforms the legacy 802.11ac by 2.08×, and an alternative benchmark system by 1.22× on average.},
keywords={antenna phased arrays;array signal processing;MIMO communication;wireless channels;wireless LAN;legacy WLAN;state-of-the-art WLAN;downlink user;highly accurate blind power azimuth spectrum estimation algorithm;low complexity algorithm;scalable beam user selection;large-scale MU-MIMO prototype;user population;scalable user selection scheme;feedback overheads;linear growth;expensive RF chains;capacity gain;network capacity;hybrid beamforming;total throughput BUSH;analog beamforming;phased-array antennas;single RF chain;multicarrier WLANs;low-overhead probing scheme;instantaneous CSI feedbacks;analog beam;Array signal processing;Radio frequency;Antennas;Throughput;Interference;Estimation;MIMO},
doi={10.1109/INFOCOM.2017.8056988},
ISSN={},
month={May},}
@INPROCEEDINGS{8056989,
author={M. Swaminathan and A. Vizziello and D. Duong and P. Savazzi and K. R. Chowdhury},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Beamforming in the body: Energy-efficient and collision-free communication for implants},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Implants are poised to revolutionize personalized healthcare by monitoring and actuating physiological functions. Such implants operate under challenging constraints of limited battery energy, heterogeneous tissue-dependent channel conditions and human-safety regulations. To address these issues, we propose a new cross-layer protocol for galvanic coupled implants wherein weak electrical currents are used in place of classical radio frequency (RF) links. As the first step, we devise a method that allows multiple implants to communicate individual sensed data to each other through CDMA code assignments, but delegates the computational burden of decoding only to the on-body surface relays. Then, we devise a distributed beamforming approach that allows coordinated transmissions from the implants to the relays by considering the specific tissue path chosen and tissue heating-related safety constraints. Our contributions are two fold: First, we devise a collision-free protocol that prevents undue interference at neighboring implants, especially for multiple deployments. Second, this is the first application of near-field distributed beamforming in human tissue. Results reveal significant improvement in the network lifetime for implants of up to 79% compared to the galvanic coupled links without beamforming.},
keywords={array signal processing;biological tissues;code division multiple access;medical signal processing;patient monitoring;prosthetics;telecommunication power management;collision-free communication;battery energy;heterogeneous tissue-dependent channel conditions;human-safety regulations;cross-layer protocol;galvanic coupled implants;classical radio frequency links;on-body surface relays;distributed beamforming approach;specific tissue path;tissue heating-related safety constraints;collision-free protocol;neighboring implants;human tissue;physiological functions;CDMA code assignments;Implants;Relays;Array signal processing;Muscles;Multiaccess communication;Transmitters;Sensors},
doi={10.1109/INFOCOM.2017.8056989},
ISSN={},
month={May},}
@INPROCEEDINGS{8056990,
author={A. Salam and M. C. Vuran},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Smart underground antenna arrays: A soil moisture adaptive beamforming approach},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Current wireless underground (UG) communication techniques are limited by their achievable distance. In this paper, a novel framework for underground beamforming using adaptive antenna arrays is presented to extend communication distances for practical applications. Based on the analysis of propagation in wireless underground channel, a theoretical model is developed which uses soil moisture information to improve wireless underground communications performance. Array element in soil is analyzed empirically and impacts of soil type and soil moisture on return loss (RL) and resonant frequency are investigated. Accordingly, beam patterns are analyzed to communicate with underground and above ground devices. Depending on the incident angle, refraction from soil-air interface has adverse effects in the UG communications. It is shown that beam steering improves UG communications by providing a high-gain lateral wave. To this end, the angle, which enhances lateral wave, is shown to be a function of dielectric properties of the soil, soil moisture, and soil texture. Evaluations show that this critical angle varies from 0° to 16° and decreases with soil moisture. Accordingly, a soil moisture adaptive beamforming (SMABF) algorithm is developed for planar array structures and evaluated with different optimization approaches to improve UG communication performance.},
keywords={adaptive antenna arrays;array signal processing;beam steering;moisture;soil;underground communication;wireless underground channel;soil moisture information;underground ground devices;soil-air interface;soil moisture adaptive beamforming algorithm;planar array structures;UG communication performance;smart underground antenna arrays;soil moisture adaptive beamforming approach;underground beamforming;adaptive antenna arrays;Soil moisture;Antenna arrays;Array signal processing;Resonant frequency;Wireless communication},
doi={10.1109/INFOCOM.2017.8056990},
ISSN={},
month={May},}
@INPROCEEDINGS{8056991,
author={J. Palacios and D. De Donno and J. Widmer},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Tracking mm-Wave channel dynamics: Fast beam training strategies under mobility},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In order to cope with the severe path loss, millimeter-wave (mm-wave) systems exploit highly directional communication. As a consequence, even a slight beam mis-alignment between two communicating devices (for example, due to mobility) can generate a significant signal drop. This leads to frequent invocations of time-consuming mechanisms for beam re-alignment, which deteriorate system performance. In this paper, we propose smart beam training and tracking strategies for fast mm-wave link establishment and maintenance under node mobility. We leverage the ability of hybrid analog-digital transceivers to collect channel information from multiple spatial directions simultaneously and formulate a probabilistic optimization problem to model the temporal evolution of the mm-wave channel under mobility. In addition, we present for the first time a beam tracking algorithm that extracts information needed to update the steering directions directly from data packets, without the need for spatial scanning during the ongoing data transmission. Simulation results, obtained by a custom simulator based on ray tracing, demonstrate the ability of our beam training/tracking strategies to keep the communication rate only 10% below the optimal bound. Compared to the state of the art, our approach provides a 40% to 150% rate increase, yet requires lower complexity hardware.},
keywords={array signal processing;optimisation;probability;radio links;ray tracing;wireless channels;mm-Wave channel dynamics;fast beam training strategies;severe path loss;millimeter-wave;highly directional communication;slight beam mis-alignment;communicating devices;time-consuming mechanisms;beam re-alignment;system performance;smart beam training;mm-wave link establishment;maintenance;node mobility;hybrid analog-digital transceivers;channel information;multiple spatial directions;probabilistic optimization problem;steering directions;spatial scanning;ongoing data transmission;signal drop;beam training-tracking strategies;Training;Radio frequency;Transceivers;Maintenance engineering;Heuristic algorithms;Probabilistic logic;Array signal processing},
doi={10.1109/INFOCOM.2017.8056991},
ISSN={},
month={May},}
@INPROCEEDINGS{8056992,
author={H. Xu and Z. Yu and C. Qian and X. Li and Z. Liu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Minimizing flow statistics collection cost of SDN using wildcard requests},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In a software defined network (SDN), the control plane needs to frequently collect flow statistics measured at the data plane switches for different applications, such as traffic engineering, flow re-routing, and attack detection. However, existing solutions for flow statistics collection may result in large bandwidth cost in the control channel and long processing delay on switches, which significantly interfere with the basic functions such as packet forwarding and route update. To address this challenge, we propose a Cost-Optimized Flow Statistics Collection (CO-FSC) scheme using wildcard-based requests. We prove that the CO-FSC problem is NP-Hard and present a rounding-based algorithm with an approximation factor f, where f is the maximum number of switches visited by each flow. Moreover, our CO-FSC problem is extended to the general case, in which only a part of flows in a network need to be collected. The extensive simulation results show that the proposed algorithms can reduce the bandwidth overhead by over 41% and switch processing delay by over 45% compared with the existing solutions.},
keywords={computational complexity;software defined networking;statistical analysis;telecommunication network routing;telecommunication traffic;software defined network;SDN;control plane;data plane switches;traffic engineering;attack detection;control channel;long processing delay;packet forwarding;route update;Cost-Optimized Flow Statistics Collection scheme;CO-FSC problem;rounding-based algorithm;switch processing delay;wildcard requests;flow statistics collection cost minimization;Control systems;Power capacitors;Bandwidth;Delays;Approximation algorithms;Algorithm design and analysis;Conferences;Software Defined Networks;Flow Statistics Collection;Cost;Wildcard;Approximation},
doi={10.1109/INFOCOM.2017.8056992},
ISSN={},
month={May},}
@INPROCEEDINGS{8056993,
author={W. Ma and O. Sandoval and J. Beltran and D. Pan and N. Pissinou},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Traffic aware placement of interdependent NFV middleboxes},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Network function virtualization enables flexible implementation of network functions, or middleboxes, as virtual machines running on standard servers. However, the flexibility also creates a challenge for efficiently placing such middleboxes, due to the availability of multiple hosting servers, capability of middleboxes to change traffic volumes, and dependency between middleboxes. In this paper, we address the optimal placement challenge of NFV middleboxes, and propose solutions for middleboxes of different traffic changing effects and with different dependency relations. First, we formulate the Traffic Aware Placement of Interdependent Middleboxes problem as a graph optimization problem. When the flow path is predetermined, we design optimal algorithms to place a non-ordered or totally-ordered middlebox set, and propose an efficient heuristic for the general scenario of a partially-ordered middlebox set after proving its NP-hardness. When the flow path is not predetermined, we show that the problem is NP-hard even for a non-ordered middlebox set, and propose a traffic and space aware routing heuristic. We have evaluated the proposed algorithms using large scale simulations and prototype experiments, and present extensive evaluation results to demonstrate the effectiveness of our design.},
keywords={computational complexity;graph theory;optimisation;telecommunication network routing;telecommunication traffic;virtual machines;virtualisation;Traffic Aware Placement;Traffic aware placement;interdependent NFV middleboxes;network function virtualization;interdependent middlebox problem;traffic changing effects;virtual machines;graph optimization problem;totally-ordered middlebox set;nonordered middlebox set;NP-hardness;space aware routing heuristic;large scale simulations;prototype experiments;Middleboxes;Servers;Algorithm design and analysis;Optimization;Computer architecture;Prototypes;Hardware;NFV;SDN;middlebox},
doi={10.1109/INFOCOM.2017.8056993},
ISSN={},
month={May},}
@INPROCEEDINGS{8056994,
author={P. Zhang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Towards rule enforcement verification for software defined networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Software defined networks (SDNs) reshape the ossified network architectures, by introducing centralized and programmable network control. Despite the huge benefits, SDNs also open doors to what we call rule modification attack, an attack largely overlooked by the community. In such an attack, the adversary can modify rules by exploiting implementation vulnerabilities of switch OSes and control channels. As a result, packets may deviate from their original paths, thereby violating network policies. To defend against rule modification attack, this paper introduces a new security primitive named rule enforcement verification (REV). REV allows a controller to check whether switches have enforced the rules installed by it, using message authentication code (MAC). Since using standard MACs will incur heavy switch-to-controller traffic, this paper proposes a new compressive MAC, which allows switches to compress MACs before reporting to the controller. Experiments show that REV based on compressive MAC can achieve a 97% reduction in switch-to-controller traffic, and a Sx increase in verification throughput.},
keywords={access protocols;computer network security;cryptography;message authentication;software defined networking;switch-to-controller traffic;software defined networks;ossified network architectures;centralized network control;programmable network control;open doors;rule modification attack;switch OSes;control channels;network policies;REV;compressive MAC;SDN;rule enforcement verification;message authentication code;Switches;Standards;Message authentication;Silicon;Ports (Computers);Software defined networks;Rule modification attack;verification;Compressive MAC},
doi={10.1109/INFOCOM.2017.8056994},
ISSN={},
month={May},}
@INPROCEEDINGS{8056995,
author={R. Jang and D. Cho and Y. Noh and D. Nyang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={RFlow+: An SDN-based WLAN monitoring and management framework},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In this work, we propose an SDN-based WLAN monitoring and management framework called RFlow+to address WiFi service dissatisfaction caused by the limited view (lack of scalability) of network traffic monitoring and absence of intelligent and timely network treatments. Existing solutions (e.g., OpenFlow and sFlow) have limited view, no generic flow description, and poor trade-off between measurement accuracy and network overhead depending on the selection of the sampling rate. To resolve these issues, we devise a two-level counting mechanism, namely a distributed local counter (on-site and real-time) and central collector (a summation of local counters). With this, we proposed a highly scalable monitoring and management framework to handle immediate actions based on short-term (e.g., 50 ms) monitoring and eventual actions based on long-term (e.g., 1 month) monitoring. The former uses the local view of each access point (AP), and the latter uses the global view of the collector. Experimental results verify that RFlow+can achieve high accuracy (less than 5% standard error for short-term and less than 1% for long-term) and fast detection of flows of interest (within 23 ms) with manageable network overhead. We prove the practicality of RFlow+by showing the effectiveness of a MAC flooding attacker quarantine in a real-world testbed.},
keywords={computer network management;computerised monitoring;IP networks;software defined networking;telecommunication security;telecommunication traffic;wireless LAN;distributed local counter;management framework;manageable network overhead;SDN;WLAN monitoring;WiFi service dissatisfaction;network traffic monitoring;intelligent network treatments;two-level counting mechanism;measurement accuracy;RFlow+;short-term monitoring;long-term monitoring;eventual actions;access point;MAC flooding attacker;sampling rate selection;central collector;time 1.0 month;time 23.0 ms;time 50.0 ms},
doi={10.1109/INFOCOM.2017.8056995},
ISSN={},
month={May},}
@INPROCEEDINGS{8056996,
author={P. Parag and A. Bura and J. Chamberland},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Latency analysis for distributed storage},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Modern communication and computation systems consist of large networks of unreliable nodes. Yet, it is well known that such systems can provide aggregate reliability via information redundancy, duplicating paths, or replicating computations. While redundancy may increase the load on a system, it can also lead to major performance improvements through the judicious management of additional system resources. Two important examples of this abstract paradigm are content access from multiple caches in content delivery networks and master/slave computations on compute clusters. Many recent articles in the area have proposed bounds on the latency performance of redundant systems, characterizing the latency-redundancy tradeoff under specific load profiles. Following a similar line of research, this article introduces new analytical bounds and approximation techniques for the latency-redundancy tradeoff for a range of system loads and two popular redundancy schemes. The proposed framework allows for approximating the equilibrium latency distribution, from which various metrics can be derived including mean, variance, and the tail decay of stationary distributions.},
keywords={cache storage;Internet;redundancy;telecommunication traffic;judicious management;content access;multiple caches;content delivery networks;compute clusters;latency performance;redundant systems;latency-redundancy tradeoff;specific load profiles;system loads;equilibrium latency distribution;stationary distributions;latency analysis;distributed storage;computation systems;unreliable nodes;aggregate reliability;information redundancy;redundancy schemes;master-slave computations;approximation techniques;Servers;Encoding;Markov processes;Redundancy;Queueing analysis;Parity check codes;Data storage;forward error correction;content delivery networks;distributed storage systems;Markov processes;queueing analysis;equilibrium distribution;waiting time},
doi={10.1109/INFOCOM.2017.8056996},
ISSN={},
month={May},}
@INPROCEEDINGS{8056997,
author={V. Aggarwal and J. Fan and T. Lan},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Taming tail latency for erasure-coded, distributee storage systems},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Distributed storage systems are known to be susceptible to long tails in response time. It has been shown that in modern online applications such as Bing, Facebook, and Amazon, the long tail of latency is of particular concern, with 99.9th percentile response times being orders of magnitude worse than the mean. As erasure codes emerge as a popular technique in distributed storage to achieve high data reliability while attaining space efficiency, taming tail latency remains an open problem due to the lack of mathematical models for analyzing such erasure-coded storage systems. In this paper, we quantify tail latency in distributed storage systems that employ erasure coding. In particular, we derive upper bounds on tail latency in closed-form for arbitrary service time distribution and heterogeneous files. Based on the model, we formulate an optimization problem to jointly minimize weighted latency tail probability of all files. The non-convex problem is solved using an efficient, alternating optimization algorithm. Simulation results show significant reduction of tail latency for erasure-coded storage systems with realistic workload.},
keywords={optimisation;probability;storage management;service time distribution;tail latency;erasure-coded distributed storage systems;optimization problem;alternating optimization algorithm;latency tail probability;erasure coding;erasure codes;percentile response times;distributed storage systems;Optimization;Probabilistic logic;Servers;Upper bound;Encoding;Queueing analysis;Conferences},
doi={10.1109/INFOCOM.2017.8056997},
ISSN={},
month={May},}
@INPROCEEDINGS{8056998,
author={S. Wei and Y. Li and Y. Xu and S. Wu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={DSC: Dynamic stripe construction for asynchronous encoding in clustered file system},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Nowadays many clustered file systems adopt asynchronous encoding which transforms replicated data into erasure coding to maintain data availability with bounded storage overhead. Existing implementations of asynchronous encoding construct coding stripes with logically sequential data blocks, which suffers from heavy cross-rack traffic and necessitates data block redistribution. Recent work [12] solves this problem by carefully distributing replicated data blocks among racks at the time when they are being written, but it is not applicable to the cases when existing systems have different data layouts or the data layout changes. In this paper, we propose Dynamic Stripe Construction (DSC) to transform N-way replication to erasure coding. DSC does not induce to any cross-rack traffic for encoding, and it does not require data block redistribution after encoding. Besides, DSC is general enough to be applied to any existing CFSes with various erasure codes, and it can also be deployed on a distributed file system in a hot-plugging-in manner. To validate the effectiveness of DSC, we implement it on HDFS. Through extensive testbed experiments in a real storage cluster, we show that DSC can significantly increase the encoding throughput and reduce the foreground user response time over the traditional approach.},
keywords={encoding;storage management;DSC;Dynamic stripe construction;asynchronous encoding;data layouts;cross-rack traffic;data block redistribution;foreground user response time;encoding throughput;storage cluster;distributed file system;erasure codes;N-way replication;Dynamic Stripe Construction;replicated data blocks;logically sequential data blocks;coding stripes;bounded storage overhead;data availability;erasure coding;clustered file system;Encoding;Fault tolerance;Fault tolerant systems;Distributed databases;Ear;Layout},
doi={10.1109/INFOCOM.2017.8056998},
ISSN={},
month={May},}
@INPROCEEDINGS{8056999,
author={X. Liu and W. Sun and W. Lou and Q. Pei and Y. Zhang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={One-tag checker: Message-locked integrity auditing on encrypted cloud deduplication storage},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In this paper, we investigate the problem of integrity auditing for cloud deduplication storage. Specifically, in addition to the outsourced data confidentiality, we also aim to ensure the integrity of the deduplicated cloud storage. With the existing works based on Provable Data Possession (PDP)/Proof of Retrievability (PoR), we are either required to rely on a fully trusted proxy server or inevitably sacrifice the privacy and efficiency. In contrast, we present a novel message-locked integrity auditing scheme without an additional proxy server, which is applicable to both file-level and chunk-level deduplication systems. In particular, our scheme is storage efficient in the sense that apart from eliminating the ciphertext redundancy, we also enable the integrity tag deduplication by a message-derived signing key, which merely incurs minimal client-side computation overhead. Besides, we can still publicly perform the integrity check over any client's cloud storage by incorporating the proxy re-signature technique. We show that the proposed scheme will not disclose the data ownership information and is provably secure under the Computational Diffie-Hellman (CDH) assumption in the random oracle model. Finally, the performance evaluation demonstrates its effectiveness and efficiency.},
keywords={cloud computing;data integrity;data privacy;digital signatures;public key cryptography;storage management;proxy re-signature technique;data ownership information;encrypted cloud deduplication storage;outsourced data confidentiality;deduplicated cloud storage;Provable Data Possession;integrity auditing scheme;chunk-level deduplication systems;integrity tag deduplication;integrity check;message-locked integrity auditing;proxy server;file-level deduplication systems;client-side computation overhead;message-derived signing key;Computational Diffie-Hellman assumption;CDH assumption;random oracle model;Cloud computing;Servers;Encryption;Public key;Protocols;Conferences},
doi={10.1109/INFOCOM.2017.8056999},
ISSN={},
month={May},}
@INPROCEEDINGS{8057000,
author={M. Dinitz and J. Fineman and S. Gilbert and C. Newport},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Load balancing with bounded convergence in dynamic networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Load balancing is an important activity in distributed systems. At a high-level of abstraction, this problem assumes processes in the system begin with a potentially uneven assignment of “work” which they must then attempt to spread evenly. There are many different approaches to solving this problem. In this paper, we focus on local load balancing-an approach in which work is balanced in an iterative and distributed manner by having processes exchange work in each round with neighbors in an underlying communication network. The goal with local load balancing is to prove that these local exchanges will lead over time to a global balance. We describe and analyze a new local load balancing strategy called max neighbor, and prove a bound on the number of rounds required for it to obtain a parameterized level of balance, with high probability, in a general dynamic network topology. We then prove this analysis tight (within logarithmic factors) by describing a network and initial work distribution for which max neighbor matches its upper bound, and then build on this to prove that no load balancing algorithm in which every node exchanges work with at most one partner per round can converge asymptotically faster than max neighbor.},
keywords={convergence;probability;resource allocation;telecommunication network topology;load balancing algorithm;node exchanges work;bounded convergence;distributed systems;underlying communication network;local exchanges;general dynamic network topology;max neighbor;local load balancing strategy;Load management;Convergence;Heuristic algorithms;Network topology;Communication networks;Conferences;Hypercubes},
doi={10.1109/INFOCOM.2017.8057000},
ISSN={},
month={May},}
@INPROCEEDINGS{8057001,
author={H. Xu and H. Huang and S. Chen and G. Zhao},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Scalable software-defined networking through hybrid switching},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Traditional networks rely on aggregate routing and decentralized control to achieve scalability. On the contrary, software-defined networks achieve near optimal network performance and policy-based management through per-flow routing and centralized control, which however face scalability challenge due to (1) limited TCAM and on-die memory for storing the forwarding table and (2) per-flow communication/computation overhead at the controller. This paper presents a novel hybrid switching design, which integrates traditional switching and SDN switching for the purpose of achieving both scalability and optimal performance. We show that the integration also leads to unexpected benefits of making both types of switching more efficient under the hybrid design. Numerical evaluation demonstrates the superior performance of hybrid switching when comparing with the state-of-the-art SDN design.},
keywords={centralised control;software defined networking;switching networks;telecommunication control;telecommunication network routing;per-flow routing;centralized control;forwarding table;novel hybrid switching design;state-of-the-art SDN design;aggregate routing;limited TCAM;on-die memory;scalable software-defined networking;near optimal network performance;policy-based management;Switches;Routing;Scalability;Ports (Computers);Centralized control;Aggregates},
doi={10.1109/INFOCOM.2017.8057001},
ISSN={},
month={May},}
@INPROCEEDINGS{8057002,
author={S. M. Irteza and H. M. Bashir and T. Anwar and I. A. Qazi and F. R. Dogar},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Load balancing over symmetric virtual topologies},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Datacenter networks are often structured as multi-rooted trees to provide high bisection bandwidth at low cost. To utilize the available bisection bandwidth, an efficient load balancing algorithm is required. Packet Spraying is known to perform well in symmetric topologies as it provides per-packet load balancing over equal cost paths. However, packet spraying performs poorly in asymmetric topologies. In this paper we ask, “How can we make packet spraying effective in asymmetric topologies while retaining its simplicity?” Towards this end, we propose SAPS, “Symmetric Adaptive Packet Spraying”, an SDN-based scheme that uses packet spraying over symmetric virtual topologies. SAPS is based on the key insight that if we provide each flow with a symmetric view of the network fabric, then packet spraying can produce near-optimal performance. We evaluate SAPS using simulations and testbed experiments. Our results indicate that SAPS performs well for a variety of application workloads and asymmetric network scenarios.},
keywords={routing protocols;software defined networking;telecommunication network topology;telecommunication traffic;trees (mathematics);bisection bandwidth;load balancing algorithm;SDN-based scheme;multirooted trees;packet spraying;Symmetric Adaptive Packet Spraying;SAPS;asymmetric topologies;symmetric topologies;symmetric virtual topologies;Topology;Spraying;Load management;Network topology;Switches;Bandwidth;Conferences},
doi={10.1109/INFOCOM.2017.8057002},
ISSN={},
month={May},}
@INPROCEEDINGS{8057003,
author={S. Roos and M. Byrenheid and C. Deusser and T. Strufe},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={BD-CAT: Balanced dynamic content addressing in trees},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Balancing the load in content addressing schemes for route-restricted networks represents a challenge with a wide range of applications. Solutions based on greedy embeddings maintain minimal state information and enable efficient routing, but any such solutions currently result in either imbalanced content addressing, overloading individual nodes, or are unable to efficiently account for network dynamics. In this work, we propose a greedy embedding in combination with a content addressing scheme that provides balanced content addressing while at the same time enabling efficient stabilization in the presence of network dynamics. We point out the tradeoff between stabilization complexity and maximal permitted imbalance when deriving upper bounds on both metrics for two variants of the proposed algorithms. Furthermore, we substantiate these bounds through a simulation study based on both real-world and synthetic data.},
keywords={content-addressable storage;greedy algorithms;resource allocation;telecommunication network routing;trees (mathematics);BD-CAT;trees;route-restricted networks;greedy embedding;minimal state information;efficient routing;imbalanced content addressing;individual nodes;network dynamics;content addressing scheme;balanced content;stabilization complexity;maximal permitted imbalance;balanced dynamic content addressing;Routing;Topology;Heuristic algorithms;Content addressable storage;Complexity theory;Conferences;Network topology},
doi={10.1109/INFOCOM.2017.8057003},
ISSN={},
month={May},}
@INPROCEEDINGS{8057004,
author={T. Jung and X. Li and W. Huang and J. Qian and L. Chen and J. Han and J. Hou and C. Su},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={AccountTrade: Accountable protocols for big data trading against dishonest consumers},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We propose AccountTrade, a set of accountable protocols, for big data trading among dishonest consumers. To secure the big data trading environment, our protocols achieve book-keeping ability and accountability against dishonest consumers who may misbehave throughout the dataset transactions. Specifically, we study the responsibilities of the consumers in the dataset trading and design AccountTrade to achieve accountability against the dishonest consumers who may try to deviate from their responsibilities. Specifically, we propose uniqueness index, a new rigorous measurement of the data uniqueness, as well as several accountable trading protocols to enable data brokers to blame the dishonest consumer when misbehavior is detected. We formally define, prove, and evaluate the accountability of our protocols by an automatic verification tool as well as extensive evaluation in real-world datasets. Our evaluation shows that AccountTrade incurs negligible constant storage overhead per file (&lt;;10KB), and it is able to handle 8-1000 concurrent data uploading per server depending on the data types.},
keywords={Big Data;electronic commerce;formal verification;transaction processing;uniqueness index;book-keeping ability;data brokers;accountable trading protocols;data uniqueness;dataset trading;big data trading environment;dishonest consumer;accountable protocols;AccountTrade;Protocols;Big Data;Monitoring;Cryptography;Data models;Conferences;Indexes},
doi={10.1109/INFOCOM.2017.8057004},
ISSN={},
month={May},}
@INPROCEEDINGS{8057005,
author={X. Yao and R. Zhang and Y. Zhang and Y. Lin},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Verifiable social data outsourcing},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Social data outsourcing is an emerging paradigm for effective and efficient access to the social data. In such a system, a third-party Social Data Provider (SDP) purchases complete social datasets from Online Social Network (OSN) operators and then resells them to data consumers who can be any individuals or entities desiring the complete social data satisfying some criteria. The SDP cannot be fully trusted and may return wrong query results to data consumers by adding fake data and deleting/modifying true data in favor of the businesses willing to pay. In this paper, we initiate the study on verifiable social data outsourcing whereby a data consumer can verify the trustworthiness of the social data returned by the SDP. We propose three schemes for verifiable queries over outsourced social data. The three schemes all require the OSN provider to generate some cryptographic auxiliary information, based on which the SDP can construct a verification object for the data consumer to verify the query-result trustworthiness. They differ in how the auxiliary information is generated and how the verification object is constructed and verified. Extensive experiments based on a real Twitter dataset confirm the high efficacy and efficiency of our schemes.},
keywords={data privacy;outsourcing;query processing;social networking (online);data consumer;verifiable social data outsourcing;third-party SDP;social datasets;OSN operators;true data modification;true data deletion;social data trustworthiness;Twitter dataset;fake data;Online Social Network operators;third-party Social Data Provider;Outsourcing;Twitter;Cryptography;Facebook;Conferences},
doi={10.1109/INFOCOM.2017.8057005},
ISSN={},
month={May},}
@INPROCEEDINGS{8057006,
author={P. Giridhar and S. Wang and T. Abdelzaher and R. Ganti and L. Kaplan and J. George},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={On localizing urban events with Instagram},
year={2017},
volume={},
number={},
pages={1-9},
abstract={This paper develops an algorithm that exploits picture-oriented social networks to localize urban events. We choose picture-oriented networks because taking a picture requires physical proximity, thereby revealing the location of the photographed event. Furthermore, most modern cell phones are equipped with GPS, making picture location, and time metadata commonly available. We consider Instagram as the social network of choice and limit ourselves to urban events (noting that the majority of the world population lives in cities). The paper introduces a new adaptive localization algorithm that does not require the user to specify manually tunable parameters. We evaluate the performance of our algorithm for various real-world datasets, comparing it against a few baseline methods. The results show that our method achieves the best recall, the fewest false positives, and the lowest average error in localizing urban events.},
keywords={meta data;social networking (online);Instagram;adaptive localization algorithm;physical proximity;picture location;urban event localization;picture-oriented social networks;photographed event location;time metadata;Algorithm design and analysis;Flickr;Twitter;Urban areas;Event detection;Metadata},
doi={10.1109/INFOCOM.2017.8057006},
ISSN={},
month={May},}
@INPROCEEDINGS{8057007,
author={Y. Yu and W. Wang and J. Zhang and K. Ben Letaief},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={LRC: Dependency-aware cache management for data analytics clusters},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Memory caches are being aggressively used in today's data-parallel systems such as Spark, Tez, and Piccolo. However, prevalent systems employ rather simple cache management policies - notably the Least Recently Used (LRU) policy - that are oblivious to the application semantics of data dependency, expressed as a directed acyclic graph (DAG). Without this knowledge, memory caching can at best be performed by “guessing” the future data access patterns based on historical information (e.g., the access recency and/or frequency), which frequently results in inefficient, erroneous caching with low hit ratio and a long response time. In this paper, we propose a novel cache replacement policy, Least Reference Count (LRC), which exploits the application-specific DAG information to optimize the cache management. LRC evicts the cached data blocks whose reference count is the smallest. The reference count is defined, for each data block, as the number of dependent child blocks that have not been computed yet. We demonstrate the efficacy of LRC through both empirical analysis and cluster deployments against popular benchmarking workloads. Our Spark implementation shows that, compared with LRU, LRC speeds up typical applications by 60%.},
keywords={cache storage;data analysis;directed graphs;LRC;dependency-aware cache management;data analytics clusters;LRU policy;data access patterns;hit ratio;response time;cache management policies;cache replacement policy;cluster deployments;empirical analysis;cached data blocks;application-specific DAG information;Least Reference Count;historical information;memory caching;directed acyclic graph;data dependency;application semantics;data-parallel systems;memory caches;Sparks;Data analysis;Semantics;Random access memory;Conferences;Benchmark testing},
doi={10.1109/INFOCOM.2017.8057007},
ISSN={},
month={May},}
@INPROCEEDINGS{8057008,
author={Y. Afek and A. Bremler-Barr and L. Shafir},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Network anti-spoofing with SDN data plane},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Traditional DDoS anti-spoofing scrubbers require dedicated middleboxes thus adding CAPEX, latency and complexity in the network. This paper starts by showing that the current SDN match-and-action model is rich enough to implement a collection of anti-spoofing methods. Secondly we develop and utilize advance methods for dynamic resource sharing to distribute the required mitigation resources over a network of switches. None of the earlier attempts to implement anti-spoofing in SDN actually directly exploited the match and action power of the switch data plane. They required additional functionalities on top of the match-and-action model, and are not implementable on an SDN switch as is. Our method builds on the premise that an SDN data path is a very fast and efficient engine to perform low level primitive operations at wire speed. The solution requires a number of flow-table rules and switch-controller messages proportional to the legitimate traffic. To scale when protecting multiple large servers the flow tables of multiple switches are harnessed in a distributed and dynamic network based solution. We have fully implemented all our methods in either Open-Flow1.5 in Open-vSwitch and in P4. The system mitigates spoofed attacks on either the SDN infrastructure itself or on downstream servers.},
keywords={computer network security;software defined networking;telecommunication traffic;network anti-spoofing;SDN data plane;traditional DDoS anti-spoofing scrubbers;anti-spoofing methods;dynamic resource;switch data plane;SDN switch;SDN data path;low level primitive operations;flow-table rules;switch-controller messages;multiple switches;distributed network based solution;dynamic network based solution;SDN infrastructure;SDN match-and-action model;mitigation resources;legitimate traffic;Open-Flow1.5;Open-vSwitch;P4;downstream servers;Computer crime;Servers;Control systems;Software;Materials handling;Conferences;Electronic mail},
doi={10.1109/INFOCOM.2017.8057008},
ISSN={},
month={May},}
@INPROCEEDINGS{8057009,
author={G. Shang and P. Zhe and X. Bin and H. Aiqun and R. Kui},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={FloodDefender: Protecting data and control plane resources under SDN-aimed DoS attacks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The separated control and data planes in software-defined networking (SDN) with high programmability introduce a more flexible way to manage and control network traffic. However, SDN will experience long packet delay and high packet loss rate when the communication link between two planes is jammed by SDN-aimed DoS attacks with massive table-miss packets. In this paper, we propose FloodDefender, an efficient and protocol-independent defense framework for SDN/OpenFlow networks to mitigate DoS attacks. It stands between the controller platform and other controller apps, and can protect both the data and control plane resources by leveraging three new techniques: table-miss engineering to prevent the communication bandwidth from being exhausted; packet filter to identify attack traffic and save computational resources of the control plane; and flow rule management to eliminate most of useless flow entries in the switch flow table. All designs of FloodDefender conform to the OpenFlow policy, requiring no additional devices. We implement a prototype of FloodDefender and evaluate its performance in both software and hardware environments. Experimental results show that FloodDefender can efficiently mitigate the SDN-aimed DoS attacks, incurring less than 0.5% CPU computation to handle attack traffic, only 18ms packet delay and 5% packet loss rate under attacks.},
keywords={computer network security;protocols;software defined networking;telecommunication network routing;telecommunication traffic;network traffic control;SDN-aimed DoS attacks;plane resources;packet loss rate;packet delay;switch flow table;flow rule management;computational resources;attack traffic;packet filter;table-miss engineering;controller apps;controller platform;SDN/OpenFlow networks;protocol-independent defense framework;massive table-miss packets;high programmability;networking;data planes;separated control;FloodDefender;time 18.0 ms;Switches;Computer crime;Bandwidth;Delays;Monitoring;Conferences},
doi={10.1109/INFOCOM.2017.8057009},
ISSN={},
month={May},}
@INPROCEEDINGS{8057010,
author={A. Bremler-Barr and E. Brosh and M. Sides},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={DDoS attack on cloud auto-scaling mechanisms},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Auto-scaling mechanisms are an important line of defense against Distributed Denial of Service (DDoS) in the cloud. Using auto-scaling, machines can be added and removed in an on-line manner to respond to fluctuating load. It is commonly believed that the auto-scaling mechanism casts DDoS attacks into Economic Denial of Sustainability (EDoS) attacks. Rather than suffering from performance degradation up to a total denial of service, the victim suffers only from the economic damage incurred by paying for the extra resources required to process the bogus traffic of the attack. Contrary to this belief, we present and analyze the Yo-Yo attack, a new attack against the auto-scaling mechanism, that can cause significant performance degradation in addition to economic damage. In the Yo-Yo attack, the attacker sends periodic bursts of overload, thus causing the auto-scaling mechanism to oscillate between scale-up and scale-down phases. The Yo-Yo attack is harder to detect and requires less resources from the attacker compared to traditional DDoS. We demonstrate the attack on Amazon EC2 [4], and analyze protection measures the victim can take by reconfiguring the auto-scaling mechanism.},
keywords={cloud computing;computer network security;Yo-Yo attack;auto-scaling mechanism;DDoS attack;cloud auto-scaling mechanisms;distributed denial of service attack;economic denial of sustainability attack;EDoS attack;Amazon EC2;Computer crime;Cloud computing;Economics;Adaptive systems;Conferences;Google;Sustainable development;Cloud attack;Auto-scaling;Denial-of-service attack;Economic-Denial-of-Sustainability attack;Distributed systems security},
doi={10.1109/INFOCOM.2017.8057010},
ISSN={},
month={May},}
@INPROCEEDINGS{8057011,
author={M. Jadin and G. Tihon and O. Pereira and O. Bonaventure},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Securing multipath TCP: Design amp;amp; implementation},
year={2017},
volume={},
number={},
pages={1-9},
abstract={MultiFath TCP (MPTCP) is a recent TCP extension that enables hosts to send data over multiple paths for a single connection. It is already deployed for various use cases, notably on smartphones. In parallel with this, there is a growing deployment of encryption and authentication techniques to counter various forms of security attacks. Tcpcrypt and TLS are some of these security solutions. In this paper, we propose MPTCPsec, a MultiPath TCP extension that closely integrates authentication and encryption inside the protocol itself. Our design relies on an adaptation for the multipath environment of the ENO option that is being discussed within the IETF tcpinc working group. We then detail how MultiPath TCP needs to be modified to authenticate and encrypt all data and authenticate the different TCP options that it uses. Finally, we implement our proposed extension in the reference implementation of MultiPath TCP in the Linux kernel and we evaluate its performance.},
keywords={cryptographic protocols;IP networks;Linux;telecommunication security;transport protocols;authentication techniques;security attacks;security solutions;MultiPath TCP extension;encryption;multipath environment;Tcpcrypt;smartphones;protocol;Linux kernel;Protocols;Encryption;Authentication;Smart phones;Linux},
doi={10.1109/INFOCOM.2017.8057011},
ISSN={},
month={May},}
@INPROCEEDINGS{8057012,
author={M. van der Boor and S. Borst and J. van Leeuwaarden},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Load balancing in large-scale systems with multiple dispatchers},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Load balancing algorithms play a crucial role in delivering robust application performance in data centers and cloud networks. Recently, strong interest has emerged in Join-the-Idle-Queue (JIQ) algorithms, which rely on tokens issued by idle servers in dispatching tasks and outperform power-of-d policies. Specifically, JiQ strategies involve minimal information exchange, and yet achieve zero blocking and wait in the many-server limit. The latter property prevails in a multiple-dispatcher scenario when the loads are strictly equal among dispatchers. For various reasons it is not uncommon however for skewed load patterns to occur. We leverage product-form representations and fluid limits to establish that the blocking and wait then no longer vanish, even for arbitrarily low overall load. Remarkably, it is the least-loaded dispatcher that throttles tokens and leaves idle servers stranded, thus acting as bottleneck. Motivated by the above issues, we introduce two enhancements of the ordinary JIQ scheme where tokens are either distributed non-uniformly or occasionally exchanged among the various dispatchers. We prove that these extensions can achieve zero blocking and wait in the many-server limit, for any subcritical overall load and arbitrarily skewed load profiles. Extensive simulation experiments demonstrate that the asymptotic results are highly accurate, even for moderately sized systems.},
keywords={cloud computing;queueing theory;resource allocation;large-scale systems;load balancing algorithms;data centers;load profiles;throttles tokens;fluid limits;leverage product-form representations;skewed load patterns;multiple-dispatcher scenario;zero blocking;JiQ strategies;dispatching tasks;Join-the-Idle-Queue algorithms;cloud networks;Servers;Load modeling;Conferences;Load management;Algorithm design and analysis;Large-scale systems},
doi={10.1109/INFOCOM.2017.8057012},
ISSN={},
month={May},}
@INPROCEEDINGS{8057013,
author={W. R. KhudaBukhsh and A. Rizk and A. Frömmgen and H. Koeppl},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Optimizing stochastic scheduling in fork-join queueing models: Bounds and applications},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Fork-Join (FJ) queueing models capture the dynamics of system parallelization under synchronization constraints, for example, for applications such as MapReduce, multipath transmission and RAID systems. Arriving jobs are first split into tasks and mapped to servers for execution, such that a job can only leave the system when all of its tasks are executed. In this paper, we provide computable stochastic bounds for the waiting and response time distributions for heterogeneous FJ systems under general parallelization benefit. Our main contribution is a generalized mathematical framework for probabilistic server scheduling strategies that are essentially characterized by a probability distribution over the number of utilized servers, and the optimization thereof. We highlight the trade-off between the scaling benefit due to parallelization and the FJ inherent synchronization penalty. Further, we provide optimal scheduling strategies for arbitrary scaling regimes that map to different levels of parallelization benefit. One notable insight obtained from our results is that different applications with varying parallelization benefits result in different optimal strategies. Finally, we complement our analytical results by applying them to various applications showing the optimality of the proposed scheduling strategies.},
keywords={optimisation;queueing theory;statistical distributions;stochastic processes;telecommunication scheduling;optimality;system parallelization;synchronization constraints;optimal strategies;stochastic scheduling optimization;fork-join queueing models;FJ queueing models;arbitrary scaling regimes;optimal scheduling strategies;FJ inherent synchronization penalty;scaling benefit;probability distribution;probabilistic server scheduling strategies;generalized mathematical framework;general parallelization benefit;heterogeneous FJ systems;waiting response time distributions;Servers;Time factors;Optimal scheduling;Computational modeling;Steady-state;Processor scheduling},
doi={10.1109/INFOCOM.2017.8057013},
ISSN={},
month={May},}
@INPROCEEDINGS{8057014,
author={A. Anand and G. de Veciana},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Measurement-based scheduler for multi-class QoE optimization in wireless networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Traditional wireless schedulers have been driven by rate-based criteria, e.g., utility maximizing/proportionally fair, and/or queue-based packet schedulers which do not directly reflect the Quality of Experience (QoE) associated with flow-based transactions and services. This paper proposes, a Measurement-Based Delay Optimal (MBDO) scheduler, which optimizes a cost function of the mean flow delays in a multi-class system, e.g,. web interactive, file downloads, etc. In this context the cost function expresses desired trade-offs amongst traffic classes reflecting heterogeneous QoE sensitivities which are nonlinear in the flow delays and/or system loads. To achieve optimality, MBDO scheduling uses measured system variables and knowledge (or measurement) of class flow-size distributions to adapt a weighted Gittins index scheduler. We show that under mild assumptions, and in a stationary regime, that MBDO scheduling is indeed asymptotically optimal. Perhaps more importantly, MBDO schedulers can self-optimize by adapting to slowly varying traffic loads, mixes and flow size distributions. Our extensive simulations confirm the effectiveness at realizing trade-offs and performance of the proposed approach.},
keywords={optimisation;quality of experience;radio networks;telecommunication scheduling;telecommunication traffic;MBDO schedulers;flow size distributions;multiclass QoE optimization;wireless networks;packet schedulers;flow-based transactions;Delay Optimal scheduler;cost function;mean flow delays;multiclass system;traffic classes;heterogeneous QoE sensitivities;system loads;MBDO scheduling;system variables;class flow-size distributions;weighted Gittins index scheduler;wireless schedulers;quality of experience;measurement-based delay optimal scheduler;Delays;Cost function;Base stations;Wireless networks},
doi={10.1109/INFOCOM.2017.8057014},
ISSN={},
month={May},}
@INPROCEEDINGS{8057015,
author={A. Davydow and P. Chuprikov and S. I. Nikolenko and K. Kogan},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Throughput optimization with latency constraints},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Modern datacenters are increasingly required to deal with latency-sensitive applications. A major question here is how to represent latency in desired objectives. Incorporation of multiple traffic characteristics (e.g., packet values and required processing requirements) significantly increases the complexity of buffer management policies. In this work, we consider weighted throughput optimization (total transmitted value) in the setting where every incoming packet is branded with intrinsic value, required processing, and slack (an offset from the arrival time when a packet should be transmitted), and the buffer is unbounded but effectively bounded by slacks. The main result is a 3-competitive algorithm as the slack-to-work ratio increases. Our results supported by a comprehensive evaluation study on CAIDA network traces.},
keywords={competitive algorithms;computer centres;Internet;telecommunication traffic;slack-to-work ratio increases;throughput optimization;latency constraints;multiple traffic characteristics;packet values;buffer management policies;datacenters;CAIDA network;Throughput;Process control;Algorithm design and analysis;Conferences;Optimization;Scheduling;Upper bound},
doi={10.1109/INFOCOM.2017.8057015},
ISSN={},
month={May},}
@INPROCEEDINGS{8057016,
author={Y. Luo and L. Pu and Y. Zhao and G. Wang and M. Song},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Optimal energy requesting strategy for RF-based energy harvesting wireless communications},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Energy harvesting is emerging as a promising alternative source to power the next generation of wireless networks. This paper introduces a new energy harvesting strategy that uses a dedicated energy source to optimally replenish energy for radio frequency (RF) based wireless communication systems. Specifically, we develop a two-step dual tunnel energy requesting (DTER) strategy that allows an energy harvesting device to effectively obtain energy from a dedicated energy source. While minimizing the system energy consumption, DTER takes into account the practical constraints on both the energy source and the energy harvesting device. Additionally, the overhead issue and the charge characteristics of an energy storage component are examined to make the proposed strategy practical. To solve the nonlinear optimization problem in DTER, we convert the design of optimal energy requesting problem into a classic shortest path problem and thus enable us to find a global optimal solution through dynamic programming algorithms. Theoretical analysis and simulation study verify that DTER outperforms two other schemes in the literature.},
keywords={energy harvesting;nonlinear programming;radio networks;telecommunication power supplies;optimal energy requesting strategy;RF;wireless communications;wireless networks;energy harvesting strategy;wireless communication systems;two-step dual tunnel energy;energy harvesting device;system energy consumption;energy storage component;nonlinear optimization problem;optimal energy requesting problem;energy source;DTER strategy;Energy consumption;Energy harvesting;Batteries;Radio frequency;Data communication;Erbium},
doi={10.1109/INFOCOM.2017.8057016},
ISSN={},
month={May},}
@INPROCEEDINGS{8057017,
author={H. Dai and X. Wang and A. X. Liu and H. Ma and G. Chen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Optimizing wireless charger placement for directional charging},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Wireless Power Transfer (WPT) technology has witnessed huge development because of its convenience and reliability. This paper concerns the fundamental issue of wireless charger PLacement with Optimized charging uTility (PLOT), that is, given a fixed number of chargers and a set of points on the plane, determining the positions and orientations of chargers such that the overall expected charging utility for all points is maximized. To address PLOT, we propose a 1 - 1/e - ε approximation algorithm. First, we present techniques to approximate the nonlinear charging power and the expected charging utility to make the problem almost linear. Second, we develop a Dominating Coverage Set extraction method to reduce the continuous search space of PLOT to a limited and discrete one without performance loss. Third, we prove that the reformulated problem is essentially maximizing a monotone submodular function subject to a matroid constraint, and propose a greedy algorithm to address this problem. We conduct both simulation and field experiments to validate our theoretical results, and the results show that our algorithm can outperform comparison algorithms by at least 46.3%.},
keywords={approximation theory;computational complexity;greedy algorithms;inductive power transmission;optimisation;set theory;ε approximation algorithm;Dominating Coverage Set extraction method;directional charging;Wireless Power Transfer technology;reliability;wireless charger placement and optimized charging utility;wireless charger PLOT;nonlinear charging power approximation;matroid constraint;monotone submodular function;Wireless communication;Wireless sensor networks;Approximation algorithms;Conferences;Greedy algorithms;Performance evaluation;Reliability},
doi={10.1109/INFOCOM.2017.8057017},
ISSN={},
month={May},}
@INPROCEEDINGS{8057018,
author={F. Wang and X. Zhang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Secure resource allocations for polarization-enabled cooperative cognitive radio networks with energy harvesting capability},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We address secure communications over energy-harvesting based OFDMA cooperative cognitive radio networks, where one primary user (PU) cooperates with several secondary users (SUs) in terms of both information transmission and energy harvesting. To improve spectrum utilization and ensure that SU transmitters can harvest as much energy as possible, we suppose that SUs are equipped with orthogonally dual-polarized antennas. Based on these setting-ups, we propose the polarization-enabled two-phase cooperative framework, where SU transmitters first apply power splitting technique to harvest energy from radio frequency signals radiated by the PU transmitter, and then use the harvested energy to concurrently transmit their own and the PU's data. Under the proposed framework, we develop the secure resource allocation schemes for the scenarios when SU receivers are untrusted users, implying that each SU receiver may overhear the PU's and the other SUs' confidential information. For this scenario, which has hardly been studied, we investigate the joint allocation of relays, subcarriers, power splitting ratios, and powers, with the objective to maximize the total secrecy rate of all SUs while guaranteeing the PU's minimum secrecy rate. Finally, we validate and evaluate our proposed cooperative framework and resource allocation schemes through numerical analyses.},
keywords={antennas;cognitive radio;cooperative communication;energy harvesting;frequency division multiple access;OFDM modulation;radio networks;radio receivers;radio transmitters;resource allocation;telecommunication security;energy harvesting capability;primary user;secondary users;information transmission;spectrum utilization;SU transmitters;dual-polarized antennas;power splitting technique;radio frequency signals;PU transmitter;SU receiver;untrusted users;power splitting ratios;communication security;resource allocation schemes;resource allocation security;PU minimum secrecy rate;energy-harvesting based OFDMA cooperative cognitive radio networks;Resource management;Transmitters;Receivers;Antennas;Fading channels;Energy harvesting;Relays;Energy harvesting;cooperative overlay CRN;orthogonally dual-polarized antennas (ODPAs);secure communication;resource allocation;RF signals},
doi={10.1109/INFOCOM.2017.8057018},
ISSN={},
month={May},}
@INPROCEEDINGS{8057019,
author={L. Yan and H. Shen and J. Zhao and C. Xu and F. Luo and C. Qiu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={CatCharger: Deploying wireless charging lanes in a metropolitan road network through categorization and clustering of vehicle traffic},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The future generation of transportation system will be featured by electrified public transportation. To fulfill metropolitan transit demands, electric vehicles (EVs) must be continuously operable without recharging downtime. Wireless Power Transfer (WPT) techniques for in-motion EV charging is a solution. It however brings up a challenge: how to deploy charging lanes in a metropolitan road network to minimize the deployment cost while enabling EVs' continuous operability. In this paper, we propose CatCharger, which is the first work that handles this challenge. From a metropolitan-scale dataset collected from multiple sources of vehicles, we observe the diversity of vehicle passing speed and daily visit frequency (called traffic attributes) at intersections (i.e., landmarks), which are important factors for charging lane deployment. To select landmarks for deployment, we first group landmarks with similar traffic attribute values using the entropy minimization clustering method, and choose better candidate landmarks from each group suitable for deployment. To determine the deployment locations from the candidate landmarks, we infer the expected vehicle residual energy at each landmark using a Kernel Density Estimator fed by the vehicles' mobility, and formulate and solve an optimization problem to minimize the total deployment cost while ensuring a certain level of expected residual energy of EVs at each landmark. Our trace-driven experiments demonstrate the superior performance of CatCharger over other methods.},
keywords={electric vehicle charging;electric vehicles;inductive power transmission;optimisation;pattern clustering;road traffic;traffic engineering computing;transportation;vehicle traffic;transportation system;electrified public transportation;metropolitan transit demands;electric vehicles;Wireless Power Transfer techniques;in-motion EV charging;metropolitan road network;CatCharger;metropolitan-scale dataset;lane deployment;group landmarks;entropy minimization clustering method;total deployment cost;wireless charging lanes;EVs continuous operability;traffic attribute values;vehicle residual energy;kernel density estimator;Roads;Charging stations;Inductive charging;Optimization;Batteries;Wireless power transfer;Conferences},
doi={10.1109/INFOCOM.2017.8057019},
ISSN={},
month={May},}
@INPROCEEDINGS{8057020,
author={X. Xia and S. Li and Y. Zhang and L. Li and T. Gu and Y. Liu and Y. Pan},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Surviving screen-off battery through out-of-band Wi-Fi coordination},
year={2017},
volume={},
number={},
pages={1-9},
abstract={This paper identifies two energy saving opportunities of Wi-Fi interface emerged during smartphone's screen-off periods. Exploiting the opportunities, we propose a new power saving strategy, BackPSM, for screen-off Wi-Fi communications. BackPSM regulates client to send and receive packets in batches and coordinates multiple clients to communicate at different slots (i.e., beacon interval). The core problem in BackPSM is how to coordinate client without incurring extra traffic overheads. To handle the problem, we propose a novel paradigm, Out-of-Band Communication (OBC), for client-to-client direct communications. OBC exploits the TIM (Traffic Indication Map) field of Wi-Fi Beacon to create a free side-channel between clients. It is based upon the observation that a client may control 1 → 0 appearing on TIM bit by locally regulating packet receiving operations. We adopt this 1 → 0 as the basic signal, and leverage the time length in between two signals to encode information. We demonstrate that OBC can be used to convey coordination information with close to 100% accuracy. We have implemented and evaluated BackPSM on a testbed. The results show that BackPSM reduces screen-off energy by up to 60%, and outperforms state-of-the-art strategies by 16%-42%.},
keywords={telecommunication power management;telecommunication traffic;wireless LAN;Wi-Fi Beacon;OBC;coordination information;BackPSM;out-of-band Wi-Fi coordination;Wi-Fi interface;power saving strategy;screen-off Wi-Fi communications;beacon interval;Out-of-Band Communication;client-to-client direct communications;TIM field;Traffic Indication Map;packet receiving operations;screen-off battery;Wireless fidelity;Switches;Smart phones;Delays;Energy consumption;Scalability},
doi={10.1109/INFOCOM.2017.8057020},
ISSN={},
month={May},}
@INPROCEEDINGS{8057021,
author={M. Jin and Y. He and D. Fang and X. Chen and X. Meng and T. Xing},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={iGuard: A real-time anti-theft system for smartphones},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Smartphone theft is a non-negligible problem that causes serious concerns on personal property, privacy, and public security. The existing solutions to this problem either provide only functions like retrieving a phone, or require dedicated hardware to detect thefts. How to protect smartphones from being stolen at all times is still an open problem. In this paper, we propose iGuard, a real-time anti-theft system for smartphones. iGuard utilizes only the inertial sensing data from the smartphone. The basic idea behind iGuard is to distinguish different people holding a smartphone, by identifying the order of the motions during the `take-out' behavior and how each motion is performed. For this purpose, we design a motion segmentation algorithm to detect the transition between two motions from the noisy sensing data. We then leverage the distinct feature contained in each sub-segment of a motion, instead of the entire motion, to estimate the probability that the motion is performed by the smartphone owner himself/herself. Based on such pre-processed data, we propose a Markov Chain based model to track the behavior of a smartphone user. According to this model, iGuard instantly alarms once the tracked data deviate from the smartphone owner's usual habit. We implement iGuard on Android and evaluate its performance in real environments. The experimental results show that iGuard is accurate and robust in various scenarios.},
keywords={human factors;Markov processes;security of data;smart phones;inertial sensing data;motion segmentation algorithm;noisy sensing data;smartphone owner;smartphone user;smartphone theft;iGuard system;realtime anti-theft system;smart phone user behavior;Markov chain based model;Android;Smart phones;Legged locomotion;Acceleration;Sensors;Feature extraction;Real-time systems;Tracking},
doi={10.1109/INFOCOM.2017.8057021},
ISSN={},
month={May},}
@INPROCEEDINGS{8057022,
author={X. Xu and H. Gao and J. Yu and Y. Chen and Y. Zhu and G. Xue and M. Li},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={ER: Early recognition of inattentive driving leveraging audio devices on smartphones},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Real-time driving behavior monitoring is a corner stone to improve driving safety. Most of the existing studies on driving behavior monitoring using smartphones only provide detection results after an abnormal driving behavior is finished, not sufficient for driver alert and avoiding car accidents. In this paper, we leverage existing audio devices on smartphones to realize early recognition of inattentive driving events including Fetching Forward, Picking up Drops, Turning Back and Eating or Drinking. Through empirical studies of driving traces collected in real driving environments, we find that each type of inattentive driving event exhibits unique patterns on Doppler profiles of audio signals. This enables us to develop an Early Recognition system, ER, which can recognize inattentive driving events at an early stage and alert drivers timely. ER employs machine learning methods to first generate binary classifiers for every pair of inattentive driving events, and then develops a modified vote mechanism to form a multi-classifier for all inattentive driving events along with other driving behaviors. It next turns the multi-classifier into a gradient model forest to achieve early recognition of inattentive driving. Through extensive experiments with 8 volunteers driving for about half a year, ER can achieve an average total accuracy of 94.80% for inattentive driving recognition and recognize over 80% inattentive driving events before the event is 50% finished.},
keywords={audio signal processing;driver information systems;learning (artificial intelligence);road accidents;road safety;signal classification;smart phones;ER;smartphones;real-time driving behavior monitoring;driving safety;abnormal driving behavior;driving environments;driving behaviors;inattentive driving recognition;audio devices;audio signals;early recognition system;machine learning;multiclassifier;Smart phones;Erbium;Doppler effect;Automobiles;Monitoring;Turning},
doi={10.1109/INFOCOM.2017.8057022},
ISSN={},
month={May},}
@INPROCEEDINGS{8057023,
author={Y. Gao and Y. Luo and D. Chen and H. Huang and W. Dong and M. Xia and X. Liu and J. Bu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Every pixel counts: Fine-grained UI rendering analysis for mobile applications},
year={2017},
volume={},
number={},
pages={1-9},
abstract={For mobile apps, user-perceived delays are critical for user satisfaction. According to our measurement, long delays are commonly caused by network and storage I/O operations while short delays are mainly caused by UI rendering. Short delays are not uncommon, which account for 55.3% in our measurement cases. Previous app performance studies have largely focused on I/O operations but the understanding of UI rendering impact is limited. In this work, we propose DRAW, a system that performs two UI rendering analyses to help app developers pinpoint rendering problems and resolve short delays. The first analysis outlines the wasted rendering time on invisible or covered UI components, namely the overdraw problem. The second analysis is to identify the responsible UI components and rendering operations that cause overall low rendering efficiency. We implement DRAW on Android and apply it to study 1,158 real-world Android apps. Results show that DRAW is helpful as it can pinpoint the responsible UI components and specific rendering operations. Four concrete case studies of real-world apps are further presented to show how DRAW can help developers improve the UI rendering performance of their apps.},
keywords={Android (operating system);mobile computing;rendering (computer graphics);smart phones;user interfaces;mobile applications;user satisfaction;I/O operations;UI rendering impact;DRAW;UI rendering analyses;invisible components;covered UI components;responsible UI components;low rendering efficiency;specific rendering operations;Android apps;fine-grained UI rendering analysis;app performance studies;Rendering (computer graphics);Androids;Humanoid robots;Mobile applications;Delays;Smart phones;Time factors},
doi={10.1109/INFOCOM.2017.8057023},
ISSN={},
month={May},}
@INPROCEEDINGS{8057024,
author={G. Gao and Y. Wen and H. Hu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={QDLCoding: QoS-differentiated low-cost video encoding scheme for online video service},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Adaptive bitrate (ABR) streaming is the de facto solution in online video services to cope with heterogeneous devices and varying network connections. However, this solution is computation intensive, demanding a large number of servers for encoding videos. Moreover, due to the time-varying nature of video generation, intelligent strategies are required in order to determine the right amount of resources for encoding. The situation is further complicated by the fact that, the two types of co-existing video content, live content and Video-on-Demand (VoD) content, have different QoS requirements for encoding. These observations posit daunting challenges for meeting the heterogeneous QoS requirements with a minimum computing capacity. This paper proposes the QoS-differentiated low-cost video encoding (QDLCoding) scheme to address these challenges. We develop a framework for scheduling the encoding workloads of the two types of videos with statistical QoS guarantees. Each type of videos is specified with a QoS criterion and a QoS loss bound. The objective is to provision the minimum amount of resources while keeping the QoS loss probabilities within the prescribed bounds. We design an online algorithm that can determine the minimum required capacity by learning content arrival distributions. The experiment results demonstrate that our method can greatly reduce the required capacity for encoding online videos while controlling the likelihood of QoS loss precisely.},
keywords={probability;quality of service;video coding;video on demand;QoS loss probabilities;online algorithm;content arrival distributions;online video service;adaptive bitrate streaming;facto solution;heterogeneous devices;varying network connections;time-varying nature;video generation;video content;live content;Video-on-Demand content;heterogeneous QoS requirements;low-cost video encoding;statistical QoS guarantees;online video encoding;QoS-differentiated low-cost video encoding scheme;QDLCoding scheme;Streaming media;Quality of service;Encoding;Servers;Resource management;Delays;Capacity planning},
doi={10.1109/INFOCOM.2017.8057024},
ISSN={},
month={May},}
@INPROCEEDINGS{8057025,
author={M. Tang and S. Wang and L. Gao and J. Huang and L. Sun},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={MOMD: A multi-object multi-dimensional auction for crowdsourced mobile video streaming},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Crowdsourced mobile video streaming enables nearby mobile video users to aggregate their network resources to improve the video streaming performance. However, users are often selfish and may not be willing to cooperate without proper incentives. Designing an incentive mechanism for such a scenario is challenging due to the users' asynchronous downloading behaviors as well as their private valuations for multi-bitrate encoded videos. In this work, we propose a multi-object multi-dimensional auction-based incentive framework, through which users can download multiple video segments with different bitrates for multiple nearby users (and themselves). Based on this incentive framework, we propose a Vickrey-score auction, which is the first multi-object multi-dimensional auction that achieves both truthfulness and efficiency. Simulations with real traces show that crowdsourced mobile streaming outperforms noncooperative streaming by 48.6% (on average) in terms of social welfare. We further implement our proposed auction mechanism in a demostration system, and show that the crowdsourced framework together with the auction mechanism can substantially increase mobile user's welfare and video service stability.},
keywords={commerce;incentive schemes;mobile computing;video coding;video streaming;Vickrey-score auction;multidimensional auction;crowdsourced mobile streaming outperforms;auction mechanism;crowdsourced framework;crowdsourced mobile video streaming;video streaming performance;incentive mechanism;multibitrate encoded videos;incentive framework;video segments;mobile video users;Streaming media;Bit rate;Adaptation models;Mobile communication;Resource management;Mobile computing;Cost accounting},
doi={10.1109/INFOCOM.2017.8057025},
ISSN={},
month={May},}
@INPROCEEDINGS{8057026,
author={K. Matsuzono and H. Asaeda and T. Turletti},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Low latency low loss streaming using in-network coding and caching},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Owing to the rapid growth in high-quality video streaming over the Internet, preserving high-level robustness against data loss and low latency, while maintaining higher data transmission rates, is becoming an increasingly important issue for high-quality real-time delay-sensitive streaming. In this paper, we propose a low latency, low loss streaming mechanism, L4C2, specialized for high-quality delay-sensitive streaming. With L4C2, nodes in the network estimate the acceptable delay and packet loss probability in their uplinks, aiming at retrieving lost data packets from in-network cache and/or coded data packets using in-network coding within an acceptable delay, by extending the Content-Centric Networking (CCN) approach. Further, L4C2 naturally provides multiple paths and multicast technologies to efficiently utilize network resources while sharing network resources fairly with competing data flows by adjusting the video quality when necessary. We validate through comprehensive simulations that L4C2 achieves a high success probability of data transmission considering the acceptable one-way delay, and higher QoE while suppressing the interest and redundant data traffic than the proposed multipath congestion control mechanism in CCN.},
keywords={delays;Internet;network coding;probability;telecommunication congestion control;telecommunication traffic;video streaming;data loss;higher data transmission rates;high-quality real-time delay-sensitive streaming;low loss streaming mechanism;L4C2;high-quality delay-sensitive streaming;network estimate;acceptable delay;lost data packets;in-network cache;in-network coding;Content-Centric Networking approach;network resources;data flows;video quality;high success probability;redundant data traffic;low latency low loss;high-quality video;high-level robustness;Streaming media;Delays;Real-time systems;Encoding;Robustness;Video recording;Quality assessment},
doi={10.1109/INFOCOM.2017.8057026},
ISSN={},
month={May},}
@INPROCEEDINGS{8057027,
author={A. Badr and A. Khisti and W. Tan and X. Zhu and J. Apostolopoulos},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={FEC for VoIP using dual-delay streaming codes},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We introduce a new class of forward error correction (FEC) codes for VoIP communications which support different recovery delay depending on the channel conditions. Specifically, our proposed class of Dual-Delay (DD) codes can recover from challenging long bursts of losses with close to theoretical minimum delay so as to meet playback deadlines for recovered packets. They further improve conversational interactivity by achieving lower recovery delay during periods of random isolated losses. These DD codes are shown to achieve lower residual loss rates when compared to existing codes over a wide range of parameters of the Gilbert-Elliott channel. Experiments over real world packet traces further show performance gains of DD codes in terms of perceptually motivated ITU-T G.107 E-model.},
keywords={delays;dual codes;forward error correction;Internet telephony;media streaming;FEC;dual-delay streaming codes;forward error correction codes;VoIP communications;channel conditions;Dual-Delay codes;playback deadlines;recovered packets;random isolated losses;DD codes;Gilbert-Elliott channel;recovery delay;conversational interactivity;residual loss rates;Delays;Decoding;Forward error correction;Linear codes;Conferences;Systematics;Channel models;VoIP Audio Quality;Interactive Multimedia Applications;Application Layer Forward Error Correction (AL-FEC);Burst Erasures;E-model},
doi={10.1109/INFOCOM.2017.8057027},
ISSN={},
month={May},}
@INPROCEEDINGS{8057028,
author={G. Tang and K. Wu and R. Brunner},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Rethinking CDN design with distributee time-varying traffic demands},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The content delivery network (CDN) intensively uses cache to push the content close to end users. Over both traditional Internet architecture and emerging cloud-based framework, cache allocation has been the core problem that any CDN operator needs to address. As the first step for cache deployment, CDN operators need to discover or estimate the distribution of user requests in different geographic areas. This step results in a statistical spatial model for the user requests, which is used as the key input to solve the optimal cache deployment problem. More often than not, the temporal information in user requests is omitted to simplify the CDN design. In this paper, we disclose that the spatial request model alone may not lead to truly optimal cache deployment. By considering the temporal information in user requests, we provide a dynamic traffic based solution to this broadly studied problem. Via experiments over the North American ISPs Points of Presence (PoPs) network, our new solution outperforms traditional CDN design method and saves the overall delivery cost by 16% to 20%.},
keywords={cache storage;cloud computing;Internet;telecommunication traffic;distributee time-varying traffic demands;content delivery network;CDN operator;statistical spatial model;optimal cache deployment problem;temporal information;spatial request model;dynamic traffic based solution;CDN design method;North American ISP Point-of-Presence network;North American ISP-PoP network;Optimization;Servers;Network topology;Topology;Internet;Conferences},
doi={10.1109/INFOCOM.2017.8057028},
ISSN={},
month={May},}
@INPROCEEDINGS{8057029,
author={S. Shukla and A. A. Abouzeid},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Proactive retention aware caching},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We consider the problem of proactive (i.e. predictive) content caching that is aware of the costs of retention of the content in the cache. Prior work on caching (whether proactive or reactive) does not explicitly take into account the storage cost due to the duration of time for which a content is cached. This new problem, which we call retention aware caching, is motivated by two recent technological developments that are described in the paper: cloud storage rental costs and flash memory damage. We consider a hierarchical network consisting of a server connected to a number of cache-enabled nodes, located either at the edge of a network (e.g. base stations) or in the core of a data center. There are two types of network costs: storage cost at the caches and download cost from the server. We formulate the problem of proactive retention aware data caching (PRAC), which minimizes the total cost subject to the node capacity constraints. We first prove that PRAC is NP-Hard in general and then analyze PRAC for two cases: (1) linear storage cost, (2) convex storage cost. We show that PRAC admits efficient polynomial time algorithms when the storage cost is linear in retention times and caches have a large capacity. Furthermore, we derive bounds on the performance of PRAC for the case when the storage cost is a practically motivated convex function. Numerical evaluations demonstrate that PRAC outperforms other state-of-the-art caching policies for a wide range of parameters of interest.},
keywords={cache storage;cloud computing;computational complexity;convex programming;flash memories;linear programming;PRAC;proactive retention aware caching;content caching;cloud storage rental costs;cache-enabled nodes;network costs;download cost;retention times;caching policies;linear storage cost;convex storage cost;flash memory damage;hierarchical network;NP-hard problem;Servers;Cloud computing;Delays;Conferences;Multicast communication;Hardware},
doi={10.1109/INFOCOM.2017.8057029},
ISSN={},
month={May},}
@INPROCEEDINGS{8057030,
author={L. Qiu and G. Cao},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Popularity-aware caching increases the capacity of wireless networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In wireless ad hoc networks, due to the interference between concurrent transmissions, the per node capacity generally decreases with the increasing number of nodes in the network. Caching can help improve the network capacity, as it shortens the content transmission distance and reduces the communication interference. However, current researches on the capacity of wireless ad hoc networks with caching generally assume that content popularity follows uniform distribution. They ignore the fact that contents in reality have skewed popularity, which may lead to totally different capacity results. In this paper, we evaluate how the distribution of the content popularity affects the network capacity, and derive different capacity scaling laws based on the skewness of the content popularity. Our results suggest that for wireless networks with caching, when contents have skewed popularity, increasing the number of nodes monotonically increases the per node capacity.},
keywords={ad hoc networks;cache storage;content popularity;node capacity;popularity-aware caching;wireless ad hoc networks;concurrent transmissions;network capacity;content transmission distance;communication interference;capacity scaling laws;Mobile ad hoc networks;Wireless networks;Interference;Conferences;Throughput;Internet},
doi={10.1109/INFOCOM.2017.8057030},
ISSN={},
month={May},}
@INPROCEEDINGS{8057031,
author={L. E. Chatzieleftheriou and M. Karaliopoulos and I. Koutsopoulos},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Caching-aware recommendations: Nudging user preferences towards better caching performance},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Caching decisions by default seek to maximize some notion of social welfare: the content to be cached is determined so that the maximum possible aggregate demand over all users served by the cache is satisfied. Recommendation systems, on the contrary, are oriented towards user individual preferences: the recommended content should be most appealing to the user so as to elicit further content consumption. In our paper we explore how these, phenomenically conflicting, objectives can be jointly addressed. To this end, we depart radically from current practice with recommender systems, and we approach them as network traffic engineering tools that can actively shape content demand towards optimizing user- and network-centric performance objectives. We formulate the resulting joint theoretical optimization problem of deciding on the cached content and the recommendations to each user so that the cache hit ratio is maximized subject to a maximum tolerable distortion that the recommendation should undergo. We conclude on its complexity, and we propose a practical algorithm for its solution. The algorithm is essentially a form of lightweight control over the user recommendations so that the recommended content is both appealing to the end user and more friendly to the caching system and the network resources.},
keywords={cache storage;optimisation;recommender systems;telecommunication traffic;user interfaces;maximum tolerable distortion;user recommendations;recommended content;end user;caching system;user preferences;caching performance;caching decisions;social welfare;maximum possible aggregate demand;recommendation systems;user individual preferences;content consumption;recommender systems;network traffic engineering tools;content demand;resulting joint theoretical optimization problem;cached content;caching-aware recommendations;user-centric performance objective optimization;network-centric performance objective optimization;cache hit ratio;Recommender systems;Wireless networks;Distortion;Conferences;Shape;Mobile computing},
doi={10.1109/INFOCOM.2017.8057031},
ISSN={},
month={May},}
@INPROCEEDINGS{8057032,
author={M. Xiao and J. Wu and S. Zhang and J. Yu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Secret-sharing-based secure user recruitment protocol for mobile crowdsensing},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Mobile crowdsensing is a new paradigm in which a requester can recruit a group of mobile users via a platform and coordinate them to perform some sensing tasks by using their smartphones. In mobile crowdsensing, each user might perform multiple tasks with different sensing qualities. An important problem is recruiting the minimum number of users while achieving a satisfactory sensing quality for each task. Meanwhile, in order to ease users' worries about privacy disclosures, the user recruitment process needs to protect each user's sensing quality information from being revealed to other users or to the platform. We prove that this problem is NP-hard. To solve this problem, we first propose a Basic User Recruitment (BUR) protocol based on a greedy strategy, which can recruit nearly the minimum amount of users while ensuring that the total sensing quality of each task is no less than a given threshold. Based on BUR, we further propose a Secure User Recruitment (SUR) protocol by using secret sharing schemes. We analyze the approximation ratio and prove the security of the SUR protocol in the semi-honest model. Moreover, we extend SUR to deal with a more general case where the total sensing quality of each task might be an increasing submodular function. Finally, we demonstrate the significant performance of the proposed protocol through extensive simulations and execution in real smartphones.},
keywords={computational complexity;cryptographic protocols;data privacy;greedy algorithms;mobile computing;smart phones;mobile crowdsensing;Basic User Recruitment protocol;smartphones;submodular function;semihonest model;SUR protocol security;approximation ratio;greedy strategy;NP-hard problem;privacy disclosures;BUR protocol;secret-sharing-based secure user recruitment protocol;user sensing quality information;sensing qualities;Sensors;Recruitment;Protocols;Mobile communication;Privacy;Cryptography;Mobile crowdsensing;privacy;sensing quality;secret sharing;user recruitment},
doi={10.1109/INFOCOM.2017.8057032},
ISSN={},
month={May},}
@INPROCEEDINGS{8057033,
author={S. Liu and Z. Zheng and F. Wu and S. Tang and G. Chen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Context-aware data quality estimation in mobile crowdsensing},
year={2017},
volume={},
number={},
pages={1-9},
abstract={With the rapid growth of smart devices, mobile crowdsensing is becoming an important paradigm to acquire information from physical environments. Considering that the sensing data collected by mobile users are normally noisy and imprecise, one of the pressing problems in mobile crowdsensing is to evaluate the data quality in real time and to steer users to acquire data with high quality. However, it is challenging to estimate the data quality without the availability of ground truth data. In this paper, we observe that sensing context has a significant impact on data quality, which motivates us to propose a context-aware data quality estimation scheme. With historical sensing data, we train a context-quality classifier, which captures the relation between context information and data quality, to estimate data quality in an online manner. We apply such a context-aware data quality estimation scheme to guide user recruitment in mobile crowdsensing. We model the process of user recruitment as a stochastic submodular maximization problem, and design a random adaptive greedy algorithm to guarantee a constant approximation ratio. We evaluate our algorithm on a real-world temperature data set. The evaluation results show that our algorithm outperforms other existing techniques, in terms of prediction accuracy.},
keywords={approximation theory;information retrieval;mobile computing;optimisation;mobile crowdsensing;ground truth data;context-aware data quality estimation scheme;historical sensing data;context-quality classifier;real-world temperature data;stochastic submodular maximization problem;user recruitment;approximation ratio;Sensors;Mobile communication;Estimation;Real-time systems;Recruitment;Noise measurement;Legged locomotion},
doi={10.1109/INFOCOM.2017.8057033},
ISSN={},
month={May},}
@INPROCEEDINGS{8057034,
author={Q. Xu and R. Zheng},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={When data acquisition meets data analytics: A distributed active learning framework for optimal budgeted mobile crowdsensing},
year={2017},
volume={},
number={},
pages={1-9},
abstract={An important category of mobile crowdsensing applications involve collecting sensor measurements from mobile devices and querying mobile users for annotations to build machine learning models for inference and prediction. Trade-offs between inference performance and the costs of data acquisition (both unlabeled and labeled) are not yet well understood. In this paper, we develop, ALSense, a distributed active learning framework for mobile crowdsensing. The goal is to minimize prediction errors for classification-based mobile crowdsensing tasks subject to upload and query cost constraints. Novel stream-based active learning strategies are developed to orchestrate queries of annotation data and the upload of unlabeled data from mobile devices. We evaluate the effectiveness of ALSense through two applications that can benefit from mobile crowdsensing, namely, WiFi fingerprint-based indoor localization and IMU-based human activity recognition. Extensive experiments demonstrate that ALSense can indeed achieve higher classification accuracy given fixed data acquisition budgets for both applications.},
keywords={data acquisition;data analysis;learning (artificial intelligence);mobile computing;pattern classification;wireless LAN;IMU-based human activity recognition;WiFi fingerprint-based indoor localization;data acquisition budgets;ALSense framework;mobile crowdsensing applications;optimal budgeted mobile crowdsensing;data analytics;unlabeled data;annotation data;active learning strategies;query cost constraints;mobile crowdsensing tasks;prediction errors;distributed active learning framework;machine learning models;querying mobile users;mobile devices;sensor measurements;Mobile handsets;Mobile communication;Servers;Sensors;Data acquisition;Predictive models},
doi={10.1109/INFOCOM.2017.8057034},
ISSN={},
month={May},}
@INPROCEEDINGS{8057035,
author={M. H. Cheung and F. Hou and J. Huang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Make a difference: Diversity-driven social mobile crowdsensing},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In a mobile crowdsensing (MCS) application, user diversity and social effect are two important phenomena that determine its profitability, where the former improves the sensing quality, while the latter incentives the users' participation. In this paper, we consider a reward mechanism design for the service provider to achieve diversity in the collected data by exploiting the users' social relationship. Specifically, we formulate a two-stage decision problem, where the service provider first optimizes its rewards for profit maximization. The users then decide their effort levels through social network interactions as a participation game. The analysis is particularly challenging due to the users' interplay in both the diversity and social graphs, which leads to a non-convex bilevel optimization problem. Surprisingly, we find that the service provider can focus on one superimposed graph that incorporates the diversity and social relationship and compute the optimal reward as the Katz centrality in closed-form. Simulation results, based on the random graph and a real Facebook trace, show that the availability of network information improves both the service provider's profit and the users' social surplus over the incomplete information cases.},
keywords={convex programming;game theory;graph theory;mobile computing;profitability;social networking (online);diversity-driven social mobile crowdsensing;mobile crowdsensing application;user diversity;social effect;profitability;sensing quality;reward mechanism design;service provider;two-stage decision problem;profit maximization;social network interactions;participation game;social graphs;nonconvex bilevel optimization problem;social relationship;optimal reward;network information availability;Cultural differences;Sensors;Mobile communication;Facebook;Games},
doi={10.1109/INFOCOM.2017.8057035},
ISSN={},
month={May},}
@INPROCEEDINGS{8057036,
author={Y. Sang and B. Ji and G. R. Gupta and X. Du and L. Ye},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Provably efficient algorithms for joint placement and allocation of virtual network functions},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Network Function Virtualization (NFV) has the potential to significantly reduce the capital and operating expenses, shorten product release cycle, and improve service agility. In this paper, we focus on minimizing the total number of Virtual Network Function (VNF) instances to provide a specific service (possibly at different locations) to all the flows in a network. Certain network security and analytics applications may allow fractional processing of a flow at different nodes (corresponding to datacenters), giving an opportunity for greater optimization of resources. Through a reduction from the set cover problem, we show that this problem is NP-hard and cannot even be approximated within a factor of (1 - o(1))lnm (where m is the number of flows) unless P=NP. Then, we design two simple greedy algorithms and prove that they achieve an approximation ratio of (1 - o(1))ln m + 2, which is asymptotically optimal. For special cases where each node hosts multiple VNF instances (which is typically true in practice), we also show that our greedy algorithms have a constant approximation ratio. Further, for tree topologies we develop an optimal greedy algorithm by exploiting the inherent topological structure. Finally, we conduct extensive numerical experiments to evaluate the performance of our proposed algorithms in various scenarios.},
keywords={computational complexity;computer network security;greedy algorithms;minimisation;open systems;optimisation;telecommunication network topology;trees (mathematics);virtualisation;simple greedy algorithms;constant approximation ratio;optimal greedy algorithm;provably efficient algorithms;joint placement;Network Function Virtualization;service agility;total number;Virtual Network Function instances;network security;analytics applications;fractional processing;virtual network functions allocation;NFV;product release cycle;VNF instance minimization;resource optimization;NP-hard problem;inherent topological structure;tree topologies;Greedy algorithms;Network topology;Resource management;Topology;Heuristic algorithms;Algorithm design and analysis;Conferences},
doi={10.1109/INFOCOM.2017.8057036},
ISSN={},
month={May},}
@INPROCEEDINGS{8057037,
author={R. Han and F. Zhang and Z. Wang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={AccurateML: Information-aggregation-based approximate processing for fast and accurate machine learning on MapReduce},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The growing demands of processing massive datasets have promoted irresistible trends of running machine learning applications on MapReduce. When processing large input data, it is often of greater values to produce fast and accurate enough approximate results than slow exact results. Existing techniques produce approximate results by processing parts of the input data, thus incurring large accuracy losses when using short job execution times, because all the skipped input data potentially contributes to result accuracy. We address this limitation by proposing AccurateML that aggregates information of input data in each map task to create small aggregated data points. These aggregated points enable all map tasks producing initial outputs quickly to save computation times and decrease the outputs' size to reduce communication times. Our approach further identifies the parts of input data most related to result accuracy, thus first using these parts to improve the produced outputs to minimize accuracy losses. We evaluated AccurateML using real machine learning applications and datasets. The results show: (i) it reduces execution times by 30 times with small accuracy losses compared to exact results; (ii) when using the same execution times, it achieves 2.71 times reductions in accuracy losses compared to existing approximate processing techniques.},
keywords={approximation theory;data analysis;learning (artificial intelligence);AccurateML;MapReduce;machine learning applications;short job execution times;map task;aggregated data points;information aggregation;information-aggregation-based approximate processing;approximate processing techniques;accuracy loss minimization;Sparks;Correlation;Clustering algorithms;Conferences;Indexes;Aggregates;MapReduce;machine learning;approximate processing;result accuracy;information aggregation},
doi={10.1109/INFOCOM.2017.8057037},
ISSN={},
month={May},}
@INPROCEEDINGS{8057038,
author={J. F. Pérez and R. Birke and L. Y. Chen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={On the latency-accuracy tradeoff in approximate MapReduce jobs},
year={2017},
volume={},
number={},
pages={1-9},
abstract={To ensure the scalability of big data analytics, approximate MapReduce platforms emerge to explicitly trade off accuracy for latency. A key step to determine optimal approximation levels is to capture the latency of big data jobs, which is long deemed challenging due to the complex dependency among data inputs and map/reduce tasks. In this paper, we use matrix analytic methods to derive stochastic models that can predict a wide spectrum of latency metrics, e.g., average, tails, and distributions, for approximate MapReduce jobs that are subject to strategies of input sampling and task dropping. In addition to capturing the dependency among waves of map/reduce tasks, our models incorporate two job scheduling policies, namely, exclusive and overlapping, and two task dropping strategies, namely, early and straggler, enabling us to realistically evaluate the potential performance gains of approximate computing. Our numerical analysis shows that the proposed models can guide big data platforms to determine the optimal approximation strategies and degrees of approximation.},
keywords={approximation theory;Big Data;data analysis;matrix algebra;parallel processing;scheduling;stochastic processes;data inputs;matrix analytic methods;approximate MapReduce jobs;input sampling;job scheduling policies;task dropping strategies;approximate computing;optimal approximation strategies;latency-accuracy tradeoff;approximate MapReduce platforms;stochastic models;complex data dependency;Big Data analytics scalability;Big Data jobs latency;latency metrics spectrum;early task dropping strategy;stragger task dropping strategy;performance gain evaluation;numerical analysis;Time factors;Analytical models;Big Data;Computational modeling;Conferences;Measurement},
doi={10.1109/INFOCOM.2017.8057038},
ISSN={},
month={May},}
@INPROCEEDINGS{8057039,
author={H. Feng and J. Llorca and A. M. Tulino and D. Raz and A. F. Molisch},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Approximation algorithms for the NFV service distribution problem},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Distributed cloud networking builds on network functions virtualization (NFV) and software defined networking (SDN) to enable the deployment of network services in the form of elastic virtual network functions (VNFs) instantiated over general purpose servers at distributed cloud locations. We address the design of fast approximation algorithms for the NFV service distribution problem (NSDP), whose goal is to determine the placement of VNFs, the routing of service flows, and the associated allocation of cloud and network resources that satisfy client demands with minimum cost. We show that in the case of load-proportional costs, the resulting fractional NSDP can be formulated as a multi-commodity-chain flow problem on a cloud-augmented graph, and design a queue-length based algorithm, named QNSD, that provides an O(ε) approximation in time O (1/ε). We then address the case in which resource costs are a function of the integer number of allocated resources and design a variation of QNSD that effectively pushes for flow consolidation into a limited number of active resources to minimize overall cloud network cost.},
keywords={approximation theory;cloud computing;computational complexity;graph theory;resource allocation;software defined networking;virtualisation;general purpose servers;distributed cloud locations;fast approximation algorithms;NFV service distribution problem;service flows;network resources;multicommodity-chain flow problem;cloud-augmented graph;queue-length based algorithm;O(ε) approximation;cloud network cost;distributed cloud networking;software defined networking;network services;elastic virtual network functions;VNF;Cloud computing;Algorithm design and analysis;Approximation algorithms;Routing;Network function virtualization;Servers;Resource management},
doi={10.1109/INFOCOM.2017.8057039},
ISSN={},
month={May},}
@INPROCEEDINGS{8057040,
author={J. Tapolcai and L. Rónyai and B. Vass and L. Gyimóthi},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={List of shared risk link groups representing regional failures with limited size},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Shared Risk Link Group (SRLG) is a failure the network is prepared for, which contains a set of links subject to a common risk of single failure. During planning a backbone network, the list of SRLGs must be defined very carefully, because leaving out one likely failure event will significantly degrade the observed reliability of the network. Regional failures are manifested at multiple locations of the network, which are physically close to each other. In this paper we show that operators should prepare a network for only a small number of possible regional failure events. In particular, we give a fast systematic approach to generate the list of SRLGs that cover every possible circular disk failure of a given radius r. We show that this list has O((n + x)σr) SRLGs, where n is the number of nodes in the network, x is the number of link crossings, and σris the maximal number of links that could be hit by a disk failure of radius r. Finally through extensive simulations we show that this list in practice has size of ≈ 1.2 n.},
keywords={optical fibre networks;telecommunication network planning;telecommunication network reliability;telecommunication network routing;shared risk link groups;regional failures;single failure;failure event;link crossings;optical backbone network;SRLG;circular disk failure;Reliability;Earthquakes;Tornadoes;Conferences;Computer network reliability;Hurricanes;Topology},
doi={10.1109/INFOCOM.2017.8057040},
ISSN={},
month={May},}
@INPROCEEDINGS{8057041,
author={Y. Zhang and W. Wu and S. Banerjee and J. Kang and M. A. Sanchez},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={SLA-verifier: Stateful and quantitative verification for service chaining},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Network verification has been recently proposed to detect network misconfigurations. Existing work focuses on the reachability. This paper proposes a framework that verifies the Service Level Agreement (SLA) compliance of the network using static verification. This work proposes a quantitative model and a set of algorithms for verifying performance properties of a network with switches and middleboxes, i.e., service chains. We develop SLA-Verifier and evaluate its efficiency using simulation on real-world data and testbed experiments. To improve the SLA violation detection accuracy, our system uses verification results to optimize online monitoring.},
keywords={contracts;formal verification;quality of service;telecommunication network management;telecommunication services;SLA violation detection accuracy;SLA-Verifier;quantitative model;static verification;Service Level Agreement compliance;network misconfigurations;network verification;service chaining;quantitative verification;Stateful verification;SLA-verifier;Bandwidth;Middleboxes;Quality of service;Load modeling;Monitoring;Noise measurement},
doi={10.1109/INFOCOM.2017.8057041},
ISSN={},
month={May},}
@INPROCEEDINGS{8057042,
author={S. Ciavarella and N. Bartolini and H. Khamfroush and T. L. Porta},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Progressive damage assessment and network recovery after massive failures},
year={2017},
volume={},
number={},
pages={1-9},
abstract={After a massive scale failure, the assessment of damages to communication networks requires local interventions and remote monitoring. While previous works on network recovery require complete knowledge of damage extent, we address the problem of damage assessment and critical service restoration in a joint manner. We propose a polynomial algorithm called Centrality based Damage Assessment and Recovery (CeDAR) which performs a joint activity of failure monitoring and restoration of network components. CeDAR works under limited availability of recovery resources and optimizes service recovery over time. We modified two existing approaches to the problem of network recovery to make them also able to exploit incremental knowledge of the failure extent. Through simulations we show that CeDAR outperforms the previous approaches in terms of recovery resource utilization and accumulative flow over time of the critical services.},
keywords={condition monitoring;failure analysis;polynomials;telecommunication network management;telecommunication network reliability;progressive damage assessment;network recovery;massive scale failure;communication networks;remote monitoring;critical service restoration;Centrality based Damage Assessment;CeDAR;failure monitoring;recovery resource utilization;polynomial algorithm;Maintenance engineering;Monitoring;Schedules;Knowledge engineering;Inspection;Communication networks;Conferences},
doi={10.1109/INFOCOM.2017.8057042},
ISSN={},
month={May},}
@INPROCEEDINGS{8057043,
author={Y. Bejerano and C. Raman and C. Yu and V. Gupta and C. Gutterman and T. Young and H. Infante and Y. Abdelmalek and G. Zussman},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={DyMo: Dynamic monitoring of large scale LTE-Multicast systems},
year={2017},
volume={},
number={},
pages={1-9},
abstract={LTE evolved Multimedia Broadcast/Multicast Service (eMBMS) is an attractive solution for video delivery to very large groups in crowded venues. However, deployment and management of eMBMS systems is challenging, due to the lack of realtime feedback from the User Equipment (UEs). Therefore, we present the Dynamic Monitoring (DyMo) system for low-overhead feedback collection. DyMo leverages eMBMS for broadcasting Stochastic Group Instructions to all UEs. These instructions indicate the reporting rates as a function of the observed Quality of Service (QoS). This simple feedback mechanism collects very limited QoS reports from the UEs. The reports are used for network optimization, thereby ensuring high QoS to the UEs. We present the design aspects of DyMo and evaluate its performance analytically and via extensive simulations. Specifically, we show that DyMo infers the optimal eMBMS settings with extremely low overhead, while meeting strict QoS requirements under different UE mobility patterns and presence of network component failures. For instance, DyMo can detect the eMBMS Signal-to-Noise Ratio (SNR) experienced by the 0.1% percentile of the UEs with Root Mean Square Error (RMSE) of 0.05% with only 5 to 10 reports per second regardless of the number of UEs.},
keywords={broadcast communication;feedback;Long Term Evolution;mean square error methods;multicast communication;multimedia communication;quality of service;video delivery;eMBMS systems;realtime feedback;Dynamic Monitoring system;low-overhead feedback collection;DyMo leverages eMBMS;Stochastic Group Instructions;reporting rates;simple feedback mechanism;optimal eMBMS settings;extremely low overhead;strict QoS requirements;QoS;UE mobility patterns;large scale LTE-multicast systems;LTE evolved multimedia broadcast-multicast service;network optimization;root mean square error;RMSE;Signal to noise ratio;Quality of service;Estimation;Wireless fidelity;Long Term Evolution;Sociology;Statistics;Wireless Monitoring;LTE;eMBMS;Multi-cast;Feedback Mechanism},
doi={10.1109/INFOCOM.2017.8057043},
ISSN={},
month={May},}
@INPROCEEDINGS{8057044,
author={M. Zhang and L. Gao and J. Huang and M. Honig},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Cooperative and competitive operator pricing for mobile crowdsourced internet access},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Mobile Crowdsourced Access (MCA) enables mobile users (MUs) to share their Internet connections by serving as tethers to other MUs, hence can improve the quality of service of MUs as well as the overall utilization of network resources. However, MCA can also reduce the revenue-generating mobile traffic and increase the network congestion for mobile network operators (MNOs), and thus has been blocked by some MNOs in practice. In this work, we reconcile the conflicting objectives of MNOs and MUs by introducing a pricing framework for MCA, where the direct traffic and tethering traffic are charged independently according to a data price and a tethering price, respectively. We derive the optimal data and tethering prices systematically for MUs with the α-fair utility in two scenarios with cooperative and competitive MNOs, respectively. We show that the optimal tethering prices are zero and the optimal usage-based data prices are identical for all MUs, in both the cooperative and competitive scenarios. Such optimal pricing schemes will lead to mutually beneficial results for MNOs and MUs. Our simulation results show that the proposed pricing scheme approximately triples both the MNOs' profit and the MUs' payoff when the MNOs cooperate, comparing to the case where MCA is blocked. Moreover, competition among MNOs will decrease MNOs' profit and further increase the MUs' payoff.},
keywords={cooperative communication;Internet;mobile communication;mobile computing;optimisation;pricing;telecommunication services;telecommunication traffic;competitive operator pricing;mobile crowdsourced internet access;MCA;mobile users;network resources;revenue-generating mobile traffic;mobile network operators;pricing framework;direct traffic;tethering traffic;data price;tethering price;optimal tethering prices;optimal pricing schemes;cooperative operator pricing;competitive MNO;MNO profit;MU payoff;Pricing;Mobile communication;Mobile computing;Internet;Wireless fidelity;Downlink;Conferences},
doi={10.1109/INFOCOM.2017.8057044},
ISSN={},
month={May},}
@INPROCEEDINGS{8057045,
author={D. Bega and M. Gramaglia and A. Banchs and V. Sciancalepore and K. Samdanis and X. Costa-Perez},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Optimising 5G infrastructure markets: The business of network slicing},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In addition to providing substantial performance enhancements, future 5G networks will also change the mobile network ecosystem. Building on the network slicing concept, 5G allows to “slice” the network infrastructure into separate logical networks that may be operated independently and targeted at specific services. This opens the market to new players: the infrastructure provider, which is the owner of the infrastructure, and the tenants, which may acquire a network slice from the infrastructure provider to deliver a specific service to their customers. In this new context, we need new algorithms for the allocation of network resources that consider these new players. In this paper, we address this issue by designing an algorithm for the admission and allocation of network slices requests that (i) maximises the infrastructure provider's revenue and (ii) ensures that the service guarantees provided to tenants are satisfied. Our key contributions include: (i) an analytical model for the admissibility region of a network slicing-capable 5G Network, (ii) the analysis of the system (modelled as a Semi-Markov Decision Process) and the optimisation of the infrastructure provider's revenue, and (iii) the design of an adaptive algorithm (based on Q-learning) that achieves close to optimal performance.},
keywords={5G mobile communication;Markov processes;mobile computing;mobility management (mobile radio);substantial performance enhancements;future 5G networks;network slicing;logical networks;network slice requests;5G infrastructure market optimisation;semi-Markov decision process;Q-learning;infrastructure provider revenue optimisation;network resources;network infrastructure;mobile network ecosystem;Base stations;5G mobile communication;Algorithm design and analysis;Throughput;Ecosystems;Adaptation models;Mobile computing},
doi={10.1109/INFOCOM.2017.8057045},
ISSN={},
month={May},}
@INPROCEEDINGS{8057046,
author={P. Caballero and A. Banchs and G. de Veciana and X. Costa-Pérez},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Network slicing games: Enabling customization in multi-tenant networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Network slicing to enable resource sharing among multiple tenants-network operators and/or services-is considered a key functionality for next generation mobile networks. This paper provides an analysis of a well-known model for resource sharing, the `share-constrained proportional allocation' mechanism, to realize network slicing. This mechanism enables tenants to reap the performance benefits of sharing, while retaining the ability to customize their own users' allocation. This results in a network slicing game in which each tenant reacts to the user allocations of the other tenants so as to maximize its own utility. We show that, under appropriate conditions, the game associated with such strategic behavior converges to a Nash equilibrium. At the Nash equilibrium, a tenant always achieves the same, or better, performance than under a static partitioning of resources, hence providing the same level of protection as such static partitioning. We further analyze the efficiency and fairness of the resulting allocations, providing tight bounds for the price of anarchy and envy-freeness. Our analysis and extensive simulation results confirm that the mechanism provides a comprehensive practical solution to realize network slicing. Our theoretical results also fill a gap in the literature regarding the analysis of this resource allocation model under strategic players.},
keywords={game theory;mobile computing;next generation networks;program slicing;resource allocation;resource sharing;multiple tenants-network operators;network slicing game;user allocations;Nash equilibrium;static partitioning;resource allocation model;next generation mobile networks;share-constrained proportional allocation mechanism;multi-tenant networks;Resource management;Base stations;Mobile communication;Mobile computing;Analytical models;Nash equilibrium;Conferences},
doi={10.1109/INFOCOM.2017.8057046},
ISSN={},
month={May},}
@INPROCEEDINGS{8057047,
author={M. Zou and R. T. B. Ma and X. Wang and Y. Xu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={On optimal service differentiation in congested network markets},
year={2017},
volume={},
number={},
pages={1-9},
abstract={As Internet applications have become more diverse in recent years, users having heavy demand for online video services are more willing to pay higher prices for better services than light users that mainly use e-mails and instant messages. This encourages the Internet Service Providers (ISPs) to explore service differentiations so as to optimize their profits and allocation of network resources. Much prior work has focused on the viability of network service differentiation by comparing with the case of a single-class service. However, the optimal service differentiation for an ISP subject to resource constraints has remained unsolved. In this work, we establish an optimal control framework to derive the analytical solution to an ISP's optimal service differentiation, i.e., the optimal service qualities and associated prices. By analyzing the structures of the solution, we reveal how an ISP should adjust the service qualities and prices in order to meet varying capacity constraints and users' characteristics. We also obtain the conditions under which ISPs have strong incentives to implement service differentiation and whether regulators should encourage such practices.},
keywords={Internet;optimal control;pricing;quality of service;online video services;e-mails;instant messages;network resource allocation;optimal service differentiation;Internet applications;congested network markets;optimal service qualities;ISP's optimal service differentiation;optimal control framework;single-class service;network service differentiation;network resources;Internet Service Providers;Pricing;Capacity planning;Optimal control;Conferences;Regulators;Quality of service;Resource management},
doi={10.1109/INFOCOM.2017.8057047},
ISSN={},
month={May},}
@INPROCEEDINGS{8057048,
author={P. Nayak and M. Garetto and E. W. Knightly},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Multi-user downlink with single-user uplink can starve TCP},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In this paper we present the first cross-layer analysis of wireless LANs operating under downlink multi-user MIMO (MU-MIMO), considering the fundamental role played by closed-loop (TCP) traffic. In particular, we consider an 802.11ac scenario in which the access point transmits on the downlink via MU-MIMO, whereas stations must employ single-user transmissions on the uplink. With the help of analytical models built for the different regimes that can occur in the considered system, we identify and explain crucial performance anomalies that can result in very low throughput in some scenarios, completely offsetting the theoretical gains achievable by MU-MIMO. We discuss solutions to mitigate the risk of this performance degradation and alternative uplink strategies allowing WLANs to approach their maximum theoretical capacity under MU-MIMO.},
keywords={MIMO communication;multiuser detection;telecommunication traffic;transport protocols;wireless LAN;MU-MIMO;multiuser downlink;single-user uplink;wireless LAN;multiuser MIMO;closed-loop TCP traffic;Downlink;Uplink;Throughput;MIMO;Servers;Wireless LAN;Standards},
doi={10.1109/INFOCOM.2017.8057048},
ISSN={},
month={May},}
@INPROCEEDINGS{8057049,
author={D. Gunatilaka and M. Sha and C. Lu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Impacts of channel selection on industrial wireless sensor-actuator networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Industrial automation has emerged as an important application of wireless sensor-actuator networks (WSANs). To meet stringent reliability requirements of industrial applications, industrial standards such as WirelessHART adopt Time Slotted Channel Hopping (TSCH) as its MAC protocol. Since every link hops through all the channels used in TSCH, a straightforward policy to ensure reliability is to retain a link in the network topology only if it is reliable in all channels used. However, this policy has surprising side effects. While using more channels may enhance reliability due to channel diversity, more channels may also reduce the number of links and route diversity in the network topology. We empirically analyze the impact of channel selection on network topology, routing, and scheduling on a 52-node WSAN testbed. We observe inherent tradeoff between channel diversity and route diversity in channel selection, where using an excessive number of channels may negatively impact routing and scheduling. We propose novel channel and link selection strategies to improve route diversity and network schedulability. Experimental results on two different testbeds show that our algorithms can drastically improve routing and scheduling of industrial WSANs.},
keywords={access protocols;diversity reception;routing protocols;telecommunication network topology;telecommunication scheduling;wireless sensor networks;industrial wireless sensor-actuator networks;industrial automation;industrial standards;Time Slotted Channel Hopping;TSCH;network topology;route diversity;channel selection;channel diversity;network schedulability;industrial WSANs;WirelessHART;MAC protocol;Routing;Reliability;Network topology;Wireless sensor networks;Wireless communication;IEEE 802.15 Standard},
doi={10.1109/INFOCOM.2017.8057049},
ISSN={},
month={May},}
@INPROCEEDINGS{8057050,
author={A. Baiocchi and I. Tinnirello and D. Garlisi and A. L. Valvo},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Random access with repeated contentions for emerging wireless technologies},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In this paper we propose ReCo, a robust contention scheme for emerging wireless technologies, whose efficiency is not sensitive to the number of contending stations and to the settings of the contention parameters (such as the contention windows and retry limits). The idea is iterating a basic contention mechanism, devised to select a sub-set of stations among the contending ones, in consecutive elimination rounds, before performing a transmission attempt. Elimination rounds can be performed in the time or frequency domain, with different overheads, according to the physical capabilities of the nodes. Closed analytical formulas are given to dimension the number of contention rounds in order to achieve an arbitrary low collision probability. Simulation results and a real implementation for the time-domain solution demonstrate the effectiveness and robustness of this approach in comparison to IEEE 802.11 DCF.},
keywords={access protocols;probability;wireless LAN;contention parameters;contention windows;retry limits;frequency domain;closed analytical formulas;contention rounds;arbitrary low collision probability;time-domain solution;random access;wireless technologies;ReCo;robust contention scheme;Frequency-domain analysis;OFDM;Wireless communication;IEEE 802.11 Standard;Protocols;Multiaccess communication},
doi={10.1109/INFOCOM.2017.8057050},
ISSN={},
month={May},}
@INPROCEEDINGS{8057051,
author={C. Shih and R. Sivakumar},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Switch: Enabling transmitter and receiver participation in seamless lightweight control},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Lightweight control planes are techniques that create a control plane in WiFi networks without any additional spectrum requirements. Flash signals are an example of such control signals that exploit the link margin that typically exists in WiFi communication. In this paper we consider the problem of allowing transmitters and receivers of a transmission to exploit such control channels while the communication is ongoing. We present a mechanism called switch that facilitates switches in communication modes (Tx to Rx and vice-versa). We then use switch as the core building block to solve problems in WiFi networks such as starvation due to hidden terminals, early collision termination, and frequency backoffs. We rely on WARP radios to experimentally verify that switch is indeed possible, and use ns-3 simulations to study the impact of using switch to solve the aforementioned WiFi problems.},
keywords={access protocols;radio receivers;radio transmitters;telecommunication control;telecommunication switching;telecommunication traffic;wireless channels;wireless LAN;receiver participation;seamless lightweight control;WiFi networks;flash signals;control signals;link margin;WiFi communication;control channels;communication modes;collision termination;transmitter participation;Switches;Receivers;Wireless fidelity;Radio transmitters;Delays},
doi={10.1109/INFOCOM.2017.8057051},
ISSN={},
month={May},}
@INPROCEEDINGS{8057052,
author={W. Cheng and X. Zhang and H. Zhang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Pilot-based full-duplex spectrum-sensing and multichannel-MAC over non-time-slotted cognitive radio networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In the non-time-slotted cognitive radio networks (CRNs), the synchronization between PUs and secondary users (SUs) cannot be guaranteed, resulting in two challenging problems: the reactivation-failure of PUs and the frequently unexpected hand-offs among SUs. The reactivation-failure of PUs is the incident that the SUs cannot detect the PUs' reactivation when the SUs are occupying the channels to transmit their data in non-time-slotted CRNs. The frequently unexpected handoffs among SUs are the events that the SU cannot distinguish between the PUs' reactivation and the other SUs' contention, thus causing many unexpected hand-offs among SUs, which severely degrade the achieved throughput of SUs in non-time-slotted CRNs. Employing the energy-detection based wireless full-duplex spectrum sensing schemes, the PUs' reactivation-failure problem can be efficiently solved, thus guaranteeing the required throughput of PUs. However, the key of CRNs is not only the throughput-guarantees for PUs, but also the throughput-boosts for SUs. To optimize the throughput of SUs in multichannel non-time-slotted CRNs, in this paper we develop the pilot-based full-duplex spectrum sensing (PF-SS) scheme and the pilot-based medium access control (P-MAC) protocol to not only guarantee the required throughput of PUs, but also significantly increase the throughput of SUs in multichannel non-time-slotted CRNs. Using the PF-SS scheme, the SUs can identify whether the PUs' signal or the SUs' signal, thus significantly reducing the frequently unexpected hand-offs among SUs. Then, based on the PF-SS scheme, the P-MAC protocol can significantly increase the throughput of SUs. We conduct extensive numerical analyses to show that our developed PF-SS scheme and P-MAC protocol can significantly increase the throughput of SUs while guaranteeing the required throughput for PUs in multichannel non-time-slotted CRNs.},
keywords={access protocols;cognitive radio;mobility management (mobile radio);radio networks;radio spectrum management;signal detection;wireless channels;pilot-based medium access control protocol;P-MAC protocol;nontime-slotted cognitive radio networks;primary user;secondary users;multichannel-MAC;PUs reactivation;frequently unexpected handoffs;reactivation-failure;full-duplex spectrum-sensing;frequently unexpected hand-offs;SUs;CRNs;Throughput;Sensors;Wireless sensor networks;Protocols;Cognitive radio;Wireless networks;Non-time-slotted cognitive radio networks;hand-offs among secondary users;pilot-based full-duplex spectrum sensing;pilot-based medium access control protocol},
doi={10.1109/INFOCOM.2017.8057052},
ISSN={},
month={May},}
@INPROCEEDINGS{8057053,
author={X. Liu and J. Xie},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A 2D heterogeneous rendezvous protocol for multi-wideband cognitive radio networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Ideally, users in cognitive radio networks (CRNs) are capable of sensing and exploiting any potential transmission opportunities in the available spectrum band ranging from 30 KHz to 300 GHz. With the multiple-diverse-band spectrum, the network can provide more radio resources and capacity to a large number of CR users. However, the multiband scenario (e.g., TV band + 2/3G band + 4/5G band) also introduces significant challenges in channel rendezvous, a fundamental operation for users in CRNs to set up their communication link on a common channel. Existing studies on channel rendezvous suffer from unacceptable long delay and high energy consumption when applied to such scenarios. In this paper, we propose a two-dimensional heterogeneous rendezvous (2D-HR) protocol which can support multi-wideband CRNs (MWB-CRNs) with a significantly reduced rendezvous delay and energy consumption for various rendezvous scenarios, such as the pair-wise rendezvous, any-wise rendezvous, and multi-wise rendezvous. The proposed design also performs better than existing efforts even when dealing with traditional single-band rendezvous. The merits of 2D-HR are proved theoretically and validated against extensive simulations. To the best of our knowledge, this is the first work that addresses heterogeneous rendezvous in MWB-CRNs.},
keywords={cognitive radio;protocols;radio links;radio spectrum management;wireless channels;MWB-CRN;multiwideband CRN;2D-HR protocol;energy consumption;any-wise rendezvous;pair-wise rendezvous;reduced rendezvous delay;two-dimensional heterogeneous rendezvous protocol;high energy consumption;channel rendezvous;TV band;multiband scenario;CR users;radio resources;multiple-diverse-band spectrum;multiwideband cognitive radio networks;multiwise rendezvous;frequency 30.0 kHz to 300.0 GHz;Delays;Algorithm design and analysis;Wideband;Energy consumption;Conferences;Protocols;Cognitive radio},
doi={10.1109/INFOCOM.2017.8057053},
ISSN={},
month={May},}
@INPROCEEDINGS{8057054,
author={C. Joo and N. B. Shroff},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A novel coupled queueing model to control traffic via QoS-aware collision pricing in cognitive radio networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We consider a cognitive radio network, where primary users have priority over the spectrum resources, and secondary users can exploit the unused resources through channel sensing. Due to sensing inaccuracy, the secondary traffic may obstruct the primary traffic. A penalty for collision has been used to protect the primary traffic, which is often designed to provide a fixed per-collision compensation or to restrict the collision rate at an acceptable level. In this work, we develop a framework that can protect the primary traffic taking into account the Quality of Service of the primary traffic. In particular, we pay attention to the delay performance, which is determined not only by the collision rate but also by the amount of traffic in both networks. We design a novel model with coupled queues, and successfully incorporate dynamic interactions between the two systems through the standard optimization problem. We also consider the practical requirement of no direct sharing of the system information between the two networks, and develop a close-to-optimal solution of per-collision price and channel sensing under mild assumptions. We evaluate its performance through simulations.},
keywords={cognitive radio;multi-access systems;optimisation;quality of service;queueing theory;telecommunication congestion control;QoS-aware collision pricing;cognitive radio network;primary users;secondary users;channel sensing;secondary traffic;primary traffic;per-collision compensation;per-collision price;control traffic;coupled queueing model;spectrum resources;quality of service;Sensors;Quality of service;Cognitive radio;Signal to noise ratio;Interference;Throughput;Load management},
doi={10.1109/INFOCOM.2017.8057054},
ISSN={},
month={May},}
@INPROCEEDINGS{8057055,
author={A. Abdelfattah and N. Malouch},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Modeling and performance analysis of Wi-Fi networks coexisting with LTE-U},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In order to cope with the exponential growth of mobile traffic, mobile operators need to access more spectrum resources. LTE in unlicensed spectrum (LTE-U) has been proposed to extend the usual operation of LTE in licensed spectrum to cover also unlicensed spectrum. However, this extension poses significant challenges especially regarding the coexistence between LTE-U and legacy systems like Wi-Fi. In case of LTE-U adopts Time-Division Multiplexing (TDM) schemes to share the spectrum with Wi-Fi, we expect performance degradations of Wi-Fi networks. In this paper, we quantify the impact of TDM schemes on Wi-Fi performance in a coexistence scenario. We provide detailed analytical models using two different random walk approaches to compute the probability of collision faced by Wi-Fi stations and their throughput performance. Besides, we derive the performance results using an exponential approximation which shows its insufficiency to capture the exact behavior. We implement the coexistence in the NS3 simulator and we show that the models estimate accurately the collision probability and the throughput experienced by Wi-Fi. The models are then used to study and compare different coexistence schemes showing for instance that the Wi-Fi frame size has a non-negligible impact on the performance of Wi-Fi users.},
keywords={Long Term Evolution;probability;time division multiplexing;wireless LAN;Wi-Fi frame size;Wi-Fi users;Wi-Fi networks;LTE-U;mobile traffic;mobile operators;spectrum resources;unlicensed spectrum;licensed spectrum;Time-Division Multiplexing schemes;performance degradations;TDM schemes;Wi-Fi performance;Wi-Fi stations;NS3;U;Wireless fidelity;Analytical models;Long Term Evolution;Media Access Protocol;Throughput;Computational modeling;Markov processes;LTE-U;CSAT;Wi-Fi;5G;Mobile Communication;Collision Probability;Performance Evaluation;Simulation},
doi={10.1109/INFOCOM.2017.8057055},
ISSN={},
month={May},}
@INPROCEEDINGS{8057056,
author={Y. Qin and R. Jin and S. Hao and K. R. Pattipati and F. Qian and S. Sen and B. Wang and C. Yue},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A control theoretic approach to ABR video streaming: A fresh look at PID-based rate adaptation},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Adaptive bitrate streaming (ABR) has become the de facto technique for video streaming over the Internet. Despite a flurry of techniques, achieving high quality ABR streaming over cellular networks remains a tremendous challenge. ABR streaming can be naturally modeled as a feedback control problem. There has been some initial work on using PID, a widely used feedback control technique, for ABR streaming. Existing studies, however, either use PID control directly without fully considering the special requirements of ABR streaming, leading to suboptimal results, or conclude that PID is not a suitable approach. In this paper, we take a fresh look at PID-based control for ABR streaming. We design a framework called PIA that strategically leverages PID control concepts and incorporates several novel strategies to account for the various requirements of ABR streaming. We evaluate PIA using simulation based on real LTE network traces, as well as using real DASH implementation. The results demonstrate that PIA outperforms state-of-the-art schemes in providing high average bitrate with significantly lower bitrate changes (reduction up to 40%) and stalls (reduction up to 85%), while incurring very small runtime overhead.},
keywords={cellular radio;feedback;Internet;Long Term Evolution;three-term control;video streaming;DASH implementation;LTE network traces;PID control concepts;Internet;de facto technique;adaptive bitrate streaming;PID-based rate adaptation;feedback control technique;ABR video streaming;control theoretic approach;feedback control problem;Streaming media;Bit rate;PI control;PD control;Bandwidth;Cellular networks},
doi={10.1109/INFOCOM.2017.8057056},
ISSN={},
month={May},}
@INPROCEEDINGS{8057057,
author={T. Zhang and F. Ren and W. Cheng and X. Luo and R. Shu and X. Liu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Modeling and analyzing the influence of chunk size variation on bitrate adaptation in DASH},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Recently, HTTP-based adaptive video streaming has been widely adopted in the Internet. Up to now, HTTP-based adaptive video streaming is standardized as Dynamic Adaptive Streaming over HTTP (DASH), where a client-side video player can dynamically pick the bitrate level according to the perceived network conditions. Actually, not only the available bandwidth is varying, but also the chunk sizes in the same bitrate level significantly fluctuate, which also influences the bitrate adaptation. However, existing bitrate adaptation algorithms do not accurately involve the chunk size variation, leading to performance losses. In this paper, we theoretically analyze the influence of chunk size variation on bitrate adaptation performance. Based on DASH system features, we build a general model describing the playback buffer evolution. Applying stochastic theories, we respectively analyze the influence of the chunk size variation on rebuffering probability and average bitrate level. Furthermore, based on theoretical insights, we provide several recommendations for algorithm designing and rate encoding, and also propose a simple bitrate adaptation algorithm. Extensive simulations verify our insights as well as the efficiency of the proposed recommendations and algorithm.},
keywords={hypermedia;Internet;probability;transport protocols;video streaming;chunk size variation;DASH;client-side video player;bitrate adaptation algorithms;bitrate adaptation performance;average bitrate level;simple bitrate adaptation algorithm;dynamic adaptive streaming over HTTP;HTTP-based adaptive video streaming;stochastic theory;Bit rate;Streaming media;Algorithm design and analysis;Bandwidth;Adaptation models;Encoding;Analytical models;Chunk Size;bitrate adaptation;rebuffering;average video rate;DASH},
doi={10.1109/INFOCOM.2017.8057057},
ISSN={},
month={May},}
@INPROCEEDINGS{8057058,
author={C. Zhang and Q. He and J. Liu and Z. Wang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Beyond the touch: Interaction-aware mobile gamecasting with gazing pattern prediction},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Recent years have witnessed an explosion of gamecasting applications in the market, in which game players (or gamers in short) broadcast their game scenes in real-time. Such pioneer applications as YouTube Gaming, Twitch, and Mobcrush have attracted a massive number of online broadcasters, and each of them can attract hundreds or thousands of fellow viewers. The growing number however has created significant challenges to the network and end-devices, particularly considering bandwidth- and battery-limited smartphones or tablets are becoming dominating for both gamers and viewers. Yet the unique touch operations of the mobile interface offer opportunities, too. In this paper, our crowdsourced measurement reveals that strong associations exist between the gamers' touch interactions and the viewers' gazing patterns. Motivated by this, we present a novel interaction-aware optimization framework to improve the energy utilization and stream quality for mobile gamecasting (MGC). Our framework incorporates a touch-assisted prediction module to extract association rules for gazing pattern prediction and a tile-based optimization module to utilize energy on mobile devices efficiently. Trace-driven simulations illustrate the effectiveness of our framework in terms of energy consumption and streaming quality. Our user study experiments also demonstrate much improved (3%-13%) quality satisfaction than the state-of-the-art solution with similar network resources.},
keywords={computer games;data mining;mobile computing;smart phones;user interfaces;interaction-aware mobile gamecasting;game players;gamers;game scenes;online broadcasters;battery-limited smartphones;tablets;crowdsourced measurement;novel interaction-aware optimization framework;energy utilization;association rules;mobile devices;energy consumption;streaming quality;mobile interface;touch-assisted prediction module;gazing pattern prediction;touch interactions;tile-based optimization module;Games;Optimization;Land mobile radio;Smart phones;YouTube},
doi={10.1109/INFOCOM.2017.8057058},
ISSN={},
month={May},}
@INPROCEEDINGS{8057059,
author={K. Diab and M. Hefeeda},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={MASH: A rate adaptation algorithm for multiview video streaming over HTTP},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Multiview videos offer unprecedented experience by allowing users to explore scenes from different angles and perspectives. Thus, such videos have been gaining substantial interest from major content providers such as Google and Facebook. Adaptive streaming of multiview videos is, however, challenging because of the Internet dynamics and the diversity of user interests and network conditions. To address this challenge, we propose a novel rate adaptation algorithm for multiview videos (called MASH). Streaming multiview videos is more user centric than single-view videos, because it heavily depends on how users interact with the different views. To efficiently support this interactivity, MASH constructs probabilistic view switching models that capture the switching behavior of the user in the current session, as well as the aggregate switching behavior across all previous sessions of the same video. MASH then utilizes these models to dynamically assign relative importance to different views. Furthermore, MASH uses a new buffer-based approach to request video segments of various views at different qualities, such that the quality of the streamed videos is maximized while the network bandwidth is not wasted. We have implemented a multiview video player and integrated MASH in it. We compare MASH versus the state-of-the-art algorithm used by YouTube for streaming multiview videos. Our experimental results show that MASH can produce much higher and smoother quality than the algorithm used by YouTube, while it is more efficient in using the network bandwidth. In addition, we conduct large-scale experiments with up to 100 concurrent multiview streaming sessions, and we show that MASH maintains fairness across competing sessions, and it does not overload the streaming server.},
keywords={interactive video;Internet;social networking (online);video coding;video streaming;single-view videos;MASH constructs probabilistic view;video segments;streamed videos;multiview video player;multiview streaming sessions;Streaming media;Multi-stage noise shaping;Switches;Adaptation models;Computational modeling;Heuristic algorithms;YouTube},
doi={10.1109/INFOCOM.2017.8057059},
ISSN={},
month={May},}
@INPROCEEDINGS{8057060,
author={I. Markwood and Y. Liu and K. Kwiat and C. Kamhoua},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Electric grid power flow model camouflage against topology leaking attacks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The power flow model for DC power grids has been used theoretically to launch false data injection attacks (FDIAs) against state estimation. We recognize FDIAs are just one possible attack using the power flow model and that the grid topology information within the model implies its discovery may also facilitate topology-based attacks. We show attackers can derive the power flow model, and thus the topology also. Indeed, with incomplete data, attackers can accurately reconstruct regions of the model, or topology, all that is necessary to launch an attack. We also illustrate how to cause such attackers to derive instead a convincing fake model by camouflaging the real model. Consequently, no sensitive information will leak, so attacks based on this fake model will be ineffective, rather alerting grid administrators to the attacker's efforts. Using five test cases included in the MATLAB power flow analysis tool MATPOWER, ranging from 9 to 300 buses, an average 67.0% of the topology may be derived with a 69.1% model accuracy. Lastly, we find reconstructions of small portions of the model sufficient for performing FDIAs with 75% success, and that camouflage prevents 93% of them in all but the 9-bus case.},
keywords={DC power transmission;load flow control;power grids;power system simulation;power system state estimation;DC power grids;false data injection attacks;FDIAs;grid topology information;convincing fake model;MATLAB power flow analysis tool MATPOWER;electric grid power flow model camouflage;state estimation;MATPOWER;MATLAB power flow analysis tool;power flow model;topology leaking attacks;Mathematical model;Data models;Topology;Computational modeling;Meters;Admittance;Power grids},
doi={10.1109/INFOCOM.2017.8057060},
ISSN={},
month={May},}
@INPROCEEDINGS{8057061,
author={Z. Lu and M. Wei and X. Lu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={How they interact? Understanding cyber and physical interactions against fault propagation in smart grid},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In the smart grid, computer networks (i.e., the cyber domain) are built upon physical infrastructures (i.e., the physical domain) to facilitate advanced functionalities that were considered not possible in legacy systems. It is envisioned that such a cyber-physical paradigm enables intelligent, collaborative controls to prevent faults from propagating along large-scale infrastructures, which is a primary cause for massive blackouts (e.g., Northeast blackout of 2003). Despite this promising vision, how effective cyber and physical interactions are against fault propagation is not yet fully investigated. In this paper, we use analysis and system-level simulations to characterize such interactions during load shedding, which is a process to stop fault propagation by shedding a computed amount of loads based on collaborative communication. Specifically, we model faults happening in the physical domain as a counting process, with each count triggering a load shedding action on the fly in the cyber domain. We show that although global load shedding design is considered optimal by globally coordinating shedding actions in power engineering, its induced failure probability (defined as the one that at least a given number of power lines fail) is scalable to the delay performance and the system size in the cyber domain, thus less likely to stop fault propagation in large systems than local shedding design that sheds loads within a limited system scope. Our study demonstrates that a joint view on cyber and physical factors is essential for failure prevention design in the smart grid.},
keywords={load shedding;power system faults;probability;smart power grids;physical interactions;fault propagation;smart grid;cyber domain;physical infrastructures;physical domain;legacy systems;cyber-physical paradigm;system-level simulations;model faults;load shedding action;global load shedding design;shedding actions;local shedding design;physical factors;cyber interactions;Smart grids;Computational modeling;Load modeling;Power system faults;Power system protection;Analytical models;Smart grid;load shedding;fault propagation;cascading failure;failure prevention;modeling and simulations},
doi={10.1109/INFOCOM.2017.8057061},
ISSN={},
month={May},}
@INPROCEEDINGS{8057062,
author={X. Lou and R. Tan and D. K. Y. Yau and P. Cheng},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Cost of differential privacy in demand reporting for smart grid economic dispatch},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Increasing dynamics of electrical loads presents uncertainty and hence new challenges for power grid controls and optimization. In economic dispatch control (EDC) for minimizing generation cost, demand reporting by customers is a promising approach for managing the uncertainty, but it raises important privacy concerns. Adding random noise to aggregate queries of demand reports can provide differential privacy (DP) for the individual customers. But the noisy query results can adversely impact the EDC's optimality. In this paper, we analyze the privacy cost in demand reporting in terms of how DP-induced noise will increase the total generation cost. Our analysis shows that the noise amounts for different customers are intricately coupled with one another in determining the total cost. In view of the coupling, we apply the principle of Shapley value to attribute fair shares of the total cost to the power grid buses. For efficient sharing of the privacy cost, in a manner scalable to large power systems with many buses, we additionally propose heuristic algorithms to approximate the Shapley value. Trace-driven simulations based on a 5-bus power system model validate our analysis and illustrate the performance of the proposed cost sharing algorithms.},
keywords={data privacy;minimisation;power generation dispatch;power generation economics;power grids;smart power grids;differential privacy;demand reporting;smart grid economic dispatch;optimization;economic dispatch control;demand reports;privacy cost;DP-induced noise;power grid buses;cost sharing algorithms;electrical loads;power grid control;privacy concerns;EDC optimality;Shapley value;total generation cost minimisation;Privacy;Power grids;Aggregates;Uncertainty;Generators;Power system dynamics},
doi={10.1109/INFOCOM.2017.8057062},
ISSN={},
month={May},}
@INPROCEEDINGS{8057063,
author={A. Sehati and M. Ghaderi},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Energy-delay tradeoff for request bundling on smartphones},
year={2017},
volume={},
number={},
pages={1-9},
abstract={To reduce the energy consumption of a smartphone, multiple data transfer requests from applications can be bundled together and granted at once in order to reduce the time the radio interface is on. The side effect of bundling is the increased delay experienced by mobile applications. While several bundling algorithms have been proposed in the literature, a general and systematic solution to balance the energy-delay tradeoff is missing. In this paper, we formulate bundling as a cost minimization problem, in which the tradeoff between energy and delay is captured by a cost function. We then propose an online algorithm for minimizing the bundling cost and show that the algorithm is 4-competitive with respect to the optimal offline algorithm that knows the entire sequence of data transfer requests a priori. We evaluate the performance of the proposed algorithm and the accuracy of our results in a range of realistic scenarios using both model-driven simulations and real experiments on a smartphone. Our results show that depending on the delay tolerance level of a user, energy savings ranging from zero (delay intolerant) to about 100% (delay tolerant) can be achieved using our algorithm.},
keywords={costing;delays;minimisation;smart phones;telecommunication power management;energy-delay tradeoff;cost minimization problem;cost function;bundling cost;optimal offline algorithm;delay tolerance level;energy savings;request bundling;energy consumption;smartphone data transfer requests;multiple data transfer requests;mobile applications;bundling algorithms;radio interface;Delays;Algorithm design and analysis;Data transfer;Smart phones;Energy consumption;Mobile communication;Conferences},
doi={10.1109/INFOCOM.2017.8057063},
ISSN={},
month={May},}
@INPROCEEDINGS{8057064,
author={L. De Carli and R. Torres and G. Modelo-Howard and A. Tongaonkar and S. Jha},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Botnet protocol inference in the presence of encrypted traffic},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Network protocol reverse engineering of botnet command and control (C&C) is a challenging task, which requires various manual steps and a significant amount of domain knowledge. Furthermore, most of today's C&C protocols are encrypted, which prevents any analysis on the traffic without first discovering the encryption algorithm and key. To address these challenges, we present an end-to-end system for automatically discovering the encryption algorithm and keys, generating a protocol specification for the C&C traffic, and crafting effective network signatures. In order to infer the encryption algorithm and key, we enhance state-of-the-art techniques to extract this information using lightweight binary analysis. In order to generate protocol specifications we infer field types purely by analyzing network traffic. We evaluate our approach on three prominent malware families: Sality, ZeroAccess and Ramnit. Our results are encouraging: the approach decrypts all three protocols, detects 97% of fields whose semantics are supported, and infers specifications that correctly align with real protocol specifications.},
keywords={computer network security;cryptographic protocols;invasive software;reverse engineering;statistical analysis;telecommunication traffic;transport protocols;botnet protocol inference;encrypted traffic;network protocol reverse engineering;C&C protocols;network traffic;botnet botnet command and control protocols;lightweight binary analysis;network signatures;Sality malware;ZeroAccess malware;Ramnit malware;Encryption;Protocols;Malware;Ciphers;Payloads},
doi={10.1109/INFOCOM.2017.8057064},
ISSN={},
month={May},}
@INPROCEEDINGS{8057065,
author={A. A. Chariton and E. Degkleri and P. Papadopoulos and P. Ilia and E. P. Markatos},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={CCSP: A compressed certificate status protocol},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Trust in SSL-based communications is provided by Certificate Authorities (CAs) in the form of signed certificates. Checking the validity of a certificate involves three steps: (i) checking its expiration date, (ii) verifying its signature, and (iii) ensuring that it is not revoked. Currently, such certificate revocation checks are done either via Certificate Revocation Lists (CRLs) or Online Certificate Status Protocol (OCSP) servers. Unfortunately, despite the existence of these revocation checks, sophisticated cyber-attackers, may trick web browsers to trust a revoked certificate, believing that it is still valid. Consequently, the web browser will communicate (over TLS) with web servers controlled by cyber-attackers. Although frequently updated, nonced, and timestamped certificates may reduce the frequency and impact of such cyber-attacks, they impose a very large overhead to the CAs and OCSP servers, which now need to timestamp and sign on a regular basis all the responses, for every certificate they have issued, resulting in a very high overhead. To mitigate this overhead and provide a solution to the described cyber-attacks, we present CCSP: a new approach to provide timely information regarding the status of certificates, which capitalizes on a newly introduced notion called signed collections. In this paper, we present the design, preliminary implementation, and evaluation of CCSP in general, and signed collections in particular. Our preliminary results suggest that CCSP (i) reduces space requirements by more than an order of magnitude, (ii) lowers the number of signatures required by 6 orders of magnitude compared to OCSP-based methods, and (iii) adds only a few milliseconds of overhead in the overall user latency.},
keywords={certification;computer network security;Internet;protocols;public key cryptography;timestamped certificates;CAs;OCSP servers;CCSP;OCSP-based methods;compressed certificate status protocol;Certificate Authorities;signed certificates;certificate revocation checks;Certificate Revocation Lists;Online Certificate Status Protocol servers;sophisticated cyber-attackers;web browser;web servers;Browsers;Web servers;Protocols;Public key;Conferences;Receivers},
doi={10.1109/INFOCOM.2017.8057065},
ISSN={},
month={May},}
@INPROCEEDINGS{8057066,
author={B. Wang and L. Zhang and N. Z. Gong},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={SybilSCAR: Sybil detection in online social networks via local rule based propagation},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Detecting Sybils in online social networks (OSNs) is a fundamental security research problem as adversaries can leverage Sybils to perform various malicious activities. Structure-based methods have been shown to be promising at detecting Sybils. Existing structure-based methods can be classified into two categories: Random Walk (RW)-based methods and Loop Belief Propagation (LBP)-based methods. RW-based methods cannot leverage labeled Sybils and labeled benign users simultaneously, which limits their detection accuracy, and they are not robust to noisy labels. LBP-based methods are not scalable, and they cannot guarantee convergence. In this work, we propose SybilSCAR, a new structure-based method to perform Sybil detection in OSNs. SybilSCAR maintains the advantages of existing methods while overcoming their limitations. Specifically, SybilSCAR is Scalable, Convergent, Accurate, and Robust to label noises. We first propose a framework to unify RW-based and LBP-based methods. Under our framework, these methods can be viewed as iteratively applying a (different) local rule to every user, which propagates label information among a social graph. Second, we design a new local rule, which SybilSCAR iteratively applies to every user to detect Sybils. We compare SybilSCAR with a state-of-the-art RW-based method and a state-of-the-art LBP-based method, using both synthetic Sybils and large-scale social network datasets with real Sybils. Our results demonstrate that SybilSCAR is more accurate and more robust to label noise than the compared state-of-the-art RW-based method, and that SybilSCAR is orders of magnitude more scalable than the state-of-the-art LBP-based method and is guaranteed to converge. To facilitate research on Sybil detection, we have made our implementation of SybilSCAR publicly available on our webpages.},
keywords={graph theory;learning (artificial intelligence);security of data;social networking (online);Sybil detection;online social networks;RW-based methods;noisy labels;LBP-based methods;label information;synthetic Sybils;large-scale social network datasets;local rule based propagation;security research problem;SybilSCAR;OSN;Robustness;Image edge detection;Training;Belief propagation;Convergence;Twitter},
doi={10.1109/INFOCOM.2017.8057066},
ISSN={},
month={May},}
@INPROCEEDINGS{8057067,
author={S. Ji and S. Yang and A. Das and X. Hu and R. Beyah},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Password correlation: Quantification, evaluation and application},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In this paper, we study the correlation between passwords across different datasets which quantitatively explains the success of existing training-based password cracking techniques. We also study the correlation between a user's password and his/her social profile. This enabled us to develop the first social profile-aware password strength meter, namely SociaLShield. Our quantification techniques and SocialShield have meaningful implications to system administrators, users, and researchers, e.g., helping them quantitatively understand the threats posed by a password leakage incident, defending against emerging profile-based password attacks, and facilitating the research of countermeasures against existing and newly developed training-based password attacks. We validate our proposed quantification techniques and SocialShield through extensive experiments by leveraging real-world leaked passwords. Experimental results demonstrate that our quantification techniques are accurate in measuring correlation among different leaked datasets and that although SocialShield is light-weight, it is effective in defending against profile-based password attacks.},
keywords={authorisation;quantification techniques;SocialShield;password leakage incident;password correlation;password cracking techniques;social profile-aware password strength meter;SociaLShield;profile-based password attacks;training-based password attacks;leaked datasets;user passwork;user social profile;Correlation;Markov processes;Meters;Measurement;Security;Standards;LinkedIn},
doi={10.1109/INFOCOM.2017.8057067},
ISSN={},
month={May},}
@INPROCEEDINGS{8057068,
author={A. Kuhnle and T. Pan and M. A. Alim and M. T. Thai},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Scalable bicriteria algorithms for the threshold activation problem in online social networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We consider the Threshold Activation Problem (TAP): given social network G and positive threshold T, find a minimum-size seed set A that can trigger expected activation of at least T. We introduce the first scalable, parallelizable algorithm with performance guarantee for TAP suitable for datasets with millions of nodes and edges; we exploit the bicriteria nature of solutions to TAP to allow the user to control the running time versus accuracy of our algorithm through a parameter α ϵ (0, 1): given η > 0, with probability 1 - η our algorithm returns a solution A with expected activation greater than T - 2αT, and the size of the solution A is within factor 1-h 4α/T + log(T) of the optimal size. The algorithm runs in time O (α-2log (n/η) (n + m)|A|), where n, m, refer to the number of nodes, edges in the network. The performance guarantee holds for the general triggering model of internal influence and also incorporates external influence, provided a certain condition is met on the cost-effectivity of seed selection.},
keywords={computational complexity;directed graphs;network theory (graphs);parallel algorithms;probability;social networking (online);minimum-size seed set;expected activation;running time;positive threshold;TAP;scalable parallelizable algorithm;online social networks;threshold activation problem;scalable bicriteria algorithms;optimal size;Social network services;Integrated circuit modeling;Computational modeling;TV;Conferences;Context modeling},
doi={10.1109/INFOCOM.2017.8057068},
ISSN={},
month={May},}
@INPROCEEDINGS{8057069,
author={X. Li and J. D. Smith and T. N. Dinh and M. T. Thai},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Why approximate when you can get the exact? Optimal targeted viral marketing at scale},
year={2017},
volume={},
number={},
pages={1-9},
abstract={One of the most central problems in viral marketing is Influence Maximization (IM), which finds a set of k seed users who can influence the maximum number of users in online social networks. Unfortunately, all existing algorithms to IM, including the state of the art SSA and IMM, have an approximation ratio of (1 - 1/e - ϵ). Recently, a generalization of IM, Cost-aware Target Viral Marketing (CTVM), asks for the most cost-effective users to influence the most relevant users, has been introduced. The current best algorithm for CTVM has an approximation ratio of (1 - 1/√e - ϵ). In this paper, we study the CTVM problem, aiming to optimally solve the problem. We first highlight that using a traditional two stage stochastic programming to exactly solve CTVM is not possible because of scalability. We then propose an almost exact algorithm TIPTOP, which has an approximation ratio of (1 - ϵ). This result significantly improves the current best solutions to both IM and CTVM. At the heart of TIPTOP lies an innovative technique that reduces the number of samples as much as possible. This allows us to exactly solve CTVM on a much smaller space of generated samples using Integer Programming. While obtaining an almost exact solution, TIPTOP is very scalable, running on billion-scale networks such as Twitter under three hours. Furthermore, TIPTOP lends a tool for researchers to benchmark their solutions against the optimal one in large-scale networks, which is currently not available.},
keywords={approximation theory;graph theory;integer programming;marketing data processing;social networking (online);stochastic programming;IM;online social networks;approximation ratio;Cost-aware Target Viral Marketing;cost-effective users;CTVM problem;Influence Maximization;two stage stochastic programming;TIPTOP exact algorithm;Twitter;Approximation algorithms;Programming;Social network services;Integrated circuit modeling;Scalability;Conferences;Viral Marketing;Influence Maximization;Algorithms;Online Social Networks;Optimization},
doi={10.1109/INFOCOM.2017.8057069},
ISSN={},
month={May},}
@INPROCEEDINGS{8057070,
author={Z. Zhang and Y. Shi and J. Willson and D. Du and G. Tong},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Viral marketing with positive influence},
year={2017},
volume={},
number={},
pages={1-8},
abstract={One model for viral marketing is the positive influence. In this model, an inactive node is changed into active if and only if at least half of its neighbors are already in active state. The positive influence model can be viewed as a special case of a general threshold model, in which the threshold function at each node has value one if at least a certain fraction of neighbors are in active state, and value 0 otherwise. This function can be proved to be monotonically increasing and nonsubmodular for any predefined fraction. Therefore, given a seed set, the number of influenced nodes is not submodular with respect to the size of the seed set. This fact makes those optimization problems related with positive influence very hard, including the minimum partial positive influence seeding problem: Given a social network G = (V, E) and a number 0 <; p <; 1, find a minimum seed set S which can positively influence at least p|V| nodes. In this paper, we present an O ((log n)2H ([pn]))-approximation algorithm for the minimum partial positive influence seeding problem, where n is the number of nodes, and H(·) is the Harmonic number.},
keywords={computational complexity;marketing;network theory (graphs);optimisation;minimum partial positive influence seeding problem;viral marketing;positive influence model;threshold function;seed set;influenced nodes;social network;harmonic number;Approximation algorithms;Greedy algorithms;Conferences;Social network services;Computer science;Electronic mail;Computational modeling;viral marketing;partial positive-influence seeding problem;approximation algorithm},
doi={10.1109/INFOCOM.2017.8057070},
ISSN={},
month={May},}
@INPROCEEDINGS{8057071,
author={C. Lee and X. X. Do and Y. Eun},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={On the rao-blackwellization and its application for graph sampling via neighborhood exploration},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We study how the so-called Rao-Blackwellization, which is a variance reduction technique via “conditioning” for Monte Carlo methods, can be judiciously applied for graph sampling through neighborhood exploration. Despite its popularity for Monte Carlo methods, it is little known for Markov chain Monte Carlo methods and has never been discussed for random walk-based graph sampling. We first propose two forms of Rao-Blackwellization that can be used as a swap-in replacement for virtually all (reversible) random-walk graph sampling methods, and prove that the `Rao-Blackwellized' estimators reduce the (asymptotic) variances of their original estimators yet maintain their inherent unbiasedness. The variance reduction can translate into lowering the number of samples required to achieve a desired sampling accuracy. However, the sampling cost for neighborhood exploration, if required, may outweigh such improvement, even leading to higher total amortized cost. Considering this, we provide a generalization of Rao-Blackwellization, which allows one to choose a suitable extent of obtaining Rao-Blackwellized samples in order to achieve a right balance between sampling cost and accuracy. We finally provide simulation results via real-world datasets that confirm our theoretical findings.},
keywords={Bayes methods;graph theory;Markov processes;Monte Carlo methods;random processes;sampling methods;neighborhood exploration;graph sampling;Markov chain Monte Carlo methods;Rao-Blackwellized estimators;sampling cost;conditioning technique;asymptotic variance reduction;total amortized cost;virtually-all random-walk graph;Monte Carlo methods;Sampling methods;Markov processes;Conferences;Random variables;Simulation;Social network services},
doi={10.1109/INFOCOM.2017.8057071},
ISSN={},
month={May},}
@INPROCEEDINGS{8057072,
author={Y. Wu and B. Zhang and S. Yang and X. Yi and X. Yang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Energy-efficient joint communication-motion planning for relay-assisted wireless robot surveillance},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In this paper, we consider a surveillance scenario where a team of sensing robots survey a sensitive area and transmit the monitored data to a remote base station through a mobile relay. In this scenario, it is challenging to autonomously adjust the position of the mobile relay for the sake of minimizing the total communication-motion energy consumption of the system, while maintaining the communication quality of the mobile sensing robots. We first derive the asymptotically optimal transmit powers of the mobile relay and of the sensing robots according to the predefined end-to-end packet error rate (PER) requirement. Then, we propose a joint communication-motion planning (JCMP) method for minimizing the total communication-motion energy consumption in both: single- and multi-sensing-robot scenarios, where the trajectories of the sensing robots are rigorously defined. We further consider the scenario where the sensing robots' trajectories are not fixed but can be optimized in restrained areas. The effectiveness of the proposed JCMP is verified by analysis and numerical results for different system configurations, showing that a substantial energy-efficiency improvement may be achieved in comparison with the benchmark that only optimizes the communication energy consumption.},
keywords={energy conservation;energy consumption;error statistics;mobile robots;relay networks (telecommunication);telecommunication network planning;telecommunication power management;remote base station;mobile relay;total communication-motion energy consumption;communication quality;mobile sensing robots;asymptotically optimal transmit powers;predefined end-to-end packet error rate requirement;joint communication-motion planning method;multisensing-robot scenarios;energy-efficient joint communication-motion planning;wireless robot surveillance;surveillance scenario;energy-efficiency improvement;Robot sensing systems;Mobile communication;Relays;Energy consumption;Base stations;Joint communication-motion planning;energy-efficient relays;optimization methods;robots;surveillance},
doi={10.1109/INFOCOM.2017.8057072},
ISSN={},
month={May},}
@INPROCEEDINGS{8057073,
author={T. Shi and S. Cheng and J. Li and Z. Cai},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Constructing connected dominating sets in battery-free networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Currently, the limitation of battery has become a serious obstacle for the development of Internet of things (IoTs). Therefore, a new network architecture, named as battery-free network, were proposed. In a typical battery-free network, the battery-free nodes are equipped with any battery and can only gain energy from the environment. Such network extremely expands the scope of the IoT applications, however, it also brings many troubles for some network operations, e.g. data collection, since the energy of each node is quite small. Considering that the Connected Dominating Sets (CDSs) are commonly used to support data collection and network communication in wireless networks, and thus we will also investigate the CDS construction problem in battery-free networks. In this paper, the problem of constructing CDS in a battery-free network is formally defined, and we prove that it is NP-Complete. Thus, four approximation algorithms were proposed to deal with the snapshot, continuous and time-window based CDS construction requirements, respectively. Finally, the extensive experiments were carried out and the results verify that the proposed algorithms have high performance in term of accuracy and efficiency.},
keywords={approximation theory;Internet of Things;radio networks;set theory;connected dominating sets;network architecture;battery-free nodes;network operations;network communication;wireless networks;battery-free network;Internet of things;IoT;CDS;CDS construction problem;approximation algorithms;Sensors;Batteries;Wireless sensor networks;Bridges;Monitoring;Windows;Conferences},
doi={10.1109/INFOCOM.2017.8057073},
ISSN={},
month={May},}
@INPROCEEDINGS{8057074,
author={Y. Gao and W. Dong and X. Zhang and W. Wu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Universal path tracing for large-scale sensor networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Most sensor networks employ dynamic routing protocols so that the routing topology can be dynamically optimized with environmental changes. The routing behaviors can be quite complex with increasing network scale and environmental dynamics. Knowledge on the routing path of each packet is certainly a great help in understanding the complex routing behaviors, allowing effective performance diagnosis and efficient network management. We propose PAT, a universal sensornet path tracing approach. PAT includes an intelligent path encoding scheme that allows efficient decoding at the PC side. To make PAT more scalable, we propose techniques to accurately estimate the degree information by exploiting timing information, allowing more compact path encoding. Moreover, we employ subpath concatenation to infer excessively long paths with a high recovery probability. We carefully evaluate PAT's performance using testbed experiments and and extensive simulations with up to 4,000 nodes. Results show that PAT significantly outperforms existing approaches.},
keywords={decoding;encoding;probability;routing protocols;telecommunication network management;telecommunication network topology;wireless sensor networks;network management;decoding;performance diagnosis;PAT;complex routing behaviors;routing path;environmental dynamics;network scale;routing topology;dynamic routing protocols;large-scale sensor networks;compact path encoding;timing information;intelligent path encoding scheme;universal sensornet path tracing approach;Routing;Topology;Network topology;Wireless sensor networks;Robot sensing systems;Encoding;Monitoring},
doi={10.1109/INFOCOM.2017.8057074},
ISSN={},
month={May},}
@INPROCEEDINGS{8057075,
author={F. Guo and B. Zhou and M. C. Vuran},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={CFOSynt: Carrier frequency offset assisted clock syntonization for wireless sensor networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={System-level timing inconsistency and wireless communication delays are the main uncertainties of existing synchronization mechanisms for wireless sensor networks. Existing solutions mainly rely on timestamp exchanges to estimate clock offset and skew, which results in frequent synchronization, high overhead, and high energy consumption to maintain a well-synchronized network. This paper introduces a novel clock syntonization approach to estimate the differences between clock frequencies of network nodes without the need for timestamp exchanges. The carrier frequency offset (CFO) assisted syntonization (CFOSynt) utilizes the carrier information obtained from wireless packet transmission for clock skew compensation. The key idea of CFOSynt is that, in any wireless communication system, where carrier modulation is employed, carrier frequency delivers information about the transmitter RF clock. Consequently, clock frequency offset between a pair of sensor nodes will result in a carrier frequency offset detected by the receiver node. By leveraging the CFO information, CFOSynt can estimate the system clock skew based on digital counter theory. Extensive experiments and numerical analysis have been demonstrated to evaluate clock skew estimation.},
keywords={clocks;delays;modulation;synchronisation;wireless sensor networks;CFOSynt;wireless sensor networks;system-level timing inconsistency;wireless communication delays;synchronization mechanisms;timestamp exchanges;frequent synchronization;high energy consumption;well-synchronized network;novel clock syntonization approach;clock frequency;network nodes;carrier information;wireless packet transmission;clock skew compensation;wireless communication system;carrier modulation;transmitter RF clock;sensor nodes;system clock;clock skew estimation;carrier frequency offset;CFO assisted syntonization;Clocks;Synchronization;Wireless sensor networks;Estimation;Wireless communication;Radio frequency;Delays},
doi={10.1109/INFOCOM.2017.8057075},
ISSN={},
month={May},}
@INPROCEEDINGS{8057076,
author={X. Lin and S. Wang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Efficient remote radio head switching scheme in cloud radio access network: A load balancing perspective},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Cloud radio access network (C-RAN) is deemed as a promising architecture to meet the exponentially increasing traffic demand in mobile networks, where baseband processing is separated from remote radio heads (RRHs) and performed in a centralized baseband unit (BBU) pool. However, the densely deployed RRHs, as well as the passive optical network which provides high capacity backhauls between the RRHs and the BBU pool, consume a large amount of energy. In this paper, we propose efficient RRH switching schemes to achieve a tradeoff between the system energy saving and the load balance among the RRHs in the C-RAN. We first develop an approximation algorithm to address the intractable user association problem for a given set of RRHs, based on which we introduce efficient local search algorithms to perform RRH selection procedure, which can reduce the load fairness index of the C-RAN by controlling the active/inactive state of each RRH. We also discuss the handover signalling overhead issue and introduce an adaptive trigger mechanism to avoid switching on/off too many RRHs simultaneously so as to keep the signalling overhead of the C-RAN below an acceptable level. Numerical results demonstrate that the proposed RRH switching schemes can improve the system performance of the C-RAN significantly. Moreover, our proposal sheds light on how to design effective and efficient handover schemes for next generation mobile networks.},
keywords={cellular radio;cloud computing;energy conservation;mobile radio;mobility management (mobile radio);radio access networks;resource allocation;search problems;telecommunication computing;telecommunication power management;telecommunication traffic;cloud radio access network;baseband processing;remote radio heads;centralized baseband unit pool;passive optical network;BBU pool;RRH selection procedure;load fairness index;RRH switching schemes;load balancing;traffic demand;local search algorithm;next generation mobile networks;remote radio head switching scheme;RRH efficiency;handover schemes;energy saving;Optical switches;Power demand;Interference;Indexes;Heuristic algorithms;Mobile communication},
doi={10.1109/INFOCOM.2017.8057076},
ISSN={},
month={May},}
@INPROCEEDINGS{8057077,
author={D. Griffith and A. Ben Mosbah and R. Rouil},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Group discovery time in device-to-device (D2D) proximity services (ProSe) networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Device-to-device (D2D) communications for Long Term Evolution (LTE) networks relies on a discovery process to enable User Equipment (UE) to determine which D2D applications and services are supported by neighboring UEs. This is especially important for groups of UEs that operate outside the coverage area of any base station. The amount of time required for discovery information to reach every UE in a group depends on the number of UEs in the group and the dimensions of the discovery resource pool associated with the Physical Sidelink Discovery Channel (PSDCH); an additional factor is the half-duplex property of current UEs. In this paper, we use a Markov chain to characterize the performance of Mode 2 direct discovery. The resulting analytical model gives the distribution of the time for a UE to discover all other UEs in its group. We validate the model using Monte Carlo and network simulations.},
keywords={Long Term Evolution;Monte Carlo methods;resource allocation;group discovery time;device-to-device proximity services;Device-to-device communications;Long Term Evolution networks;neighboring UEs;discovery information;discovery resource pool;Physical Sidelink Discovery Channel;Mode 2 direct discovery;LTE network;D2D communication;Monte Carlo;Device-to-device communication;Markov processes;Mathematical model;Superluminescent diodes;Analytical models;Indexes;Long Term Evolution},
doi={10.1109/INFOCOM.2017.8057077},
ISSN={},
month={May},}
@INPROCEEDINGS{8057078,
author={R. Calvo-Palomino and D. Giustiniano and V. Lenders and A. Fakhreddine},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Crowdsourcing spectrum data decoding},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Crowdsourced signal monitoring systems are gaining attention for capturing the wireless spectrum at large geographical scale. Yet, most of the current systems are still limited to simple power spectrum measurements reported by each sensor. Our objective is to enhance such systems with signal decoding capabilities performed in the backend while retaining the original vision of a low-cost and crowdsourced setup. We propose a distributed system architecture for collaborative radio signal monitoring and decoding that builds on $12 low-cost radio frequency (RF) frontends and embedded boards and that takes into consideration the limited network bandwidth from the sensors to the backend. We present a distributed time multiplexing mechanism to sample the spectrum in a coordinated fashion that exploits the similarity of the radio signal received by more than one RF frontend in the same radio coverage. We address the strict time synchronization required among sensors to reconstruct the signal from the samples they receive when in the same radio coverage. We study and implement techniques to identify and overcome errors in the timing information in the presence of noise sources and decode the data in the backend. We provide an evaluation based on simulations and on real signals transmitted by Long-Term Evolution (LTE) base stations. Our results show that we can reliably reconstruct and decode radio signals received by low-cost crowdsourced sensors.},
keywords={cognitive radio;crowdsourcing;decoding;radio receivers;synchronisation;radio coverage;backend;decode radio signals;spectrum data;crowdsourced signal monitoring systems;wireless spectrum;geographical scale;signal decoding capabilities;distributed system architecture;collaborative radio signal monitoring;consideration the limited network bandwidth;distributed time;time synchronization;crowdsourced sensors;Decoding;Radio frequency;Synchronization;Monitoring;Bandwidth;Internet;Economic indicators},
doi={10.1109/INFOCOM.2017.8057078},
ISSN={},
month={May},}
@INPROCEEDINGS{8057079,
author={Y. Zhang and L. Deng and M. Chen and P. Wang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Joint bidding and geographical load balancing for datacenters: Is uncertainty a blessing or a curse?},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We consider the scenario where a cloud service provider (CSP) operates multiple geo-distributed datacenters to provide Internet-scale service. Our objective is to minimize the total electricity and bandwidth cost by jointly optimizing electricity procurement from wholesale markets and geographical load balancing (GLB), i.e., dynamically routing workloads to locations with cheaper electricity. Under the ideal setting where exact values of market prices and workloads are given, this problem reduces to a simple LP and is easy to solve. However, under the realistic setting where only distributions of these variables are available, the problem unfolds into a non-convex infinite-dimensional one and is challenging to solve. Our main contribution is to develop an algorithm that is proven to solve the challenging problem optimally and efficiently, by exploring the full design space of strategic bidding. Trace-driven evaluations corroborate our theoretical results, demonstrate fast convergence of our algorithm, and show that it can reduce the cost for the CSP by up to 20% as compared to baseline alternatives. Our study highlights the intriguing role of uncertainty. While variability in workloads deteriorates the cost-saving performance of joint electricity procurement and GLB, counter-intuitively, variability in market prices can be exploited to achieve a cost reduction even larger than the setting without price variability.},
keywords={cloud computing;computer centres;concave programming;convex programming;cost reduction;Internet;power aware computing;power markets;pricing;resource allocation;tendering;cloud service provider;CSP;Internet-scale service;joint bidding and geographical load balancing;multiple geo-distributed datacenters;joint electricity procurement optimisation;dynamic routing workloads;nonconvex infinite-dimensional problem;trace-driven evaluations;total electricity minimization;bandwidth cost minimization;price variability;cost reduction;cost-saving performance;strategic bidding;market prices;GLB;wholesale markets;Real-time systems;Procurement;Standards;Load management;Electricity supply industry;Conferences;Uncertainty},
doi={10.1109/INFOCOM.2017.8057079},
ISSN={},
month={May},}
@INPROCEEDINGS{8057080,
author={A. Kabbani and M. Sharif},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Flier: Flow-level congestion-aware routing for direct-connect data centers},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Various topologies have been proposed in the context of high-performance computing and data center networking. Direct-connect topologies generally offer large capacity with high path diversity and are highly cost effective for general data center traffic patterns. However, the lack of simple yet efficient load balancing techniques for direct-connect fabrics has hindered these networks from gaining traction in data centers. This paper presents the design, implementation, and evaluation of Flicr, a light-weight host-based load balancing mechanism for direct-connect data centers. Flicr dynamically reroutes traffic through minimal and non-minimal routes to avoid congesting the fabric. This enables Flicr to efficiently minimize networking resource consumption while exploiting high path diversity in direct-connect fabrics to balance the network and gracefully handle link failures. Flicr requires only a simple kernel modification and is readily deployable in commodity data centers today. Our evaluations show that Flicr consistently outperforms other state-of-the-art load balancing designs, achieving 25-60% lower average flow completion time compared to adaptive routing. Flicr is also more robust against link failures and has 5-8 χ better performance relative to other schemes in the presence of link failures.},
keywords={computer centres;computer networks;data communication;telecommunication network routing;telecommunication network topology;telecommunication traffic;Flow-level congestion-aware routing;high-performance computing;data center networking;high path diversity;general data center traffic patterns;Flicr;load balancing mechanism;commodity data centers today;fabrics;direct-connect topologies;networking resource consumption;Routing;Load management;Fabrics;Topology;Network topology;Robustness},
doi={10.1109/INFOCOM.2017.8057080},
ISSN={},
month={May},}
@INPROCEEDINGS{8057081,
author={R. Zhu and D. Niu and B. Li and Z. Li},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Optimal multicast in virtualized datacenter networks with software switches},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Virtualized datacenter networks have been deployed in production platforms, e.g., Amazon VPC and VMware's NVP, to offer the flexibility of network management to enterprise-level clients. A common characteristic of these platforms is that they adopt software switches, such as Open vSwitch (OvS), instead of hardware switches to transfer data between VMs. Although group communication is common in enterprise applications, the unique characteristics of software switches have posed new challenges to the design of multicast protocols. How logical multicast can be optimally performed with software switches is still not well understood. In this paper, we observe that unlike hardware switches, the per-stream output rate in a software switch critically depends on the packet processing overhead of flow cloning. We study the optimal OvS multicast topology with or without the help of additional dedicated software switches called service nodes, and formulate the throughput maximization as a new class of degree-supervised combinatorial graph problems due to the presence of flow cloning costs. We propose a lineartime optimal solution that translates into simple forwarding rules installed at each software switch. Through emulation-based OvS profiling and extensive simulation results, we demonstrate that our proposed logical multicast solutions can significantly improve session throughput with the ability to handle load balancing and latency issues, as compared to the state-of-the-art in the literature.},
keywords={computer centres;computer network management;graph theory;multicast protocols;telecommunication network topology;telecommunication switching;virtualisation;virtualized datacenter networks;hardware switches;software switches;network management;Open vSwitch;multicast protocols;OvS multicast topology;degree-supervised combinatorial graph problem;Software;Virtual machine monitors;Throughput;Topology;Hardware;Cloning;Network topology},
doi={10.1109/INFOCOM.2017.8057081},
ISSN={},
month={May},}
@INPROCEEDINGS{8057082,
author={Z. Li and W. Bai and K. Chen and D. Han and Y. Zhang and D. Li and H. Yu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Rate-aware flow scheduling for commodity data center networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Flow completion times (FCTs) are critical for many cloud applications. To minimize the average FCT, recent transport designs, such as pFabric, PASE, and PIAS, approximate the Shortest Remaining Time First (SRTF) scheduling. A common, implicit assumption of these solutions is that the remaining time is only determined by the remaining flow size. However, this assumption does not hold in many real-world scenarios where applications generate data at diverse rates that are smaller than the network capacity. In this paper, we look into this issue from system perspective and find that the operating system (OS) kernel can be exploited to better estimate the remaining time of a flow. In particular, we use the rate of copying data from user space to kernel space to measure the data generation rate. We design RAX, a rate aware flow scheduling method, that calculates the remaining time of a flow more accurately, based on not only the flow size but also the data generation rate. We have implemented a RAX prototype in Linux kernel and evaluated it through testbed experiments and ns-2 simulations. Our testbed results show that RAX reduces FCT by up to 14.9%/41.8% and 7.8%/22.9% over DCTCP and PIAS for all/medium flows respectively.},
keywords={computer centres;computer networks;Linux;telecommunication scheduling;transport protocols;network capacity;operating system kernel;data generation rate;rate aware flow scheduling method;commodity data center networks;flow completion times;cloud applications;diverse rates;FCT;shortest remaining time first schedulling approximation;SRTF scheduling;Linux kernel;RAX prototype;ns-2 simulations;Kernel;Servers;Schedules;Throughput;Conferences;Hard disks},
doi={10.1109/INFOCOM.2017.8057082},
ISSN={},
month={May},}
@INPROCEEDINGS{8057083,
author={Y. Li and W. Gao},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Interconnecting heterogeneous devices in the personal mobile cloud},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Recent diversification of mobile computing devices allows a mobile user to own multiple types of devices for different application scenarios, but also results in various restrictions on the performance and usability of these devices. A viable solution to such restriction is to incorporate and interconnect mobile devices towards a personal mobile cloud where these devices can complement each other via cooperative resource sharing, but is challenging due to the heterogeneity of mobile devices in both hardware and software aspects. In this paper, we propose a novel design of resource sharing framework to address these challenges and generically interconnect heterogeneous mobile devices. Our basic idea is to mask the hardware and software heterogeneity in mobile systems by exploiting the existing mobile OS services as the interface of resource sharing, and further develop the resource sharing framework as a middleware in the mobile OS. We have implemented our design over various mobile platforms with diverse characteristics and resource limits, and demonstrated that our design can efficiently support generic resource sharing among heterogeneous mobile devices without incurring significant system overhead or requiring individual system modification.},
keywords={cloud computing;middleware;mobile computing;operating systems (computers);resource allocation;heterogeneous devices;personal mobile cloud;mobile computing devices;mobile user;interconnect mobile devices;software aspects;generically interconnect heterogeneous mobile devices;software heterogeneity;mobile systems;resource sharing framework;mobile platforms;cooperative resource sharing;hardware heterogeneity;middleware;mobile OS;Mobile communication;Hardware;Resource management;Mobile handsets;Mobile applications;Androids;Humanoid robots},
doi={10.1109/INFOCOM.2017.8057083},
ISSN={},
month={May},}
@INPROCEEDINGS{8057084,
author={L. Chen and H. Shen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Considering resource demand misalignments to reduce resource over-provisioning in cloud datacenters},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Previous resource provisioning strategies in cloud datacenters allocate physical resources to virtual machines (VMs) based on the predicted resource utilization pattern of VMs. The pattern for VMs of a job is usually derived from historical utilizations of multiple VMs of the job. We observed that these utilization curves are usually misaligned in time, which would lead to resource over-prediction and hence over-provisioning. Since this resource utilization misalignment problem has not been revealed and studied before, in this paper, we study the VM resource utilization from public datacenter traces to verify the existence of the utilization misalignments. Then, to reduce resource over-provisioning, we propose three VM resource utilization pattern refinement algorithms to improve the original generated pattern by lowering the cap of the pattern, reducing cap provision duration and varying the minimum value of the pattern. These algorithms can be used in any resource provisioning strategy that considers predicted resource utilizations of VMs of a job. We then adopt these refinement algorithms in an initial VM allocation mechanism and test them in trace-driven experiments and real-world cluster experiments. The experimental results show that each improved mechanism can increase resource efficiency up to 74%, and reduce the number of PMs needed to satisfy tenant requests up to 47% while conforming the SLO requirement.},
keywords={cloud computing;computer centres;resource allocation;virtual machines;resource demand misalignments;cloud datacenters;physical resources;predicted resource utilization pattern;historical utilizations;utilization curves;resource utilization misalignment problem;VM resource utilization;public datacenter;cap provision duration;resource provisioning strategy;resource utilizations;resource efficiency;resource over-provisioning reduction;VM allocation mechanism;SLO requirement;Resource management;Google;Correlation;Prediction algorithms;Clustering algorithms;Extraterrestrial measurements},
doi={10.1109/INFOCOM.2017.8057084},
ISSN={},
month={May},}
@INPROCEEDINGS{8057085,
author={J. P. Champati and B. Liang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Efficient minimization of sum and differential costs on machines with job placement constraints},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We revisit the problem of assigning n jobs to m machines/servers. We study this problem under more general settings, which capture important aspects of applications that arise in networking and information systems. In particular, we consider jobs that have placement constraints and machines that are heterogeneous. The cost incurred at a machine is given by any general convex function on the number of jobs assigned to it. We aim to minimize the sum cost and the maximum differential cost. Through a network-flow equivalence transformation, we observe how these two objectives are fundamentally related, showing that sum-cost minimization implies maximum-differential-cost minimization. We propose an efficient algorithm termed Maximum Edge-Cost Cycle Cancelling (MEC<sup>3</sup>) to solve the sum-cost minimization problem with O(n<sup>2</sup> m<sup>2</sup>) time complexity. Furthermore, for applications where only the maximum differential cost is of concern, we further improve the efficiency of MEC<sup>3</sup> by proposing an early stop condition. We implement MEC<sup>3</sup> and two other algorithms from the literature. Using benchmark input instances, we show that MEC<sup>3</sup> has substantially lower run time than the other algorithms.},
keywords={computational complexity;convex programming;graph theory;minimisation;scheduling;job placement constraints;job assignment;differential cost minimization;MEC3;O(n2 m2) time complexity;sum-cost minimization problem;Maximum Edge-Cost Cycle Cancelling;maximum-differential-cost minimization;network-flow equivalence transformation;general convex function;Minimization;Cost function;Time complexity;Conferences;Benchmark testing;Algorithm design and analysis;Information systems},
doi={10.1109/INFOCOM.2017.8057085},
ISSN={},
month={May},}
@INPROCEEDINGS{8057086,
author={W. Jiang and Z. Yin and S. M. Kim and T. He},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Transparent cross-technology communication over data traffic},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Cross-technology communication (CTC) techniques are introduced in recent literatures to explore the opportunities of collaboration between heterogeneous wireless technologies, such as WiFi and ZigBee. Their applications include context-aware services and global channel coordination. However, state-of-the-art CTC schemes either suffer from channel inefficiency, low throughput, or disruption to existing networks. This paper presents the CTC via data packets (DCTC), which takes advantage of abundant existing data packets to construct recognizable energy patterns. DCTC features (i) a significant enhancement in CTC throughput while (ii) keeping transparent to upper layer protocols and applications. Our design also features advanced functions including multiplexing to support concurrent transmissions of multiple DCTC senders and adaptive rate control according to the traffic volume. Testbed implementations across WiFi and ZigBee platforms demonstrate reliable bidirectional communication of over 95% in accuracy while achieving throughput 2.3x of the state of the art. Meanwhile, experiment results show that DCTC has little and bounded impact on the delay and throughput of original data traffic.},
keywords={multiplexing;protocols;telecommunication network reliability;telecommunication traffic;wireless channels;wireless LAN;Zigbee;cross-technology communication techniques;heterogeneous wireless technologies;context-aware services;global channel coordination;bidirectional communication;data traffic;WiFi;ZigBee;adaptive rate control;Throughput;Wireless fidelity;ZigBee;Delays;Wireless communication;Protocols;Wireless sensor networks},
doi={10.1109/INFOCOM.2017.8057086},
ISSN={},
month={May},}
@INPROCEEDINGS{8057087,
author={J. Ren and L. Gao and H. Wang and Z. Wang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Optimise web browsing on heterogeneous mobile platforms: A machine learning based approach},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Web browsing is an activity that billions of mobile users perform on a daily basis. Battery life is a primary concern to many mobile users who often find their phone has died at most inconvenient times. The heterogeneous multi-core architecture is a solution for energy-efficient processing. However, the current mobile web browsers rely on the operating system to exploit the underlying hardware, which has no knowledge of individual web contents and often leads to poor energy efficiency. This paper describes an automatic approach to render mobile web workloads for performance and energy efficiency. It achieves this by developing a machine learning based approach to predict which processor to use to run the web rendering engine and at what frequencies the processors should operate. Our predictor learns offline from a set of training web workloads. The built predictor is then integrated into the browser to predict the optimal processor configuration at runtime, taking into account the web workload characteristics and the optimisation goal: whether it is load time, energy consumption or a trade-off between them. We evaluate our approach on a representative ARM big.LITTLE mobile architecture using the hottest 500 webpages. Our approach achieves 80% of the performance delivered by an ideal predictor. We obtain, on average, 45%, 63.5% and 81% improvement respectively for load time, energy consumption and the energy delay product, when compared to the Linux heterogeneous multi-processing scheduler.},
keywords={Internet;learning (artificial intelligence);Linux;microprocessor chips;mobile computing;multiprocessing systems;online front-ends;power aware computing;search engines;telecommunication power management;telecommunication scheduling;heterogeneous mobile platforms;machine learning;mobile users;battery life;heterogeneous multicore architecture;energy-efficient processing;operating system;web rendering engine;optimal processor configuration;energy consumption;mobile architecture;energy delay product;Linux heterogeneous multiprocessing scheduler;Web browsing;Web contents;energy efficiency;mobile Web browsers;Web workload characteristics;ARM big.LITTLE mobile architecture;Mobile communication;Rendering (computer graphics);Optimization;Energy consumption;Browsers;Measurement;Feature extraction;Mobile Web Browsing;Energy Optimisation;big.LITTLE;Mobile Workloads},
doi={10.1109/INFOCOM.2017.8057087},
ISSN={},
month={May},}
@INPROCEEDINGS{8057088,
author={Q. Xiao and Y. Zhou and S. Chen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Better with fewer bits: Improving the performance of cardinality estimation of large data streams},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Cardinality estimation is the task of determining the number of distinct elements (or the cardinality) in a data stream, under a stringent constraint that the input data stream can be scanned by just a single pass. This is a fundamental problem with many practical applications, such as traffic monitoring of high-speed networks and query optimization of Internetscale database. To solve the problem, we propose an algorithm named HLL-TailCut+, which implements the estimation standard error 1.0/√m using the memory units of three bits each, whose cost is much smaller than the five-bit memory units used by HyperLogLog, the best previously known cardinality estimator. This makes it possible to reduce the memory cost of HyperLogLog by 45%. For example, when the target estimation error is 1.1%, state-of-the-art HyperLogLog needs 5.6 kilobytes memory. By contrast, our new algorithm only needs 3 kilobytes memory consumption for attaining the same accuracy. Additionally, our algorithm is able to support the estimation of very large stream cardinalities, even on the Tera and Peta scale.},
keywords={data handling;storage management;cardinality estimation;stringent constraint;high-speed networks;query optimization;five-bit memory units;memory cost;target estimation error;stream cardinalities;data stream;memory consumption;estimation standard error;HyperLogLog;word length 3.0 bit;memory size 5.6 KByte;memory size 3.0 KByte;Registers;Estimation;Radiation detectors;Memory management;Google;Algorithm design and analysis;Monitoring},
doi={10.1109/INFOCOM.2017.8057088},
ISSN={},
month={May},}
@INPROCEEDINGS{8057089,
author={A. Furno and M. Fiore and R. Stanica},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Joint spatial and temporal classification of mobile traffic demands},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Mobile traffic data collected by network operators is a rich source of information about human habits, and its analysis provides insights relevant to many fields, including urbanism, transportation, sociology and networking. In this paper, we present an original approach to infer both spatial and temporal structures hidden in the mobile demand, via a first-time tailoring of Exploratory Factor Analysis (EFA) techniques to the context of mobile traffic datasets. Casting our approach to the time or space dimensions of such datasets allows solving different problems in mobile traffic analysis, i.e., network activity profiling and land use detection, respectively. Tests with real-world mobile traffic datasets show that, in both its variants above, the proposed approach (i) yields results whose quality matches or exceeds that of state-of-the-art solutions, and (ii) provides additional joint spatiotemporal knowledge that is critical to result interpretation.},
keywords={mobile computing;spatiotemporal phenomena;statistical analysis;telecommunication traffic;network activity;joint spatial and temporal classification;EFA techniques;real-world mobile traffic datasets;land use detection;mobile traffic analysis;Exploratory Factor Analysis techniques;first-time tailoring;mobile demand;temporal structures;spatial structures;original approach;mobile traffic data;mobile traffic demands;Mobile communication;Loading;Mobile computing;Maximum likelihood estimation;Sociology;Mathematical model;Conferences},
doi={10.1109/INFOCOM.2017.8057089},
ISSN={},
month={May},}
@INPROCEEDINGS{8057090,
author={J. Wang and J. Tang and Z. Xu and Y. Wang and G. Xue and X. Zhang and D. Yang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Spatiotemporal modeling and prediction in cellular networks: A big data enabled deep learning approach},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In this paper, we propose to leverage the emerging deep learning techniques for spatiotemporal modeling and prediction in cellular networks, based on big system data. First, we perform a preliminary analysis for a big dataset from China Mobile, and use traffic load as an example to show non-zero temporal autocorrelation and non-zero spatial correlation among neighboring Base Stations (BSs), which motivate us to discover both temporal and spatial dependencies in our study. Then we present a hybrid deep learning model for spatiotemporal prediction, which includes a novel autoencoder-based deep model for spatial modeling and Long Short-Term Memory units (LSTMs) for temporal modeling. The autoencoder-based model consists of a Global Stacked AutoEncoder (GSAE) and multiple Local SAEs (LSAEs), which can offer good representations for input data, reduced model size, and support for parallel and application-aware training. Moreover, we present a new algorithm for training the proposed spatial model. We conducted extensive experiments to evaluate the performance of the proposed model using the China Mobile dataset. The results show that the proposed deep model significantly improves prediction accuracy compared to two commonly used baseline methods, ARIMA and SVR. We also present some results to justify effectiveness of the autoencoder-based spatial model.},
keywords={cellular radio;learning (artificial intelligence);recurrent neural nets;telecommunication computing;LSAE;multiple local SAE;spatial modeling;spatiotemporal prediction;hybrid deep learning model;spatial dependencies;temporal dependencies;Base Stations;big system data;spatiotemporal modeling;deep learning approach;big data;cellular networks;China Mobile dataset;spatial model;Global Stacked AutoEncoder;temporal modeling;Long Short-Term Memory units;Correlation;Predictive models;Data models;Machine learning;Load modeling;Spatiotemporal phenomena;Mobile communication;Cellular Network;Big Data;Spatiotemporal Modeling;Deep Learning;Autoencoder;Recurrent Neural Network},
doi={10.1109/INFOCOM.2017.8057090},
ISSN={},
month={May},}
@INPROCEEDINGS{8057091,
author={N. Bartolini and T. He and H. Khamfroush},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Fundamental limits of failure identifiability by boolean network tomography},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Boolean network tomography is a powerful tool to infer the state (working/failed) of individual nodes from path-level measurements obtained by egde-nodes. We consider the problem of optimizing the capability of identifying network failures through the design of monitoring schemes. Finding an optimal solution is NP-hard and a large body of work has been devoted to heuristic approaches providing lower bounds. Unlike previous works, we provide upper bounds on the maximum number of identifiable nodes, given the number of monitoring paths and different constraints on the network topology, the routing scheme, and the maximum path length. The proposed upper bounds represent a fundamental limit on the identifiability of failures via Boolean network tomography. This analysis provides insights on how to design topologies and related monitoring schemes to achieve the maximum identifiability under various network settings. Through analysis and experiments we demonstrate the tightness of the bounds and efficacy of the design insights for engineered as well as real networks.},
keywords={Boolean functions;optimisation;telecommunication network topology;path-level measurements;monitoring schemes;NP-hard;failure identifiability;network settings;maximum identifiability;boolean network tomography;network topology;monitoring paths;identifiable nodes;network failures;egde-nodes;NP;Monitoring;Routing;Upper bound;Tomography;Testing;Encoding;Network topology},
doi={10.1109/INFOCOM.2017.8057091},
ISSN={},
month={May},}
@INPROCEEDINGS{8057092,
author={S. I. Nikolenko and K. Kogan and A. F. Anta},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Network simplification preserving bandwidth and routing capabilities},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We introduce structural transformations that allow simplifying a given network while preserving its original “bandwidth” and “routing” capabilities, transparently to specific allocations. We minimize a certain objective such as the aggregate capacity of network links, number of nodes, or number of links, in such a way that all the bandwidth that could be routed in the original network can also be routed in the reduced one. This improves cost-efficiency for both inter- and intra-datacenter connections and simplifies network management. We also identify a fundamental tradeoff between extra added capacity and simplicity of representation for a given network. Our analytic results are supported by extensive simulation results on hundreds of real network topologies. One result is that by adding 10-30% extra capacity to evaluated real-world networks one can simplify them down to a star topology with a single switch, while all routing and bandwidth allocation decisions on the simplified topology can be mapped back to the original network. This is an important step towards simplifying network management via a reduced virtualized network infrastructure.},
keywords={bandwidth allocation;computer centres;computer networks;quality of service;telecommunication network management;telecommunication network routing;telecommunication network topology;telecommunication traffic;extra added capacity;given network;network topologies;real-world networks;network management;reduced virtualized network infrastructure;network simplification preserving bandwidth;routing capabilities;structural transformations;original bandwidth;aggregate capacity;network links;intra-datacenter connections;extensive simulation;Bandwidth;Channel allocation;Routing;Network topology;Capacity planning;Topology;Conferences},
doi={10.1109/INFOCOM.2017.8057092},
ISSN={},
month={May},}
@INPROCEEDINGS{8057093,
author={J. Doncel and S. Aalto and U. Ayesta},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Economies of scale in parallel-server systems},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We consider a parallel-server system with K homogeneous servers where incoming tasks, arriving at rate λ, are dispatched by n dispatchers. Servers are FCFS queues and dispatchers implement a size-based policy such that the servers are equally loaded. We compare the performance of a system with n> 1 dispatchers and of a system with a single dispatcher. Every dispatcher handles a fraction 1/n of the incoming traffic and balances the load to K/n servers. We show that the performance of a system with n dispatchers, K servers and arrival rate λ coincides with that of a system with one dispatcher, K/n servers and arrival rate λ/n. Therefore, the performance comparison can be interpreted as the economies of scale in a system with one dispatcher when we scale up the number of servers and the arrival rate proportionately. We consider two continuous service time distributions: uniform and Bounded Pareto that have increasing and decreasing failure rates, respectively; and a discrete distribution with two values, which is the distribution that maximizes the variance for a given mean. We show that the performance degradation is small for uniformly distributed job sizes, but that for Bounded Pareto and two points distributions it can be unbounded.},
keywords={Pareto distribution;queueing theory;resource allocation;telecommunication traffic;parallel-server system;K homogeneous servers;n dispatchers;single dispatcher;incoming traffic;load balancing;FCFS queues;size-based policy;continuous service time distributions;uniform distribution;bounded Pareto distribution;failure rates;discrete distribution;variance maximization;Servers;Degradation;Routing;Economies of scale;Conferences;Time factors},
doi={10.1109/INFOCOM.2017.8057093},
ISSN={},
month={May},}
@INPROCEEDINGS{8057094,
author={X. Fu and Z. Xu and Q. Peng and L. Fu and X. Wang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Complexity vs. optimality: Unraveling source-destination connection in uncertain graphs},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Determination of source-destination connectivity in networks has long been a fundamental problem, where most existing works are based on deterministic graphs that overlook the inherent uncertainty in network links. To overcome such limitation, this paper models the network as an uncertain graph where each edge e exists independently with some probability p(e). The problem examined is that of determining whether a given pair of nodes, a source s and a destination t, are connected by a path or separated by a cut. Assuming that during each determining process we are associated with an underlying graph, the existence of each edge can be unraveled through edge testing at a cost of c(e). Our goal is to find an optimal strategy incurring the minimum expected testing cost with the expectation taken over all possible underlying graphs that form a product distribution. Formulating it into a combinatorial optimization problem, we first characterize the computational complexity of optimally determining source-destination connectivity in uncertain graphs. Specifically, through proving the NP-hardness of two closely related problems, we show that, contrary to its counterpart in deterministic graphs, this problem cannot be solved in polynomial time unless P=NP. Driven by the necessity of designing an exact algorithm, we then apply the Markov Decision Process framework to give a dynamic programming algorithm that derives the optimal strategies. As the exact algorithm may have prohibitive time complexity in practical situations, we further propose two more efficient approximation schemes compromising the optimality. The first one is a simple greedy approach with linear approximation ratio. Interestingly, we show that naive as it is, it has comparable performance than some other seemingly more sophisticated algorithms. Second, by harnessing the sub-modularity of the problem, we further design a more elaborate algorithm with better approximation ratio. The effectiveness of the proposed algorithms are justified through extensive simulations on three real network datasets, from which we demonstrate that the proposed algorithms yield strategies with smaller expected cost than conventional heuristics.},
keywords={approximation theory;computational complexity;dynamic programming;graph theory;greedy algorithms;Markov processes;optimisation;probability;uncertain graph;source-destination connectivity;deterministic graphs;network links;edge testing;combinatorial optimization problem;computational complexity;exact algorithm;Markov Decision Process framework;dynamic programming algorithm;prohibitive time complexity;network datasets;smaller expected cost;source-destination connection;linear approximation ratio;approximation scheme;Approximation algorithms;Testing;Heuristic algorithms;Algorithm design and analysis;Markov processes;Dynamic programming;Reliability},
doi={10.1109/INFOCOM.2017.8057094},
ISSN={},
month={May},}
@INPROCEEDINGS{8057095,
author={K. Chen and G. Tan},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={SatProbe: Low-energy and fast indoor/outdoor detection based on raw GPS processing},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Indoor-outdoor (IO) detection provides very useful hints for a mobile device to perform context-aware services. To that end, GPS presents a viable solution by relating a device's IO status with its positioning performance, which depends on the device's exposure to the open sky. This approach, however, is prohibitively expensive in terms of energy consumption and response time. Recent work has thus been focused on exploiting low-energy sensors such as light, cellular, and magnetic sensors to infer the IO status indirectly, at the cost of reduced adaptability or explicit user involvement. In this paper, we propose an improving solution to this problem. Our method, called SatProbe, reverts to the GPS approach for its directness and robustness, but avoids its drawback by extracting only the number of visible satellites from the raw GPS data, instead of going through extensive computation to obtain a final position. This metric provides a clear indicator of the IO status, yet can be obtained with great efficiency. Experiments on 79 raw GPS traces with 2595 detection points across a variety of environments show that SatProbe produces higher detection accuracy than previous solutions, with more than an order of magnitude reductions in energy consumption and detection time.},
keywords={Global Positioning System;magnetic sensors;mobile computing;ubiquitous computing;fast indoor/outdoor detection;raw GPS processing;indoor-outdoor detection;mobile device;context-aware services;IO status;SatProbe;Global Positioning System;Satellites;Receivers;Magnetic sensors;Correlation;Multiaccess communication},
doi={10.1109/INFOCOM.2017.8057095},
ISSN={},
month={May},}
@INPROCEEDINGS{8057096,
author={J. Wang and N. Tan and J. Luo and S. J. Pan},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={WOLoc: WiFi-only outdoor localization using crowdsensed hotspot labels},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Given the ever-expanding scale of WiFi deployments in metropolitan areas, we have reached the point where accurate GPS-free outdoor localization becomes possible by relying solely on the WiFi infrastructure. Nevertheless, the existing industrial practices do not seem to have the right implementation to achieve an adequate accuracy, while the academic researches that are mostly attracted by indoor localization have largely neglected this outdoor aspect. In this paper, we propose WOLoc (WiFi-only Outdoor Localization) as a solution that offers meter-level accuracy, by holistically treating the large number of WiFi hotspot labels gather by crowdsensing. On one hand, we do not take these labels as fingerprints as it is almost impossible to extend indoor localization mechanisms by fingerprinting metropolitan areas. On the other hand, we avoid the over-simplified local synthesis methods (e.g., centroid) that significantly lose the information contained in the labels. Instead, we accommodate all the labeled and unlabeled data for a given area using a semi-supervised manifold learning technique, and the output concerning the unlabeled part will become the estimated locations for both users and WiFi hotspots. We conduct extensive experiments with WOLoc in several outdoor areas, and the results have strongly indicated the efficacy of our solution.},
keywords={Global Positioning System;learning (artificial intelligence);wireless LAN;WOLoc;WiFi-only outdoor localization;crowdsensed hotspot labels;WiFi deployments;metropolitan areas;WiFi infrastructure;WiFi-only Outdoor Localization;meter-level accuracy;WiFi hotspot labels;indoor localization mechanisms;GPS-free outdoor localization;Wireless fidelity;Urban areas;Manifolds;Conferences;Global Positioning System;Estimation;Roads},
doi={10.1109/INFOCOM.2017.8057096},
ISSN={},
month={May},}
@INPROCEEDINGS{8057097,
author={R. Margolies and R. Becker and S. Byers and S. Deb and R. Jana and S. Urbanek and C. Volinsky},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Can you find me now? Evaluation of network-based localization in a 4G LTE network},
year={2017},
volume={},
number={},
pages={1-9},
abstract={User location is of critical importance to cellular network operators. It is often used for network capacity planning and to aid in the analysis of service and network diagnostics. However, existing localization techniques rely on user-provided information (e.g., Angle-of-Arrival), which are not available to the operator, and often require a significant effort to collect training data. Our main contribution is the design and evaluation of the Network-Based Localization (NBL) System for localizing a user in a 4G LTE network. The NBL System consists of 2 stages. In an offline stage, we develop RF coverage maps based on a large-scale crowd-sourced channel measurement campaign. Then, in an online stage, we present a localization algorithm to quickly match RF measurements (which are already collected as part of normal network operation) to coverage map locations. The system is more practical than related works, as it does not make any assumptions about user mobility, nor does it require expensive manual training measurements. Despite the realistic assumptions, our extensive evaluations in a national 4G LTE network show that the NBL System achieves a localization accuracy which is comparable to related works (i.e., a median accuracy of 5% of the cell's coverage region).},
keywords={4G mobile communication;cellular radio;Long Term Evolution;mobility management (mobile radio);telecommunication network planning;wireless channels;cellular network operators;RF coverage maps;RF measurements;user mobility;localization techniques;4G LTE network;network capacity planning;network-based localization system;crowd-sourced channel measurement;Radio frequency;Long Term Evolution;Training;Cellular networks;Training data;Area measurement;Localization;wireless networks;crowd-sourcing},
doi={10.1109/INFOCOM.2017.8057097},
ISSN={},
month={May},}
@INPROCEEDINGS{8057098,
author={M. Fida and A. Lutu and M. K. Marina and Ö. Alay},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={ZipWeave: Towards efficient and reliable measurement based mobile coverage maps},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The accuracy of measurement-driven mobile coverage maps depends on the quality, density and pattern of the signal strength observations. Thus, identifying an efficient measurement data collection methodology is essential, especially when considering the cost associated with the measurement collection approaches (e.g., drive tests, crowd approaches). We propose ZipWeave, a novel measurement data collection and fusion framework for building efficient and reliable measurement-based mobile coverage maps. ZipWeave incorporates a novel nonuniform sampling strategy to achieve reliable coverage maps with reduced sample size. Assuming prior knowledge of the propagation characteristics of the region of interest, we first examine the potential gains of this non-uniform sampling strategy in different cases via a measurement-based statistical analysis methodology; this involves irregular spatial tessellation of the region of interest into sub-regions with internally similar radio propagation characteristics and sampling based on these sub-regions. We then present a practical form of ZipWeave nonuniform sampling strategy that can be used even without any prior information. In all our evaluations, we show that the ZipWeave non-uniform sampling approach reduces the samples by half compared to the common systematic-random sampling, while maintaining similar accuracy. Moreover, we show that the other key feature of ZipWeave to combine high-quality controlled measurements (that present limited geographic footprint similar to drive tests) with crowdsourced measurements (that cover a wider footprint) leads to more reliable mobile coverage maps overall.},
keywords={mobile radio;quality control;radiowave propagation;sampling methods;signal sampling;statistical analysis;reliable measurement;measurement-driven mobile coverage maps;measurement data collection;radio propagation characteristics;measurement data collection methodology;nonuniform sampling strategy;reliable mobile coverage maps;crowdsourced measurements;high-quality controlled measurements;systematic-random sampling;ZipWeave nonuniform sampling strategy;statistical analysis methodology;reduced sample size;fusion framework;crowd approaches;drive tests;measurement collection approaches;signal strength observations;Reliability;Mobile communication;Atmospheric measurements;Battery charge measurement;Particle measurements;Monitoring;Urban areas},
doi={10.1109/INFOCOM.2017.8057098},
ISSN={},
month={May},}
@INPROCEEDINGS{8057099,
author={J. Tan and C. Nguyen and X. Wang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={SilentTalk: Lip reading through ultrasonic sensing on mobile phones},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The recently enhanced computing capability and rich sensing functionality on mobile devices lead to the ubiquitous application of speech recognition. Traditional speech recognition records acoustic signals or visual images to interpret speech. However, the acoustic based scheme has many drawbacks. It is easily affected by the environmental noise when users are in the factory or market, and can not be used in a place where people need to be quite such as library. Specifically the current design is not suitable for people with speaking or hearing difficulties. Unfortunately, the visual-based approach is sensitive to fight conditions which shows poor performance in the dark area. As a result, it is necessary to provide an new human-computer interaction channel to assist speech recognition. This paper presents SilentTalk, a non-invasive lip reading system based on ultrasonic Doppler effect The main idea is to generate ultrasonic signals from a mobile phone, then capture the reflections and analyze the fine-grained frequency shift caused by mouth movements. A Frequency Shift Detection Model (FSDM) is proposed to quantify the correlation between frequency variations and mouth movements that form different syllables. SilentTalk then applies a Continuous Lip Reading Model (CLRM) on top of FSDM to realize continuous lip reading. Based on Markov assumption, CLRM effectively combines pronunciation rules and context knowledge to connect isolated syllables to words and sentences. Experiments show that SilentTalk can identify 12 basic mouth motions up to 95.4% accuracy in English. The system can also recognize short sentences up to six words with an average accuracy of 74.8%.},
keywords={Doppler effect;feature extraction;mobile handsets;speech recognition;SilentTalk;ultrasonic sensing;mobile phone;rich sensing functionality;mobile devices;ubiquitous application;visual images;acoustic based scheme;environmental noise;speaking hearing difficulties;visual-based approach;human-computer interaction channel;noninvasive lip reading system;ultrasonic signals;fine-grained frequency shift;mouth movements;Frequency Shift Detection Model;FSDM;frequency variations;Continuous Lip Reading Model;speech recognition;ultrasonic Doppler effect;Markov assumption;Conferences},
doi={10.1109/INFOCOM.2017.8057099},
ISSN={},
month={May},}
@INPROCEEDINGS{8057100,
author={Y. Wei and H. Wu and H. Wang and H. Tsai and K. C. Lin and R. Boubezari and H. Le Minh and Z. Ghassemlooy},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={LiCompass: Extracting orientation from polarized light},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Accurate orientation information is the key in many applications, ranging from map reconstruction with crowdsourcing data, location data analytics, to accurate indoor localization. Many existing solutions rely on noisy magnetic and inertial sensor data, leading to limited accuracy, while others leverage multiple, dense anchor points to improve the accuracy, requiring significant deployment efforts. This paper presents LiCompass, the first system that enables a commodity camera to accurately estimate the object orientation using just a single optical anchor. Our key idea is to allow a camera to observe varying intensity level of polarized light when it is in different orientations and, hence, perform estimation directly from image pixel intensity. As the estimation relies only on pixel intensity, instead of the location of the anchor in an image, the system performs reliably at long distance, with low resolution images, and with large perspective distortion. LiCompass' core designs include an elaborate optical anchor design and a series of signal processing techniques based on trigonometric properties, which extend the range of orientation estimation to full 360 degrees. Our prototype evaluation shows that LiCompass produces very accurate estimates with median errors of merely 2.5 degrees at 5 meters and 7.4 degrees at 2.5 meters with an irradiance angle of 55 degrees.},
keywords={cameras;image resolution;image sensors;light polarisation;navigation;accurate indoor localization;noisy magnetic sensor data;commodity camera;object orientation;single optical anchor;varying intensity level;polarized light;image pixel intensity;low resolution images;orientation estimation;LiCompass core design;optical anchor design;Cameras;Estimation;Optical polarization;Adaptive optics;Optical imaging;Optical distortion;Optical receivers},
doi={10.1109/INFOCOM.2017.8057100},
ISSN={},
month={May},}
@INPROCEEDINGS{8057101,
author={H. Chen and F. Li and Y. Wang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={EchoTrack: Acoustic device-free hand tracking on smart phones},
year={2017},
volume={},
number={},
pages={1-9},
abstract={This paper explores the limits of acoustic ranging on smart phone in the scenario of device-free hand tracking. Tracking the hand is challenging since it requires continuously locating the moving hand in the air with fine resolution. Existing work on hand tracking relies on special hardware or requires users hold the mobile device. This paper presents EchoTrack, which continuously locates the hand by leveraging mobile audio hardware advances without special infrastructure supported. EchoTrack measures the distance from the hand to the speaker array embedded in smart phone via the chirp's Time of Flight (TOF). The speaker array and hand yield a unique triangle. The hand can be located with this triangular geometry. The trajectory accuracy can be improved with the method of Doppler shift compensation and trajectory correction (i.e., roughness penalty smoothing method). We implement a prototype on smart phone and the evaluation shows that EchoTrack can achieve tracking accuracy within about three centimeters of 76% and two centimeters of 48%.},
keywords={Doppler shift;gesture recognition;human computer interaction;smart phones;tracking accuracy;EchoTrack;Acoustic device-free hand tracking;smart phone;acoustic ranging;moving hand;mobile device;speaker array;mobile audio hardware;Doppler shift compensation;Chirp;Microphones;Smart phones;Distance measurement;Trajectory;Hardware;Doppler shift},
doi={10.1109/INFOCOM.2017.8057101},
ISSN={},
month={May},}
@INPROCEEDINGS{8057102,
author={H. Xu and Z. Yang and Z. Zhou and K. Yi and C. Peng},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={TUM: Towards ubiquitous multi-device localization for cross-device interaction},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Cross-device interaction is becoming an increasingly hot topic as we often have multiple devices at our immediate disposal in this era of mobile computing. Various cross-device applications such as file sharing, multi-screen display, and cross-device authentication have been proposed and investigated. However, one of the most fundamental enablers remains unsolved: How to achieve ubiquitous multi-device localization? Though pioneer efforts have resorted to gesture-assisted or sensing-assisted localization, they either require extensive user participation or impose some strong assumptions on device sensing abilities. This introduces extra costs and constraints, and thus degrades their practicality. To overcome these limitations, we propose TUM, an acoustic-assisted localization scheme Towards Ubiquitous Multi-device localization. The basic idea of TUM is to utilize the dual-microphones and speakers to obtain distance cues among devices. At the same time it resolves the location ambiguity with the help of MEMS sensors. We devise techniques for distance constraint extraction, static localization, continuous localization, and multi-device localization, and build a prototype that runs on commodity devices. Extensive experiments show that TUM provides a real-time 3D relative localization service under 10cm mean error for both static and continuous localization.},
keywords={array signal processing;microphones;mobile computing;peer-to-peer computing;sensors;ubiquitous computing;wireless sensor networks;TUM;ubiquitous multidevice localization;cross-device interaction;cross-device authentication;device sensing abilities;continuous localization;acoustic-assisted localization scheme;dual-microphones;MEMS sensors;distance constraint extraction;static localization;multidevice localization;static localization;Microphones;Acoustics;Sensors;Smart devices;Distance measurement;Three-dimensional displays;Clocks},
doi={10.1109/INFOCOM.2017.8057102},
ISSN={},
month={May},}
@INPROCEEDINGS{8057103,
author={W. Jiang and J. Wu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Active opinion-formation in online social networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Recommendation systems usually try to “guess” a user's preferences from the system's view. We study another side of recommendation: active opinion-formation from the perspective of the user. In real life, a user's opinion evolves with time and refines when new evidence occurs. Then, how does an online user form his/her own opinion actively in large social networks? The problem has three challenges: the factor, the effect and the open environment. To address those challenges, we investigate: (1) what factors or channels a user will consider, (2) how those channels will take effect, and (3) an incremental approach to incorporate multiple channels. We explore three types of channels: the internal opinion of an individual user, influences from trusted friends, and influences from public channels. A novel simulator, OpinionFormer, is proposed to incorporate those channels incrementally. It differentiates the effects of friends and public channels as well as positive and negative opinions. We validate the performance of OpinionFormer by predicting users' opinions using real-world data sets. Experimental results show that our model can improve accuracy over other models that ignore some channels or that neglect the evolving features.},
keywords={recommender systems;social networking (online);internal opinion;individual user;public channels;positive opinions;negative opinions;active opinion-formation;online social networks;recommendation systems;online user;OpinionFormer simulator;friends;user preferences;Conferences;Social network services;Containers;Open systems;Temperature measurement;Frequency modulation;opinion formation;internal opinion;trusted friend;public channel;fluid dynamics},
doi={10.1109/INFOCOM.2017.8057103},
ISSN={},
month={May},}
@INPROCEEDINGS{8057104,
author={A. Zhiyuli and X. Liang and Z. Xu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Learning distributed representations for large-scale dynamic social networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Learning distributed representations of symbolic data were introduced by Hinton[1], and first developed in modeling networks for learning the node vectors by Perozzi et al (2014). In this work, we proposed Dnps, a novel nodes embedding approach for acquiring distributed representations of large-scale dynamic social networks. Dnps is suitable for many types of social networks: dynamic/static, directed/undirected, and weighted/unweighted. Recently, several works of nodes embedding were proposed. However, they were designed for static networks, such as language networks. To address this problem, first, we develop a damping based positive sampling (DpS) algorithm to learn the hierarchical structure of social networks. Then, we devise a local search based DpS algorithm to obtain incremental information of network evolution. Finally, we show Dnps's potentials on future link prediction task for three real-life large-scale dynamic social networks. The results show that Dnps consistently outperforms all baseline methods and exhibits an improvement of 12%, 6%, 4% on Digg, Flickr and YouTube over the second-highest level, respectively. Moreover, Dnps is also scalable. For example, Dnps can speed up the training process in 2 ~ 36 times compared with benchmarks on Flickr network. The source codes of the project is available online<sup>1</sup>.},
keywords={computational complexity;graph theory;learning (artificial intelligence);search problems;social networking (online);speech processing;statistical distributions;network evolution;large-scale dynamic social networks;Dnps;Flickr network;distributed representations;static networks;language networks;node embedding;symbolic data;node vectors;damping based positive sampling algorithm;DpS algorithm;Digg;YouTube;Social network services;Damping;Heuristic algorithms;Training;Algorithm design and analysis;Prediction algorithms;Conferences},
doi={10.1109/INFOCOM.2017.8057104},
ISSN={},
month={May},}
@INPROCEEDINGS{8057105,
author={W. Chen and C. G. Brinton and D. Cao and M. Chiang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Behavior in social learning networks: Early detection for online short-courses},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We study learning outcome prediction for online courses. Whereas prior work has focused on semester-long courses with frequent student assessments, we focus on short-courses that have single outcomes assigned by instructors at the end. The lack of performance data makes the behavior of learners, captured as they interact with course content and with one another in Social Learning Networks (SLN), essential for prediction. Our method defines several (machine) learning features based on behaviors collected on the modes of (human) learning in a course, and uses them in appropriate classifiers. Through evaluation on data captured from three two-week courses hosted through our delivery platforms, we make three key observations: (i) behavioral data is predictive of learning outcomes in short-courses (our classifiers achieving AUCs ≥ 0.8 after the two weeks), (ii) it has an early detection capability (AUCs ≥ 0.7 with the first week of data), and (iii) the content features have an “earliest” detection capability (with higher AUC in the first few days), while the SLN features become the more predictive set over time, as the network matures. We also discuss how our method can generate behavioral analytics for instructors.},
keywords={computer aided instruction;distance learning;educational courses;learning (artificial intelligence);pattern classification;social networking (online);social learning networks;outcome prediction;semester-long courses;performance data;course content;two-week courses;behavioral data;early detection capability;behavioral analytics;student assessment;online short-courses;Feature extraction;Training;Programmable logic arrays;Data models;Computational modeling;Conferences;Discussion forums},
doi={10.1109/INFOCOM.2017.8057105},
ISSN={},
month={May},}
@INPROCEEDINGS{8057106,
author={G. Liu and Q. Chen and Q. Yang and B. Zhu and H. Wang and W. Wang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={OpinionWalk: An efficient solution to massive trust assessment in online social networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Massive trust assessment (MTA) in an Online Social Network (OSN), i.e., computing the trustworthiness of all users in the network, is crucial in various OSN-related applications. Existing solutions are either too slow or inaccurate in addressing the MTA problem. We propose the OpinionWalk algorithm that accurately and efficiently conducts MTA in an OSN. OpinionWalk models trust by the Dirichlet distribution and uses a matrix to represent the direct trust relations among users. From the perspective of a user, other users' trustworthiness are stored in a column vector that is iteratively updated when the algorithm “walks” through the network, in a breadth-first search manner. We identify the overlapping subproblems property in MTA and prove OpinionWalk is a more efficient solution. The accuracy and execution time of OpinionWalk are evaluated and compared to benchmark algorithms including EigenTrust, TrustRank, MoleTrust, TidalTrust and AssessTrust, using two real-world datasets (Advogato and Pretty Good Privacy). Experimental results indicate that OpinionWalk is an efficient and accurate solution to MTA, compared to previous algorithms.},
keywords={data privacy;security of data;social networking (online);trusted computing;OpinionWalk models trust;direct trust relations;benchmark algorithms;OSN;Pretty Good Privacy;Advogato;AssessTrust;TidalTrust;MoleTrust;TrustRank;EigenTrust;breadth-first search;column vector;Dirichlet distribution;OpinionWalk algorithm;MTA problem;OSN-related applications;online social networks;massive trust assessment;Computational modeling;Social network services;Time complexity;Statistical distributions;Network topology;Algorithm design and analysis;Conferences;Massive trust assessments;online social networks;threes-valued subjective logic;trust model},
doi={10.1109/INFOCOM.2017.8057106},
ISSN={},
month={May},}
@INPROCEEDINGS{8057107,
author={Z. Yin and W. Jiang and S. M. Kim and T. He},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={C-Morse: Cross-technology communication with transparent Morse coding},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Recent research on CTC (cross-technology communication) demonstrates the viability of direct coordination among heterogeneous devices (e.g., WiFi and ZigBee) with incompatible physical layers. Although encouraging, current solutions suffer from either severe inefficiency in channel utilization or low throughput using limited beacons. To address these limitations, this paper presents C-Morse, which leverages all traffic (such as through data packets, beacons and other control frames) to achieve a high cross-technology communication throughput. The key idea of C-Morse is to slightly perturb the transmission timing of existing WiFi packets to construct recognizable radio energy patterns without introducing noticeable delays to upper layers. At the receiver side, ZigBee captures such patterns by sensing the RSSI value, and then decodes the transmitted symbols. C-Morse also introduces a novel timing-based multiplexing technique to allow the coexistence of multiple C-Morse access points and reject other interference, showing a reliable symbol delivery ratio. As a result, C-Morse achieves a free side-channel, whose CTC throughput is as much as 9 χ of the present state of the art, while maintaining the through traffic within a negligible delay that goes unnoticed by applications and end-users.},
keywords={channel coding;decoding;interference suppression;multiplexing;radiofrequency interference;telecommunication traffic;wireless channels;wireless LAN;wireless sensor networks;Zigbee;recognizable radio energy patterns;multiple C-Morse access points;transparent Morse coding;heterogeneous devices;channel utilization;data packets;CTC;cross-technology communication;WiFi packets;ZigBee;telecommunication traffic;radio energy pattern;RSSI;decoding;timing-based multiplexing technique;interference;symbol delivery ratio;free side-channel;Wireless fidelity;ZigBee;Delays;Wireless communication;Throughput;Wireless sensor networks},
doi={10.1109/INFOCOM.2017.8057107},
ISSN={},
month={May},}
@INPROCEEDINGS{8057108,
author={X. Guo and X. Zheng and Y. He},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={WiZig: Cross-technology energy communication over a noisy channel},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The proliferation of loT applications drives the need of ubiquitous connections among heterogeneous wireless devices. Cross-Technology Communication (CTC) is a significant technique to directly exchange information among heterogeneous devices that follow different standards. By exploiting a side-channel like frequency, amplitude or temporal modulation, the existing works enable CTC but have limited performance under channel noise. In this paper, we propose WiZig, a novel CTC technique that employs modulation techniques in both the amplitude and temporal dimensions to optimize the throughput over a noisy channel. We establish a theoretical model of the energy communication channel to clearly understand the channel capacity. We then devise an online rate adaptation algorithm to adjust the modulation strategy according to the channel condition. Based on the theoretical model, WiZig can accurately control the number of encoded energy amplitudes and the length of a receiving window, so as to optimize the CTC throughput. We implement a prototype of WiZig on a software radio platform and a commercial ZigBee device. The evaluation show that WiZig achieves a throughput of 153.85 bps with less than 1 % symbol error rate in a real environment. The results demonstrate that WiZig realizes efficient and reliable CTC under varied channel conditions.},
keywords={channel capacity;error statistics;modulation;software radio;telecommunication power management;wireless channels;Zigbee;heterogeneous devices;temporal modulation;channel noise;WiZig;novel CTC technique;modulation techniques;temporal dimensions;noisy channel;theoretical model;energy communication channel;channel capacity;online rate adaptation algorithm;modulation strategy;channel condition;encoded energy amplitudes;CTC throughput;commercial ZigBee device;varied channel conditions;Cross-technology energy communication;ubiquitous connections;heterogeneous wireless devices;loT applications;symbol error rate;Receivers;Wireless communication;Throughput;Error analysis;Noise measurement;Wireless sensor networks;Modulation},
doi={10.1109/INFOCOM.2017.8057108},
ISSN={},
month={May},}
@INPROCEEDINGS{8057109,
author={Z. Chi and Z. Huang and Y. Yao and T. Xie and H. Sun and T. Zhu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={EMF: Embedding multiple flows of information in existing traffic for concurrent communication among heterogeneous IoT devices},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The exponentially increasing number of IoT devices makes the unlicensed industrial, scientific, and medical (ISM) radio bands (e.g., 2.4 GHz) extremely crowded. Currently, there is no efficient solution to coordinate the large amount heterogeneous IoT devices that have different communication technologies (e.g., WiFi and ZigBee). To fill this gap, in this paper, we introduce embedded multiple flows (EMF) communication method, which (i) embeds different pieces of information in existing traffic and (ii)concurrently sends out these information from one IoT sender to multiple IoT receivers that have a different communication technology from the sender. By doing this, our EMF method (i) enables cross-technology communication among heterogeneous IoT devices, (ii) does not introduce any extra control traffic, and (iii) is transparent to the higher layer applications. Our approach is implemented on USRPs and commercial off-the-shelf (COTS) ZigBee devices. We also conducted extensive experiments to evaluate our approach in real-world settings. The evaluation results show that EMF's throughput is more than 14 times higher than the latest cross-technology communication technique (i.e. FreeBee[1]).},
keywords={embedded systems;Internet of Things;Zigbee;IoT sender;multiple IoT receivers;EMF method;Embedding multiple flows;concurrent communication;heterogeneous IoT devices;embedded multiple flows communication method;USRP;commercial off-the-shelf ZigBee devices;cross-technology communication technique;unlicensed industrial scientific and medical radio bands;ISM bands;frequency 2.4 GHz;Wireless fidelity;ZigBee;Receivers;Modulation;Throughput;Bit error rate;Logic gates},
doi={10.1109/INFOCOM.2017.8057109},
ISSN={},
month={May},}
@INPROCEEDINGS{8057110,
author={T. Osuki and K. Sakai and S. Fukumoto},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Contact avoidance routing in delay tolerant networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Delay tolerant networks (DTNs) are widely adopted to many network applications, such as disaster recovery and battlefield communications. Such critical network scenarios call for an outright prevention mechanism against contact-based attacks, e.g., blackmailing a legitimate user to compromise sensitive information at a contact. To the best of our knowledge, there is no work on secure routing protocol against contact-based attacks in DTNs. Therefore, in this paper, we first formulate the problem of contact avoidance routing, in which the node holding a message tries to avoid having a contact with an adversary. By applying the phase-type distribution, we build the secure opportunistic path model, which integrates the delivery probability within the deadline and the safety of opportunistic paths. Then, we propose a contact avoidance routing (CAR) protocol to securely deliver a message to its destination against the contact-based compromise attack. In addition, we further propose an adaptive CAR (A-CAR) to accommodate complicated network scenarios, where the capabilities of adversaries are parameterized. The extensive simulations using real traces as well as random graphs demonstrate that the proposed CAR and A-CAR protocols achieve their design goals.},
keywords={computer network security;delay tolerant networks;probability;routing protocols;delay tolerant networks;DTNs;network applications;critical network scenarios;outright prevention mechanism;secure routing protocol;secure opportunistic path model;contact avoidance routing protocol;phase-type distribution;contact-based attacks;Routing;Routing protocols;Automobiles;Security;Delays;Adaptation models;Contact avoidance routing;network security;delay tolerant networks;DTNs},
doi={10.1109/INFOCOM.2017.8057110},
ISSN={},
month={May},}
@INPROCEEDINGS{8057111,
author={H. Jin and L. Su and K. Nahrstedt},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={CENTURION: Incentivizing multi-requester mobile crowd sensing},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The recent proliferation of increasingly capable mobile devices has given rise to mobile crowd sensing (MCS) systems that outsource the collection of sensory data to a crowd of participating workers that carry various mobile devices. Aware of the paramount importance of effectively incentivizing participation in such systems, the research community has proposed a wide variety of incentive mechanisms. However, different from most of these existing mechanisms which assume the existence of only one data requester, we consider MCS systems with multiple data requesters, which are actually more common in practice. Specifically, our incentive mechanism is based on double auction, and is able to stimulate the participation of both data requesters and workers. In real practice, the incentive mechanism is typically not an isolated module, but interacts with the data aggregation mechanism that aggregates workers' data. For this reason, we propose CENTURION, a novel integrated framework for multi-requester MCS systems, consisting of the aforementioned incentive and data aggregation mechanism. CENTURION's incentive mechanism satisfies truthfulness, individual rationality, computational efficiency, as well as guaranteeing non-negative social welfare, and its data aggregation mechanism generates highly accurate aggregated results. The desirable properties of CENTURION are validated through both theoretical analysis and extensive simulations.},
keywords={data aggregation;mobile computing;sensor fusion;mobile crowd sensing systems;data aggregation mechanism;multirequester MCS systems;mobile devices;CENTURION incentive mechanism;multirequester mobile crowd sensing systems;sensory data collection;Sensors;Reliability;Data aggregation;Mobile communication;Conferences;Mobile handsets},
doi={10.1109/INFOCOM.2017.8057111},
ISSN={},
month={May},}
@INPROCEEDINGS{8057112,
author={C. Huang and D. Wang and S. Zhu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Where are you from: Home location profiling of crowd sensors from noisy and sparse crowdsourcing data},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Crowdsourcing has emerged as an important data collection paradigm in participatory and human-centric sensing applications. While many crowdsourcing studies focus on sensing and recovering the status of the physical world, this paper investigates the problem of profiling the crowd sensors (i.e., humans). In particular, we study the problem of accurately inferring the home locations of people from the noisy and sparse crowdsourcing data they contribute. In this study, we propose a semi-supervised framework, Where Are You From (WAYF), to accurately infer the home locations of people by explicitly exploring the localness of people and the dependency between people based on their check-in behaviors under a rigorous analytical framework. We perform extensive experiments to evaluate the performance of our scheme and compared it to the state-of-the-art techniques using three real world data traces collected from Foursquare. The results showed the effectiveness of our scheme in accurately profiling the home locations of people.},
keywords={mobile computing;sensor fusion;social networking (online);crowd sensors;home locations;noisy crowdsourcing data;sparse crowdsourcing data;Home location profiling;participatory;human-centric sensing;data collection;where are you from;WAYF;Foursquare;Urban areas;Crowdsourcing;Sensors;Social network services;Manganese;Noise measurement;Conferences;Home Location Profiling;Crowdsourcing;Location Based Social Networks (LBSN)},
doi={10.1109/INFOCOM.2017.8057112},
ISSN={},
month={May},}
@INPROCEEDINGS{8057113,
author={A. Chakraborty and M. S. Rahman and H. Gupta and S. R. Das},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={SpecSense: Crowdsensing for efficient querying of spectrum occupancy},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We describe an end-to-end platform called SpecSense to support large scale spectrum monitoring. SpecSense crowdsources spectrum monitoring to low-cost, low-power commodity SDR/embedded platforms and provides necessary analytics support in a central spectrum server. In this work, we describe SpecSense and address specific challenges related to accurately estimate spectrum occupancy on demand with low overhead. To address the accuracy question, we augment state-of-the-art spatial interpolation techniques to accommodate scenarios where RF propagation characteristics change across space. To address the overhead question, we solve the sensor selection problem to select the minimum number of spectrum sensors that can best estimate the spectrum at the requested locations.},
keywords={cognitive radio;crowdsourcing;embedded systems;interpolation;query processing;radio spectrum management;software radio;telecommunication computing;efficient querying;end-to-end platform;central spectrum server;spectrum sensors;spectrum occupancy estimation;spectrum monitoring;RF propagation characteristics;SpecSense;low-cost low-power commodity SDR;embedded platforms;spatial interpolation techniques;sensor selection problem;Interpolation;Radio frequency;Monitoring;RF signals;Crowdsourcing;Sensors;Conferences},
doi={10.1109/INFOCOM.2017.8057113},
ISSN={},
month={May},}
@INPROCEEDINGS{8057114,
author={C. Miao and L. Su and W. Jiang and Y. Li and M. Tian},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A lightweight privacy-preserving truth discovery framework for mobile crowd sensing systems},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The recent proliferation of human-carried mobile devices has given rise to the mobile crowd sensing (MCS) systems. However, the sensory data provided by the participating workers are usually not reliable. As an efficient technique to extract truthful information from unreliable data, truth discovery has drawn significant attention. Currently, the privacy concern of the participating workers poses a major challenge on the design of truth discovery mechanisms. Although the existing mechanism can conduct truth discovery with high accuracy and strong privacy guarantee, tremendous overhead is incurred on the worker side. In this paper, we propose a novel lightweight privacy preserving truth discovery framework, L-PPTD, which is implemented by involving two non-colluding cloud platforms and adopting additively homomorphic cryptosystem. This framework not only achieves the protection of each worker's sensory data and reliability information but also introduces little overhead to the workers. In order to further reduce each worker's overhead in the scenarios where only the sensory data need to be protected, we propose another more lightweight framework named L<sup>2</sup>-PPTD. The desirable performance of the proposed frameworks is verified through extensive experiments conducted on real world MCS systems.},
keywords={cloud computing;data privacy;mobile computing;homomorphic cryptosystem;privacy concern;unreliable data;human-carried mobile devices;mobile crowd sensing systems;lightweight privacy-preserving truth discovery framework;world MCS systems;sensory data;reliability information;noncolluding cloud platforms;worker side;strong privacy guarantee;truth discovery mechanisms;participating workers;Reliability;Sensors;Cryptography;Privacy;Mobile handsets;Zinc},
doi={10.1109/INFOCOM.2017.8057114},
ISSN={},
month={May},}
@INPROCEEDINGS{8057115,
author={J. Li and Y. Zhu and J. Yu and C. Long and G. Xue and S. Qian},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Online auction for IaaS clouds: Towards elastic user demands and weighted heterogeneous VMs},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Auctions have been adopted by many major cloud providers, such as Amazon EC2. Unfortunately, only simple auctions have been implemented. Such simple auction has serious limitations, such as being unable to accept elastic user demands and having to allocate different types of VMs independently. These limitations create a big gap between the real needs of cloud users and the available services of cloud providers. In response to the limitations of the existing auction mechanisms, this paper proposes a novel online auction mechanism for IaaS clouds, with the unique features of an elastic model for inputting time-varying user demands and a unified model for requesting heterogeneous VMs together. However, several major challenges should be addressed, such as NP hardness of optimal VM allocation, time-varying user demands and potential misreports of private information of cloud users. We propose a truthful online auction mechanism for maximizing the profit of the cloud provider in IaaS clouds, which is composed of a price-based allocation rule and a payment rule. In the allocation rule, the online auction mechanism determines the number of VMs of each type to each user. In the payment rule, by introducing a marginal price function for each type of VMs, the mechanism determines how much the cloud provider should charge each cloud user. With solid theoretical analysis and trace-driven simulations, we demonstrate that our mechanism is truthful and individually rational, and has a polynomial-time complexity.},
keywords={cloud computing;computational complexity;electronic commerce;pricing;resource allocation;virtual machines;IaaS clouds;cloud provider;cloud user;marginal price function;unified model;time-varying user demands;weighted heterogeneous VM;elastic user demands;online auctions;payment rule;price-based allocation rule;Cloud computing;Resource management;Cost accounting;Servers;Computational modeling;Pricing;Upper bound;IaaS Clouds;online auctions;time-varying;elastic user demands;weighted heterogeneous VMs;profit maximization},
doi={10.1109/INFOCOM.2017.8057115},
ISSN={},
month={May},}
@INPROCEEDINGS{8057116,
author={H. Tan and Z. Han and X. Li and F. C. M. Lau},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Online job dispatching and scheduling in edge-clouds},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In edge-cloud computing, a set of edge servers are deployed near the mobile devices such that these devices can offload jobs to the servers with low latency. One fundamental and critical problem in edge-cloud systems is how to dispatch and schedule the jobs so that the job response time (defined as the interval between the release of a job and the arrival of the computation result at its device) is minimized. In this paper, we propose a general model for this problem, where the jobs are generated in arbitrary order and times at the mobile devices and offloaded to servers with both upload and download delays. Our goal is to minimize the total weighted response time over all the jobs. The weight is set based on how latency sensitive the job is. We derive the first online job dispatching and scheduling algorithm in edge-clouds, called OnDisc, which is scalable in the speed augmentation model; that is, OnDisc is (1 + ε)-speed O(1/ε)-competitive for any constant ε ϵ (0,1). Moreover, OnDisc can be easily implemented in distributed systems. Extensive simulations on a real-world data-trace from Google show that OnDisc can reduce the total weighted response time dramatically compared with heuristic algorithms.},
keywords={cloud computing;mobile computing;telecommunication scheduling;upload delays;download delays;OnDisc;speed augmentation model;distributed systems;edge servers;edge-cloud computing;scheduling algorithm;online job dispatching;total weighted response time;mobile devices;job response time;edge-cloud systems;Servers;Cloud computing;Mobile handsets;Algorithm design and analysis;Dispatching;Time factors;Mobile communication},
doi={10.1109/INFOCOM.2017.8057116},
ISSN={},
month={May},}
@INPROCEEDINGS{8057117,
author={R. Zhu and D. Niu and Z. Li},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Robust web service recommendation via quantile matrix factorization},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We study the problem of personalized Quality of Service (QoS) estimation for web services. State-of-the-art methods use matrix factorization or collaborative prediction to estimate web service response times and throughput for each user based on partial measurements collected from past invocations. We point out that in reality, both the response times and through-put of web services follow highly skewed distributions. In this case, the conditional mean QoS estimates generated by traditional matrix completion approaches can be heavily biased toward a few outliers, leading to poor web service recommendation performance. In this paper, we propose the Quantile Matrix Factorization (QMF) technique for web service recommendation by introducing quantile regression into the matrix factorization framework. We propose a novel and efficient algorithm based on Iterative Reweighted Least Squares (IRLS) to solve the QMF problem involving a non-smooth objective function. We further extend the proposed QMF approach to take into account user and service side attributes. Extensive evaluation based on a large-scale QoS dataset has shown that our schemes significantly outperform various state-of-the-art web service QoS estimation schemes in terms of personalized recommendation performance.},
keywords={iterative methods;least squares approximations;matrix decomposition;quality of service;recommender systems;regression analysis;Web services;robust web service recommendation;web service response times;Quantile Matrix Factorization technique;quantile regression;QMF problem;quality of service estimation;throughput estimation;conditional mean QoS estimation;matrix completion approaches;iterative reweighted least squares;IRLS;nonsmooth objective function;user side attribute;service side attribute;Web services;Quality of service;Time factors;Throughput;Estimation;Measurement;Collaboration},
doi={10.1109/INFOCOM.2017.8057117},
ISSN={},
month={May},}
@INPROCEEDINGS{8057118,
author={X. Zhang and C. Wu and Z. Li and F. C. M. Lau},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Proactive VNF provisioning with multi-timescale cloud resources: Fusing online learning and online optimization},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Network Function Virtualization (NFV) represents a new paradigm of network service provisioning. NFV providers acquire cloud resources, install virtual network functions (VNFs), assemble VNF service chains for customer usage, and dynamically scale VNF deployment against input traffic fluctuations. While existing literature on VNF scaling mostly adopts a reactive approach, we target a proactive approach that is more practical given the time overhead for VNF deployment. We aim to effectively estimate upcoming traffic rates and adjust VNF deployment a priori, for flow service quality assurance and resource cost minimization. We adapt online learning techniques for predicting future service chain workloads. We further combine the online learning method with a multi-timescale online optimization algorithm for VNF scaling, through minimization of the regret due to inaccurate demand prediction and minimization of the cost incurred by sub-optimal online decisions in a joint online optimization framework. The resulting proactive online VNF provisioning algorithm achieves a good performance guarantee, as shown by both theoretical analysis and simulation under realistic settings.},
keywords={cloud computing;learning (artificial intelligence);optimisation;telecommunication traffic;virtualisation;NFV;network service provisioning;input traffic fluctuations;VNF scaling;reactive approach;proactive approach;traffic rates;flow service quality assurance;resource cost minimization;online learning techniques;future service chain workloads;online learning method;sub-optimal online decisions;joint online optimization framework;multitimescale cloud resources;Network Function Virtualization;dynamically scale VNF deployment;proactive online VNF provisioning algorithm;Prediction algorithms;Algorithm design and analysis;Cloud computing;Optimization;Minimization;Hardware;Dynamic scheduling},
doi={10.1109/INFOCOM.2017.8057118},
ISSN={},
month={May},}
@INPROCEEDINGS{8057119,
author={S. Sharifian and F. Lin and R. Safavi-Naini},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Secret key agreement using a virtual wiretap channel},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Key agreement using physical layer properties of communication channels is a well studied problem. iJam is a physical layer key agreement protocol that achieves security by creating a “virtual” wiretap channel for the adversary through a subprotocol between the sender and the receiver that uses self-jamming by the receiver. The protocol was implemented and its security was shown through extensive experiments. The self-jamming subprotocol of iJam was later modelled as a wiretap channel and used for designing a secure message transmission protocol with provable security. We use the same wiretap model of the subprotocol to design secret key agreement protocols with provable security. We propose two protocols that use the wiretap channel once from Alice to Bob, and a protocol that uses two wiretap channels, one from Alice to Bob, and one in the opposite direction. We provide security proof and efficiency analysis for the protocols. The protocols effectively give physical layer security protocols that can be implemented and have provable security. We discuss our results and propose directions for future research.},
keywords={cryptographic protocols;jamming;telecommunication security;virtual wiretap channel;self-jamming subprotocol;iJam;secure message transmission protocol;provable security;wiretap model;secret key agreement protocols;security proof;physical layer security protocols;physical layer properties;communication channels;physical layer key agreement protocol;Protocols;Security;Receivers;Jamming;Random variables;Physical layer;Wireless communication},
doi={10.1109/INFOCOM.2017.8057119},
ISSN={},
month={May},}
@INPROCEEDINGS{8057120,
author={P. Wang and R. Safavi-Naini},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Interactive message transmission over adversarial wiretap channel II},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In Wyner wiretap II model of communication, Alice and Bob are connected by a channel that can be eavesdropped by an adversary with unlimited computation who can select a fraction of communication to view, and the goal is to provide perfect information theoretic security. Information theoretic security is increasingly important because of the threat of quantum computers that can effectively break algorithms and protocols that are used in today's public key infrastructure. We consider interactive protocols for wiretap II channel with active adversary who can eavesdrop and add adversarial noise to the eavesdropped part of the codeword. These channels capture wireless setting where malicious eavesdroppers at reception distance of the transmitter can eavesdrop the communication and introduce jamming signal to the channel. We derive a new upperbound R ≤ 1 - ρ for the rate of interactive protocols over two-way wiretap II channel with active adversaries, and construct a perfectly secure protocol family with achievable rate 1 - 2ρ + ρ2. This is strictly higher than the rate of the best one round protocol which is 1 - 2ρ, hence showing that interaction improves rate. We also prove that even with interaction, reliable communication is possible only if ρ <; 1/2. An interesting aspect of this work is that our bounds will also hold in network setting when two nodes are connected by n paths, a ρ of which is corrupted by the adversary. We discuss our results, give their relations to the other works, and propose directions for future work.},
keywords={cryptographic protocols;jamming;telecommunication security;wireless channels;interactive message transmission;adversarial wiretap channel II;Wyner wiretap II model;perfect information theoretic security;quantum computers;public key infrastructure;interactive protocols;active adversary;adversarial noise;perfectly secure protocol family;wireless setting;two-way wiretap II channel;jamming signal;Protocols;Reliability;Security;Mobile communication;Random variables;Upper bound;Conferences},
doi={10.1109/INFOCOM.2017.8057120},
ISSN={},
month={May},}
@INPROCEEDINGS{8057121,
author={K. Jiang and T. Jing and Z. Li and Y. Huo and F. Zhang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Analysis of secrecy performance in fading multiple access wiretap channel with SIC receiver},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Recently, a new paradigm of multiple access (MAC) along with one eavesdropper to achieve secrecy transmissions has been getting in focus. However, all existing work on such multiple access wiretap channel (MAC-WT) mainly concentrates on the secrecy performance of the system as a whole from an information theoretic perspective. In this work, we investigate the secrecy performance of a single transmitter in the quasi-static Rayleigh fading MAC-WT on basis of two decoding methods, zero-forcing (ZF) and minimum mean-square error (MMSE), jointly with successive interference cancellation (SIC). We evaluate the secrecy performance in three metrics: positive secrecy capacity probability, secrecy outage probability and effective secrecy throughput. The analytical and simulation results show that, 1) the SIC order has great impacts on the secrecy performance for both methods; 2) MMSE-SIC outperforms ZF-SIC, while the performance gap can be overcome via adjusting SIC order, or increasing SNR, or enhancing the spatial diversity gain; 3) in high SNR regime, the secrecy performance is only determined by the relative distance to eavesdropper over legitimate receiver rather than the SNR.},
keywords={channel capacity;decoding;fading channels;least mean squares methods;multi-access systems;radio receivers;Rayleigh channels;telecommunication security;wireless channels;secrecy performance;multiple access wiretap channel;secrecy transmissions;quasistatic Rayleigh fading MAC-WT;positive secrecy capacity probability;secrecy outage probability;effective secrecy throughput;SIC;Decoding;Silicon carbide;Signal to noise ratio;Transmitters;Fading channels;Interference;Receivers},
doi={10.1109/INFOCOM.2017.8057121},
ISSN={},
month={May},}
@INPROCEEDINGS{8057122,
author={M. Bradbury and A. Jhumka},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Understanding source location privacy protocols in sensor networks via perturbation of time series},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Source location privacy (SLP) is becoming an important property for a large class of security-critical wireless sensor network applications such as monitoring and tracking. Much of the previous work on SLP has focused on the development of various protocols to enhance the level of SLP imparted to the network, under various attacker models and other conditions. Other work has focused on analysing the level of SLP being imparted by a specific protocol. In this paper, we adopt a different approach where we model the attacker movement as a time series and use information theoretic concepts to infer the properties of a routing protocol that imparts high levels of SLP. We propose the notion of a properly competing path that causes an attacker to “stall” when moving towards the source. This concept provides the basis for developing a perturbation model, similar to those in privacy-preserving data mining. We then show how to use properly competing paths to develop properties of an SLP-aware routing protocol. Further, we show how different SLP-aware routing protocols can be obtained through different instantiations of the framework. Those instantiations are obtained based on a notion of information loss achieved through the use of the perturbation model proposed.},
keywords={data mining;data privacy;routing protocols;telecommunication security;time series;wireless sensor networks;attacker movement;time series;information theoretic concepts;perturbation model;privacy-preserving data mining;SLP-aware routing protocol;source location privacy protocols;security-critical wireless sensor network applications;properly-competing path;Routing protocols;Privacy;Routing;Time series analysis;Wireless sensor networks;Phantoms;Source Location Privacy;Wireless Sensor Networks;Entropy;Mutual Information;Time Series},
doi={10.1109/INFOCOM.2017.8057122},
ISSN={},
month={May},}
@INPROCEEDINGS{8057123,
author={L. Zheng and C. Joe-Wong and J. Chen and C. G. Brinton and C. W. Tan and M. Chiang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Economic viability of a virtual ISP},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Growing mobile data usage has led to end users paying substantial data costs, while Internet service providers (ISPs) struggle to upgrade their networks to keep up with demand and maintain high quality-of-service (QoS). This problem is particularly severe for smaller ISPs with less capital. Instead of simply upgrading their network infrastructure, ISPs can pool their networks to provide a good QoS and attract more users. Such a vISP (virtual ISP), for example, Google's Project Fi, allows users to access any of its partner ISPs' networks. We provide the first systematic analysis of a vISP's economic impact, showing that the vISP provides a viable solution for smaller ISPs attempting to attract more users, but may not maintain a positive profit if users' data demands evolve. To do so, we consider users' decisions of whether to defect from their current ISP to the vISP, as well as ISPs' decisions on whether to partner with the vISP. We derive the vISP's dependence on user behavior and partner ISPs: users with very light or very heavy usage are the most likely to defect, while ISPs with heavy-usage customers can benefit from declining to partner with the vISP. Our analytical results are verified with extensive numerical simulations.},
keywords={Internet;microeconomics;mobile radio;quality of service;Internet service providers struggle;quality-of-service;network infrastructure;virtual ISP;current ISP;economic viability;mobile data usage;end users;QoS;ISP networks;Google Project Fi;ISP decisions;Mobile communication;Quality of service;Throughput;Economics;Wireless fidelity;Switches;Pricing},
doi={10.1109/INFOCOM.2017.8057123},
ISSN={},
month={May},}
@INPROCEEDINGS{8057124,
author={D. Mitra and Q. Wang and A. Hong},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Emerging internet content and service providers' relationships: Models and analyses of engineering, business and policy impact},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We study engineering and business relationships between Content Providers and broadband access ISPs in various organizational and policy environments. We focus on pricing and capacity decisions for bandwidth and caches for delivery of the CP's content over the “last mile” of the ISP's infrastructure. We model the CP-ISP interaction by the Stackelberg “leader-follower” game and the Integrated Operations model. We consider cases where premium bandwidth is offered to subscribers of the CP's service over the last mile, and cases where this is prohibited by Net Neutrality regulations. We develop a uniform solution procedure for all four resulting models. We explore the connections between optimal bandwidth and cache deployments, and, together with fees, their impact on the number of users, and related business and policy topics. We show that the decrease in profitability of caching due to Net Neutrality regulations is greater than the decrease from Integrated Operations to the Stackelberg game. In the Stackelberg game we prove that if a certain condition is satisfied, then with Net Neutrality the ISP will increase the cache price so that it is unprofitable for the CP to use caches. Moreover, this condition is satisfied in a typical case studied in detail.},
keywords={game theory;Internet;pricing;policy impact;capacity decisions;CP-ISP interaction;Stackelberg leader-follower game;Integrated Operations model;Net Neutrality regulations;optimal bandwidth;cache deployments;cache price;Internet service providers;pricing decisions;Internet content providers;broadband access ISP;CP content;CP service;Bandwidth;Artificial neural networks;Network neutrality;Elasticity;Business;Internet},
doi={10.1109/INFOCOM.2017.8057124},
ISSN={},
month={May},}
@INPROCEEDINGS{8057125,
author={J. Z. F. Pang and H. Fu and W. I. Lee and A. Wierman},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={The efficiency of open access in platforms for networked cournot markets},
year={2017},
volume={},
number={},
pages={1-9},
abstract={This paper studies how the efficiency of an online platform is impacted by the degree to which access of platform participants is open or controlled. The study is motivated by an emerging trend within platforms to impose increasingly fine-grained control over the options available to platform participants. While early online platforms allowed open access, e.g., Ebay allows any seller to interact with any buyer; modern platforms often impose matches directly, e.g., Uber directly matches drivers to riders. This control is performed with the goal of achieving more efficient market outcomes. However, the results in this paper highlight that imposing matches may create new strategic incentives that lead to increased inefficiency. In particular, in the context of networked Cournot competition, we prove that open access platforms guarantee social welfare within 7/16 of the optimal; whereas controlled allocation platforms can have social welfare unboundedly worse than optimal.},
keywords={Internet;networked cournot markets;online platform;platform participants;fine-grained control;open access platforms;controlled allocation platforms;Open Access;Resource management;Production;Economics;Conferences;Market research;Companies},
doi={10.1109/INFOCOM.2017.8057125},
ISSN={},
month={May},}
@INPROCEEDINGS{8057126,
author={D. X. Mendes and E. de Souza e Silva and D. Menasché and R. Leão and D. Towsley},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={An experimental reality check on the scaling laws of swarming systems},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Swarming systems, such as BitTorrent, are one of the most common solutions for scalable, robust and inexpensive content distribution. Although the service capacity of swarming systems has been studied for decades through modeling and analysis, there is a lack of experimental evidence about how the throughput of such systems behaves in under-provisioned regimes. The aim of this paper is to fill this gap. In this paper, we consider a closed-loop model to assess the throughput of peer-to-peer systems. Then, we show through controlled experiments using BitTorrent clients that some analytical findings recently reported in the literature, such as the missing piece syndrome, occur in practice. In particular, we indicate that when seeds have a small effective service capacity, or when seeds are intermittent, the throughput saturates as the population size grows. Finally, we discuss the implications of such findings on the modeling and design of swarming systems.},
keywords={peer-to-peer computing;scaling laws;modeling analysis;closed-loop model;peer-to-peer systems;BitTorrent clients;content distribution;swarming systems;missing piece syndrome;Throughput;Sociology;Statistics;Analytical models;Computational modeling;Scalability;Peer-to-peer computing},
doi={10.1109/INFOCOM.2017.8057126},
ISSN={},
month={May},}
@INPROCEEDINGS{8057127,
author={C. Lin and Y. Chen and K. C. Lin and W. Chen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={acPad: Enhancing channel utilization for 802.11ac using packet padding},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Multi-User Multiple Input Multiple Output (MU-MIMO) enables a multi-antenna access point (AP) to serve multiple users simultaneously, and has been adopted as the IEEE 802.11ac standard. While several PHY-MAC designs have recently been proposed to improve the throughput performance of a MU-MIMO WLAN, they, however, usually assume that all the concurrent streams are of roughly equal length. In reality, users usually have frames with heterogeneous lengths even after aggregation, leading to different lengths of transmission time. Hence, the concurrent transmission opportunities might not always be fully utilized when some streams finish earlier than the others in a transmission opportunity (TXOP). To resolve this inefficiency, this paper presents acPad, a PHY-MAC design that adds additional frames to fill up the idle channel time and better utilize the spatial multiplexing gain. Our acPad identifies proper users as the padding so as to improve the padding gain, while preventing this padding from harming all the ongoing streams. Our evaluation via large-scale trace-driven simulations demonstrates that acPad improves the throughput by up to 2.83×, or by 1.36× on average, as compared to the conventional 802.11ac.},
keywords={access protocols;antenna arrays;MIMO communication;multiplexing;wireless channels;wireless LAN;PHY-MAC design;acPad;channel utilization;packet padding;MultiUser Multiple Input Multiple Output;IEEE 802.11ac standard;MU-MIMO WLAN;multiantenna access point;spatial multiplexing gain;Signal to noise ratio;Interference;Precoding;Throughput;MIMO;Standards;Array signal processing},
doi={10.1109/INFOCOM.2017.8057127},
ISSN={},
month={May},}
@INPROCEEDINGS{8057128,
author={S. Kim and J. Yi and Y. Son and S. Yoo and S. Choi},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Quiet ACK: ACK transmit power control in IEEE 802.11 WLANs},
year={2017},
volume={},
number={},
pages={1-9},
abstract={With increasing demand for wireless connectivity, IEEE 802.11 WLANs have become ubiquitous and continue to grow in number. This leads to high density of basic service sets with the significant co-channel interference (CCI) among them. This paper sheds light on the CCI caused by 802.11 MAC ACK frames, which has been less studied than the CCI caused by data frames. Based on stochastic geometry analysis, we propose Quiet ACK (QACK), a dynamic transmit power control algorithm for ACK frames. Fine-grained transmit power adjustment is enabled by CCI detection and CCI power estimation in the middle of a data frame reception. Our prototype using software-defined radio shows the feasibility and performance gain of QACK, i.e., 1.5x higher throughput than the legacy 802.11 WLANs. The performance of QACK is further evaluated in more general WLAN environments via extensive simulations using ns-3.},
keywords={access protocols;cochannel interference;power control;wireless LAN;wireless connectivity;MAC ACK frames;Quiet ACK;QACK;dynamic transmit power control algorithm;fine-grained transmit power adjustment;CCI detection;data frame reception;ACK transmit power control;IEEE 802.11 WLAN;co-channel interference;software-defined radio;Interference;Power control;IEEE 802.11 Standard;Wireless LAN;Power system reliability;Probability;Heuristic algorithms},
doi={10.1109/INFOCOM.2017.8057128},
ISSN={},
month={May},}
@INPROCEEDINGS{8057129,
author={G. Lee and Y. Shin and J. Koo and J. Choi and S. Choi},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={ACT-AP: ACTivator access point for multicast over WLAN},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Multicast is a major solution in supporting the explosive growth of the wireless video traffic demand. Also, power saving is a key technology to extend battery life of mobile devices. To meet both purposes, IEEE 802.11 wireless local area network (WLAN) supports power save mode (PSM) for station (STA) while receiving multicast packets. According to recent studies, off-the-shelf chipsets configured to use PSM show un-desired functions, thus resulting in many multicast packet losses. From our extensive measurement, we also verify degradation of multicast performance with widely-used off-the-shelf chipsets using PSM, and present previously-unknown undesired functions. Without modification of the chipsets, STA in PSM cannot enjoy reliable multicast service. Given this, we develop a practical and readily-applicable AP-side solution, called ACT-AP, which avoids multicast packet losses by preventing STA from operating in PSM. Our prototype implementation with off-the-shelf chipsets demonstrates that ACT-AP improves packet delivery ratio by up to 216% with little additional protocol overhead. To our best knowledge, ACT-AP is the first practical effort to support multicast to real devices with undesired functions in PSM.},
keywords={mobile radio;multicast communication;protocols;telecommunication power management;telecommunication traffic;wireless LAN;IEEE 802.11 wireless local area network;WLAN;PSM;multicast packet losses;reliable multicast service;readily-applicable AP-side;packet delivery ratio;ACTivator access point;wireless video traffic demand;mobile devices;ACT-AP;power save mode;Power system management;Wireless LAN;Unicast;Packet loss;Semiconductor device measurement;IEEE 802.11 Standard},
doi={10.1109/INFOCOM.2017.8057129},
ISSN={},
month={May},}
@INPROCEEDINGS{8057130,
author={R. K. Sheshadri and D. Koutsonikolas},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={On packet loss rates in modern 802.11 networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The knowledge of link packet loss rates (PLRs) at different PHY layer configurations is vital for a number of wireless network optimization schemes. However, the very large number of PHY layer configurations offered by modern 802.11 n/ac networks has made probing-based PLR estimation at each available configuration extremely challenging. In this paper, we seek to answer the question “How to estimate the PLRs at each available PHY layer configuration with minimal overhead?” Our analysis of the PLR datasets collected from three 802.11 n/ac testbeds reveals that, for any given link, there are several configurations with similar PLR. However, capturing this similarity using well-known link quality indicators like RSSI, or PHY layer features such as MCS or number of MIMO streams is hard. Consequently, we explore the approach of clustering the available PHY layer configurations into a small number of clusters with similar PLR, independent of any other parameter, and only probe one representative configuration in each cluster. Using two real-world case studies - rate adaptation and multihop routing, we show that the proposed clustering-based PLR estimation helps network optimization schemes to reach optimal configurations faster leading to significant performance improvements.},
keywords={optimisation;telecommunication network routing;wireless LAN;multihop routing;rate adaptation;PHY layer configurations;PHY layer features;link quality indicators;PLR estimation;802.11 n/ac networks;link packet loss rates;MIMO;IEEE 802.11n Standard;Bit rate;Estimation;Probes},
doi={10.1109/INFOCOM.2017.8057130},
ISSN={},
month={May},}
@INPROCEEDINGS{8057131,
author={A. Marcone and M. Pierobon and M. Magarini},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A parity check analog decoder for molecular communication based on biological circuits},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Molecular Communication (MC) is an enabling paradigm for the interconnection of future devices and networks in the biological environment, with applications ranging from bio-medicine to environmental monitoring and control. The engineering of biological circuits, which allows to manipulate the molecular information processing abilities of biological cells, is a candidate technology for the realization of MC-enabled devices. In this paper, inspired by recent studies favoring the efficiency of analog computation over digital in biological cells, an analog decoder design is proposed based on biological circuit components. In particular, this decoder computes the a-posteriori log-likelihood ratio of parity-check-encoded bits from a binary-modulated concentration of molecules. The proposed design implements the required L-value and the box-plus operations entirely in the biochemical domain by using activation and repression of gene expression, and reactions of molecular species. Each component of the circuit is designed and tuned in this paper by comparing the resulting functionality with that of the corresponding analytical expression. Despite evident differences with classical electronics, biochemical simulation data of the resulting biological circuit demonstrate very close performance in terms of Mean Squared Error (MSE) and Bit Error Rate (BER), and validate the proposed approach for the future realization of MC components.},
keywords={computational complexity;decoding;error statistics;mean square error methods;molecular communication (telecommunication);parity check codes;parity check analog decoder;molecular communication;biological circuits;biological environment;environmental monitoring;molecular information processing abilities;biological cells;analog computation;molecular species;MC components;biomedicine;a-posteriori log-likelihood ratio;binary-modulated concentration;L-value;box-plus operations;biochemical simulation data;mean squared error;MSE;bit error rate;BER;gene expression;Biological information theory;Cells (biology);Decoding;Proteins;Transmitters;Receivers;Mathematical model},
doi={10.1109/INFOCOM.2017.8057131},
ISSN={},
month={May},}
@INPROCEEDINGS{8057132,
author={S. Basagni and V. Di Valerio and P. Gjanci and C. Petrioli},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Finding MARLIN: Exploiting multi-modal communications for reliable and low-latency underwater networking},
year={2017},
volume={},
number={},
pages={1-9},
abstract={This paper concerns the smart exploitation of multimodal communication capabilities of underwater nodes to enable reliable and swift underwater networking. To contrast adverse and highly varying channel conditions we define a smart framework enabling nodes to acquire knowledge on the quality of the communication to neighboring nodes over time. Following a model-based reinforcement learning approach, our framework allows senders to select the best forwarding relay for its data jointly with the best communication device to reach that relay. We name the resulting forwarding method MARLIN, for MultimodAl Reinforcement Learning-based RoutINg. Applications can choose whether to seek reliable routes to the destination, or whether faster packet delivery is more desirable. We evaluate the performance of MARLIN in varying networking scenarios where nodes communicate through two acoustic modems with widely different characteristics. MARLIN is compared to state-of-the-art forwarding protocols, including a channel-aware solution, a machine learning-based solution and to a flooding protocol extended to use multiple modems. Our results show that a smartly learned selection of relay and modem is key to obtain a packet delivery ratio that is twice as much that of other protocols, while maintaining low latencies and energy consumption.},
keywords={learning (artificial intelligence);marine communication;modems;radio networks;routing protocols;MARLIN;forwarding method;MultimodAl Reinforcement Learning-based RoutINg;flooding protocol;multiple modems;packet delivery ratio;smartly learned selection;machine learning;channel-aware solution;state-of-the-art forwarding protocols;acoustic modems;networking scenarios;faster packet delivery;reliable routes;communication device;forwarding relay;reinforcement learning approach;neighboring nodes;smart framework;channel conditions;swift underwater networking;reliable networking;underwater nodes;multimodal communications;Modems;Reliability;Protocols;Relays;Routing;Computer network reliability;Quality of service;Underwater Wireless Sensor Networks;multimodal communications;reinforcement learning-based routing},
doi={10.1109/INFOCOM.2017.8057132},
ISSN={},
month={May},}
@INPROCEEDINGS{8057133,
author={G. E. Santagati and T. Melodia},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={An implantable low-power ultrasonic platform for the Internet of Medical Things},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Wirelessly networked systems of implantable medical devices endowed with sensors and actuators will be the basis of many innovative, sometimes revolutionary therapies. The biggest obstacle in realizing this vision of networked implantable devices is posed by the dielectric nature of the human body, which strongly attenuates radio-frequency (RF) electromagnetic waves. In this paper we present the first hardware and software architecture of an Internet of Medical Things (IoMT) platform with ultrasonic connectivity for intra-body communications that can be used as a basis for building future IoT-ready medical implantable and wearable devices. We show that ultrasonic waves can be efficiently generated and received with low-power and mm-sized components, and that despite the conversion loss introduced by ultrasonic transducers the gap in attenuation between 2.4 GHz RF and ultrasonic waves is still substantial, e.g., ultrasounds offer 70 dB less attenuation over 10 cm. We show that the proposed IoMT platform requires much lower transmission power compared to 2.4 GHz RF with equal reliability in tissues, e.g., 35 dBm lower over 12 cm for 10<sup>-3</sup> Bit Error Rate (BEr) leading to lower energy per bit and longer device lifetime. Finally, we show experimentally that 2.4 GHz RF links are not functional at all above 12 cm, while ultrasonic links achieve a reliability of 10<sup>-6</sup> up to 20 cm with less than 0 dBm transmission power.},
keywords={biomedical communication;biomedical ultrasonics;error statistics;Internet of Things;low-power electronics;prosthetics;ultrasonic transducers;low-power ultrasonic platform;wirelessly networked systems;implantable medical devices;sensors;actuators;revolutionary therapies;networked implantable devices;dielectric nature;human body;radio-frequency electromagnetic waves;software architecture;ultrasonic connectivity;wearable devices;ultrasonic waves;mm-sized components;ultrasonic transducers the gap;attenuation;IoMT platform;lower transmission power;longer device lifetime;ultrasonic links;intrabody communications;RF links;transmission power;Internet of Medical Things platform;frequency 2.4 GHz;noise figure 70.0 dB;size 10.0 cm;size 12.0 cm;size 20.0 cm;Acoustics;Radio frequency;Sensors;Implants;Hardware;Wireless communication;Field programmable gate arrays},
doi={10.1109/INFOCOM.2017.8057133},
ISSN={},
month={May},}
@INPROCEEDINGS{8057134,
author={Z. Zhang and P. Kumar},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={mEEC: A novel error estimation code with multi-dimensional feature},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Error estimation code estimates the bit error ratio of the received data bits with low overhead. It has many applications, especially in estimating the number of errors in a packet transmitted over a wireless link. In this paper, we propose a novel error estimation code, mEEC, that outperforms the existing code by more than 10%-20% depending on the packet sizes, at the same time being less biased. mEEC is mainly based on the idea of grouping multiple blocks of sampled data bits into a super-block, thus creating a multi-dimensional feature. It then compresses these features into a single number, called the color, as the coded bits. Through an intelligent coloring scheme, the blocks in a super-block share the cost of covering low probability events, which allows the decoder to recover the actual feature values from the color even in the presence of error. mEEC also adopts a lightweight redistribution step, which is guided by the solution of an optimization problem and further reduces the estimation errors and bias. We also show that mEEC can be implemented with reasonable storage sizes and low time complexity.},
keywords={error correction codes;estimation theory;mEEC;estimation errors;multidimensional feature;bit error ratio;coded bits;data bits;error estimation code;Conferences},
doi={10.1109/INFOCOM.2017.8057134},
ISSN={},
month={May},}
@INPROCEEDINGS{8057135,
author={H. Pan and G. Xie and Z. Li and P. He and L. Mathy},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={FlowConvertor: Enabling portability of SDN applications},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Software-Defined Networking (SDN) provides network administrators opportunities to control network devices more simply and easily than in traditional networking. However, heterogeneity in switch hardware, especially in forwarding pipeline architecture, renders the task of network application developers and network administrators tedious, by hampering portability across switch models. In this paper, we propose FlowConvertor, an algorithm capable of converting rules from any forwarding pipeline to any other different forwarding pipeline, as long as both pipelines offer compatible operations. More precisely, FlowConvertor is an online algorithm that operates on flow updates issued to the origin pipeline and computes the corresponding updates for the target pipeline in real time. Performance evaluation shows that the latency introduced by FlowConvertor on the path between the SDN controller and the target switch is of the order of 1ms in most cases, and is thus acceptable for practical deployment.},
keywords={computer network management;pipelines;software defined networking;forwarding pipeline architecture;switch hardware;traditional networking;network devices;network administrators opportunities;Software-Defined Networking;SDN applications;target switch;SDN controller;target pipeline;origin pipeline;flow updates;online algorithm;FlowConvertor;switch models;network application developers;time 1.0 ms;Switches;Pipelines;Metadata;Pipeline processing;Hardware;Engines;Ports (Computers)},
doi={10.1109/INFOCOM.2017.8057135},
ISSN={},
month={May},}
@INPROCEEDINGS{8057136,
author={K. Poularakis and G. Iosifidis and G. Smaragdakis and L. Tassiulas},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={One step at a time: Optimizing SDN upgrades in ISP networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Nowadays, there is a fast-paced shift from legacy telecommunication systems to novel Software Defined Network (SDN) architectures that can support on-the-fly network reconfiguration, therefore, empowering advanced traffic engineering mechanisms. Despite this momentum, migration to SDN cannot be realized at once especially in high-end cost networks of Internet Service Providers (ISPs). It is expected that ISPs will gradually upgrade their networks to SDN over a period that spans several years. In this paper, we study the SDN upgrading problem in an ISP network: which nodes to upgrade and when. We consider a general model that captures different migration costs and network topologies, and two plausible ISP objectives; first, the maximization of the traffic that traverses at least one SDN node, and second, the maximization of the number of dynamically selectable routing paths enabled by SDN nodes. We leverage the theory of submodular and supermodular functions to devise algorithms with provable approximation ratios for each objective. Using real-world network topologies and traffic matrices, we evaluate the performance of our algorithms and show up to 54% gains over state-of-the-art methods. Moreover, we describe the interplay between the two objectives; maximizing one may cause a factor of 2 loss to the other.},
keywords={Internet;optimisation;software defined networking;telecommunication network routing;telecommunication network topology;telecommunication traffic;ISP network;Software Defined Network architectures;on-the-fly network reconfiguration;advanced traffic engineering mechanisms;Internet Service Providers;telecommunication systems;network topologies;traffic maximization;Approximation algorithms;Routing;Network topology;Routing protocols;Conferences;Optimization},
doi={10.1109/INFOCOM.2017.8057136},
ISSN={},
month={May},}
@INPROCEEDINGS{8057137,
author={H. Wang and A. Srivastava and L. Xu and S. Hong and G. Gu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Bring your own controller: Enabling tenant-defined SDN apps in IaaS clouds},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The need of customized network functions for enterprises in Infrastructure-as-a-Service (IaaS) clouds is emerging. However, existing network functions in IaaS clouds are very limited, inflexible, and hard to control by the tenants. Recently, the introduction of Software-Defined Networking (SDN) technology brings the hope of flexible control of network flows and creation of diverse network functions. Unfortunately, enterprises lose access to the SDN controller when they move to clouds. Moreover, the cloud SDN controller is only managed by the provider administrators for security and performance reasons. To allow enterprise tenants to develop and deploy their own SDN apps in the cloud, in this paper, we introduce a new cloud usage paradigm: Bring Your Own Controller (BYOC). BYOC offers each tenant an individual SDN controller, where tenants can deploy SDN apps to manage their network. To manage these tenant SDN controllers, we propose BYOC-Visor, a new SDN-based virtualization platform. BYOC-VISOR addresses several security and performance challenges which are specific to IaaS clouds. We show that BYOC-Visor supports different controller platforms and diverse SDN security applications such as firewall, IDS, and access control. We implement a prototype system and the performance evaluation results show that our system has low overhead.},
keywords={Bring Your Own Device;cloud computing;computer network management;computer network security;software defined networking;virtualisation;IaaS clouds;customized network functions;Software-Defined Networking technology;diverse network functions;cloud SDN controller;enterprise tenants;cloud usage paradigm;individual SDN controller;tenant SDN controllers;BYOC-Visor;diverse SDN security applications;tenant-defined SDN apps;infrastructure-as-a-service clouds;flexible network flow control;Bring Your Own Controller;BYOC;network management;SDN-based virtualization platform;prototype system;performance evaluation;BYOC;Cloud computing;Topology;Network topology;Security;Control systems;Virtualization;Prototypes},
doi={10.1109/INFOCOM.2017.8057137},
ISSN={},
month={May},}
@INPROCEEDINGS{8057138,
author={H. Mekky and F. Hao and S. Mukherjee and T. V. Lakshman and Z. Zhang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Network function virtualization enablement within SDN data plane},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Software Defined Networking (SDN) can benefit a Network Function Virtualization solution by chaining a set of network functions (NF) to create a network service. Currently, control on NFs is isolated from the SDN, which creates routing inflexibility, flow imbalance and choke points in the network as the controller remains oblivious to the number, capacity and placement of NFs. Moreover, a NF may modify packets in the middle, which makes flow identification at a SDN switch challenging. In this paper, we postulate native NFs within the SDN data plane, where the same logical controller controls both network services and routing. This is enabled by extending SDN to support stateful flow handling based on higher layers in the packet beyond layers 2-4. As a result, NF instances can be chained on demand, directly on the data plane. We present an implementation of this architecture based on Open vSwitch, and show that it enables popular NFs effectively using detailed evaluation and comparison with other alternative solutions.},
keywords={computer networks;software defined networking;virtualisation;SDN data plane;logical controller;network services;Software Defined Networking;network functions;network service;routing inflexibility;flow imbalance;flow identification;network function virtualization;SDN switch;Switches;Noise measurement;Kernel;Routing;Inductors},
doi={10.1109/INFOCOM.2017.8057138},
ISSN={},
month={May},}
@INPROCEEDINGS{8057139,
author={C. Q. Wu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Bandwidth scheduling in overlay networks with linear capacity constraints},
year={2017},
volume={},
number={},
pages={1-9},
abstract={An increasing number of high-performance networks are built over the existing IP network infrastructure to provision dedicated channels for big data transfer. The links in these overlay networks correspond to underlying paths and may share lower-level link segments. We consider a model of overlay networks that incorporates correlated link capacities and linear capacity constraints (LCCs) to formulate such shared bottleneck components. The overlay links are typically shared by multiple users through advance reservations, resulting in varying bandwidth availability in future time. Therefore, efficient bandwidth scheduling algorithms are needed to improve the network resource utilization and also meet the user's transport requirements. We investigate two advance scheduling problems in overlay networks with LCCs: Fixed-Bandwidth Path and Varying-Bandwidth Path, with the objective to minimize the data transfer end time for a given data size. We prove that both problems are NP-complete and non-approximable, and propose heuristic algorithms using a gradual relaxation procedure on the maximum number of links from each LCC allowed for path computation. The performance superiority of these heuristics is verified by extensive simulation results in comparison with optimal and greedy strategies.},
keywords={Big Data;computational complexity;data communication;greedy algorithms;IP networks;minimisation;overlay networks;telecommunication channels;telecommunication links;telecommunication scheduling;overlay networks;linear capacity constraints;high-performance networks;lower-level link segments;link capacities;overlay links;network resource utilization;advance scheduling problems;Fixed-Bandwidth Path;Varying-Bandwidth Path;IP network infrastructure;bandwidth scheduling algorithms;Big Data transfer;LCC;data transfer end time minimisation;NP-complete;heuristic algorithms;gradual relaxation procedure;greedy strategy;Bandwidth;Overlay networks;Scheduling;Processor scheduling;IP networks;Data transfer;Approximation algorithms;bandwidth scheduling;overlay networks;approximate algorithm},
doi={10.1109/INFOCOM.2017.8057139},
ISSN={},
month={May},}
@INPROCEEDINGS{8057140,
author={P. Rahimzadeh and C. Joe-Wong and K. Shin and Y. Im and J. Lee and S. Ha},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={SVC-TChain: Incentivizing good behavior in layered P2P video streaming},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Video streaming applications based on Peer-to-Peer (P2P) systems are popular for their scalability, which is hard to achieve with traditional client-server approaches. In particular, layered video streaming has been much-studied due to its ability to differentiate users' streaming qualities in heterogeneous user environments. Previous work, however, has shown that user misbehavior (e.g., free-riding and protocol deviation) poses a serious threat to P2P systems that are not equipped with proper incentive mechanisms. We propose a method to disincentivize such misbehavior. Our SVC-TChain is a layered P2P video streaming method based on scalable video coding (SVC), which uses the recently proposed T-Chain incentive mechanism to discourage free-riding. After introducing T-Chain, we present the first analytical framework to study SVC piece selection with multiple video layers, using it to efficiently choose SVC-TChain's optimal piece selection parameters and thus discourage deviations from the piece selection policy. Extensive experimental results show that SVC-TChain outperforms layered extensions of BiTos and Give-to-Get, two popular P2P video streaming approaches, both in the absence of user misbehavior and when some users misbehave.},
keywords={peer-to-peer computing;video coding;video streaming;layered P2P video streaming method;scalable video coding;free-riding;SVC piece selection;multiple video layers;piece selection policy;user misbehavior;video streaming applications;Peer-to-Peer systems;traditional client-server approaches;particular video streaming;layered video streaming;heterogeneous user environments;protocol deviation;incentive mechanisms;T-Chain incentive mechanism;P2P video streaming approaches;Streaming media;Static VAr compensators;Video coding;Peer-to-peer computing;Standards;Encryption},
doi={10.1109/INFOCOM.2017.8057140},
ISSN={},
month={May},}
@INPROCEEDINGS{8057141,
author={O. Bilgen and A. B. Wagner},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A new stable peer-to-peer protocol with non-persistent peers},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Recent studies have suggested that the stability of peer-to-peer networks may rely on persistent peers, who dwell on the network after they obtain the entire file. In the absence of such peers, one piece becomes extremely rare in the network, which leads to instability. Technological developments, however, are poised to reduce the incidence of persistent peers, giving rise to a need for a protocol that guarantees stability with nonpersistent peers. We propose a novel peer-to-peer protocol, the group suppression protocol, to ensure the stability of peer-to-peer networks under the scenario that all the peers adopt non-persistent behavior. Using a suitable Lyapunov potential function, the group suppression protocol is proven to be stable when the file is broken into two pieces, and detailed experiments demonstrate the stability of the protocol for arbitrary number of pieces. Straightforward incorporation of the group suppression protocol into BitTorrent while retaining most of BitTorrent's core mechanisms is also presented. Subsequent simulations show that under certain assumptions, BitTorrent with the official protocol cannot escape from the missing piece syndrome, but BitTorrent with group suppression does.},
keywords={Lyapunov methods;peer-to-peer computing;protocols;persistent peers;nonpersistent peers;peer-to-peer protocol;group suppression protocol;peer-to-peer network stability;Lyapunov potential function;BitTorrent;Protocols;Stability analysis;Peer-to-peer computing;Wireless communication;Conferences;Mobile communication;Servers},
doi={10.1109/INFOCOM.2017.8057141},
ISSN={},
month={May},}
@INPROCEEDINGS{8057142,
author={B. Spang and A. Sabnis and R. Sitaraman and D. Towsley and B. DeCleene},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={MON: Mission-optimized overlay networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Large organizations often have users in multiple sites which are connected over the Internet. Since resources are limited, communication between these sites needs to be carefully orchestrated for the most benefit to the organization. We present a Mission-optimized Overlay Network (MON), a hybrid overlay network architecture for maximizing utility to the organization. We combine an offline and an online system to solve non-concave utility maximization problems. The offline tier, the Predictive Flow Optimizer (PFO), creates plans for routing traffic using a model of network conditions. The online tier, MONtra, is aware of the precise local network conditions and is able to react quickly to problems within the network. Either tier alone is insufficient. The PFO may take too long to react to network changes. MONtra only has local information and cannot optimize non-concave mission utilities. However, by combining the two systems, MON is robust and achieves near-optimal utility under a wide range of network conditions. While best-effort overlay networks are well studied, our work is the first to design overlays that are optimized for mission utility.},
keywords={Internet;optimisation;overlay networks;peer-to-peer computing;telecommunication network routing;telecommunication traffic;nonconcave utility maximization problems;Predictive Flow Optimizer;precise local network conditions;near-optimal utility;mission-optimized overlay networks;hybrid overlay network architecture;Internet;MON;routing traffic;PFO offline tier;MONtra online tier;local network conditions;nonconcave mission utilities optimization;Optimization;Organizations;Overlay networks;Computer architecture;Routing;Internet telephony},
doi={10.1109/INFOCOM.2017.8057142},
ISSN={},
month={May},}
@INPROCEEDINGS{8057143,
author={L. Toka and B. Lajtha and É. Hosszu and B. Formanek and D. Géhberger and J. Tapolcai},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A resource-aware and time-critical IoT framework},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Internet of Things (IoT) systems produce great amount of data, but usually have insufficient resources to process them in the edge. Several time-critical IoT scenarios have emerged and created a challenge of supporting low latency applications. At the same time cloud computing became a success in delivering computing as a service at affordable price with great scalability and high reliability. We propose an intelligent resource allocation system that optimally selects the important IoT data streams to transfer to the cloud for processing. The optimization runs on utility functions computed by predictor algorithms that forecast future events with some probabilistic confidence based on a dynamically recalculated data model. We investigate ways of reducing specifically the upload bandwidth of IoT video streams and propose techniques to compute the corresponding utility functions. We built a prototype for a smart squash court and simulated multiple courts to measure the efficiency of dynamic allocation of network and cloud resources for event detection during squash games. By continuously adapting to the observed system state and maximizing the expected quality of detection within the resource constraints our system can save up to 70% of the resources compared to the naive solution.},
keywords={cloud computing;Internet of Things;resource allocation;sport;video streaming;intelligent resource allocation system;dynamically recalculated data model;IoT video streams;dynamic cloud resource allocation;dynamic network resource allocation;smart squash court;upload bandwidth;IoT data streams;Internet of Things;resource-aware time-critical IoT scenarios;utility functions;cloud computing;Cloud computing;Bandwidth;Streaming media;Cameras;Quality of service;Uplink;Resource management;Internet of Things;cloud computing;cloud control;resource provisioning;adaptive;dynamic;QoS;QoE},
doi={10.1109/INFOCOM.2017.8057143},
ISSN={},
month={May},}
@INPROCEEDINGS{8057144,
author={D. Trihinas and G. Pallis and M. D. Dikaiakos},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={ADMin: Adaptive monitoring dissemination for the Internet of Things},
year={2017},
volume={},
number={},
pages={1-9},
abstract={As more knowledge is vastly added to the devices fuelling the Internet of Things (IoT) energy efficiency and real-time data processing are great challenges that must be tackled. In this paper, we introduce ADMin, a low-cost IoT framework that reduces on device energy consumption and the volume of data disseminated across the network. This is achieved by efficiently adapting the rate at which IoT devices disseminate monitoring streams based on run-time knowledge of the stream evolution, variability and seasonal behavior. Rather than transmitting the entire stream, ADMin favors sending updates for its estimation model from which values can be inferred, triggering dissemination only when shifts in the stream evolution are detected. Results on real-life testbeds, show that ADMin is able to reduce energy consumption by at least 83%, data volume by 71%, shift detection delays by 61% while maintaining accuracy above 91% in comparison to other IoT frameworks.},
keywords={data dissemination;energy consumption;Internet of Things;telecommunication power management;ADMin;low-cost IoT framework;run-time knowledge;data volume;IoT frameworks;adaptive monitoring dissemination;real-time data processing;IoT devices;Internet of Things energy efficiency;triggering dissemination;device energy consumption reduction;Measurement;Monitoring;Estimation;Adaptation models;Energy consumption;Receivers;Sensors},
doi={10.1109/INFOCOM.2017.8057144},
ISSN={},
month={May},}
@INPROCEEDINGS{8057145,
author={J. Zhang and Z. Wang and Z. Yang and Q. Zhang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Proximity based IoT device authentication},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Internet of Things (IoT) devices are largely embedded devices which lack a sophisticated user interface, e.g., touch screen, keyboard, etc. As a consequence, traditional Pre-Shared Key (PSK) based authentication for mobile devices becomes difficult to apply. For example, according to our study on home automation devices which leverage smartphone for PSK input, the current process does not protect against active impersonating attack and also leaks the Wi-Fi password to eavesdroppers, i.e., currently these IoT devices can be exploited to enter into critical infrastructures, e.g., home networks. Motivated by this real-world security vulnerability, in this paper we propose a novel proximity-based mechanism for IoT device authentication, called Move2Auth, for the purpose of enhancing IoT device security. In Move2Auth, we require user to hold smartphone and perform one of two hand-gestures (moving towards and away, and rotating) in front of IoT device. By combining (1) large RSS-variation and (2) matching between RSS-trace and smartphone sensor-trace, Move2Auth can reliably detect proximity and authenticate IoT device accordingly. Based on our implementation on Samsung Galaxy smartphone and commodity Wi-Fi adapter, we prove Move2Auth can protect against powerful active attack, i.e., the false-positive rate is consistently lower than 0.5%.},
keywords={computer network security;cryptographic protocols;Internet;Internet of Things;message authentication;mobile computing;security of data;smart phones;telecommunication security;user interfaces;wireless LAN;IoT device authentication;called Move2Auth;smartphone sensor-trace;proximity;authenticate IoT device;Key based authentication;mobile devices;home automation devices;IoT device security;Wi-Fi password;Wireless fidelity;Authentication;Home automation;Cryptography;Performance evaluation;Phase shift keying},
doi={10.1109/INFOCOM.2017.8057145},
ISSN={},
month={May},}
@INPROCEEDINGS{8057146,
author={S. Park and S. Lim and D. Jeong and J. Lee and J. Yang and H. Lee},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={PUFSec: Device fingerprint-based security architecture for Internet of Things},
year={2017},
volume={},
number={},
pages={1-9},
abstract={A low-end embedded platform for Internet of Things (IoT) often suffers from a critical trade-off dilemma between security enhancement and computation overhead. We propose PUFSec, a new device fingerprint-based security architecture for IoT devices. By leveraging intrinsic hardware characteristics, we aim to design a computationally lightweight security software system architecture so that complex cryptography computation can dramatically be prohibited. We exploit the innovative idea of Public Physical Unclonable Functions (PPUFs) that fundamentally protects attackers from recovering the secret key from public gate delay information. We implement its hardware logic in a real-world FPGA board. On top of the PPUF fingerprint hardware, we present an adaptive security control mechanism consisting of adaptive key generation and key exchange protocol, which adjusts security strength depending on system load dynamics. We demonstrate that our PPUF FPGA implementation embeds distinctive variability enough to distinguish between two different PPUFs with high fidelity. We validate our PUFSec architecture by implementing necessary algorithms and protocols in a real-world IoT platform, and performing empirical evaluation in terms of computation and memory usages, proving its practical feasibility.},
keywords={cryptographic protocols;embedded systems;field programmable gate arrays;Internet of Things;private key cryptography;public key cryptography;telecommunication security;low-end embedded platform;security enhancement;computation overhead;IoT devices;intrinsic hardware characteristics;computationally lightweight security software system architecture;complex cryptography computation;Public Physical Unclonable Functions;secret key;public gate delay information;hardware logic;real-world FPGA board;PPUF fingerprint hardware;adaptive security control mechanism;adaptive key generation;key exchange protocol;security strength;system load dynamics;PPUF FPGA implementation;PUFSec architecture;real-world IoT platform;device fingerprint-based security architecture;Internet of Things;Hardware;Logic gates;Delays;Cryptography;Software;Radiation detectors},
doi={10.1109/INFOCOM.2017.8057146},
ISSN={},
month={May},}
@INPROCEEDINGS{8057147,
author={K. Sucipto and D. Chatzopoulos and S. Kosta± and P. Hui},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Keep your nice friends close, but your rich friends closer — Computation offloading using NFC},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The increasing complexity of smartphone applications and services necessitate high battery consumption but the growth of smartphones' battery capacity is not keeping pace with these increasing power demands. To overcome this problem, researchers gave birth to the Mobile Cloud Computing (MCC) research area. In this paper we advance on previous ideas, by proposing and implementing the first known Near Field Communication (NFC)-based computation offloading framework. This research is motivated by the advantages of NFC's short distance communication, with its better security, and its low battery consumption. We design a new NFC communication protocol that overcomes the limitations of the default protocol; removing the need for constant user interaction, the one-way communication restraint, and the limit on low data size transfer. We present experimental results of the energy consumption and the time duration of two computationally intensive representative applications: (i) RSA key generation and encryption, and (ii) gaming/puzzles. We show that when the helper device is more powerful than the device offloading the computations, the execution time of the tasks is reduced. Finally, we show that devices that offload application parts considerably reduce their energy consumption due to the low-power NFC interface and the benefits of offloading.},
keywords={cloud computing;cryptography;mobile computing;near-field communication;smart phones;low-power NFC interface;smartphone applications;Mobile Cloud Computing research area;NFC communication protocol;constant user interaction;one-way communication restraint;low data size transfer;energy consumption;computationally intensive representative applications;encryption;device offloading;offload application parts;power demands;smartphone application complexity;near field communication based computation offloading framework;NFC-based computation offloading framework;NFC short distance communication;RSA key generation;Protocols;Batteries;Wireless fidelity;Androids;Humanoid robots;Mobile communication;Bluetooth},
doi={10.1109/INFOCOM.2017.8057147},
ISSN={},
month={May},}
@INPROCEEDINGS{8057148,
author={S. Jošilo and G. Dán},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A game theoretic analysis of selfish mobile computation offloading},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Offloading computation to a mobile cloud is a promising approach for enabling the use of computationally intensive applications by mobile devices. In this paper we consider autonomous devices that maximize their own performance by choosing one of many wireless access points for computation offloading. We develop a game theoretic model of the problem, prove the existence of pure strategy Nash equilibria, and provide a polynomial time algorithm for computing an equilibrium. For the case when the cloud computing resources scale with the number of mobile devices we show that all improvement paths are finite. We provide a bound on the price of anarchy of the game, thus our algorithm serves as an approximation algorithm for the global computation offloading cost minimization problem. We use extensive simulations to provide insight into the performance and the convergence time of the algorithms in various scenarios. Our results show that the equilibrium cost may be close to optimal, and the convergence time is almost linear in the number of mobile devices.},
keywords={cloud computing;game theory;minimisation;mobile computing;polynomial approximation;computationally intensive applications;mobile devices;autonomous devices;wireless access points;game theoretic model;pure strategy Nash equilibria;polynomial time algorithm;approximation algorithm;global computation offloading cost minimization problem;convergence time;game theoretic analysis;selfish mobile computation offloading;mobile cloud;cloud computing resources;Mobile handsets;Games;Cloud computing;Mobile communication;Approximation algorithms;Computational modeling;Performance evaluation},
doi={10.1109/INFOCOM.2017.8057148},
ISSN={},
month={May},}
@INPROCEEDINGS{8057149,
author={J. P. Champati and B. Liang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Single restart with time stamps for computational offloading in a semi-online setting},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We study the problem of scheduling n tasks on m + m' parallel processors, where the processing times on m processors are known while those on the remaining m' processors are not known a priori. This semi-online model is an abstraction of certain heterogeneous computing systems, e.g., with the m known processors representing local CPU cores and the unknown processors representing remote servers with uncertain availability of computing cycles. Our objective is to minimize the makespan of all tasks. We initially focus on the case m' = 1 and propose a semi-online algorithm termed Single Restart with Time Stamps (SRTS), which has time complexity O(n log n). We derive its competitive ratio in comparison with the optimal offline solution. If the unknown processing times are deterministic, the competitive ratio of SRTS is shown to be either always constant or asymptotically constant in practice, respectively in cases where the processing times are independent and dependent on m. A similar result is obtained when the unknown processing times are random. Furthermore, extending the ideas of SRTS, we propose a heuristic algorithm termed SRTS-Multiple (SRTS-M) for the case m' &gt; 1. Besides the proven competitive ratios, simulation results further suggest that SRTS and SRTS-M give superior performance on average over randomly generated task processing times, substantially reducing the makespan over the best known alternatives. Interestingly, the performance gain is more significant for task processing times sampled from heavy-tailed distributions.},
keywords={computational complexity;processor scheduling;SRTS-M;randomly generated task processing times;computational offloading;parallel processors;local CPU cores;unknown processors;computing cycles;semionline algorithm;competitive ratio;unknown processing times;task scheduling;heterogeneous computing systems;task makespan minimization;single restart-with-time stamps;O(n log n) time complexity;SRTS-multiple heuristic algorithm;Program processors;Processor scheduling;Cloud computing;Computational modeling;Servers;Heuristic algorithms},
doi={10.1109/INFOCOM.2017.8057149},
ISSN={},
month={May},}
@INPROCEEDINGS{8057150,
author={M. Chen and B. Liang and M. Dong},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Joint offloading and resource allocation for computation and communication in mobile cloud with computing access point},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We consider a general multi-user mobile cloud computing system with a computing access point (CAP), where each mobile user has multiple independent tasks that may be processed locally, at the CAP, or at a remote cloud server. The CAP serves both as the network access gateway and a computation service provider to the mobile users. We aim to jointly optimize the offloading decisions of all users' tasks as well as the allocation of computation and communication resources, to minimize the overall cost of energy, computation, and delay for all users. This problem is NP-hard in general. We propose an efficient three-step algorithm comprising of semidefinite relaxation (SDR), alternating optimization (AO), and sequential tuning (ST). It is shown to always compute a locally optimal solution, and give nearly optimal performance under a wide range of parameter settings. Through evaluating the performance of different combinations of the three components of this SDR-AO-ST algorithm, we provide insights into their roles and contributions in the overall solution. We further compare the performance of SDR-AO-ST against a lower bound to the minimum cost, purely local processing, purely cloud processing, and hybrid local-cloud processing without using the CAP. Our numerical results demonstrate the effectiveness of the proposed algorithm in the joint management of computation and communication resources in mobile cloud computing systems with a CAP.},
keywords={cloud computing;mathematical programming;mobile computing;resource allocation;resource allocation;computing access point;multiuser mobile cloud computing system;CAP;mobile user;multiple independent tasks;remote cloud server;network access gateway;computation service provider;offloading decisions;locally optimal solution;optimal performance;SDR-AO-ST algorithm;purely local processing;purely cloud processing;joint offloading;computation resource allocation;communication resource allocation;overall cost of energy minimization;NP-hard problem;semidefinite relaxation;alternating optimization;sequential tuning;lower bound;hybrid local-cloud processing;Mobile communication;Delays;Cloud computing;Servers;Resource management;Mobile handsets},
doi={10.1109/INFOCOM.2017.8057150},
ISSN={},
month={May},}
@INPROCEEDINGS{8057151,
author={Y. Zhang and D. Li and T. Tian and P. Zhong},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={CubeX: Leveraging glocality of cube-based networks for RAM-based key-value store},
year={2017},
volume={},
number={},
pages={1-9},
abstract={RAM-based storage aggregates the RAM of servers in data center networks (DCN) to provide extremely high storage performance. For quick recovery of storage server failures, Mem-Cube [1] exploits the proximity of the BCube network to limit the recovery traffic to the recovery servers' 1-hop neighborhood. However, previous design is applicable only to BCube, and has suboptimal recovery performance due to congestion and contention. To address these problems, in this paper we propose CubeX, which generalizes the “1-hop” principle of MemCube for all cube-based networks, and improves the throughput and recovery performance of RAM-based key-value (KV) store via cross-layer optimizations. At the core of CubeX is to leverage the glocality (= globality + locality) of cube-based networks: it scatters backup data across a large number of disks globally distributed throughout the cube, and restricts all recovery traffic within the small local range of each server node. Our evaluation shows that CubeX efficiently supports RAM-based KV store for cube-based networks, and CubeX remarkably outperforms MemCube in both throughput and recovery time.},
keywords={computer centres;hypercube networks;optimisation;random-access storage;storage management;telecommunication traffic;data center networks;storage server failures;BCube network;recovery traffic;recovery servers;CubeX;cube-based networks;Mem-Cube;RAM-based key-value store;cross-layer optimizations;glocality;Servers;Random access memory;Throughput;Hypercubes;Bandwidth;Conferences},
doi={10.1109/INFOCOM.2017.8057151},
ISSN={},
month={May},}
@INPROCEEDINGS{8057152,
author={F. Wang and L. Gao and S. Xiaozhe and H. Harai and K. Fujikawa},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Towards reliable and lightweight source switching for datacenter networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={A low-latency and reliable message switching network is critical for constructing high-speed datacenter networks. In this paper, we present the design, implementation, and evaluation of a novel Location basEd Source Switching (LESS) for datacenter networks. LESS enables lightweight source switching through a location-based addressing scheme. Each switch and host can independently derive a source route to reach a destination without requiring the full knowledge of the network topology. We demonstrate that using location-based source routes as forwarding labels allows LESS to eliminate the need for routing tables and integrate with minimum required functionality for packet forwarding. Moreover, we propose a fast rerouting solution to address the issue of fault tolerance in source routing. Each switch can locally derive an alternative source route during a failure. The paper evaluates the performance of LESS. Our evaluation results suggest that LESS improves the performance of datacenter networks in terms of latency, throughput, and reliability.},
keywords={computer centres;fault tolerance;telecommunication network routing;telecommunication network topology;telecommunication switching;fault tolerance;packet forwarding;LESS;lightweight source switching;alternative source route;source routing;routing tables;location-based source routes;network topology;novel Location basEd Source Switching;high-speed datacenter networks;reliable message switching network;Switches;Ports (Computers);Topology;Network topology;Routing;Reliability;IP networks},
doi={10.1109/INFOCOM.2017.8057152},
ISSN={},
month={May},}
@INPROCEEDINGS{8057153,
author={J. Fan and C. Guan and Y. Zhao and C. Qiao},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Availability-aware mapping of service function chains},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Network Function Virtualization (NFV) is a promising technique to greatly improve the effectiveness and flexibility of network services through a process named Service Function Chain (SFC) mapping, with which different network services are deployed over virtualized and shared platforms in data centers. However, such an evolution towards software-defined network functions introduces new challenges to network services which require high availability. One effective way of protecting the network services is to use sufficient redundancy. By doing so, however, the efficiency of physical resources may be greatly decreased. To address such an issue, this paper defines an optimal availability-aware SFC mapping problem and presents a novel online algorithm that can minimize the physical resources consumption while guaranteeing the required high availability within a polynomial time. Simulation results show that our proposed algorithm can significantly improve SFC mapping request acceptance ratio and reduce resource consumption.},
keywords={computer centres;computer network management;virtualisation;availability-aware SFC mapping problem;data centers;resource consumption;Service Function Chain mapping;Network Function Virtualization;service function chains;availability-aware mapping;SFC mapping request acceptance ratio;software-defined network functions;virtualized shared platforms;Redundancy;Delays;Computational modeling;Bandwidth;Conferences;Approximation algorithms},
doi={10.1109/INFOCOM.2017.8057153},
ISSN={},
month={May},}
@INPROCEEDINGS{8057154,
author={H. Saito and H. Honda and R. Kawahara},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Disaster avoidance control against heavy rainfall},
year={2017},
volume={},
number={},
pages={1-9},
abstract={This paper proposes a disaster avoidance control method for use against heavy rainfall and discusses its effectiveness through actual weather data. The proposed control method uses geographical information data including weather data, hazard area data, and physical network data. By applying technologies related to meteorology, erosion control, and civil engineering to such data, the proposed method can evaluate the risk of a physical network being disconnected. On the basis of the evaluated risk, the proposed method reconfigures a logical network to reduce service disruption. The proposed method is applied to a cloud computing service network where, in addition to route changes, the relocation of virtual machines is possible, increasing its effectiveness. By using empirical data, we show that the proposed method reduces the probability of service disconnection to almost zero even for heavy rainfall causing landslides. Finally, an experimental system of the proposed method was implemented through software defined network technology and successfully controlled the experimental network.},
keywords={cloud computing;disasters;emergency management;erosion;geographic information systems;geomorphology;rain;software defined networking;virtual machines;erosion control;civil engineering;logical network;service disruption;service disconnection;software defined network technology;disaster avoidance control method;geographical information data;hazard area data;physical network data;heavy rainfall;weather data;landslides;meteorology;virtual machines relocation;cloud computing service network;Hazards;Measurement;Rain;Control systems;Conferences;Terrain factors},
doi={10.1109/INFOCOM.2017.8057154},
ISSN={},
month={May},}
@INPROCEEDINGS{8057155,
author={M. Ashour and J. Wang and C. Lagoa and N. Aybat and H. Che},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Non-concave network utility maximization: A distributed optimization approach},
year={2017},
volume={},
number={},
pages={1-9},
abstract={This paper proposes an algorithm for optimal decentralized traffic engineering in communication networks. We aim at distributing the traffic among the available routes such that the network utility is maximized. In some practical applications, modeling network utility using non-concave functions is of particular interest, e.g., video streaming. Therefore, we tackle the problem of optimizing a generalized class of non-concave utility functions. The approach used to solve the resulting non-convex network utility maximization (NUM) problem relies on designing a sequence of convex relaxations whose solutions converge to that of the original problem. A distributed algorithm is proposed for the solution of the convex relaxation. Each user independently controls its traffic in a way that drives the overall network traffic allocation to an optimal operating point subject to network capacity constraints. All computations required by the algorithm are performed independently and locally at each user using local information and minimal communication overhead. The only non-local information needed is binary feedback from congested links. The robustness of the algorithm is demonstrated, where the traffic is shown to be automatically rerouted in case of a link failure or having new users joining the network. Numerical simulation results are presented to validate our findings.},
keywords={concave programming;convex programming;distributed algorithms;radio networks;telecommunication network routing;telecommunication traffic;nonconcave network utility maximization;distributed optimization approach;optimal decentralized traffic engineering;communication networks;nonconcave utility functions;distributed algorithm;convex relaxation;network traffic allocation;network capacity constraints;nonconvex network utility maximization problem;convex relaxations;binary feedback;Resource management;Optimization;Quality of service;Distributed algorithms;Conferences;Communication networks;Streaming media;Distributed optimization;non-concave utility maximization;traffic engineering},
doi={10.1109/INFOCOM.2017.8057155},
ISSN={},
month={May},}
@INPROCEEDINGS{8057156,
author={P. Wan and F. Al-dhelaan and H. Yuan and S. Ji},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Fractional wireless link scheduling and polynomial approximate capacity regions of wireless networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Fractional Link scheduling is one of the most fundamental problems in wireless networks. The prevailing approach for shortest fractional link scheduling is based on a reduction to the maximum-weighted independent set problem, which itself may not admit efficient approximation algorithms. In addition, except for the wireless networks under the protocol interference model, none of the existing scheduling algorithms can produce a link schedule with explicit upper bounds on its length in terms of the link demands. As the result, the polynomial approximate capacity regions in these networks remain blank. This paper develops a purely combinatorial paradigm for fractional link scheduling in wireless networks. In addition to the superior efficiency, it is able to provide explicit upper bounds on the lengths of the produced link schedule. By exploiting these upper bounds, polynomial approximate capacity regions are derived. The effectiveness of this new paradigm is demonstrated by its applications in wireless networks under the physical interference model and wireless MIMO networks under the protocol interference model.},
keywords={approximation theory;computational complexity;MIMO communication;polynomial approximation;protocols;radio links;radiofrequency interference;scheduling;set theory;telecommunication scheduling;fractional wireless link scheduling;polynomial approximate capacity regions;shortest fractional link scheduling;maximum-weighted independent set problem;protocol interference model;explicit upper bounds;wireless MIMO networks;combinatorial paradigm;approximation algorithms;Wireless networks;Interference;Schedules;Games;Approximation algorithms;Protocols;Upper bound},
doi={10.1109/INFOCOM.2017.8057156},
ISSN={},
month={May},}
@INPROCEEDINGS{8057157,
author={H. Yu and M. J. Neely},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A new backpressure algorithm for joint rate control and routing with vanishing utility optimality gaps and finite queue lengths},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The backpressure algorithm has been widely used as a distributed solution to the problem of joint rate control and routing in multi-hop data networks. By controlling a parameter V in the algorithm, the backpressure algorithm can achieve an arbitrarily small utility optimality gap. However, this in turn brings in a large queue length at each node and hence causes large network delay. This phenomenon is known as the fundamental utility-delay tradeoff. The best known utility-delay tradeoff for general networks is [O(1/V), O(V)] and is attained by a backpressure algorithm based on a drift-pluspenalty technique. This may suggest that to achieve an arbitrarily small utility optimality gap, the existing backpressure algorithms necessarily yield an arbitrarily large queue length. However, this paper proposes a new backpressure algorithm that has a vanishing utility optimality gap, so utility converges to exact optimality as the algorithm keeps running, while queue lengths are bounded throughout by a finite constant. The technique uses backpressure and drift concepts with a new method for convex programming.},
keywords={data communication;delays;distributed algorithms;optimisation;queueing theory;telecommunication network routing;backpressure algorithm;finite queue lengths;multihop data networks;queue length;joint rate control and routing;utility-delay tradeoff;small utility optimality gap;network delay;drift-pluspenalty technique;convex programming;Routing;Delays;Heuristic algorithms;Approximation algorithms;Conferences;Optimization;Network topology},
doi={10.1109/INFOCOM.2017.8057157},
ISSN={},
month={May},}
@INPROCEEDINGS{8057158,
author={S. Vargaftik and I. Keslassy and A. Orda},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Stable user-defined priorities},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Network providers now want to enable users to define their own flow priorities, and commercial devices already implement this ability. However, it has been shown that directly applying arbitrary user-defined priorities can fundamentally destabilize a network. In this paper, we show that it is possible to apply user-defined priorities while keeping the network stable. We introduce U-BP, a scalable approach that extends backpressure-based scheduling techniques to service user-defined flow priorities and rates while maintaining throughput optimality and strong network performance. We explain how our approach relies on a dual-layer scheme with an exponential convergence to requested priorities. We further prove analytically the network stability of our solution, and show how it achieves a strong performance for high-priority flows.},
keywords={quality of service;radio networks;telecommunication scheduling;network providers;commercial devices;arbitrary user-defined priorities;network stable;scalable approach;user-defined flow priorities;requested priorities;high-priority flows;network performance;backpressure-based scheduling techniques;U-BP;Stability analysis;Heuristic algorithms;Standards;Topology;Delays;Conferences;Throughput},
doi={10.1109/INFOCOM.2017.8057158},
ISSN={},
month={May},}
@INPROCEEDINGS{8057159,
author={J. Liu and M. Chen and S. Chen and Q. Pan and L. Chen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Tag-compass: Determining the spatial direction of an object with small dimensions},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Identifying an object's spatial direction (or orientation) plays a fundamental role in a variety of applications, such as automatic assembly, indoor navigation, and robot driving. In this paper, we design a fine-grained direction finding system called Tag-Compass that attaches a single tag to an object (whose size may be small) and identifies the tagged object's orientation by determining the spatial direction of the tag. We exploit the polarization properties of the RF waves used in the communications between an RFID reader and the tag on the object. Polarization mismatch between the tag and the reader's antenna affects the received signal strength at the reader. From the measured signal strength values, we are able to deduce the tag's direction through a series of transformations and deviation minimization. We propose a system design for Tag-Compass and implement a prototype. We evaluate the performance of TagCompass through extensive experiments using the prototype. The experimental results show that Tag-Compass provides accurate direction estimates with a median error of just 2.5° when the tag's position is known and a median error of 3.8° when the tag's position is unknown.},
keywords={electromagnetic wave polarisation;radiofrequency identification;RSSI;fine-grained direction finding system;Tag-compass;Tag-Compass;spatial direction;tagged object;Robots;Radiofrequency identification;Receivers;Transmitters;Object recognition;Directive antennas;Radio frequency},
doi={10.1109/INFOCOM.2017.8057159},
ISSN={},
month={May},}
@INPROCEEDINGS{8057160,
author={S. Zhang and X. Liu and J. Wang and J. Cao},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Tag size profiling in multiple reader RFID systems},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In this paper, we study the tag size profiling (TSP) problem in RFID systems with multiple readers, which is to estimate the number of tags in every subregion in the system covered by different set of readers. The TSF problem is vitally important to reader scheduling and many other related operations in large scale multi-reader RFID systems. To our knowledge, however, it is not well solved in previous researches. We propose a novel approach to the TSF problem. The key idea is to treat the size of subregions as variables and construct a linear system in these variables to solve them. We theoretically prove that for any multi-reader RFID system, a linear system that can be used to uniquely solve the variables corresponding to the subregion sizes can always be constructed. We then propose a time-efficient algorithm that uses two heuristics to quickly find enough linearly independent equations to construct the linear system. Extensive simulation results show that the proposed approach achieves very high accuracy. When the estimation results of individual readers contain 5% errors, our approach achieves median estimation error of smaller than 0.02 and 90-percentile estimation error of smaller than 0.04 in large systems containing more than one hundred readers.},
keywords={radiofrequency identification;telecommunication scheduling;multiple reader RFID systems;tag size profiling problem;multiple readers;TSF problem;reader scheduling;scale multireader RFID systems;linear system;multireader RFID system;subregion sizes;linearly independent equations;individual readers;TSP;Conferences;Radio Frequency Identification;Multiple Readers;Reader Scheduling;RFID Estimation},
doi={10.1109/INFOCOM.2017.8057160},
ISSN={},
month={May},}
@INPROCEEDINGS{8057161,
author={C. Duan and X. Rao and L. Yang and Y. Liu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Fusing RFID and computer vision for fine-grained object tracking},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In recent years, both the RFID and computer vision technologies have been widely employed in indoor scenarios aimed at different goals while faced with respective limitations. For example, the RFID-based EAS system is useful in quickly identifying tagged objects but the accompanying false alarm problem is troublesome and hard to tackle with except that the accurate trajectory of the target tag can be easily acquired. On the other side, the CV system performs fairly well in tracking multiple moving objects precisely while finding it difficult to screen out the specific target among them. To overcome the above limitations, we present TagVision, a hybrid RFID and computer vision system for fine-grained localization and tracking of tagged objects. A fusion algorithm is proposed to organically combine the position information given by the CV subsystem, and phase data output by the RFID subsystem. In addition, we employ the probabilistic model to eliminate the measurement error caused by thermal noise and device diversity. We have implemented TagVision with COTS camera and RFID devices and evaluated it extensively in our lab environment. Experimental results show that TagVision can achieve 98% blob matching accuracy and 10.33mm location tracking precision.},
keywords={computer vision;image fusion;motion estimation;object detection;object tracking;probability;radiofrequency identification;RFID-based EAS system;hybrid RFID system fusion;computer vision system fusion;probabilistic model;thermal noise;device diversity;COTS camera;blob matching accuracy;fine-grained localization;TagVision;multiple moving objects;CV system;tagged objects;indoor scenarios;fine-grained object tracking;Cameras;Radiofrequency identification;Target tracking;Computer vision;Trajectory;Optical imaging;RFID;computer vision;tracking;TagVision},
doi={10.1109/INFOCOM.2017.8057161},
ISSN={},
month={May},}
@INPROCEEDINGS{8057162,
author={C. Yang and J. Gummeson and A. Sample},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Riding the airways: Ultra-wideband ambient backscatter via commercial broadcast systems},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Communication costs dominate the energy consumption, and ultimately limit the utility, of low power devices and sensor nodes. Backscatter communication based on deliberate and ambient sources has the potential to radically alter this paradigm by offering two to three orders of magnitude better communication efficiency (in terms of nJ/Bit) then conventional radio architectures. Initial work on ambient backscatter shows promising results but has focused on narrow band operation in well controlled laboratory settings. The goal of this work is to enable the ubiquitous deployment of ultra-low power nodes that communicate via ambient backscatter to wired Universal Backscatter Readers, in real-world environments. This is accomplished through ultra-wideband backscatter techniques that leverage the breath of commercial broadcast signals in the 80 MHz to 900 MHz range from FM radios, digital TVs, and cellular networks. Additionally the use of powered Universal Backscatter Readers allows a network of ultra-low power nodes to operate on ambient carriers as low as -80 dBm, which is typical for indoor home and office environments. For the first time we demonstrate the simultaneous use of 17 ambient signal sources to achieve node-to-reader communication distances of 50 meters, with data rates up to 1 kbps.},
keywords={backscatter;cellular radio;digital television;radio broadcasting;Ultra-wideband ambient backscatter;commercial broadcast systems;communication costs;energy consumption;low power devices;sensor nodes;Backscatter communication;ambient sources;initial work;narrow band operation;ultra-low power nodes;wired Universal Backscatter Readers;ultra-wideband backscatter techniques;commercial broadcast signals;ambient carriers;node-to-reader communication distances;radio architectures;universal backscatter readers;ambient signal sources;FM radios;digital TV;cellular networks;frequency 80.0 MHz to 900.0 MHz;Backscatter;Poles and towers;Radio frequency;Frequency modulation;Ultra wideband technology;Digital TV;Ultra-Wideband;Ambient Backscatter Communication;Sensor Node;Energy Harvesting},
doi={10.1109/INFOCOM.2017.8057162},
ISSN={},
month={May},}
@INPROCEEDINGS{8057163,
author={C. Wu and Y. Zhang and L. Zhang and B. Yang and X. Chen and W. Zhu and L. Qiu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={ButterFly: Mobile collaborative rendering over GPU workload migration},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The ever increasing of display resolution on mobile devices raises high demand for GPU rendering details. However, the challenge of poor hardware support but fine-grained rendering details often makes user unsatisfied especially in calling for high frame rate scenarios, e.g., game. To resolve such issue, we propose BUTTERFLY, a novel system which collaboratively utilizes mobile GPUs to process high-quality rendering details for on-the-go mobile users. In particular, ButterFly achieves two technical contributions for the collaborative design: (1) a mobile device can migrate GPU workloads in buffer queue to peers, and (2) the collaborative rendering mechanism benefits user high quality details while significant power saving performance. Both techniques are compatible with the OpenGL ES standards. Furthermore, a 40-person survey perceives that ButterFly can provide excellent user experience of both rendering details and frame rate over Wi-Fi network. In addition, our comprehensive trace-driven experiments on Android prototype reveal the benefits of Butterfly have more superior performance over state-of-the-art systems, which achieves more than 28.3% power saving.},
keywords={Android (operating system);graphics processing units;mobile computing;rendering (computer graphics);screens (display);ButterFly;mobile collaborative rendering;GPU workload migration;display resolution;mobile device;BUTTERFLY;Butterfly;Android prototype;GPU rendering;mobile GPU;Rendering (computer graphics);Graphics processing units;Mobile communication;Collaboration;Androids;Humanoid robots;Hardware},
doi={10.1109/INFOCOM.2017.8057163},
ISSN={},
month={May},}
@INPROCEEDINGS{8057164,
author={C. Pei and Z. Wang and Y. Zhao and Z. Wang and Y. Meng and D. Pei and Y. Peng and W. Tang and X. Qu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Why it takes so long to connect to a WiFi access point},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Today's WiFi networks deliver a large fraction of traffic. However, the performance and quality of WiFi networks are still far from satisfactory. Among many popular quality metrics (throughput, latency), the probability of successfully connecting to WiFi APs and the time cost of the WiFi connection set-up process are the two of the most critical metrics that affect WiFi users' experience. To understand the WiFi connection set-up process in real-world settings, we carry out measurement studies on 5 million mobile users from 4 representative cities associating with 7 million APs in 0.4 billion WiFi sessions, collected from a mobile “WiFi Manager” App that tops the Android/iOS App market. To the best of our knowledge, we are the first to do such large scale study on: how large the WiFi connection set-up time cost is, what factors affect the WiFi connection set-up process, and what can be done to reduce the WiFi connection set-up time cost. Based on our data-driven measurement and analysis, we reveal the insights as follows: (1) Connection set-up failure and large connection set-up time cost are common in today's WiFi use. As large as 45% of the users suffer connection set-up failures, and 15% (5%) of them have large connection set-up time costs over 5 seconds (10 seconds). (2) Contrary to the state-of-the-art work, scan, one of the subphase of four phases in the connection set-up process, contributes the most (47%) to the overall connection set-up time cost. (3) Mobile device model and AP model can greatly help us to predict the connection set-up time cost if we can make good use of the hidden information. Based on the measurement analysis, we develop a machine learning based AP selection strategy that can significantly improve WiFi connection set-up performance, against the conventional strategy purely based on signal strength, by reducing the connection set-up failures from 33% to 3.6% and reducing 80% time costs of the connection set-up processes by more than 10 times.},
keywords={learning (artificial intelligence);mobile radio;wireless LAN;WiFi access point;WiFi networks;mobile WiFi Manager App;WiFi users experience;WiFi connection set-up process;WiFi connection set-up time cost;connection set-up failures;mobile device model;AP model;machine learning based AP selection strategy;time 5.0 s;time 10.0 s;Wireless fidelity;Mobile handsets;IP networks;Authentication;Mobile communication;Switches;Measurement},
doi={10.1109/INFOCOM.2017.8057164},
ISSN={},
month={May},}
@INPROCEEDINGS{8057165,
author={F. Ahmed and J. Erman and Z. Ge and A. X. Liu and J. Wang and H. Yan},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Monitoring quality-of-experience for operational cellular networks using machine-to-machine traffic},
year={2017},
volume={},
number={},
pages={1-9},
abstract={It is crucial for cellular data network operators to understand the service quality perceived by its customers. The state-of-art systems deployed in cellular networks mostly report service quality aggregated on cell site level, which is typically an aggregation of tens or hundreds of customers depending on the locations of the cell sites. In this paper, we propose to enhance the measurement of customer-perceived service quality by leveraging M2M devices as sensors in the field, which provide an unprecedented opportunity for cellular network operators to measure what end-users experience with better accuracy and coverage. Our approach is to identify a set of M2M devices which are stationary and communicate continuously over the cellular network over an indefinite period of time. We use these M2M devices to estimate the customer-perceived service quality during cell site outages. We implement our methodology as a system called M2MScan and evaluate M2MScan with both synthetic outages and real outages from a large-scale operational cellular network. To the best of our knowledge, this is the first work that employs M2M devices to measure the service quality perceived by customers in operational cellular networks at a large scale.},
keywords={cellular radio;customer services;machine-to-machine communication;quality of experience;quality of service;telecommunication traffic;service quality;cell site outages;M2MScan;large-scale operational cellular network;quality-of-experience;machine-to-machine traffic;cellular data network operators;cell site level;M2M devices;Machine-to-machine communications;Cellular networks;Monitoring;Object recognition;Performance evaluation;Sensors;Level measurement},
doi={10.1109/INFOCOM.2017.8057165},
ISSN={},
month={May},}
@INPROCEEDINGS{8057166,
author={L. Xue and X. Ma and X. Luo and L. Yu and S. Wang and T. Chen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Is what you measure what you expect? Factors affecting smartphone-based mobile network measurement},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Many apps have been developed to measure the performance of mobile networks. Unfortunately, their measurement results may not be what users expect, because the results could be biased by various factors and the apps' descriptions may confuse users. Although a few recent studies pointed out several factors, they missed other important factors and lacked of finegrained analysis on the factors and measurement apps. Moreover, none has studied whether or not the descriptions of such apps will mislead users. In this paper, we conduct the first systematic study of the factors that could bias the result from measurement apps and their descriptions. We identify new factors, revisit known factors, and propose a novel approach with new tools to discover these factors in proprietary apps. We also develop a new measurement app named MobiScope for demonstrating how to mitigate the negative effects of these factors. Furthermore, we construct enhanced descriptions for measurement apps to provide users more information about what is measured. The extensive experimental results illustrate the negative effects of various factors, the improvement in performance measurement brought by MobiScope, and the clarity of the enhanced descriptions.},
keywords={smart phones;measurement app;performance measurement;mobile network measurement;MobiScope;Androids;Humanoid robots;Runtime;Delays;Mobile communication;Protocols;Smart phones},
doi={10.1109/INFOCOM.2017.8057166},
ISSN={},
month={May},}
@INPROCEEDINGS{8057167,
author={Z. Wang and Y. Zhang and Y. Li and Q. Wang and F. Xia},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Exploiting social influence for context-aware event recommendation in event-based social networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Event-based Social Networks (EBSNs) which bridge the gap between online and offline interactions among users have received increasing popularity. The unique cold-start nature makes event recommendation more challenging than traditional recommendation problems, since even for two events with the same content, they may not happen at the same time, the same location, or be organized by the same host. Existing event recommendation algorithms mainly exploit the basic context information (e.g., location, time and content), while the social influence of event hosts and group members have been ignored. In this paper, we propose a Social Information Augmented Recommender System (SIARS), which fully exploits the social influence of event hosts and group members together with basic context information for event recommendation. In particular, we combine the information of EBSNs and other social networks to characterize the social influence of event hosts, and take interactions between group members into consideration for event recommendation. In addition, we propose a new content-aware recommendation model using the topic model to find the most similar topic the event belongs to, and a new location-aware recommendation model integrating location popularity with location distribution for event recommendation. Extensive experiments on real-world datasets demonstrate that SIARS outperforms other recommendation algorithms.},
keywords={mobile computing;recommender systems;social networking (online);event hosts;group members;content-aware recommendation model;location-aware recommendation model;social influence;context-aware event recommendation;social networks;event recommendation algorithms;Social Information Augmented Recommender System;context information;recommendation problems;event-based social networks;EBSN;SIARS;Recommender systems;Conferences;Facebook;Twitter;Computer security;Collaboration},
doi={10.1109/INFOCOM.2017.8057167},
ISSN={},
month={May},}
@INPROCEEDINGS{8057168,
author={B. Samanta and A. De and N. Ganguly},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={STRM: A sister tweet reinforcement process for modeling hashtag popularity},
year={2017},
volume={},
number={},
pages={1-9},
abstract={With social media platform such as Twitter becoming the de facto destination for users' views and opinions, it is of great importance to forecast an information outbreak. In Twitter, tweets are often annotated with hashtags to help its users to quickly extract their contents. The existing approaches for modeling the dynamics of tweet-messages are usually limited to individual or simple aggregates of tweets rather than the underlying hashtags. In this paper, we develop, STRM, a novel point process driven model that considers the effect of cross-tweet impact in hashtag popularity. STRM, by assuming hashtag to be a heterogeneous collection of tweet-chains. Through extensive experimentation, we find that our algorithm - STRM, shows consistent performance boosts with six diverse real datasets against several strong baselines. Moreover surprisingly, it also offers significant accuracy gains in popularity-prediction for individual tweets as compared with the existing paradigms.},
keywords={information retrieval;social networking (online);cross-tweet impact;tweet-chains;popularity-prediction;STRM process;tweet-messages;information outbreak;Twitter;social media platform;hashtag popularity;sister tweet reinforcement process;Twitter;Tagging;Computational modeling;Predictive models;Data models;Proposals;History},
doi={10.1109/INFOCOM.2017.8057168},
ISSN={},
month={May},}
@INPROCEEDINGS{8057169,
author={X. Xu and C. Lee and D. Y. Eun},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Challenging the limits: Sampling online social networks with cost constraints},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Graph sampling techniques via random walk crawling have been popular for analyzing statistical characteristics of large online social networks due to simple implementation and provable guarantees on unbiased estimates. Despite the growing popularity, the `cost' of sampling and its true impact on the accuracy of estimates still have not been carefully studied. In addition, the random walk-based methods inherently suffer from the sluggish nature of random walks and the `slow-mixing' structure of social graphs, thereby leading to high correlation in the samples obtained. With these in mind, in this paper, we develop a mathematical framework such that the cost of sampling is properly taken into account, which in turn re-defines a widely used asymptotic variance into a cost-based asymptotic variance. Our new metric enables us to compare a class of sampling policies under the same cost constraint, integrating “random skipping” (bypassing nodes without sampling) into the random walk-based sampling. We obtain an optimal policy striking the right balance between sampling quality (less correlation) and sampling quantity (higher cost per sample), which greatly improves over the usual skip-free crawling-based samplers. We further extend our framework, enabling one to design more sophisticated sampling strategies with an array of control knobs, which all produce unbiased estimates under the same cost constraint.},
keywords={graph theory;information retrieval;network theory (graphs);random processes;sampling methods;social networking (online);sampling strategies;cost-based asymptotic variance;random walk-based sampling;sampling quantity;skip-free crawling-bsaed samplers;online social networks sampling;provable guarantees;simple implementation;statistical characteristics;random walk crawling;graph sampling;sampling quality;random skipping;cost constraint;sampling policies;social graphs;slow-mixing structure;unbiased estimates;Web pages;Crawlers;Social network services;Conferences;Correlation;Measurement;Sampling methods},
doi={10.1109/INFOCOM.2017.8057169},
ISSN={},
month={May},}
@INPROCEEDINGS{8057170,
author={Q. Lin and L. Yang and Y. Liu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={TagScreen: Synchronizing social televisions through hidden sound markers},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Millions of people nowadays share their television (TV) experience with other people through social media like Twitter or Facebook with mobile devices. It is generally believed that TV has been repurposed for social networks, called as social television. A key functionality of social television is that it allows the viewers to interchange their comments through mobile devices, thus creating the impression of watching TV like alongside a group of friends. To do so, mobile devices have to be aware of the current media context (identifier and progress) of what the TV is playing, known as synchronizing context from TVs to mobile devices. Unfortunately, most legacy systems are not able to track the playing progresses or need to upgrade TV devices. To address the issue, we design a purely software-based solution, called as TagScreen, which inserts a series of hidden sound markers into the audio of the content. TagScreen supports longrange or multipath-resistant synchronization at second-level, being independent of device diversity over severely frequency-selective acoustic channels. We implement TagScreen by using COTS TVs and mobile devices. The system has been extensively tested on 150 movies and 150 TV series across five different environments. Results show that TagScreen has a mean recognition accuracy of 98% up to 35m, and a mean tracking accuracy of 97%.},
keywords={mobile handsets;mobile television;social networking (online);social networks;social television;mobile devices;TV devices;TagScreen;hidden sound markers;device diversity;television experience;social media;TV series;synchronizing context;multipath-resistant synchronization;frequency-selective acoustic channels;COTS TVs;TV;Mobile handsets;Media;Social network services;Synchronization;Motion pictures;Frequency modulation},
doi={10.1109/INFOCOM.2017.8057170},
ISSN={},
month={May},}
@INPROCEEDINGS{8057171,
author={Q. Liang and E. Modiano},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Coflow scheduling in input-queued switches: Optimal delay scaling and algorithms},
year={2017},
volume={},
number={},
pages={1-9},
abstract={A coflow is a collection of parallel flows belonging to the same job. It has the all-or-nothing property: a coflow is not complete until the completion of all its constituent flows. In this paper, we focus on optimizing coflow-level delay, i.e., the time to complete all the flows in a coflow, in the context of an N × N input-queued switch. In particular, we develop a throughput-optimal scheduling policy that achieves the best scaling of coflow-level delay as N → ∞. We first derive lower bounds on the coflow-level delay that can be achieved by any scheduling policy. It is observed that these lower bounds critically depend on the variability of flow sizes. Then we analyze the coflow-level performance of some existing coflow-agnostic scheduling policies and show that none of them achieves provably optimal performance with respect to coflow-level delay. Finally, we propose the Coflow-Aware Batching (CAB) policy which achieves the optimal scaling of coflow-level delay under some mild assumptions.},
keywords={computer networks;delays;optimisation;queueing theory;scheduling;telecommunication scheduling;throughput-optimal scheduling policy;coflow-level delay;coflow-level performance;Coflow-Aware Batching policy;Optimal delay scaling;coflow-agnostic scheduling policies;CAB policy;Delays;Scheduling;Optimal scheduling;Processor scheduling;Ports (Computers)},
doi={10.1109/INFOCOM.2017.8057171},
ISSN={},
month={May},}
@INPROCEEDINGS{8057172,
author={W. Wang and S. Ma and B. Li and B. Li},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Coflex: Navigating the fairness-efficiency tradeoff for coflow scheduling},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Fair and efficient coflow scheduling improves application-level networking performance in today's datacenters. Ideally, a coflow scheduler should provide isolation guarantees on the minimum coflow progress to achieve predictable networking performance. Network operators, on the other hand, strive to decrease the average coflow completion time (CCT). Unfortunately, optimal isolation guarantees and minimum average CCT are conflicting objectives and cannot be achieved at the same time. Existing coflow schedulers either optimize isolation guarantees at the expense of long CCTs (e.g., HUG [1]), or decrease the average CCT without performance isolation (e.g., Varys and Aalo [2], [3]). The lack of a smooth tradeoff in between poses a dilemma between low efficiency and no performance isolation. To bridge this gap, we develop a new coflow scheduler, Coflex, to navigate this tradeoff. Coflex allows network operators to specify the desired level of isolation guarantee using a tunable fairness knob, while at the same time decreasing the average CCT. Both our real-world deployments and trace-driven simulations have shown that Coflex offers a smooth tradeoff between fairness and efficiency. At an appropriate tradeoff level, Coflex outperforms fair schedulers by 2 × in minimizing the average CCT.},
keywords={computer centres;computer networks;telecommunication scheduling;Coflex;fairness-efficiency tradeoff;application-level networking performance;coflow scheduler;minimum coflow progress;predictable networking performance;network operators;average coflow completion time;optimal isolation guarantees;minimum average CCT;performance isolation;coflow scheduling;Bandwidth;Resource management;Fabrics;Navigation;Production;Silicon;Conferences},
doi={10.1109/INFOCOM.2017.8057172},
ISSN={},
month={May},}
@INPROCEEDINGS{8057173,
author={C. Wang and S. T. Maguluri and T. Javidi},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Heavy traffic queue length behavior in switches with reconfiguration delay},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Optical switches have been drawing attention due to their large data bandwidth and low power consumption. However, scheduling policies need to account for the schedule reconfiguration delay of optical switches to achieve good performance. The Adaptive MaxWeight policy achieves optimal throughput for switches with nonzero reconfiguration delay, and has been shown in simulation to have good delay performance. In this paper, we analyze the queue length behavior of a switch with nonzero reconfiguration delay operating under the Adaptive MaxWeight. We first show that the Adaptive MaxWeight policy exhibits a weak state space collapse behavior in steady-state, which could be viewed as an inheritance of the MaxWeight policy in a switch with zero reconfiguration delay. We then use the weak state space collapse result to obtain a steady state delay bound under the Adaptive MaxWeight algorithm in heavy traffic by applying a recently developed drift technique. The resulting delay bound is dependent on the expected schedule duration. We then derive the relation between the expected schedule duration and the steady state queue length through drift analysis, and obtain asymptotically tight queue length bounds in the heavy traffic regime.},
keywords={delays;optical switches;queueing theory;telecommunication scheduling;telecommunication traffic;Adaptive MaxWeight policy;weak state space collapse behavior;drift analysis;schedule reconfiguration delay;scheduling policies;optical switches;heavy traffic regime;asymptotically tight queue length;steady state queue length;expected schedule duration;Adaptive MaxWeight algorithm;steady state delay;weak state space collapse result;nonzero reconfiguration delay;queue length behavior;Schedules;Delays;Optical switches;Steady-state;Throughput;Ports (Computers);Queueing analysis},
doi={10.1109/INFOCOM.2017.8057173},
ISSN={},
month={May},}
@INPROCEEDINGS{8057174,
author={S. Yang and B. Lin and P. Tune and J. J. Xu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A simple re-sequencing load-balanced switch based on analytical packet reordering bounds},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Chang et al. proposed the load-balanced switch in their seminal work [1], which has received wide attention due to its inherent scalability properties in both size and speed. These scalability properties continue to be of significant interest due to the relentless exponential growth in Internet traffic. The main drawback of the load-balanced switch is that packets can depart out-of-order from the switch, which can significantly degrade network performance by negatively interacting with TCP congestion control. Hence, a large body of subsequent work has proposed a variety of modifications for ensuring packet ordering, but all the proposed approaches tend to increase packet delay significantly in comparison to the basic load-balanced switch. In this paper, we show that the amount of packet reordering that can occur with the load-balanced switch is actually quite limited, which means that packet reordering can simply be rectified by employing reordering buffers at the switch outputs. In particular, we formally bound the worst-case amount of time that a packet has to wait in these output reordering buffers before it is guaranteed to be ready for in-order departure with high probability, and we prove that this bound is linear with respect to the switch size. This linear bound is significant because previous approaches can add quadratic or cubic delays to the load-balanced switch. In addition, we use a hash-grouping method that further reduces resequencing delays significantly. Although simple and intuitive, our experimental results show that our output packet reordering approach substantially outperforms existing load-balanced switch architectures.},
keywords={Internet;packet switching;probability;resource allocation;telecommunication congestion control;telecommunication traffic;transport protocols;switch size;load-balanced switch architectures;analytical packet;basic load-balanced switch;analytical packet reordering bounds;simple resequencing load-balanced switch;Internet traffic;TCP congestion control;hash-grouping method;Ports (Computers);Optical switches;Delays;Out of order;Semantics;Computer architecture},
doi={10.1109/INFOCOM.2017.8057174},
ISSN={},
month={May},}
@INPROCEEDINGS{8057175,
author={J. Lin and M. Li and D. Yang and G. Xue and J. Tang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Sybil-proof incentive mechanisms for crowdsensing},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The rapid growth of sensor-embedded smartphones has led to a new data sensing and collecting paradigm, known as crowdsensing. Many auction-based incentive mechanisms have been proposed to stimulate smartphone users to participate in crowdsensing. However, none of them have taken into consideration the Sybil attack where a user illegitimately pretends multiple identities to gain benefits. This attack may undermine existing inventive mechanisms. To deter the Sybil attack, we design Sybil-proof auction-based incentive mechanisms for crowdsensing in this paper. We investigate both the single-minded and multi-minded cases and propose SPIM-S and SPIM-M, respectively. SPIM-S achieves computational efficiency, individual rationality, truthfulness, and Sybil-proofness. SPIM-M achieves individual rationality, truthfulness, and Sybil-proofness. We evaluate the performance and validate the desired properties of SPIM-S and SPIM-M through extensive simulations.},
keywords={incentive schemes;mobile computing;smart phones;Sybil-proofness;SPIM-S;SPIM-M;Sybil-proof incentive mechanisms;crowdsensing;smartphone users;Sybil attack;inventive mechanisms;sensor-embedded smart phones;data sensing paradigm;data collecting paradigm;Sybil-proof auction-based incentive mechanisms;Sensors;Smart phones;Cost function;Conferences;Electronic mail;Computational modeling},
doi={10.1109/INFOCOM.2017.8057175},
ISSN={},
month={May},}
@INPROCEEDINGS{8057176,
author={C. Liu and S. Wang and L. Ma and X. Cheng and R. Bie and J. Yu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Mechanism design games for thwarting malicious behavior in crowdsourcing applications},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Crowdsourcing applications are vulnerable to malicious behaviors, posing serious threats to their adoption and large deployment. Based on the notion that the requestor (i.e., the crowdsourcer) can block malicious behaviors via leveraging the market power through task allocation and pricing, we propose two novel frameworks based on the mechanism design game theory (i.e., the reverse game theory). To the best of our knowledge, we are the first to exploit the market power and to apply the mechanism design game theory in thwarting malicious behaviors in crowdsourcing. The first proposed framework is built on a requestor-dominant mechanism design game (Rd-MDG), where the game rule is determined solely by the requestor. The second proposed framework is based on the worker-assisted mechanism design game (WaMDG), where the worker (i.e., the contributor) can assist the requestor to determine the game rules by offering advices. These two frameworks have the following salient features: i) neither of them requires the workers to reveal their private information; ii) the game rules of each framework are designed to be able to force the workers to calculate their best strategies based on their actual private information; iii) our theoretical analysis shows that equilibriums exist for both frameworks; and iv) our extensive simulation results demonstrate that these two frameworks can thwart malicious behaviors by driving the workers with a higher attack intent into obtaining lower utilities.},
keywords={crowdsourcing;data privacy;game theory;security of data;malicious behavior;crowdsourcing applications;reverse game theory;requestor-dominant mechanism design game;game rule;Rd-MDG;worker-assisted mechanism design game;WaMDG;private information;Games;Crowdsourcing;Game theory;Resource management;Electronic mail;Conferences},
doi={10.1109/INFOCOM.2017.8057176},
ISSN={},
month={May},}
@INPROCEEDINGS{8057177,
author={F. Qiu and Z. He and L. Kong and F. Wu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={MAGIK: An efficient key extraction mechanism based on dynamic geomagnetic field},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Secret key establishment is a fundamental requirement for private communication between two wireless entities. An intriguing solution is to extract secret keys from the inherent randomness shared between them. Although several works have been done to extract secret keys from different kinds of mediums (e.g., RSSI, CSI, CIR), the efficiency and security problems are not fully solved. In this paper, we consider the problem of secret key establishment for wireless devices, and propose MAGIK, a secure and efficient scheme based on dynamic geoMAGnetic field in Indoor environment for Key establishment. We carefully study the feasibility of utilizing indoor geomagnetic field for key extraction through extensive measurements. Our results demonstrate that geomagnetic field has several dynamic properties, including space-varying, time-varying, sensitive to measurement device, and correlative between two observed points in proximity. We also optimize the key extraction process and present two rotation-angle-based quantification methods, which can achieve faster key generation rates and lower bit mismatching ratios. Besides, we build a prototype on commodity mobile devices, and evaluate its performance by conducting real-word experiments in indoor scenarios. The experiment results confirm that our system is efficient, in terms of key extraction rate, and robust in secret key establishment without requiring additional overhead on mobile devices.},
keywords={private key cryptography;quantum cryptography;telecommunication security;wireless channels;MAGIK;key extraction mechanism;key generation rates;secure scheme;secret keys;dynamic geomagnetic field;secret key establishment;key extraction rate;indoor geomagnetic field;Mobile handsets;Wireless communication;Communication system security;Magnetometers;Wireless sensor networks;Security;Extraterrestrial measurements},
doi={10.1109/INFOCOM.2017.8057177},
ISSN={},
month={May},}
@INPROCEEDINGS{8057178,
author={B. Wang and W. Song and W. Lou and Y. T. Hou},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Privacy-preserving pattern matching over encrypted genetic data in cloud computing},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Personalized medicine performs diagnoses and treatments according to the DNA information of the patients. The new paradigm will change the health care model in the future. A doctor will perform the DNA sequence matching instead of the regular clinical laboratory tests to diagnose and medicate the diseases. Additionally, with the help of the affordable personal genomics services such as 23andMe, personalized medicine will be applied to a great population. Cloud computing will be the perfect computing model as the volume of the DNA data and the computation over it are often immense. However, due to the sensitivity, the DNA data should be encrypted before being outsourced into the cloud. In this paper, we start from a practical system model of the personalize medicine and present a solution for the secure DNA sequence matching problem in cloud computing. Comparing with the existing solutions, our scheme protects the DNA data privacy as well as the search pattern to provide a better privacy guarantee. We have proved that our scheme is secure under the well-defined cryptographic assumption, i.e., the sub-group decision assumption over a bilinear group. Unlike the existing interactive schemes, our scheme requires only one round of communication, which is critical in practical application scenarios. We also carry out a simulation study using the real-world DNA data to evaluate the performance of our scheme. The simulation results show that the computation overhead for real world problems is practical, and the communication cost is small. Furthermore, our scheme is not limited to the genome matching problem but it applies to general privacy preserving pattern matching problems which is widely used in real world.},
keywords={biology computing;cloud computing;cryptography;data privacy;diseases;DNA;genetics;genomics;health care;pattern matching;genome matching problem;privacy-preserving pattern matching;encrypted genetic data;cloud computing;personalized medicine;DNA information;health care model;regular clinical laboratory tests;secure DNA sequence matching problem;DNA data privacy;privacy guarantee;DNA data;personal genomics services;DNA;Cloud computing;Testing;Computational modeling;Encryption},
doi={10.1109/INFOCOM.2017.8057178},
ISSN={},
month={May},}
@INPROCEEDINGS{8057179,
author={Q. Yin and J. Kaur and F. D. Smith},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={TCP Rapid: From theory to practice},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Delay and rate-based alternatives to TCP congestion-control have been around for nearly three decades and have seen a recent surge in interest. However, such designs have faced significant resistance in being deployed on a wide-scale across the Internet - this has been mostly due to serious concerns about noise in delay measurements, pacing inter-packet gaps, and/or required changes to the standard TCP stack/headers. With the advent of high-speed networking, some of these concerns become even more significant. In this paper, we consider Rapid, a recent proposal for ultra-high speed congestion control, which perhaps stretches each of these challenges to the greatest extent. Rapid adopts a framework of continuous fine-scale bandwidth probing, which requires a potentially different and finely-controlled gap for every packet, high-precision timestamping of received packets, and reliance on fine-scale changes in inter-packet gaps. While simulation-based evaluations of Rapid show that it has outstanding performance gains along several important dimensions, these will not translate to the real-world unless the above challenges are addressed. We design a Linux implementation of Rapid after carefully considering each of these challenges. Our evaluations on a 10Gbps testbed confirm that the implementation can indeed achieve the claimed performance gains, and that it would not have been possible unless each of the above challenges was addressed.},
keywords={Internet;Linux;telecommunication congestion control;transport protocols;TCP Rapid;TCP congestion-control;delay measurements;required changes;standard TCP stack/headers;high-speed networking;greatest extent;continuous fine-scale bandwidth probing;finely-controlled gap;high-precision timestamping;fine-scale changes;Internet;interpacket gaps;ultrahigh speed congestion control;Linux;bit rate 10 Gbit/s;Bandwidth;Protocols;Probes;Noise measurement;Delays;Performance gain;Loss measurement},
doi={10.1109/INFOCOM.2017.8057179},
ISSN={},
month={May},}
@INPROCEEDINGS{8057180,
author={T. Bonald and J. Roberts and C. Vitale},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Convergence to multi-resource fairness under end-to-end window control},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The paper relates to multi-resource sharing between flows with heterogeneous requirements as arises in networks with wireless links or software routers implementing network function virtualization. Bottleneck max fairness (BMF) is a sharing objective in this context with good performance. The paper shows that BMF results when local fairness is imposed at each resource while flow rates are controlled by an end-to-end window. We analytically prove convergence to BMF under a fluid model when flows share a network limited to 2 resources while numerical results confirm BMF convergence for larger networks. Simulation results illustrate the impact of packetized transmission.},
keywords={Internet;radio links;telecommunication network routing;virtualisation;end-to-end window control;multiresource fairness;bottleneck max fairness;network function virtualization;software routers;wireless links;multiresource sharing;Resource management;Convergence;Mathematical model;Wireless communication;Bit rate},
doi={10.1109/INFOCOM.2017.8057180},
ISSN={},
month={May},}
@INPROCEEDINGS{8057181,
author={D. Shan and F. Ren},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Improving ECN marking scheme with micro-burst traffic in data center networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In data centers, micro-burst is a common traffic pattern. The packet dropping caused by it usually leads to serious performance degradations. Therefore, much attention has been paid to avoiding buffer overflow caused by micro-burst traffic. In particular, ECN is widely used in data centers to keep persistent queue occupancy low, so that enough buffer space can be available as headroom to absorb micro-burst traffic. However, we find that instantaneous-queue-length-based ECN may cause problems in another direction - buffer underflow. Specifically, current ECN marking scheme in data centers is easy to trigger spurious congestion signals, which may result in overreaction of senders and queue length oscillations in switches. Since ECN threshold is low, the buffer may underflow and link capacity is not fully used. In this paper, we reveal this problem by experiments and simulations. Besides, we theoretically deduce the amplitude of queue length oscillations. The analysis result shows that overreaction of senders is caused by ECN mis-marking. Therefore, we propose Combined Enqueue and Dequeue Marking (CEDM), which can mark packets more accurately. Through simulations, we show that CEDM can greatly reduce throughput loss and improve flow completion time.},
keywords={computer centres;computer networks;queueing theory;telecommunication congestion control;telecommunication switching;telecommunication traffic;transport protocols;microburst traffic;data center networks;common traffic pattern;buffer space;instantaneous-queue-length;buffer underflow;queue length oscillations;ECN threshold;ECN mis-marking;buffer avoidance;low persistent queue occupancy;ECN marking scheme;combined enqueue and dequeue marking scheme;CEDM scheme;Throughput;Oscillators;Integrated circuits;Conferences;Proposals;Delays;ECN marking;Micro-burst traffic;Large Segment Offload;Interrupt Coalescing},
doi={10.1109/INFOCOM.2017.8057181},
ISSN={},
month={May},}
@INPROCEEDINGS{8057182,
author={J. Zhao and J. Liu and H. Wang and C. Xu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Multipath TCP for datacenters: From energy efficiency perspective},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Nearly 50% of the energy overhead in today's datacenters comes from host-to-host data transfers, which largely depend on the transport layer performance. Multipath TCP (MPTCP) has recently been suggested as a promising transport protocol to improve datacenter network throughput, yet it also increases the host CPU power consumption. It remains unclear whether datacenters can indeed benefit from using MPTCP from the perspective of energy efficiency. By analyzing the performance of MPTCP, we show that (1) despite consuming higher host CPU power than TCP, MPTCP can largely reduce the long flow completion time and thus save the aggregated energy; (2) link-sharing subflows in MPTCP not only has negative impact on both throughput-sensitive long flows and latency-sensitive short flows, but also noticeably increases the host CPU power, especially for short flows. We present MPTCP-D, an energy-efficient variant of multipath TCP for datacenters. MPTCP-D incorporates a novel congestion control algorithm that can provide energy efficiency by minimizing the flow completion time, and an extra subflow elimination mechanism that can preclude link-sharing subflows from increasing the host CPU power. We implement MPTCP-D in the Linux kernel and study its performance by experiments on Amazon EC2. Our results show that, without degrading the performance of the long flow throughput and short flow completion time, MPTCP-D reduces the long flow energy consumption by up to 72% compared to DCTCP for data transfers, and reduces the short flow power consumption by up to 46% compared to MPTCP with linksharing subflows.},
keywords={computer centres;computer networks;energy conservation;Linux;power consumption;telecommunication congestion control;telecommunication power management;transport protocols;Linux kernel;transport protocol;short flow power consumption;long flow energy consumption;short flow completion time;long flow throughput;MPTCP-D;energy-efficient variant;latency-sensitive short flows;throughput-sensitive long flows;link-sharing subflows;aggregated energy;long flow completion time;datacenter network throughput;transport layer performance;host-to-host data transfers;energy overhead;energy efficiency perspective;multipath TCP;Throughput;Energy consumption;Data transfer;Servers;Power demand;Transport protocols;Interference},
doi={10.1109/INFOCOM.2017.8057182},
ISSN={},
month={May},}
@INPROCEEDINGS{8057183,
author={J. Palacios and P. Casari and J. Widmer},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={JADE: Zero-knowledge device localization and environment mapping for millimeter wave systems},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Device localization is a highly important functionality for a range of applications. It is particularly beneficial in mmWave networks, where it can be used to reduce the beam training overhead and anticipate handovers between access points. In this paper, we present JADE, an algorithm that estimates the location of a mobile user in an indoor space without any knowledge about the surrounding environment (floor plan, location of walls and presence of reflective surfaces) or about the location and number of access points available therein. JADE leverages the beam procedure used in pre-standard and commercial mmWave equipment to estimate the angle-of-arrival of multipath components of the signal sent by visible access points. This information is then employed to localize the mobile user, estimate the position of access points and finally form a map of the environment. No radar-like ranging operations are required for this. Our results demonstrate that JADE can localize a user with sub-meter accuracy in the broad majority of the cases, and that it even outperforms localization algorithms that require full knowledge of the environment and access point positions.},
keywords={array signal processing;direction-of-arrival estimation;indoor radio;mobility management (mobile radio);multi-access systems;multipath channels;environment mapping;millimeter wave systems;mmWave networks;beam training;JADE;mobile user;indoor space;beam procedure;visible access points;radar-like ranging operations;Zero-knowledge device localization;multipath components;angle-of-arrival;joint anchor and device location estimation;Signal processing algorithms;Phased arrays;Mobile communication;Simultaneous localization and mapping;Estimation;Algorithm design and analysis;Millimeter wave;localization;indoor;virtual anchors;mobility;simulation},
doi={10.1109/INFOCOM.2017.8057183},
ISSN={},
month={May},}
@INPROCEEDINGS{8057184,
author={Z. Zhao and J. Wang and X. Zhao and C. Peng and Q. Guo and B. Wu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={NaviLight: Indoor localization and navigation under arbitrary lights},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Thanks to the highly-dense lighting infrastructure in public areas, visible light emerges as a promising means to indoor localization and navigation. State-of-the-art techniques generally require customized hardware (sensing boards), and mainly work with one single light source (e.g., customized LEDs). This greatly limits their application scope. In this paper, we propose NaviLight, a generic indoor localization and navigation framework based on existing lighting infrastructure with any unmodified light sources (e.g., LED, fluorescent, and incandescent lights). NaviLight simply adopts commercial off-the-shelf mobile phones as receivers, and light intensity values as location signatures. Unlike existing WiFi systems, a single light intensity value is not discriminative enough over space though the light intensity field does vary, which makes our design more challenging. We thus propose a LightPrint as a location signature using a vector of multiple light intensity values obtained during user's walks. Such LightPrints are created by leveraging any user movement (of varying distance and direction) in order to minimize user efforts. A set of techniques are proposed to achieve quick LightPrint matching, which includes a coarse-grained classification and a fine-grained matching over dynamic time warping. We have implemented NaviLight to provide real-time service on Android phones in three typical indoor environments, covering a total area size over 1000m<sup>2</sup>. Our experiments show that NaviLight can achieve sub-meter localization accuracy to meet practical engineering requirements.},
keywords={indoor communication;indoor radio;light sources;lighting;navigation;smart phones;wireless LAN;light intensity field;location signature;multiple light intensity;NaviLight;highly-dense lighting infrastructure;visible light;sensing boards;generic indoor localization;navigation framework;incandescent lights;indoor environments;light source;lighting infrastructure;mobile phones;WiFi systems;light intensity value;LightPrint matching;LED;size 1000.0 m;Light emitting diodes;Navigation;Indoor environments;Light sources;High intensity discharge lamps;Sensors},
doi={10.1109/INFOCOM.2017.8057184},
ISSN={},
month={May},}
@INPROCEEDINGS{8057185,
author={X. Chen and C. Ma and M. Allegue and X. Liu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Taming the inconsistency of Wi-Fi fingerprints for device-free passive indoor localization},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Device-free Passive (DfP) indoor localization releases the users from the burden of wearing sensors or carrying smartphones. Instead of locating devices, DfP technology directly locates human bodies. This promising technology upgrades and even redefines many services, such as intruder alarm, fire rescue, fall detection, baby monitoring, etc. Using Wi-Fi based fingerprints, DfP approaches can achieve a nearly perfect accuracy with a resolution less than one meter. However, Wi-Fi localization profiles may easily drift with a minor environment change, resulting in an inconsistency between fingerprints and new profiles. This inconsistency issue could lead to large errors, and may quickly ruin the whole system. To address this issue, we propose a approach named AutoFi to automatically calibrate the localization profiles in an unsupervised manner. AutoFi embraces a new technique that online estimates and cancels profile contaminants introduced by environment changes. It applies an autoencoder to preserve critical features of fingerprints, and reproduces them later in new localization profiles. Experiment results demonstrate that AutoFi indeed rescues the Wi-Fi fingerprints from variations in the surrounding. The localization accuracy is improved from 18.8% (before auto-calibration) to 84.9% (after auto-calibration).},
keywords={fingerprint identification;indoor radio;telecommunication security;wireless LAN;profile contaminants;AutoFi;inconsistency issue;environment change;Wi-Fi localization profiles;Wi-Fi based fingerprints;fire rescue;intruder alarm;DfP technology;device-free passive indoor localization;Wi-Fi fingerprints;localization accuracy;baby monitoring;direct human bodies localization;Wireless fidelity;Antenna arrays;Databases;Wireless communication;Signal resolution;OFDM;Array signal processing},
doi={10.1109/INFOCOM.2017.8057185},
ISSN={},
month={May},}
@INPROCEEDINGS{8057186,
author={R. Gao and B. Zhou and F. Ye and Y. Wang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Knitter: Fast, resilient single-user indoor floor plan construction},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Lacking of floor plans is a fundamental obstacle to ubiquitous indoor location-based services. Recent work have made significant progress to accuracy, but they largely rely on slow crowdsensing that may take weeks or even months to collect enough data. In this paper, we propose Knitter that can generate accurate floor maps by a single random user's one hour data collection efforts. Knitter extracts high quality floor layout information from single images, calibrates user trajectories and filters outliers. It uses a multi-hypothesis map fusion framework that updates landmark positions/orientations and accessible areas incrementally according to evidences from each measurement. Our experiments on 3 different large buildings and 30+ users show that Knitter produces correct map topology, and 90-percentile landmark location and orientation errors of 3 ~ 5m and 4 ~ 6°, comparable to the state-of-the-art at more than 20× speed up: data collection can finish in about one hour even by a novice user trained just a few minutes.},
keywords={calibration;construction;floors;planning;buildings;crowdsensing;floor maps;ubiquitous indoor location-based services;floor plans;single-user indoor floor plan construction;data collection;correct map topology;multihypothesis map fusion framework;filters outliers;high quality floor layout information;Knitter;time 1.0 hour;size 5.0 m;Trajectory;Calibration;Geometry;Gyroscopes;Layout;Data collection;Cleaning},
doi={10.1109/INFOCOM.2017.8057186},
ISSN={},
month={May},}
@INPROCEEDINGS{8057187,
author={R. Naves and H. Khalifé and G. Jakllari and V. Conan and A. Beylot},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A framework for evaluating physical-layer network coding gains in multi-hop wireless networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We investigate the potential gains of Physical-Layer Network Coding (PLNC) in multi-hop wireless networks. Physical-Layer Network Coding was first introduced as a solution to increase the throughput of a two-way relay channel communication. Unlike most wireless communications techniques which try to avoid collisions, PLNC allows two simultaneous transmissions to a common receiver. Such transmitted messages are summed at signal level and then decoded at packet level. In basic topologies, Physical-Layer Network Coding has been shown to significantly enhance the throughput performance compared to classical communications. However, the impact of PLNC in large multi-hop networks remains an open question. We therefore exploit Linear Programming to evaluate the impact of this paradigm in large realistic radio deployments. Our numerical results show that PLNC can increase the throughput in large multi-hop topologies by 30%. Such gains set theoretical benchmarks for designing new access methods and routing protocols to efficiently exploit the Physical-Layer Network Coding concept.},
keywords={linear programming;network coding;relay networks (telecommunication);routing protocols;telecommunication network topology;wireless channels;multihop wireless networks;PLNC;wireless communications techniques;multihop topologies;Physical-Layer Network Coding concept;two-way relay channel communication;throughput performance enhancement;linear programming;large realistic radio deployments;routing protocols;Network coding;Throughput;Network topology;Topology;Spread spectrum communication;Wireless communication;Computational modeling},
doi={10.1109/INFOCOM.2017.8057187},
ISSN={},
month={May},}
@INPROCEEDINGS{8057188,
author={A. Zhou and X. Zhang and H. Ma},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Beam-forecast: Facilitating mobile 60 GHz networks via model-driven beam steering},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Low robustness under mobility is the Achilles' heel of the emerging 60 GHz networking technology. Instead of using omni-directional antennas as in existing Wi-Fi/cellular networks, 60 GHz radios communicate via highly-directional links formed by phased-array beam-forming, so as to upgrade wireless link throughput to multi-Gbps. However, user motion causes misalignment between the Tx's and Rx's beam directions, and often leads to link outage. Legacy 60 GHz protocols realign the beams by scanning alternative Tx/Rx beams. But unfortunately this tedious process can easily overwhelm the useful channel time, leaving the Tx/Rx in misalignment most of the time during mobility. In this paper, we propose Beam-forecast, a novel model-driven beam steering approach that can sustain high performance for mobile 60 GHz links. Beam-forecast is built on the observation that 60 GHz channel profiles at nearby locations are highly-correlated. By exploiting this correlation, Beam-forecast can reconstruct the channel profile as the Tx/Rx moves, without explicit channel scanning. In this way, it can predict new optimal beams and realign links for mobile users with minimal overhead. We evaluate Beam-forecast using a reconfigurable 60 GHz testbed along with a trace-driven simulator. Our experiments demonstrate multi-fold throughput gain compared with state-of-the-art under various practical scenarios.},
keywords={array signal processing;beam steering;cellular radio;directive antennas;mobile radio;protocols;radio links;wireless LAN;channel scanning;wireless link;omnidirectional antennas;cellular networks;mobile links;model-driven beam steering;Wi-Fi;networking technology;channel profiles;beam directions;highly-directional links;mobile 60 GHz networks;optimal beams;beam steering approach;Beam-forecast;alternative Tx/Rx beams;legacy 60 GHz protocols;phased-array beam-forming;frequency 60.0 GHz;Correlation;Mobile communication;Horn antennas;Array signal processing;Antenna measurements;Beam steering;Wireless communication},
doi={10.1109/INFOCOM.2017.8057188},
ISSN={},
month={May},}
@INPROCEEDINGS{8057189,
author={Q. Chen and H. Gao and Y. Li and S. Cheng and J. Li},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Edge-based beaconing schedule in duty-cycled multihop wireless networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Beaconing is a fundamental networking service where each node broadcasts a packet to all its neighbors locally. Unfortunately, the problem Minimum Latency Beaconing Schedule (MLBS) in duty-cycled scenarios is not well studied. Existing works always have rigid assumption that each node is only active once per working cycle. Aiming at making the work more practical and general, MLBS problem in duty-cycled network where each node is allowed to active multiple times in each working cycle (MLBSDCA for short) is investigated in this paper. Firstly, a modified first-fit coloring based algorithm is proposed for MLBSDCA under protocol interference model. After that, a (ρ + 1)2*|W|-approximation algorithm is proposed to further reduce the beaconing latency, where ρ denotes the interference radius, and |W| is the maximum number of active time slots per working cycle. When ρ and |W| is equal to 1, the approximation ratio is only 4, which is better than the one (i.e., 10) in existing works. Furthermore, two approximation algorithms for MLBSDCA under physical interference model are also investigated. The theoretical analysis and experimental results demonstrate the efficiency of the proposed algorithms in term of latency.},
keywords={approximation theory;graph colouring;protocols;radiofrequency interference;telecommunication scheduling;wireless sensor networks;multihop wireless networks;fundamental networking service;duty-cycled scenarios;duty-cycled network;active multiple times;MLBSDCA;first-fit coloring based algorithm;active time slots;edge-based beaconing schedule;Minimum Latency Beaconing Schedule;protocol interference model;physical interference model;Interference;Approximation algorithms;Protocols;Schedules;Scheduling algorithms;Algorithm design and analysis;Scheduling},
doi={10.1109/INFOCOM.2017.8057189},
ISSN={},
month={May},}
@INPROCEEDINGS{8057190,
author={S. Saha and M. C. Chan},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Design and application of a many-to-one communication protocol},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In this paper, we address the fundamental problem of improving the performance of many-to-one and many-to-many communications. Our approach is Time Division Multiple Access (TDMA) based but addresses the limitations of existing TDMA implementations in a novel way. In a nutshell, we combine packets from many senders into a single large packet transmission by exploiting capture effect achieved through fine grained power control at the level of segments within a single packet. We applied our technique to the design of a one-hop, many-to-one communication protocol, SyncMerge, and a multi-hop, many-to-many communication protocol, ByteCast. Our evaluation shows that SyncMerge is able to achieve 2 to 15 times improvement over traditional many-to-one communication schemes. In addition, ByteCast is able to disseminate 1 byte of data from every node to all other nodes in about 600 ms with 99.5% reliability on a 90 node testbed. Compared to the state-of-the-art protocols such as LWB and Chaos, ByteCast is able to reduce the radio-on time by up to 90% while achieving similar reliability.},
keywords={power control;telecommunication network topology;time division multiple access;state-of-the-art protocols;Time Division Multiple Access;single large packet transmission;fine grained power control;communication schemes;many-to-one communication protocol;many-to-many communication;TDMA implementations;SyncMerge protocol;ByteCast protocol;memory size 1.0 Byte;Protocols;Time division multiple access;Power control;Synchronization;Reliability;Conferences;Chaos},
doi={10.1109/INFOCOM.2017.8057190},
ISSN={},
month={May},}
@INPROCEEDINGS{8057191,
author={H. Dai and B. Liu and H. Yuan and P. Crowley and J. Lu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Analysis of tandem PIT and CS with non-zero download delay},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Collapsed forwarding has long been used in cache systems to reduce the load on servers by aggregating requests for the same content. Named Data Networking (NDN) as a future Internet architecture incorporates this technique through a data structure called Pending Interest Table (PIT). The request aggregation feature suggests that PIT can be viewed as a nonreset time-to-live (TTL) based cache. The Content Store (CS) is a content cache placed in front of the PIT on the NDN forwarding path, so they make up a tandem cache network. To investigate the metrics of interest in this network, like the hit probability for the PIT and the CS, the expected PIT size, non-zero download delay (non-ZDD) should be taken into consideration. Caching policies usually assume zero download delay (ZDD), i.e., request and object arrive simultaneously, and numerous analytical methods have been proposed to study the ZDD caching policies. In this paper, after dissecting the LRU policy, we for the first time propose two LRU variants considering non-ZDD by defining separate operations for the request and object arrivals. When CS adopts the proposed LRU variants, the analysis of the CS-PIT network can still take advantage of the existing models, so the metrics of interest can be computed. Especially, the distribution for the “inter-miss” time of this network can be derived, which has not been achieved by prior works. Finally, the analytical results are verified through simulations.},
keywords={cache storage;data structures;Internet;future Internet architecture;data structure;Pending Interest Table;request aggregation feature;Content Store;content cache;NDN forwarding path;tandem cache network;nonzero download delay;nonZDD;zero download delay;ZDD caching policies;LRU policy;LRU variants;CS-PIT network;cache systems;Named Data Networking;nonreset time-to-live cache;TTL based cache;inter-miss time;Delays;Computational modeling;Random variables;Conferences;Data structures;Analytical models},
doi={10.1109/INFOCOM.2017.8057191},
ISSN={},
month={May},}
@INPROCEEDINGS{8057192,
author={G. Neglia and D. Carra and P. Michiardi},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Cache policies for linear utility maximization},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Cache policies to minimize the content retrieval cost have been studied through competitive analysis when the miss costs are additive and the sequence of content requests is arbitrary. More recently, a cache utility maximization problem has been introduced, where contents have stationary popularities and utilities are strictly concave in the hit rates. This paper bridges the two formulations, considering linear costs and content popularities. We show that minimizing the retrieval cost corresponds to solving an online knapsack problem, and we propose new dynamic policies inspired by simulated annealing, including DynqLRU, a variant of qLRU. For such policies we prove asymptotic convergence to the optimum under the characteristic time approximation. In a real scenario, popularities vary over time and their estimation is very difficult. DynqLRU does not require popularity estimation, and our realistic, trace-driven evaluation shows that it significantly outperforms state-of-the-art policies, with up to 45% cost reduction.},
keywords={cache storage;information retrieval;knapsack problems;simulated annealing;cache utility maximization problem;linear costs;online knapsack problem;DynqLRU;cost reduction;retrieval cost;trace-driven evaluation;simulated annealing;content requests;miss costs;content retrieval cost;linear utility maximization;cache policies;Simulated annealing;Estimation;Approximation algorithms;Space exploration;Detectors;Conferences;Heuristic algorithms},
doi={10.1109/INFOCOM.2017.8057192},
ISSN={},
month={May},}
@INPROCEEDINGS{8057193,
author={M. Zhang and V. Lehman and L. Wang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Scalable name-based data synchronization for named data networking},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In Named Data Networking (NDN), data synchronization plays an important role similar to transport protocols in IP. Many distributed applications, including pub-sub applications such as news and weather services, require a synchronization protocol where each consumer can subscribe to a different subset of a producer's data streams. However, existing Sync protocols support only full-data synchronization, which is a special case of this problem. We propose PSync to efficiently address different types of data synchronization. Names are used in PSync messages to carry producers' latest namespace information and each consumer's subscription information, which allows producers to maintain a single state for all consumers and enables consumers to synchronize with any producer that replicates the same data. We have implemented PSync in the NDN codebase and used it to develop a prototype pub-sub module for building management. Our experimental results show that PSync scales well as the number of consumers, subscriptions, and data streams increases and it outperforms the state-of-the-art Sync protocol in supporting full-data synchronization.},
keywords={Internet;synchronisation;telecommunication traffic;transport protocols;named data networking;synchronization protocol;Sync protocols support;full-data synchronization;data streams increases;state-of-the-art Sync protocol;scalable name-based data synchronization;transport protocols;PSync messages;NDN codebase;Synchronization;Protocols;Arrays;Distributed databases;Conferences;Buildings},
doi={10.1109/INFOCOM.2017.8057193},
ISSN={},
month={May},}
@INPROCEEDINGS{8057194,
author={J. Choi and S. Moon and J. Woo and K. Son and J. Shin and Y. Yi},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Rumor source detection under querying with untruthful answers},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Social networks are the major routes for most individuals to exchange their opinions about new products, social trends and political issues via their interactions. It is often of significant importance to figure out who initially diffuses the information, i.e., finding a rumor source or a trend setter. It is known that such a task is highly challenging and the source detection probability cannot be beyond 31% for regular trees, if we just estimate the source from a given diffusion snapshot. In practice, finding the source often entails the process of querying that asks “Are you the rumor source?” or “Who tells you the rumor?” that would increase the chance of detecting the source. In this paper, we consider two kinds of querying: (a) simple batch querying and (b) interactive querying with direction under the assumption that queriees can be untruthful with some probability. We propose estimation algorithms for those queries, and quantify their detection performance and the amount of extra budget due to untruthfulness, analytically showing that querying significantly improves the detection performance. We perform extensive simulations to validate our theoretical findings over synthetic and real-world social network topologies.},
keywords={probability;query processing;social networking (online);social sciences computing;social trends;political issues;source detection probability;simple batch querying;detection performance;rumor source detection;untruthful answers;social networks;diffusion snapshot;social network topologies;Network topology;Topology;Conferences;Computational modeling;Internet;Maximum likelihood estimation},
doi={10.1109/INFOCOM.2017.8057194},
ISSN={},
month={May},}
@INPROCEEDINGS{8057195,
author={S. Achleitner and T. L. Porta and P. McDaniel and S. V. Krishnamurthy and A. Poylisher and C. Serban},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Stealth migration: Hiding virtual machines on the network},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Live virtual machine (VM) migration is commonly used for enabling dynamic resource or fault management, or for load balancing in datacenters or cloud platforms. A service hosted by a VM may also be migrated to prevent its visibility to an external adversary who may seek to disrupt its operation by launching a DDoS attack against it. We first show that current systems cannot adequately hide a VM migration from an external adversary. The key reason for this is that a migration typically manifests a traffic pattern with distinguishable statistical properties. We introduce two new attacks that can allow an adversary to effectively track a migration in progress, by leveraging observations of these properties. As our primary contribution, we design and implement a stealth migration framework that causes migration traffic to be indistinguishable from regular Internet traffic, with a negligible latency overhead of approximately 0.37 seconds, on average.},
keywords={cloud computing;computer centres;computer network security;resource allocation;telecommunication traffic;virtual machines;live virtual machine migration;dynamic resource;live VM migration;statistical properties;fault management;datacenters;traffic pattern;latency overhead;migration traffic;stealth migration framework;DDoS attack;external adversary;cloud platforms;load balancing;Virtual machining;Virtual machine monitors;Hardware;Timing;Conferences;Cloud computing},
doi={10.1109/INFOCOM.2017.8057195},
ISSN={},
month={May},}
@INPROCEEDINGS{8057196,
author={Y. Xiao and M. Krunz},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={QoE and power efficiency tradeoff for fog computing networks with fog node cooperation},
year={2017},
volume={},
number={},
pages={1-9},
abstract={This paper studies the workload offloading problem for fog computing networks in which a set of fog nodes can offload part or all the workload originally targeted to the cloud data centers to further improve the quality-of-experience (QoE) of users. We investigate two performance metrics for fog computing networks: users' QoE and fog nodes' power efficiency. We observe a fundamental tradeoff between these two metrics for fog computing networks. We then consider cooperative fog computing networks in which multiple fog nodes can help each other to jointly offload workload from cloud data centers. We propose a novel cooperation strategy referred to as offload forwarding, in which each fog node, instead of always relying on cloud data centers to process its unprocessed workload, can also forward part or all of its unprocessed workload to its neighboring fog nodes to further improve the QoE of its users. A distributed optimization algorithm based on distributed alternating direction method of multipliers (ADMM) via variable splitting is proposed to achieve the optimal workload allocation solution that maximizes users' QoE under the given power efficiency. We consider a fog computing platform that is supported by a wireless infrastructure as a case study to verify the performance of our proposed framework. Numerical results show that our proposed approach significantly improves the performance of fog computing networks.},
keywords={cloud computing;evolutionary computation;resource allocation;fog computing networks;fog node cooperation;cloud data centers;fog computing platform;distributed alternating direction method of multipliers;distributed optimization algorithm;ADMM;variable splitting;user QoE;quality of experience;power efficiency;Edge computing;Cloud computing;Resource management;Power demand;Measurement;Fog computing;response-time analysis;power efficiency;offload forwarding;ADMM},
doi={10.1109/INFOCOM.2017.8057196},
ISSN={},
month={May},}
@INPROCEEDINGS{8057197,
author={R. D. Yates and M. Tavan and Y. Hu and D. Raychaudhuri},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Timely cloud gaming},
year={2017},
volume={},
number={},
pages={1-9},
abstract={This work introduces a new model for cloud gaming systems aimed at optimizing the timeliness of video frames based on an age of information (AoI) metric. Mobile clients submit actions through an access network to a game server. The game server generates video frames at a constant frame rate. At the mobile device, the display of these frames represent game status updates. We develop a Markov model to characterize the frame delivery process in low-latency edge cloud gaming systems. Based on this model, we derive a simple formula for the average status age of a tightly synchronized low-latency mobile gaming system in which the inter-frame period is a significant contributor to the system latency. We validate the model by ns-3 simulation of a low-latency edge cloud gaming system. Our evaluation scenarios included single-player games as well as multi-player games in which the game processing was conducted by a combination of a centralized game server and edge cloud renderers.},
keywords={cloud computing;computer games;Markov processes;rendering (computer graphics);edge cloud renderers;timely cloud gaming;cloud gaming systems;video frames;game status updates;Markov model;frame delivery process;low-latency edge cloud gaming system;low-latency mobile gaming system;inter-frame period;single-player games;multiplayer games;game processing;centralized game server;Servers;Mobile communication;Delays;Cloud gaming;Streaming media;Mobile computing},
doi={10.1109/INFOCOM.2017.8057197},
ISSN={},
month={May},}
@INPROCEEDINGS{8057198,
author={X. Wang and X. Chen and W. Wu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Towards truthful auction mechanisms for task assignment in mobile device clouds},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Despite the increased capabilities of mobile devices, resource-demanded mobile applications still transcend what can be accomplished on a single device. As such, mobile device cloud (MDC), an environment that enables computation-intensive tasks to be performed among a set of nearby mobile devices, offers a promising architecture to support real-time mobile applications. To stimulate mobile devices to execute tasks for others, it is essential to design an incentive mechanism that appropriately charges the owners of the tasks, acted as the buyers, and rewards the mobile devices, acted as the sellers. In this paper, we propose two truthful auction mechanisms for two different task models, heterogeneous and homogeneous task models, which assume the different and the same resource requirements of the tasks, respectively. Specifically, for heterogeneous task model, we propose an efficient heuristic winning bids determination algorithm to allocate the tasks, and decide the payment of each seller for its winning bids. For homogeneous task model, we design an optimal winning bid determination algorithm, and propose a Vickrey-Clarke-Groves (VCG) based auction mechanism to determine the payment of each bid. Both theoretical analysis and simulations show that the proposed auction mechanisms achieve several desirable properties such as individual rationality, truthfulness and computational efficiency.},
keywords={cloud computing;mobile computing;resource allocation;mobile device cloud;computation-intensive tasks;real-time mobile applications;truthful auction mechanisms;heterogeneous task models;homogeneous task models;Vickrey-Clarke-Groves based auction mechanism;task assignment;mobile devices;VCG based auction mechanism;winning bid determination algorithm;Mobile handsets;Computational modeling;Cloud computing;Mobile communication;Resource management;Batteries},
doi={10.1109/INFOCOM.2017.8057198},
ISSN={},
month={May},}
@INPROCEEDINGS{8057199,
author={C. Hu and A. Alhothaily and A. Alrawais and X. Cheng and C. Sturtivant and H. Liu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A secure and verifiable outsourcing scheme for matrix inverse computation},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Matrix inverse computation is one of the most fundamental mathematical problems in large-scale data analytics and computing. It is often too expensive to be solved in resource-constrained devices such as sensors. Outsourcing the computation task to a cloud server or a fog server is a potential approach as the server is able to perform large-scale scientific computations on behalf of resource-constrained users with special software. However, outsourcing brings in new security concerns and challenges such as data privacy violations and result invalidation. In this paper, we propose a secure and verifiable outsourcing scheme to compute the matrix inverse in a server. In our scheme, the client generates two secret key sets based on two chaotic systems, which are utilized to create two sparse matrices whose permuted versions are used for matrix encryption and decryption to protect input and output privacy. The server computes the inverse over the ciphertext matrix and returns the result to the client who can verify the validity of the inverse. We analyze the proposed scheme in terms of correctness, security, verifiability, and attack resistance, and compare its performance (computation, storage, and communication overheads) with those of the state-of-the-art. Our theoretical results and comparison study demonstrate that the proposed scheme provides a secure and efficient outsourcing mechanism for matrix inverse computation.},
keywords={cryptography;data privacy;matrix inversion;computation task;cloud server;fog server;large-scale scientific computations;resource-constrained users;verifiable outsourcing scheme;matrix encryption;ciphertext matrix;secure outsourcing mechanism;matrix inverse computation;large-scale data analytics;resource-constrained devices;secure outsourcing scheme;matrix decryption;Servers;Outsourcing;Sparse matrices;Computational modeling;Encryption;Iterative methods;Matrix inversion;cloud/fog computing;secure outsourcing;verification;data privacy;chaotic systems},
doi={10.1109/INFOCOM.2017.8057199},
ISSN={},
month={May},}
@INPROCEEDINGS{8057200,
author={X. Feng and Z. Zheng and D. Cansever and A. Swami and P. Mohapatra},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A signaling game model for moving target defense},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Incentive-driven advanced attacks have become a major concern to cyber-security. Traditional defense techniques that adopt a passive and static approach by assuming a fixed attack type are insufficient in the face of highly adaptive and stealthy attacks. In particular, a passive defense approach often creates information asymmetry where the attacker knows more about the defender. To this end, moving target defense (MTD) has emerged as a promising way to reverse this information asymmetry. The main idea of MTD is to (continuously) change certain aspects of the system under control to increase the attacker's uncertainty, which in turn increases attack cost/complexity and reduces the chance of a successful exploit in a given amount of time. In this paper, we go one step beyond and show that MTD can be further improved when combined with information disclosure. In particular, we consider that the defender adopts a MTD strategy to protect a critical resource across a network of nodes, and propose a Bayesian Stackelberg game model with the defender as the leader and the attacker as the follower. After fully characterizing the defender's optimal migration strategies, we show that the defender can design a signaling scheme to exploit the uncertainty created by MTD to further affect the attacker's behavior for its own advantage. We obtain conditions under which signaling is useful, and show that strategic information disclosure can be a promising way to further reverse the information asymmetry and achieve more efficient active defense.},
keywords={Bayes methods;game theory;security of data;signaling game model;cyber-security;traditional defense techniques;static approach;fixed attack type;highly adaptive attacks;stealthy attacks;passive defense approach;information asymmetry;MTD strategy;Bayesian Stackelberg game model;signaling scheme;strategic information disclosure;efficient active defense;moving target defense;incentive-driven advanced attacks;attack cost-complexity;Games;Computational modeling;Bayes methods;Uncertainty;Face;Servers;Conferences},
doi={10.1109/INFOCOM.2017.8057200},
ISSN={},
month={May},}
@INPROCEEDINGS{8057201,
author={J. Chen and S. Yao and Q. Yuan and R. Du and G. Xue},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Checks and balances: A tripartite public key infrastructure for secure web-based connections},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Recent real-world attacks against Certification Authorities (CAs) and fraudulently issued certificates arouse the public to rethink the security of public key infrastructure for web-based connections. To distribute the trust of CAs, notaries, as an independent party, are introduced to record certificates, and a client can request an audit proof of certificates from notaries directly. However, there are two challenges. On one hand, existing works consider the security of notaries insufficiently. Due to lack of systematic mutual verification, notaries might bring safety bottlenecks. On the other hand, the service of these works is not sustainable, when any party leaks its private key or fails. In this paper, we propose a Tripartite Public Key Infrastructure (TriPKI), using Certificates Authorities, Integrity Log Servers, and Domain Name Servers, to provide a basis for establishing secure SSL/TLS connections. Specifically, we apply checks-and balances among those three parties in the structure to make them verify mutually, which avoids any single party compromise. Furthermore, we design a collaborative certificate management scheme to provide sustainable services. The security analysis and experiment results demonstrate that our scheme is suitable for practical usage with moderate overhead.},
keywords={authorisation;certification;Internet;public key cryptography;balances;tripartite public key infrastructure;Certification Authorities;CAs;fraudulently issued certificates;notaries;independent party;record certificates;systematic mutual verification;party leaks;private key;Certificates Authorities;secure SSL/TLS;checks;single party compromise;collaborative certificate management scheme;security analysis;secure Web-based connections;Public key;Servers;Collaboration;Electronic mail;Conferences;Authentication;Public Key Infrastructure;DNS-based;Mutual Verification},
doi={10.1109/INFOCOM.2017.8057201},
ISSN={},
month={May},}
@INPROCEEDINGS{8057202,
author={A. Klein and H. Shulman and M. Waidner},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Internet-wide study of DNS cache injections},
year={2017},
volume={},
number={},
pages={1-9},
abstract={DNS caches are an extremely important tool, providing services for DNS as well as for a multitude of applications, systems and security mechanisms, such as anti-spam defences, routing security (e.g., RPKI), firewalls. Subverting the security of DNS is detrimental to the stability and security of the clients and services, and can facilitate attacks, circumventing even cryptographic mechanisms. We study the caching component of DNS resolution platforms in diverse networks in the Internet, and evaluate injection vulnerabilities allowing cache poisoning attacks. Our evaluation includes networks of leading Internet Service Providers and enterprises, and professionally managed open DNS resolvers. We test injection vulnerabilities against known payloads as well as a new class of indirect attacks that we define in this work. Our Internet evaluation indicates that more than 92% of the Internet's DNS resolution platforms are vulnerable to records injection and can be persistently poisoned.},
keywords={cache storage;computer network security;Internet;DNS cache injections;security mechanisms;cryptographic mechanisms;caching component;injection vulnerabilities;cache poisoning attacks;open DNS resolvers;indirect attacks;Internet evaluation;DNS resolution platforms;antispam defences;routing security;firewalls;Internet service providers;Electronic mail;IP networks;Servers;Payloads;Internet;Computer crime},
doi={10.1109/INFOCOM.2017.8057202},
ISSN={},
month={May},}
@INPROCEEDINGS{8057203,
author={Y. Cheng and H. Jiang and F. Wang and Y. Hua and D. Feng},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={BlitzG: Exploiting high-bandwidth networks for fast graph processing},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Nowadays, high-bandwidth networks are easily accessible in data centers. However, existing distributed graph-processing frameworks fail to efficiently utilize the additional bandwidth capacity in these networks for higher performance, due to their inefficient computation and communication models, leading to very long waiting times experienced by users for the graph-computing results. The root cause lies in the fact that the computation and communication models of these frameworks generate, send and receive messages so slowly that only a small fraction of the available network bandwidth is utilized. In this paper, we propose a high-performance distributed graph-processing framework, called BlitzG, to address this problem. This framework fully exploits the available network bandwidth capacity for fast graph processing. Our approach aims at significant reduction in (i) the computation workload of each vertex for fast message generation by using a new slimmed-down vertex-centric computation model and (ii) the average message overhead for fast message delivery by designing a lightweight message-centric communication model. Evaluation on a 40Gbps Ethernet, driven by real-world graph datasets, shows that BlitzG outperforms the state-of-the-art distributed graph-processing frameworks by up to 27x with an average of 20.7x.},
keywords={computer centres;distributed processing;graph theory;local area networks;network theory (graphs);communication models;long waiting times;high-performance distributed graph-processing framework;fast graph processing;computation workload;fast message generation;vertex-centric computation model;fast message delivery;lightweight message-centric communication model;real-world graph datasets;high-bandwidth networks;BlitzG;network bandwidth capacity;data centers;slimmed-down vertex-centric computation model;Ethernet;bit rate 40 Gbit/s;Computational modeling;Message systems;Kernel;Conferences;Bandwidth;Receivers;Scalability},
doi={10.1109/INFOCOM.2017.8057203},
ISSN={},
month={May},}
@INPROCEEDINGS{8057204,
author={T. Shu and C. Q. Wu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Performance optimization of Hadoop workflows in public clouds through adaptive task partitioning},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Cloud computing provides a cost-effective computing platform for big data workflows where moldable parallel computing models such as MapReduce are widely applied to meet stringent performance requirements. The granularity of task partitioning in each moldable job has a significant impact on workflow completion time and financial cost. We investigate the properties of moldable jobs and design a big-data workflow mapping model, based on which, we formulate a workflow mapping problem to minimize workflow makespan under a budget constraint in public clouds. We show this problem to be strongly NP-complete and design i) a fully polynomial-time approximation scheme (FPTAS) for a special case with a pipeline-structured workflow executed on virtual machines in a single class, and ii) a heuristic for a generalized problem with an arbitrary directed acyclic graph-structured workflow executed on virtual machines in multiple classes. The performance superiority of the proposed solution is illustrated by extensive simulation-based results in Hadoop/YARN in comparison with existing workflow mapping models and algorithms.},
keywords={Big Data;cloud computing;computational complexity;directed graphs;optimisation;parallel processing;scheduling;performance optimization;Hadoop workflows;public clouds;cloud computing;cost-effective computing platform;moldable parallel computing models;task partitioning;workflow completion time;financial cost;big-data workflow mapping model;workflow mapping problem;virtual machines;generalized problem;performance superiority;Hadoop/YARN;workflow mapping models;adaptive task partitioning;Big Data workflow;NP-complete design;fully polynomial-time approximation scheme;arbitrary directed acyclic graph-structured workflow;Cloud computing;Computational modeling;Big Data;Optimization;Scheduling;Program processors},
doi={10.1109/INFOCOM.2017.8057204},
ISSN={},
month={May},}
@INPROCEEDINGS{8057205,
author={X. Mei and X. Chu and H. Liu and Y. Leung and Z. Li},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Energy efficient real-time task scheduling on CPU-GPU hybrid clusters},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Conserving the energy consumption of large data centers is of critical significance, where a few percent in consumption reduction translates into millions-dollar savings. This work studies energy conservation on emerging CPU-GPU hybrid clusters through dynamic voltage and frequency scaling (DVFS). We aim at minimizing the total energy consumption of processing a sequence of real-time tasks under deadline constraints. We compute the appropriate voltage/frequency setting for each task through mathematical optimization, and assign multiple tasks to the cluster with heuristic scheduling algorithms. In performance evaluation driven by real-world power measurement traces, our scheduling algorithm shows comparable energy savings to the theoretical upper bound. With a GPU scaling interval where analytically at most 38% of energy can be saved, we record 30-36% of energy savings. Our results are applicable to energy management on modern heterogeneous clusters. In particular, our model stresses the nonlinear relationship between task execution time and processor speed for GPU-accelerated applications, for more accurately capturing real-world GPU energy consumption.},
keywords={graphics processing units;multiprocessing systems;performance evaluation;power aware computing;processor scheduling;upper bound;GPU energy consumption;total energy consumption reduction;dynamic voltage and frequency scaling;GPU-accelerated applications;task execution time;GPU scaling interval;real-world power measurement traces;heuristic scheduling algorithms;data centers;CPU-GPU hybrid clusters;energy efficient real-time task scheduling;Graphics processing units;Energy consumption;Servers;Runtime;Power demand;Scheduling algorithms},
doi={10.1109/INFOCOM.2017.8057205},
ISSN={},
month={May},}
@INPROCEEDINGS{8057206,
author={D. Cheng and Y. Chen and X. Zhou and D. Gmach and D. Milojicic},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Adaptive scheduling of parallel jobs in spark streaming},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Streaming data analytics has become increasingly vital in many applications such as dynamic content delivery (e.g., advertisements), Twitter sentiment analysis, and security event processing (e.g., intrusion detection systems, and spam filters). Emerging stream processing systems, such as Spark Streaming, treat the continuous stream as a series of micro-batches of data and continuously process these micro-batch jobs. Such micro-batch based stream processing provides several advantages over traditional stream processing systems, which process streaming data one record at a time, including fast recovery from failures, better load balancing and scalability. However, efficient scheduling of micro-batch jobs to achieve high throughput and low latency is very challenging due to the complex data dependency and dynamism inherent in streaming workloads. In this paper, we propose A-scheduler, an adaptive scheduling approach that dynamically schedules parallel micro-batch jobs in Spark Streaming and automatically adjusts scheduling parameters to improve performance and resource efficiency. Specifically, A-scheduler dynamically schedules multiple jobs concurrently using different policies based on their data dependencies and automatically adjusts the level of job parallelism and resource shares among jobs based on workload properties. We implemented A-scheduler and evaluated it with a real-time security event processing workload. Our experimental results show that A-scheduler can reduce end-to-end latency by 42% and improve workload throughput and energy efficiency by 21% and 13%, respectively, compared to the default Spark Streaming scheduler.},
keywords={adaptive scheduling;data analysis;dynamic scheduling;parallel processing;resource allocation;scheduling;security of data;parallel jobs;data analytics;dynamic content delivery;intrusion detection systems;microbatch jobs;microbatch based stream processing;traditional stream processing systems;adaptive scheduling approach;data dependencies;job parallelism;resource shares;real-time security event processing workload;streaming data;data dependency;A-scheduler;Spark Streaming scheduler;Sparks;Parallel processing;Resource management;Throughput;Schedules;Real-time systems},
doi={10.1109/INFOCOM.2017.8057206},
ISSN={},
month={May},}
@INPROCEEDINGS{8057207,
author={W. Wang and Y. Chen and L. Yang and Q. Zhang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Detecting on-body devices through creeping wave propagation},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The ability to detect which wearables and smartphones are on the same body has the potential to support a wealth of applications, including user authentication, automatic data synchronization, and personalized profile loading. This paper brings this feature to commercial off-the-shelf (COTS) wearables and smartphones, by creating a virtual “on-body detection sensor” based on devices' inherent wireless capabilities. We investigate using the peculiar propagation characteristics of creeping waves to discern on-body wearables. To this end, we decompose signals into multiple independent components to exploit the variation features of creeping waves. We implement our system on COTS wearables and a smartphone. Extensive experiments are conducted in a lab, apartments, malls, and outdoor areas, involving 12 volunteer subjects of different age groups, to demonstrate the robustness of our system. Results show that our system can identify on-body devices at 92.3% average true positive rate and 5% average false positive rate.},
keywords={body sensor networks;electromagnetic wave propagation;object detection;smart phones;on-body devices;smartphones;user authentication;automatic data synchronization;virtual on-body detection sensor;on-body wearables;COTS wearables;smartphone;creeping wave propagation;personalized profile loading;commercial off-the-shelf wearables;Smart phones;Feature extraction;Radio propagation;Wireless sensor networks;Wireless communication;Dynamics;Biomedical monitoring;Creeping waves;wearables;on-body detection},
doi={10.1109/INFOCOM.2017.8057207},
ISSN={},
month={May},}
@INPROCEEDINGS{8057208,
author={X. Guo and J. Liu and Y. Chen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={FitCoach: Virtual fitness coach empowered by wearable mobile devices},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Acknowledging the powerful sensors on wearables and smartphones enabling various applications to improve users' life styles and qualities (e.g., sleep monitoring and running rhythm tracking), this paper takes one step forward developing FitCoach, a virtual fitness coach leveraging users' wearable mobile devices (including wrist-worn wearables and arm-mounted smartphones) to assess dynamic postures (movement patterns &amp; positions) in workouts. FitCoach aims to help the user to achieve effective workout and prevent injury by dynamically depicting the short-term and long-term picture of a user's workout based on various sensors in wearable mobile devices. In particular, FitCoach recognizes different types of exercises and interprets fine-grained fitness data (i.e., motion strength and speed) to an easy-to-understand exercise review score, which provides a comprehensive workout performance evaluation and recommendation. FitCoach has the ability to align the sensor readings from wearable devices to the human coordinate system, ensuring the accuracy and robustness of the system. Extensive experiments with over 5000 repetitions of 12 types of exercises involve 12 participants doing both anaerobic and aerobic exercises in indoors as well as outdoors. Our results demonstrate that FitCoach can provide meaningful review and recommendations to users by accurately measure their workout performance and achieve 93% accuracy for workout analysis.},
keywords={accelerometers;assisted living;biomechanics;body sensor networks;medical computing;mobile computing;patient rehabilitation;sleep;smart phones;FitCoach;wearable mobile devices;smartphones;sleep monitoring;running rhythm tracking;dynamic postures;fine-grained fitness data;exercise review score;comprehensive workout performance evaluation;sensor readings;wearable devices;workout analysis;virtual fitness coach;wrist-worn wearables;arm-mounted smart phones;movement pattern-and-positions;Biomedical monitoring;Performance evaluation;Wearable sensors;Monitoring;Smart phones},
doi={10.1109/INFOCOM.2017.8057208},
ISSN={},
month={May},}
@INPROCEEDINGS{8057209,
author={L. Xie and X. Dong and W. Wang and D. Huang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Meta-activity recognition: A wearable approach for logic cognition-based activity sensing},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Activity sensing has become a key technology for many ubiquitous applications, such as exercise monitoring and elder care. Most traditional approaches track the human motions and perform activity recognition based on the waveform matching schemes in the raw data representation level. In regard to the complex activities with relatively large moving range, they usually fail to accurately recognize these activities, due to the inherent variations in human activities. In this paper, we propose a wearable approach for logic cognition-based activity sensing scheme in the logical representation level, by leveraging the meta-activity recognition. Our solution extracts the angle profiles from the raw inertial measurements, to depict the angle variation of limb movement in regard to the consistent body coordinate system. It further extracts the meta-activity profiles to depict the sequence of small-range activity units in the complex activity. By leveraging the least edit distance-based matching scheme, our solution is able to accurately perform the activity sensing. Based on the logic cognition-based activity sensing, our solution achieves lightweight-training recognition, which requires a small quantity of training samples to build the templates, and user-independent recognition, which requires no training from the specific user. The experiment results in real settings shows that our meta-activity recognition achieves an average accuracy of 92% for user-independent activity sensing.},
keywords={cognition;feature extraction;geriatrics;medical signal processing;patient monitoring;ubiquitous computing;wearable computers;meta-activity recognition;wearable approach;raw data representation level;complex activity;human activities;logical representation level;meta-activity profiles;small-range activity units;user-independent activity sensing;logic cognition-based activity sensing;ubiquitous applications;exercise monitoring;elder care;human motions;waveform matching;least edit distance-based matching scheme;limb movement;Coordinate measuring machines;Motion measurement;Training;Activity recognition;Transforms;Optical wavelength conversion},
doi={10.1109/INFOCOM.2017.8057209},
ISSN={},
month={May},}
@INPROCEEDINGS{8057210,
author={X. Liang and T. Yun and R. Peterson and D. Kotz},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={LightTouch: Securely connecting wearables to ambient displays with user intent},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Wearables are small and have limited user interfaces, so they often wirelessly interface with a personal smartphone/computer to relay information from the wearable for display or other interactions. In this paper, we envision a new method, LightTouch, by which a wearable can establish a secure connection to an ambient display, such as a television or a computer monitor, while ensuring the user's intention to connect to the display. LightTouch uses standard RF methods (like Bluetooth) for communicating the data to display, securely bootstrapped via the visible-light communication (the brightness channel) from the display to the low-cost, low-power, ambient light sensor of a wearable. A screen `touch' gesture is adopted by users to ensure that the modulation of screen brightness can be securely captured by the ambient light sensor with minimized noise. Wireless coordination with the processor driving the display establishes a shared secret based on the brightness channel information. We further propose novel onscreen localization and correlation algorithms to improve security and reliability. Through experiments and a preliminary user study we demonstrate that LightTouch is compatible with current display and wearable designs, is easy to use (about 6 seconds to connect), is reliable (up to 98% success connection ratio), and is secure against attacks.},
keywords={interactive devices;mobile handsets;security of data;smart phones;user interfaces;LightTouch;ambient display;user intent;user interfaces;standard RF methods;visible-light communication;ambient light sensor;screen touch gesture;screen brightness;wireless coordination;brightness channel information;preliminary user study;wearable designs;personal smartphone;personal computer;connection ratio;Brightness;Biomedical monitoring;Radio frequency;Security;Monitoring;Correlation;Light sources},
doi={10.1109/INFOCOM.2017.8057210},
ISSN={},
month={May},}
@INPROCEEDINGS{8057211,
author={Z. Li and K. G. Shin and L. Zhen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={When and how much to neutralize interference?},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Interference management (IM) is essential to wireless communication networks, but interference suppression, a key component of IM, is known to degrade users' achievable spectral efficiency (SE). It is thus important to select an appropriate IM method with optimal operating parameters according to diverse network deployments, transmit power differences of various communication equipments, and dynamically changing channel conditions so as to balance the benefits brought by and the cost of IM. Interference neutralization (IN) has recently been receiving considerable attention, with which a duplicate of interference of the same strength and opposite phase w.r.t. the original interfering signal is generated to neutralize the disturbance at the intended receiver. However, to the best of our knowledge, all existing IN schemes assume that interference is completely neutralized without accounting for their power consumption. To remedy this deficiency, we propose a novel scheme, called dynamic interference neutralization (DIN). By intelligently determining the appropriate portion of interference to be neutralized, we balance the transmitter's power used for IN and the desired signal's transmission. We then present a new way to adaptively select one of DIN and other IM methods by taking into account the cost of multiple IM methods and their benefits. Our analysis has shown that DIN can include complete IN and non-interference management (non-IM) as special cases. The proposed strategy is shown via simulation to be able to make better use of the transmit power than existing IM methods, hence enhancing users' SE.},
keywords={interference suppression;radio networks;radiofrequency interference;interference management;wireless communication networks;interference suppression;optimal operating parameters;diverse network deployments;transmit power differences;communication equipments;channel conditions;opposite phase w.r.t;original interfering signal;intended receiver;dynamic interference neutralization;nonIM;noninterference management;multiple IM methods;transmitter;DIN;power consumption;Interference;Transmitters;Receivers;Relays;Macrocell networks;Conferences;Array signal processing},
doi={10.1109/INFOCOM.2017.8057211},
ISSN={},
month={May},}
@INPROCEEDINGS{8057212,
author={K. Hsu and K. C. Lin and H. Wei},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Inter-client interference cancellation for full-duplex networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Recent studies have experimentally shown the gains of full-duplex radios. However, due to its relatively higher cost and complexity, we can envision a more practical step in the network evolution is to have a full-duplex access point (AP) but keep the clients half-duplex. Unfortunately, the full-duplex gains can hardly be extracted in practice as the uplink transmission from a half-duplex client introduces inter-client interference to another downlink client. To address this issue, we present the design and implementation of IC2 (Inter-Client Interference Cancellation), the first physical layer solution that exploits the AP's full-duplex capability to actively cancel the interference at the downlink client. Such active cancellation not only improves the achievable capacity, but also better tolerates imperfect user pairing, simplifying the MAC design as a result. We build a prototype of IC2 on USRP-N200 and evaluate its performance via both testbed experiments and large-scale trace-driven simulations. The results show that, without IC2, about 60% of client pairs produce no gain from full-duplex transmissions, while, with IC2 the median throughput gain over conventional half-duplex networks can be 1.65× even when clients are simply paired randomly.},
keywords={access protocols;interference suppression;radio networks;radiofrequency interference;software radio;wireless LAN;active cancellation;client pairs;full-duplex transmissions;Inter-client interference cancellation;full-duplex networks;full-duplex radios;full-duplex access point;downlink client;AP's full-duplex capability;Inter-Client Interference Cancellation;IC2;Downlink;Uplink;Interference cancellation;Throughput;Hardware;Relays},
doi={10.1109/INFOCOM.2017.8057212},
ISSN={},
month={May},}
@INPROCEEDINGS{8057213,
author={T. Vermeulen and M. Laghate and G. Hattab and D. Cabric and S. Pollin},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Towards instantaneous collision and interference detection using in-band full duplex},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Wireless devices are ubiquitous nowadays and, since most of them use the same unlicensed frequency bands, the high number of packet losses due to interference and collisions degrade performance. Reliability, energy consumption, and latency are key challenges for future dense networks. Allowing the transmitter to take action, i.e., vacating the channel, as soon as a collision or interference is detected is crucial in improving these metrics. In-band full duplex radios enable the transmitter to simultaneously transmit packets and sense the spectrum for collisions and interference. This paper studies two important questions regarding transmitter-based collision and interference detection: (1) from an overall system perspective, does such detection outperform receiver-based detection and (2) which test statistic is the most accurate and sensitive at detecting collisions and interference. First, NS-3 simulations are used to show that transmitter-based detection reduces the energy consumption while improving the throughput in a typical star topology network. Next, we present a measurement-based study of four different techniques for transmitter-based collision and interference detection. In particular, we compare the energy detector with three goodness-of-fit tests in terms of probability of detection and false alarm. Our analysis shows that transmitter-based detection can detect between 80% to 100% of the collisions and interference occurring at the receiver, depending on the distance between the transmitter and the receiver. Of those detectable by the transmitter, our measurement results show that goodness-of-fit tests can detect nearly 100% of the collisions and have at least 10 dB better sensitivity as compared to the commonly proposed energy detection test. In general, the proposed techniques can detect interfering signals that are up to 25 dB below the remaining self-interference power.},
keywords={energy consumption;interference suppression;probability;radio receivers;radio transmitters;radiofrequency interference;signal detection;telecommunication network reliability;telecommunication network topology;wireless channels;interference detection;unlicensed frequency bands;collision;In-band full duplex radios;goodness-of-fit tests;remaining self-interference power;instantaneous collision;energy detection test;latency;transmitter-based collision;NS-3 simulations;energy consumption reduction;star topology network;false alarm;probability of detection;receiver;interfering signal detection;Interference;Receivers;Radio transmitters;Silicon;Reliability;Energy consumption},
doi={10.1109/INFOCOM.2017.8057213},
ISSN={},
month={May},}
@INPROCEEDINGS{8057214,
author={G. Naik and J. Liu and J. J. Park},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Coexistence of Dedicated Short Range Communications (DSRC) and Wi-Fi: Implications to Wi-Fi performance},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The 5.9 GHz band is being actively explored for possible spectrum sharing opportunities between Dedicated Short Range Communications (DSRC) and IEEE 802.11ac networks in order to address the increasing demand for bandwidth-intensive Wi-Fi applications. In this paper, we study the implications of this spectrum sharing to the performance of Wi-Fi systems. Through experiments performed on our testbed, we first investigate band sharing options available for Wi-Fi devices. Using experimental results, we show the need for using conservative Wi-Fi transmission parameters to enable harmonious coexistence between DSRC and Wi-Fi. Moreover, we show that under the current 802.11ac standard, certain channelization options, particularly the high bandwidth ones, cannot be used by Wi-Fi devices without causing interference to the DSRC nodes. Under these constraints, we propose a Real-time Channelization Algorithm (RCA) for Wi-Fi Access Points (APs) operating in the shared spectrum. Evaluation of the proposed algorithm using a prototype implementation on commodity hardware as well as via simulations show that informed channelization decisions can significantly increase Wi-Fi throughput compared to static channelization schemes.},
keywords={radio spectrum management;real-time systems;wireless LAN;real-time channelization algorithms;spectrum sharing;conservative Wi-Fi transmission parameters;band sharing options;Wi-Fi systems;bandwidth-intensive Wi-Fi applications;IEEE 802.11ac networks;Wi-Fi performance;DSRC;dedicated short range communications;shared spectrum;Wi-Fi Access Points;Wi-Fi devices;frequency 5.9 GHz;Wireless fidelity;Transmitters;IEEE 802.11 Standard;Bandwidth;Hardware;Performance evaluation},
doi={10.1109/INFOCOM.2017.8057214},
ISSN={},
month={May},}
@INPROCEEDINGS{8057215,
author={R. Ben Basat and G. Einziger and R. Friedman and Y. Kassner},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Randomized admission policy for efficient top-k and frequency estimation},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Network management protocols often require timely and meaningful insight about per flow network traffic. This paper introduces Randomized Admission Policy (RAP) - a novel algorithm for the frequency and top-k estimation problems, which are fundamental in network monitoring. We demonstrate space reductions compared to the alternatives by a factor of up to 32 on real packet traces and up to 128 on heavy-tailed workloads. For top-k identification, RAP exhibits memory savings by a factor of between 4 and 64 depending on the workloads' skewness. These empirical results are backed by formal analysis, indicating the asymptotic space improvement of our probabilistic admission approach. Additionally, we present d-Way RAP, a hardware friendly variant of RAP that empirically maintains its space and accuracy benefits.},
keywords={frequency estimation;protocols;telecommunication network management;telecommunication traffic;top-k estimation problems;Randomized admission policy;frequency estimation;network management protocols;flow network traffic;Randomized Admission Policy;Radiation detectors;Frequency estimation;Monitoring;Random access memory;Probabilistic logic;Algorithm design and analysis;Estimation},
doi={10.1109/INFOCOM.2017.8057215},
ISSN={},
month={May},}
@INPROCEEDINGS{8057216,
author={R. Ben Basat and G. Einziger and R. Friedman and Y. Kassner},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Optimal elephant flow detection},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Monitoring the traffic volumes of elephant flows, including the total byte count per flow, is a fundamental capability for online network measurements. We present an asymptotically optimal algorithm for solving this problem in terms of both space and time complexity. This improves on previous approaches, which can only count the number of packets in constant time. We evaluate our work on real packet traces, demonstrating an up to X2.5 speedup compared to the best alternative.},
keywords={computational complexity;optimisation;telecommunication network management;telecommunication traffic;optimal elephant flow detection;traffic volumes;fundamental capability;online network measurements;asymptotically optimal algorithm;time complexity;constant time;byte count;space complexity;Radiation detectors;Maintenance engineering;Monitoring;Data structures;Software algorithms;Runtime;Real-time systems},
doi={10.1109/INFOCOM.2017.8057216},
ISSN={},
month={May},}
@INPROCEEDINGS{8057217,
author={G. Xie and K. Xie and J. Huang and X. Wang and Y. Chen and J. Wen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Fast low-rank matrix approximation with locality sensitive hashing for quick anomaly detection},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Detecting anomalous traffic is a critical task for advanced Internet management. The traditional approaches based on Principal Component Analysis (PCA) are effective only when the corruption is caused by small additive i.i.d. Gaussian noise. The recent Direct Robust Matrix Factorization (DRMF) is proven to be more robust and accurate in anomaly detection, but it incurs a high computation cost due to its need of singular value decomposition (SVD) for low-rank matrix approximation and the iterative use of SVD execution to find the final solution. To enable the anomaly detection for large traffic matrix with the use of DRMF, we formulate the low-rank matrix approximation problem as a problem of searching for the subspace to project the traffic matrix with the minimum error. We propose a novel approach, LSH-subspace, for fast low-rank matrix approximation. To facilitate the matrix partition for the quick search of the subspace, we propose several novel techniques: a multi-layer locality sensitive hashing (LSH) table to reorder the OD pairs based on LSH function, a partition principle to guide the partition to minimize the projection error, and a lightweight algorithm to exploit the sparsity of the outlier matrix to update the LSH table at low overhead. Our extensive simulations based on real trace data demonstrate that our LSH-subspace is 3 times faster than DRMF with high anomaly detection accuracy.},
keywords={approximation theory;Internet;matrix decomposition;security of data;singular value decomposition;fast low-rank matrix approximation;quick anomaly detection;DRMF;traffic matrix;low-rank matrix approximation problem;LSH-subspace;matrix partition;multilayer locality sensitive hashing table;outlier matrix;high anomaly detection accuracy;anomalous traffic detection;direct robust matrix factorization;SVD execution;singular value decomposition;multilayer locality sensitive hashing;partition principle;Anomaly detection;Principal component analysis;Matrix decomposition;Robustness;Approximation algorithms;Noise measurement;Sparse matrices;Low-Rank Matrix Approximation;Anomaly Detection},
doi={10.1109/INFOCOM.2017.8057217},
ISSN={},
month={May},}
@INPROCEEDINGS{8057218,
author={K. Xie and C. Peng and X. Wang and G. Xie and J. Wen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Accurate recovery of internet traffic data under dynamic measurements},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The inference of the network traffic matrix from partial measurement data becomes increasingly critical for various network engineering tasks, such as capacity planning, load balancing, path setup, network provisioning, anomaly detection, and failure recovery. The recent study shows it is promising to more accurately interpolate the missing data with a three-dimensional tensor as compared to interpolation methods based on two-dimensional matrix. Despite the potential, it is difficult to form a tensor with measurements taken at varying rate in a practical network. To address the issues, we propose Reshape-Align scheme to form the regular tensor with data from dynamic measurements, and introduce user-domain and temporal-domain factor matrices which takes full advantage of features from both domains to translate the matrix completion problem to the tensor completion problem based on CP decomposition for more accurate missing data recovery. Our performance results demonstrate that our Reshape-Align scheme can achieve significantly better performance in terms of two metrics: error ratio and mean absolute error (MAE).},
keywords={Internet;interpolation;matrix algebra;telecommunication traffic;tensors;internet traffic data;dynamic measurements;mean absolute error;capacity planning;data recovery;tensor completion problem;matrix completion problem;temporal-domain factor matrices;user-domain;regular tensor;Reshape-Align scheme;practical network;two-dimensional matrix;interpolation methods;three-dimensional tensor;failure recovery;anomaly detection;network provisioning;path setup;load balancing;network engineering tasks;partial measurement data;network traffic matrix;Tensile stress;Matrix decomposition;Monitoring;Correlation;Indexes;Interpolation;Computational modeling;Internet traffic data recovery;Matrix completion;Tensor completion},
doi={10.1109/INFOCOM.2017.8057218},
ISSN={},
month={May},}
@INPROCEEDINGS{8057219,
author={F. Dang and P. Zhou and Z. Li and E. Zhai and A. Mohaisen and Q. Wen and M. Li},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Large-scale invisible attack on AFC systems with NFC-equipped smartphones},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Automated Fare Collection (AFC) systems have been globally deployed for decades, particularly in public transportation. Although the transaction messages of AFC systems are mostly transferred in plaintext, which is obviously insecure, system operators do not need to pay much attention to this issue, since the AFC network is well isolated from public network (e.g., the Internet). Nevertheless, in recent years, the advent of Near Field Communication (NFC)-equipped smartphones has bridged the gap between the AFC network and the Internet through Host-based Card Emulation (HCE). Motivated by this fact, we design and practice a novel paradigm of attack on modern distance-based pricing AFC systems, enabling users to pay much less than actually required. Our constructed attack has two important properties: 1) it is invisible to AFC system operators because the attack never causes any inconsistency in the backend database of the operators; and 2) it can be scalable to large number of users (e.g., 10,000) by maintaining a moderate-sized AFC card pool (e.g., containing 150 cards). Based upon this constructed attack, we developed an HCE app, named LessPay. Our real-world experiments on LessPay demonstrate not only the feasibility of our attack (with 97.6% success rate), but also its low-overhead in terms of bandwidth and computation.},
keywords={Internet;mobile computing;near-field communication;security of data;smart cards;smart phones;large-scale invisible attack;NFC-equipped smartphones;automated fare collection systems;host-based card emulation;HCE;distance-based pricing AFC systems;backend database;LessPay;moderate-sized AFC card pool;Near Field Communication;public network;AFC network;transaction messages;public transportation;Protocols;Authentication;Smart phones;Conferences;Urban areas;Web servers},
doi={10.1109/INFOCOM.2017.8057219},
ISSN={},
month={May},}
@INPROCEEDINGS{8057220,
author={Y. Chen and J. Sun and X. Jin and T. Li and R. Zhang and Y. Zhang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Your face your heart: Secure mobile face authentication with photoplethysmograms},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Face authentication emerges as a powerful method for preventing unauthorized access to mobile devices. It is, however, vulnerable to photo-based forgery attacks (PFA) and videobased forgery attacks (VFA), in which the adversary exploits a photo or video containing the user's frontal face. Effective defenses against PFA and VFA often rely on liveness detection, which seeks to find a live indicator that the submitted face photo or video of the legitimate user is indeed captured in real time. In this paper, we propose FaceHeart, a novel and practical face authentication system for mobile devices. FaceHeart simultaneously takes a face video with the front camera and a fingertip video with the rear camera on COTS mobile devices. It then achieves liveness detection by comparing the two photoplethysmograms independently extracted from the face and fingertip videos, which should be highly consistent if the two videos are for the same live person and taken at the same time. As photoplethysmograms are closely tied to human cardiac activity and almost impossible to forge or control, FaceHeart is strongly resilient to PFA and VFA. Extensive user experiments on Samsung Galaxy S5 have confirmed the high efficacy and efficiency of FaceHeart.},
keywords={face recognition;feature extraction;mobile computing;security of data;smart phones;video cameras;video signal processing;photoplethysmograms;unauthorized access;PFA;videobased forgery attacks;VFA;liveness detection;live indicator;legitimate user;FaceHeart;face video;fingertip video;rear camera;COTS mobile devices;fingertip videos;live person;mobile face authentication security;photo-based forgery attack;face photo;face authentication system;Samsung Galaxy S5;extensive user experiments;Face;Authentication;Mobile handsets;Cameras;Streaming media;Feature extraction;Mobile communication},
doi={10.1109/INFOCOM.2017.8057220},
ISSN={},
month={May},}
@INPROCEEDINGS{8057221,
author={H. Fu and Z. Zheng and S. Bose and M. Bishop and P. Mohapatra},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={LeakSemantic: Identifying abnormal sensitive network transmissions in mobile applications},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Mobile applications (apps) often transmit sensitive data through network with various intentions. Some transmissions are needed to fulfill the app's functionalities. However, transmissions with malicious receivers may lead to privacy leakage and tend to behave stealthily to evade detection. The problem is twofold: how does one unveil sensitive transmissions in mobile apps, and given a sensitive transmission, how does one determine if it is legitimate? In this paper, we propose LeakSemantic, a framework that can automatically locate abnormal sensitive network transmissions from mobile apps. LeakSemantic consists of a hybrid program analysis component and a machine learning component. Our program analysis component combines static analysis and dynamic analysis to precisely identify sensitive transmissions. Compared to existing taint analysis approaches, LeakSemantic achieves better accuracy with fewer false positives and is able to collect runtime data such as network traffic for each transmission. Based on features derived from the runtime data, machine learning classifiers are built to further differentiate between the legal and illegal disclosures. Experiments show that LeakSemantic achieves 91% accuracy on 2279 sensitive connections from 1404 apps.},
keywords={data privacy;learning (artificial intelligence);mobile computing;pattern classification;program diagnostics;security of data;system monitoring;runtime data;LeakSemantic;mobile applications;hybrid program analysis component;network traffic;abnormal sensitive network transmission identification;sensitive data transmission;dynamic analysis;machine learning classifiers;legal disclosures;illegal disclosures;Runtime;Mobile communication;Androids;Humanoid robots;Conferences;Analytical models;Privacy},
doi={10.1109/INFOCOM.2017.8057221},
ISSN={},
month={May},}
@INPROCEEDINGS{8057222,
author={Y. Yang and J. Sun},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Energy-efficient W-layer for behavior-based implicit authentication on mobile devices},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Motivated by the great potential of implicit and seamless user authentication, we attempt to build an efficient middle layer running on mobile devices to support implicit authentication (IA) systems with adaptive sampling. Various activities, such as user location, application usage, user motion, and battery usage have been popular choices to generate behaviors, the soft biometrics, for implicit authentication. Unlike password-based or hard biometric-based authentication, implicit authentication does not require explicit user action or expensive hardware. However, user behaviors can change unpredictably which renders it more challenging to develop systems that depend on them. Various machine learning algorithms have been used to address this challenge. The expensive training process is usually outsourced to the remote server but this can potentially increase the chance of data leakage. In addition, mobile devices may not always have reliable network connections to send real-time data to the server for training. Motivated by these limitations, we propose a W-layer, an overlay that provides an energy-efficient solution for real-time implicit authentication on mobile devices. The size of the data the system needs to collect at different times depends on the legitimacy of the user. This in turn affects how the sampling rate is adjusted which can reduce energy consumption. To evaluate our method, we conducted several experiments on both synthetic and real datasets. The average accuracy of identifying legitimate users is 96.73% using the synthetic dataset and 96.70% using the real dataset. Furthermore, we tested the power consumption on a low-end Nexus S smartphone to obtain a more pessimistic result. We found that our method consumed 14.5% of the device's total battery usage. The power consumption performance is expected to improve significantly on high-end mobile devices.},
keywords={biometrics (access control);energy conservation;learning (artificial intelligence);message authentication;mobile computing;mobile handsets;power aware computing;smart phones;implicit authentication systems;user location;user motion;user behaviors;energy-efficient solution;real-time implicit authentication;legitimate users;high-end mobile devices;energy-efficient W-layer;implicit user authentication;seamless user authentication;user action;Nexus S smartphone;power consumption;energy consumption reduction;battery usage;Authentication;Sensors;Real-time systems;Mobile handsets;Biometrics (access control);Servers;Training},
doi={10.1109/INFOCOM.2017.8057222},
ISSN={},
month={May},}
@INPROCEEDINGS{8057223,
author={R. Vaze},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Online knapsack problem and budgeted truthful bipartite matching},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Two related online problems: knapsack and truthful bipartite matching are considered. For these two problems, the common theme is how to `match' an arriving left vertex in an online fashion with any of the available right vertices, if at all, so as to maximize the sum of the value of the matched edges, subject to satisfying a sum-weight constraint on the matched left vertices. Assuming that the left vertices arrive in an uniformly random order (secretary model), two almost similar algorithms are proposed for the two problems, that are 2e competitive and 24 competitive, respectively. The proposed online bipartite matching algorithm is also shown to be truthful: there is no incentive for any left vertex to misreport its bid/weight. Direct applications of these problems include job allocation with load balancing, generalized adwords, crowdsourcing auctions, and matching wireless users to cooperative relays in device-to-device communication enabled cellular network.},
keywords={combinatorial mathematics;knapsack problems;sum-weight constraint;bipartite matching algorithm;online knapsack problem;truthful bipartite matching;left vertex;job allocation;load balancing;generalized adwords;crowdsourcing auctions;wireless users;cooperative relays;device-to-device communication enabled cellular network;Device-to-device communication;Impedance matching;Conferences;Resource management;Load management;Crowdsourcing;Wireless communication},
doi={10.1109/INFOCOM.2017.8057223},
ISSN={},
month={May},}
@INPROCEEDINGS{8057224,
author={J. Zhang and E. Modiano},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Robust routing in interdependent networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We consider a model of two interdependent networks, where every node in one network depends on one or more supply nodes in the other network and a node fails if it loses all of its supply nodes. We develop algorithms to compute the failure probability of a path, and obtain the most reliable path between a pair of nodes in a network, under the condition that each supply node fails independently with a given probability. Our work generalizes the classical shared risk group model, by considering multiple risks associated with a node and letting a node fail if all the risks occur. Moreover, we study the diverse routing problem by considering two paths between a pair of nodes. We define two paths to be d-failure resilient if at least one path survives after removing d or fewer supply nodes, which generalizes the concept of disjoint paths in a single network, and risk-disjoint paths in a classical shared risk group model. We compute the probability that both paths fail, and develop algorithms to compute the most reliable pair of paths.},
keywords={probability;telecommunication network reliability;telecommunication network routing;interdependent networks;supply node;risk-disjoint paths;shared risk group model;failure probability;diverse routing problem;Routing;Computer network reliability;Robustness;Computational modeling;Approximation algorithms;Algorithm design and analysis},
doi={10.1109/INFOCOM.2017.8057224},
ISSN={},
month={May},}
@INPROCEEDINGS{8057225,
author={J. Yu and X. Ning and Y. Sun and S. Wang and Y. Wang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Constructing a self-stabilizing CDS with bounded diameter in wireless networks under SINR},
year={2017},
volume={},
number={},
pages={1-9},
abstract={As a virtual backbone structure, connected dominating sets (CDSs) play an important role in topology control for wireless networks. In this paper, we develop a distributed self-stabilizing CDS construction algorithm under the SINR model (also known as the physical interference model), a more practical yet more challenging interference model for distributed algorithm design. Specifically, we propose a randomized distributed algorithm that can construct a CDS in O (log n) timeslots with a high probability, where n is the total number of nodes in the network. The constructed CDS achieves constant approximation in both density and diameter. To the best of our knowledge, this is the first known asymptotically optimal self-stabilizing result in terms of both density and diameter for distributed CDS construction under the practical SINR model.},
keywords={approximation theory;distributed algorithms;probability;radiofrequency interference;set theory;telecommunication network topology;wireless sensor networks;bounded diameter;wireless networks;virtual backbone structure;topology control;distributed self-stabilizing CDS construction algorithm;physical interference model;distributed algorithm design;randomized distributed algorithm;practical SINR model;asymptotically optimal self-stabilizing result;Interference;Signal to noise ratio;Algorithm design and analysis;Approximation algorithms;Wireless networks;Wireless sensor networks},
doi={10.1109/INFOCOM.2017.8057225},
ISSN={},
month={May},}
@INPROCEEDINGS{8057226,
author={W. Li and J. Zhang and Y. Zhao},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Conflict graph embedding for wireless network optimization},
year={2017},
volume={},
number={},
pages={1-9},
abstract={With the dense deployment of wireless infrastructure such as radio towers and WiFi access points, wireless network optimization becomes very important for improving the network capacity and enhancing the communication quality of wireless links. Most optimization algorithms rely on the conflict graph to describe the interference situation. However, building a conflict graph requires exhaustive measurements of the whole network and the existing estimation approaches are static and inaccurate. In this paper, we propose a conflict graph embedding approach to assess network interference situations by representing the wireless nodes with low-dimensional vectors while preserving their conflict relationships. Specifically, our approach introduces a sliding-window based partial measurement method to sample the interference graph in the network, then adopts a learning algorithm to obtain the vector representation of the nodes, and then infers the interference situations by exploring the feature vectors. The proposed approach has been proved to be low measurement overhead, low computational cost, and self-adaptive, which is suitable for large-scale dynamic wireless networks. We illustrate that conflict graph embedding can be used for interference-aware wireless network optimizations. We conduct extensive experiments based on real wireless network datasets, which show the efficiency of the proposed approach.},
keywords={graph theory;optimisation;radio networks;radiofrequency interference;sliding-window based partial measurement method;large-scale dynamic wireless networks;conflict graph embedding;interference-aware wireless network optimizations;wireless network datasets;interference situation;optimization algorithms;wireless links;network capacity;wireless network optimization;WiFi access points;Interference;Wireless networks;Optimization;Extraterrestrial measurements;Time measurement},
doi={10.1109/INFOCOM.2017.8057226},
ISSN={},
month={May},}
@INPROCEEDINGS{8057227,
author={S. Lin and I. F. Akyildiz},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Dynamic base station formation for solving NLOS problem in 5G millimeter-wave communication},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Millimeter-wave communication is one of the enabling technologies to meet high data-rate requirements of 5G wireless systems. Millimeter-wave systems due large available bandwidth enable gigabit-per-second data rates for line-of-sight (LOS) transmissions in short distances. However, for non-line-of-sight (NLOS) transmissions, millimeter-wave systems suffers performance degradation because the received signal strengths at user equipments (UEs) are not satisfactory. In this paper, the NLOS problem in millimeter-wave systems is treated from SoftAir (a wireless software-defined networking architecture) perspective. In particular, a so-called dynamic base station (BS) formation is introduced, which adaptively coordinates BSs and their multiple antennas to always satisfy UEs' quality-of-service (QoS) requirements in NLOS cases. First, the architecture for software-defined millimeter-wave system is introduced, where remote radio heads (RRHs) coordination is explained and millimeter-wave channel model between RRHs and UEs is analyzed. A ubiquitous millimeter-wave coverage problem is formulated, which jointly optimizes RRH-UE associations and beamforming weights of RRHs to maximize the UE sum-rate while guaranteeing QoS and system-level constraints. After proving the np-hardness of the coverage optimization problem with non-convex constraints, an iterative algorithm is developed for dynamic BS formation that achieves ubiquitous coverage with high data rates in LOS and NLOS cases. Through successive convex approximations, the proposed dynamic BS formation algorithm transforms the original mixed-integer nonlinear programming into a mixed-integer second-order cone programming, which is efficiently solved by convex tools. Simulations validate the efficacy of our solution that completely solves NLOS problem by facilitating ubiquitous coverage in 5G millimeter-wave systems.},
keywords={5G mobile communication;approximation theory;array signal processing;concave programming;convex programming;integer programming;iterative methods;quality of service;RSSI;software defined networking;5G millimeter-wave communication;line-of-sight transmission;received signal strengths;user equipments;SoftAir;wireless software-defined networking architecture;multiple antennas;quality-of-service;software-defined millimeter-wave system;remote radio heads;system-level constraints;coverage optimization problem;nonconvex constraints;beamforming weights;5G wireless systems;mixed-integer second-order cone programming;mixed-integer nonlinear programming;convex approximations;iterative algorithm;nonline-of-sight transmissions;NLOS problem;ubiquitous millimeter-wave coverage problem;millimeter-wave channel model;NLOS cases;dynamic base station formation;Millimeter wave communication;Nonlinear optics;5G mobile communication;Array signal processing;Computer architecture;Antennas},
doi={10.1109/INFOCOM.2017.8057227},
ISSN={},
month={May},}
@INPROCEEDINGS{8057228,
author={S. Borst and A. Ö. Kaya and D. Calin and H. Viswanathan},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Dynamic path selection in 5G multi-RAT wireless networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Emerging 5G networks will not only offer higher link rates, but also integrate a variety of Radio Access Technologies (RATs) in order to provide ultra-reliable broadband access to a wide range of applications with high throughput and low latency requirements. SDN-enabled dynamic path selection is of critical importance in exploiting the collective transmission resources in such heterogeneous multi-RAT environments and delivering excellent user performance. In the present paper we propose the `best-rate' path selection algorithm for multi-RAT networks with various types of traffic flows. The best-rate algorithm accounts for the radio conditions and performance requirements of individual flows as well as the load conditions at the various access points. We analytically establish that the rates received by the various flows under the best-rate path selection, in conjunction with local fair resource sharing at the individual access points, are close to globally Proportional Fair. Detailed simulation experiments demonstrate that the best-rate algorithm achieves significant gains in terms of user-perceived throughput performance over various baseline policies.},
keywords={5G mobile communication;radio access networks;resource allocation;software defined networking;traffic flows;best-rate algorithm accounts;radio conditions;individual access points;Radio Access Technologies;ultra-reliable broadband access;collective transmission resources;heterogeneous multiRAT environments;best-rate path selection algorithm;5G multi-RAT wireless networks;local fair resource sharing;SDN-enabled dynamic path selection;Resource management;Throughput;Heuristic algorithms;Streaming media;Sociology;Statistics;Load management},
doi={10.1109/INFOCOM.2017.8057228},
ISSN={},
month={May},}
@INPROCEEDINGS{8057229,
author={J. Kuo and S. Shen and H. Kang and D. Yang and M. Tsai and W. Chen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Service chain embedding with maximum flow in software defined network and application to the next-generation cellular network architecture},
year={2017},
volume={},
number={},
pages={1-9},
abstract={With software-defined network (SDN) and network function virtualization (NFV) techniques, we can embed the service chain consisting of a sequence of virtualized network functions (VNFs), i.e., we can determine the flow path and deploy the VNFs contained in the service chain at any place on the path. In the literature, the methods of service chain embedding bound the number of VNFs at a node, whereas the link capacities are disregarded and the amount of flows is not considered, which could cause serious congestion. In addition, according to our experiment, the process overhead on a computation node is linear to the total amount of flows processed. In this paper, we propose a method of service chain embedding to maximize the total amount of flows while bounding the process overhead of the flows on a node by its computation capability and the total amount of flows on an link by its bandwidth capacity. To our knowledge, our method is the first approximation algorithm of service chain embedding with considering flow in the literature. Simulations show our algorithm has good performance in terms of the total amount of flows.},
keywords={cellular radio;embedded systems;mobile computing;software defined networking;virtualisation;software defined network;next-generation cellular network architecture;virtualized network functions;flow path;VNF;SDN;service chain embedding;Routing;Encapsulation;Approximation algorithms;Bandwidth;Servers;Software;Network function virtualization},
doi={10.1109/INFOCOM.2017.8057229},
ISSN={},
month={May},}
@INPROCEEDINGS{8057230,
author={V. Sciancalepore and K. Samdanis and X. Costa-Perez and D. Bega and M. Gramaglia and A. Banchs},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Mobile traffic forecasting for maximizing 5G network slicing resource utilization},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The emerging network slicing paradigm for 5G provides new business opportunities by enabling multi-tenancy support. At the same time, new technical challenges are introduced, as novel resource allocation algorithms are required to accommodate different business models. In particular, infrastructure providers need to implement radically new admission control policies to decide on network slices requests depending on their Service Level Agreements (SLA). When implementing such admission control policies, infrastructure providers may apply forecasting techniques in order to adjust the allocated slice resources so as to optimize the network utilization while meeting network slices' SLAs. This paper focuses on the design of three key network slicing building blocks responsible for (i) traffic analysis and prediction per network slice, (ii) admission control decisions for network slice requests, and (iii) adaptive correction of the forecasted load based on measured deviations. Our results show very substantial potential gains in terms of system utilization as well as a trade-off between conservative forecasting configurations versus more aggressive ones (higher gains, SLA risk).},
keywords={5G mobile communication;contracts;quality of service;resource allocation;telecommunication congestion control;telecommunication traffic;different business models;infrastructure providers;admission control policies;allocated slice resources;network utilization;traffic analysis;admission control decisions;network slice requests;forecasted load;trade-off between conservative forecasting configurations;mobile traffic forecasting;resource utilization;emerging network slicing paradigm;business opportunities;multitenancy support;resource allocation algorithms;5G network slicing resource utilization;service level agreements;SLA;slicing building blocks;Forecasting;Admission control;5G mobile communication;3GPP;Prediction algorithms;Training;Resource management},
doi={10.1109/INFOCOM.2017.8057230},
ISSN={},
month={May},}
@INPROCEEDINGS{8057231,
author={S. Pradhan and L. Qiu and A. Parate and K. Kim},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Understanding and managing notifications},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In today's always-connected world, we receive a large number of notifications on our mobile devices. These notifications cause interruptions, stress, and even impact users' lifestyle. To understand how users respond to notifications, we develop an application that monitors various features (e.g., importance) of the notifications, users' actions, and the level of users' engagement with the notifications. We recruit 30 users to use the application and monitor over 30 days, and subsequently find that 20% to 50% of the notifications generally get ignored by the users. In addition, we also solicit explicit feedback about the importance of notifications from 12 users over 14 days and identify the relation between perceived importance and users' engagement level. Based on this study, we identify the key characteristics of notifications and users' engagement, which is further substantiated by an onfine survey of 400+ users. In addition, we develop a notification manager that includes a machine learning based prediction model and that shows only the important notifications and delays the unimportant notifications. Our experimental results show that our notification manager automatically assesses the importance of notifications with more than 87% accuracy. We believe this work is a promising step toward intelligent personal assistant that manages notifications.},
keywords={learning (artificial intelligence);mobile computing;user interfaces;notification manager;important notifications;notification understanding;notification management;always-connected world;mobile devices;user engagement;machine learning based prediction model;Electronic mail;Monitoring;Vibrations;Delays;Conferences;Mobile communication;Operating systems},
doi={10.1109/INFOCOM.2017.8057231},
ISSN={},
month={May},}
@INPROCEEDINGS{8057232,
author={Y. Chen and X. Jin and J. Sun and R. Zhang and Y. Zhang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={POWERFUL: Mobile app fingerprinting via power analysis},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Which apps a mobile user has and how they are used can disclose significant private information about the user. In this paper, we present the design and evaluation of POWERFUL, a new attack which can fingerprint sensitive mobile apps (or infer sensitive app usage) by analyzing the power consumption profiles on Android devices. POWERFUL works on the observation that distinct apps and their different usage patterns all lead to distinguishable power consumption profiles. Since the power profiles on Android devices require no permission to access, POWERFUL is very difficult to detect and can pose a serious threat against user privacy. Extensive experiments involving popular and sensitive apps in Google Play Store show that POWERFUL can identify the app used at any particular time with accuracy up to 92.9%, demonstrating the feasibility of POWERFUL.},
keywords={data privacy;mobile computing;power aware computing;security of data;smart phones;power analysis;mobile user;POWERFUL;sensitive app usage;Android devices;power profiles;sensitive mobile app fingerprinting;power consumption profiles;usage patterns;Google Play Store;user privacy;Androids;Humanoid robots;Mobile communication;Feature extraction;Power demand;Google;Mobile handsets},
doi={10.1109/INFOCOM.2017.8057232},
ISSN={},
month={May},}
@INPROCEEDINGS{8057233,
author={Z. Li and M. Li and P. Mohapatra and J. Han and S. Chen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={iType: Using eye gaze to enhance typing privacy},
year={2017},
volume={},
number={},
pages={1-9},
abstract={This paper presents iType, a system that uses eye gaze for typing private information on commodity mobile platforms. The design combats three primary challenges: 1) relatively low accuracy of mobile gaze tracking; 2) difficulties in correcting input errors due to lacking the comparison with the true text-entry value; and 3) device motions and other noises that may interfere gaze tracking accuracy and thus the iType performance. We devise a set of effective techniques, including leveraging a collective behavior of the gaze tracking results, unique correlation of the typing error spatial distributions, and motion sensor hints from mobile devices, to address above challenges. A set of enhancement techniques are applied to further improve iType's robustness and reliability. We consolidate above designs and implement iType on iOS platform. Evaluations show that iType achieves high keystroke detection accuracy for the secure typing within a reasonable short latency.},
keywords={data privacy;eye;gaze tracking;mobile computing;iOS platform;high keystroke detection accuracy;secure typing;eye gaze;typing privacy;private information;commodity mobile platforms;mobile gaze tracking;input errors;text-entry value;gaze tracking accuracy;iType performance;leveraging a collective behavior;typing error spatial distributions;motion sensor hints;mobile devices;enhancement techniques;reliability;Gaze tracking;Mobile handsets;Cameras;Keyboards;Engines;Mobile communication;Privacy},
doi={10.1109/INFOCOM.2017.8057233},
ISSN={},
month={May},}
@INPROCEEDINGS{8057234,
author={P. Y. Cao and G. Li and A. C. Champion and D. Xuan and S. Romig and W. Zhao},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={On human mobility predictability via WLAN logs},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In this research, we conduct a comprehensive measurement study on the predictability of human mobility with respect to demographic differences. We leverage an extensive WLAN dataset collected on a large university campus. Specifically, our dataset includes over 41 million WLAN entries gathered from over 5,000 students (with demographic information) during a four-month period in 2015. We observed surprising patterns on large increases of long-term mobility entropy by age, and the impact of academic majors on students long-term mobility entropy. The distribution of long-term entropy follows a bimodal distribution, which is different from previous studies. We also find that the predictability of students' short-term (daily or weekly) mobility varies on different days of the week and with student gender. Because of the large campus size, our results can mimic people's mobility patterns in metropolitan areas. We also anticipate that our results will provide insight that guides academic administrators' decisions regarding facilities planning, emergency management, etc. on campus.},
keywords={demography;educational institutions;entropy;Internet;mobile computing;social sciences computing;wireless LAN;human mobility predictability;WLAN logs;comprehensive measurement study;demographic differences;university campus;demographic information;academic majors;WLAN dataset;student long-term mobility entropy;metropolitan area;campus size;student gender;bimodal distribution;Entropy;Wireless LAN;Mobile handsets;Buildings;Mobile communication;Global Positioning System;Conferences},
doi={10.1109/INFOCOM.2017.8057234},
ISSN={},
month={May},}

