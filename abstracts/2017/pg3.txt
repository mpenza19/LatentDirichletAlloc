@INPROCEEDINGS{8057141,
author={O. Bilgen and A. B. Wagner},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A new stable peer-to-peer protocol with non-persistent peers},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Recent studies have suggested that the stability of peer-to-peer networks may rely on persistent peers, who dwell on the network after they obtain the entire file. In the absence of such peers, one piece becomes extremely rare in the network, which leads to instability. Technological developments, however, are poised to reduce the incidence of persistent peers, giving rise to a need for a protocol that guarantees stability with nonpersistent peers. We propose a novel peer-to-peer protocol, the group suppression protocol, to ensure the stability of peer-to-peer networks under the scenario that all the peers adopt non-persistent behavior. Using a suitable Lyapunov potential function, the group suppression protocol is proven to be stable when the file is broken into two pieces, and detailed experiments demonstrate the stability of the protocol for arbitrary number of pieces. Straightforward incorporation of the group suppression protocol into BitTorrent while retaining most of BitTorrent's core mechanisms is also presented. Subsequent simulations show that under certain assumptions, BitTorrent with the official protocol cannot escape from the missing piece syndrome, but BitTorrent with group suppression does.},
keywords={Lyapunov methods;peer-to-peer computing;protocols;persistent peers;nonpersistent peers;peer-to-peer protocol;group suppression protocol;peer-to-peer network stability;Lyapunov potential function;BitTorrent;Protocols;Stability analysis;Peer-to-peer computing;Wireless communication;Conferences;Mobile communication;Servers},
doi={10.1109/INFOCOM.2017.8057141},
ISSN={},
month={May},}
@INPROCEEDINGS{8057142,
author={B. Spang and A. Sabnis and R. Sitaraman and D. Towsley and B. DeCleene},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={MON: Mission-optimized overlay networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Large organizations often have users in multiple sites which are connected over the Internet. Since resources are limited, communication between these sites needs to be carefully orchestrated for the most benefit to the organization. We present a Mission-optimized Overlay Network (MON), a hybrid overlay network architecture for maximizing utility to the organization. We combine an offline and an online system to solve non-concave utility maximization problems. The offline tier, the Predictive Flow Optimizer (PFO), creates plans for routing traffic using a model of network conditions. The online tier, MONtra, is aware of the precise local network conditions and is able to react quickly to problems within the network. Either tier alone is insufficient. The PFO may take too long to react to network changes. MONtra only has local information and cannot optimize non-concave mission utilities. However, by combining the two systems, MON is robust and achieves near-optimal utility under a wide range of network conditions. While best-effort overlay networks are well studied, our work is the first to design overlays that are optimized for mission utility.},
keywords={Internet;optimisation;overlay networks;peer-to-peer computing;telecommunication network routing;telecommunication traffic;nonconcave utility maximization problems;Predictive Flow Optimizer;precise local network conditions;near-optimal utility;mission-optimized overlay networks;hybrid overlay network architecture;Internet;MON;routing traffic;PFO offline tier;MONtra online tier;local network conditions;nonconcave mission utilities optimization;Optimization;Organizations;Overlay networks;Computer architecture;Routing;Internet telephony},
doi={10.1109/INFOCOM.2017.8057142},
ISSN={},
month={May},}
@INPROCEEDINGS{8057143,
author={L. Toka and B. Lajtha and É. Hosszu and B. Formanek and D. Géhberger and J. Tapolcai},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A resource-aware and time-critical IoT framework},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Internet of Things (IoT) systems produce great amount of data, but usually have insufficient resources to process them in the edge. Several time-critical IoT scenarios have emerged and created a challenge of supporting low latency applications. At the same time cloud computing became a success in delivering computing as a service at affordable price with great scalability and high reliability. We propose an intelligent resource allocation system that optimally selects the important IoT data streams to transfer to the cloud for processing. The optimization runs on utility functions computed by predictor algorithms that forecast future events with some probabilistic confidence based on a dynamically recalculated data model. We investigate ways of reducing specifically the upload bandwidth of IoT video streams and propose techniques to compute the corresponding utility functions. We built a prototype for a smart squash court and simulated multiple courts to measure the efficiency of dynamic allocation of network and cloud resources for event detection during squash games. By continuously adapting to the observed system state and maximizing the expected quality of detection within the resource constraints our system can save up to 70% of the resources compared to the naive solution.},
keywords={cloud computing;Internet of Things;resource allocation;sport;video streaming;intelligent resource allocation system;dynamically recalculated data model;IoT video streams;dynamic cloud resource allocation;dynamic network resource allocation;smart squash court;upload bandwidth;IoT data streams;Internet of Things;resource-aware time-critical IoT scenarios;utility functions;cloud computing;Cloud computing;Bandwidth;Streaming media;Cameras;Quality of service;Uplink;Resource management;Internet of Things;cloud computing;cloud control;resource provisioning;adaptive;dynamic;QoS;QoE},
doi={10.1109/INFOCOM.2017.8057143},
ISSN={},
month={May},}
@INPROCEEDINGS{8057144,
author={D. Trihinas and G. Pallis and M. D. Dikaiakos},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={ADMin: Adaptive monitoring dissemination for the Internet of Things},
year={2017},
volume={},
number={},
pages={1-9},
abstract={As more knowledge is vastly added to the devices fuelling the Internet of Things (IoT) energy efficiency and real-time data processing are great challenges that must be tackled. In this paper, we introduce ADMin, a low-cost IoT framework that reduces on device energy consumption and the volume of data disseminated across the network. This is achieved by efficiently adapting the rate at which IoT devices disseminate monitoring streams based on run-time knowledge of the stream evolution, variability and seasonal behavior. Rather than transmitting the entire stream, ADMin favors sending updates for its estimation model from which values can be inferred, triggering dissemination only when shifts in the stream evolution are detected. Results on real-life testbeds, show that ADMin is able to reduce energy consumption by at least 83%, data volume by 71%, shift detection delays by 61% while maintaining accuracy above 91% in comparison to other IoT frameworks.},
keywords={data dissemination;energy consumption;Internet of Things;telecommunication power management;ADMin;low-cost IoT framework;run-time knowledge;data volume;IoT frameworks;adaptive monitoring dissemination;real-time data processing;IoT devices;Internet of Things energy efficiency;triggering dissemination;device energy consumption reduction;Measurement;Monitoring;Estimation;Adaptation models;Energy consumption;Receivers;Sensors},
doi={10.1109/INFOCOM.2017.8057144},
ISSN={},
month={May},}
@INPROCEEDINGS{8057145,
author={J. Zhang and Z. Wang and Z. Yang and Q. Zhang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Proximity based IoT device authentication},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Internet of Things (IoT) devices are largely embedded devices which lack a sophisticated user interface, e.g., touch screen, keyboard, etc. As a consequence, traditional Pre-Shared Key (PSK) based authentication for mobile devices becomes difficult to apply. For example, according to our study on home automation devices which leverage smartphone for PSK input, the current process does not protect against active impersonating attack and also leaks the Wi-Fi password to eavesdroppers, i.e., currently these IoT devices can be exploited to enter into critical infrastructures, e.g., home networks. Motivated by this real-world security vulnerability, in this paper we propose a novel proximity-based mechanism for IoT device authentication, called Move2Auth, for the purpose of enhancing IoT device security. In Move2Auth, we require user to hold smartphone and perform one of two hand-gestures (moving towards and away, and rotating) in front of IoT device. By combining (1) large RSS-variation and (2) matching between RSS-trace and smartphone sensor-trace, Move2Auth can reliably detect proximity and authenticate IoT device accordingly. Based on our implementation on Samsung Galaxy smartphone and commodity Wi-Fi adapter, we prove Move2Auth can protect against powerful active attack, i.e., the false-positive rate is consistently lower than 0.5%.},
keywords={computer network security;cryptographic protocols;Internet;Internet of Things;message authentication;mobile computing;security of data;smart phones;telecommunication security;user interfaces;wireless LAN;IoT device authentication;called Move2Auth;smartphone sensor-trace;proximity;authenticate IoT device;Key based authentication;mobile devices;home automation devices;IoT device security;Wi-Fi password;Wireless fidelity;Authentication;Home automation;Cryptography;Performance evaluation;Phase shift keying},
doi={10.1109/INFOCOM.2017.8057145},
ISSN={},
month={May},}
@INPROCEEDINGS{8057146,
author={S. Park and S. Lim and D. Jeong and J. Lee and J. Yang and H. Lee},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={PUFSec: Device fingerprint-based security architecture for Internet of Things},
year={2017},
volume={},
number={},
pages={1-9},
abstract={A low-end embedded platform for Internet of Things (IoT) often suffers from a critical trade-off dilemma between security enhancement and computation overhead. We propose PUFSec, a new device fingerprint-based security architecture for IoT devices. By leveraging intrinsic hardware characteristics, we aim to design a computationally lightweight security software system architecture so that complex cryptography computation can dramatically be prohibited. We exploit the innovative idea of Public Physical Unclonable Functions (PPUFs) that fundamentally protects attackers from recovering the secret key from public gate delay information. We implement its hardware logic in a real-world FPGA board. On top of the PPUF fingerprint hardware, we present an adaptive security control mechanism consisting of adaptive key generation and key exchange protocol, which adjusts security strength depending on system load dynamics. We demonstrate that our PPUF FPGA implementation embeds distinctive variability enough to distinguish between two different PPUFs with high fidelity. We validate our PUFSec architecture by implementing necessary algorithms and protocols in a real-world IoT platform, and performing empirical evaluation in terms of computation and memory usages, proving its practical feasibility.},
keywords={cryptographic protocols;embedded systems;field programmable gate arrays;Internet of Things;private key cryptography;public key cryptography;telecommunication security;low-end embedded platform;security enhancement;computation overhead;IoT devices;intrinsic hardware characteristics;computationally lightweight security software system architecture;complex cryptography computation;Public Physical Unclonable Functions;secret key;public gate delay information;hardware logic;real-world FPGA board;PPUF fingerprint hardware;adaptive security control mechanism;adaptive key generation;key exchange protocol;security strength;system load dynamics;PPUF FPGA implementation;PUFSec architecture;real-world IoT platform;device fingerprint-based security architecture;Internet of Things;Hardware;Logic gates;Delays;Cryptography;Software;Radiation detectors},
doi={10.1109/INFOCOM.2017.8057146},
ISSN={},
month={May},}
@INPROCEEDINGS{8057147,
author={K. Sucipto and D. Chatzopoulos and S. Kosta± and P. Hui},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Keep your nice friends close, but your rich friends closer — Computation offloading using NFC},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The increasing complexity of smartphone applications and services necessitate high battery consumption but the growth of smartphones' battery capacity is not keeping pace with these increasing power demands. To overcome this problem, researchers gave birth to the Mobile Cloud Computing (MCC) research area. In this paper we advance on previous ideas, by proposing and implementing the first known Near Field Communication (NFC)-based computation offloading framework. This research is motivated by the advantages of NFC's short distance communication, with its better security, and its low battery consumption. We design a new NFC communication protocol that overcomes the limitations of the default protocol; removing the need for constant user interaction, the one-way communication restraint, and the limit on low data size transfer. We present experimental results of the energy consumption and the time duration of two computationally intensive representative applications: (i) RSA key generation and encryption, and (ii) gaming/puzzles. We show that when the helper device is more powerful than the device offloading the computations, the execution time of the tasks is reduced. Finally, we show that devices that offload application parts considerably reduce their energy consumption due to the low-power NFC interface and the benefits of offloading.},
keywords={cloud computing;cryptography;mobile computing;near-field communication;smart phones;low-power NFC interface;smartphone applications;Mobile Cloud Computing research area;NFC communication protocol;constant user interaction;one-way communication restraint;low data size transfer;energy consumption;computationally intensive representative applications;encryption;device offloading;offload application parts;power demands;smartphone application complexity;near field communication based computation offloading framework;NFC-based computation offloading framework;NFC short distance communication;RSA key generation;Protocols;Batteries;Wireless fidelity;Androids;Humanoid robots;Mobile communication;Bluetooth},
doi={10.1109/INFOCOM.2017.8057147},
ISSN={},
month={May},}
@INPROCEEDINGS{8057148,
author={S. Jošilo and G. Dán},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A game theoretic analysis of selfish mobile computation offloading},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Offloading computation to a mobile cloud is a promising approach for enabling the use of computationally intensive applications by mobile devices. In this paper we consider autonomous devices that maximize their own performance by choosing one of many wireless access points for computation offloading. We develop a game theoretic model of the problem, prove the existence of pure strategy Nash equilibria, and provide a polynomial time algorithm for computing an equilibrium. For the case when the cloud computing resources scale with the number of mobile devices we show that all improvement paths are finite. We provide a bound on the price of anarchy of the game, thus our algorithm serves as an approximation algorithm for the global computation offloading cost minimization problem. We use extensive simulations to provide insight into the performance and the convergence time of the algorithms in various scenarios. Our results show that the equilibrium cost may be close to optimal, and the convergence time is almost linear in the number of mobile devices.},
keywords={cloud computing;game theory;minimisation;mobile computing;polynomial approximation;computationally intensive applications;mobile devices;autonomous devices;wireless access points;game theoretic model;pure strategy Nash equilibria;polynomial time algorithm;approximation algorithm;global computation offloading cost minimization problem;convergence time;game theoretic analysis;selfish mobile computation offloading;mobile cloud;cloud computing resources;Mobile handsets;Games;Cloud computing;Mobile communication;Approximation algorithms;Computational modeling;Performance evaluation},
doi={10.1109/INFOCOM.2017.8057148},
ISSN={},
month={May},}
@INPROCEEDINGS{8057149,
author={J. P. Champati and B. Liang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Single restart with time stamps for computational offloading in a semi-online setting},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We study the problem of scheduling n tasks on m + m' parallel processors, where the processing times on m processors are known while those on the remaining m' processors are not known a priori. This semi-online model is an abstraction of certain heterogeneous computing systems, e.g., with the m known processors representing local CPU cores and the unknown processors representing remote servers with uncertain availability of computing cycles. Our objective is to minimize the makespan of all tasks. We initially focus on the case m' = 1 and propose a semi-online algorithm termed Single Restart with Time Stamps (SRTS), which has time complexity O(n log n). We derive its competitive ratio in comparison with the optimal offline solution. If the unknown processing times are deterministic, the competitive ratio of SRTS is shown to be either always constant or asymptotically constant in practice, respectively in cases where the processing times are independent and dependent on m. A similar result is obtained when the unknown processing times are random. Furthermore, extending the ideas of SRTS, we propose a heuristic algorithm termed SRTS-Multiple (SRTS-M) for the case m' &gt; 1. Besides the proven competitive ratios, simulation results further suggest that SRTS and SRTS-M give superior performance on average over randomly generated task processing times, substantially reducing the makespan over the best known alternatives. Interestingly, the performance gain is more significant for task processing times sampled from heavy-tailed distributions.},
keywords={computational complexity;processor scheduling;SRTS-M;randomly generated task processing times;computational offloading;parallel processors;local CPU cores;unknown processors;computing cycles;semionline algorithm;competitive ratio;unknown processing times;task scheduling;heterogeneous computing systems;task makespan minimization;single restart-with-time stamps;O(n log n) time complexity;SRTS-multiple heuristic algorithm;Program processors;Processor scheduling;Cloud computing;Computational modeling;Servers;Heuristic algorithms},
doi={10.1109/INFOCOM.2017.8057149},
ISSN={},
month={May},}
@INPROCEEDINGS{8057150,
author={M. Chen and B. Liang and M. Dong},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Joint offloading and resource allocation for computation and communication in mobile cloud with computing access point},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We consider a general multi-user mobile cloud computing system with a computing access point (CAP), where each mobile user has multiple independent tasks that may be processed locally, at the CAP, or at a remote cloud server. The CAP serves both as the network access gateway and a computation service provider to the mobile users. We aim to jointly optimize the offloading decisions of all users' tasks as well as the allocation of computation and communication resources, to minimize the overall cost of energy, computation, and delay for all users. This problem is NP-hard in general. We propose an efficient three-step algorithm comprising of semidefinite relaxation (SDR), alternating optimization (AO), and sequential tuning (ST). It is shown to always compute a locally optimal solution, and give nearly optimal performance under a wide range of parameter settings. Through evaluating the performance of different combinations of the three components of this SDR-AO-ST algorithm, we provide insights into their roles and contributions in the overall solution. We further compare the performance of SDR-AO-ST against a lower bound to the minimum cost, purely local processing, purely cloud processing, and hybrid local-cloud processing without using the CAP. Our numerical results demonstrate the effectiveness of the proposed algorithm in the joint management of computation and communication resources in mobile cloud computing systems with a CAP.},
keywords={cloud computing;mathematical programming;mobile computing;resource allocation;resource allocation;computing access point;multiuser mobile cloud computing system;CAP;mobile user;multiple independent tasks;remote cloud server;network access gateway;computation service provider;offloading decisions;locally optimal solution;optimal performance;SDR-AO-ST algorithm;purely local processing;purely cloud processing;joint offloading;computation resource allocation;communication resource allocation;overall cost of energy minimization;NP-hard problem;semidefinite relaxation;alternating optimization;sequential tuning;lower bound;hybrid local-cloud processing;Mobile communication;Delays;Cloud computing;Servers;Resource management;Mobile handsets},
doi={10.1109/INFOCOM.2017.8057150},
ISSN={},
month={May},}
@INPROCEEDINGS{8057151,
author={Y. Zhang and D. Li and T. Tian and P. Zhong},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={CubeX: Leveraging glocality of cube-based networks for RAM-based key-value store},
year={2017},
volume={},
number={},
pages={1-9},
abstract={RAM-based storage aggregates the RAM of servers in data center networks (DCN) to provide extremely high storage performance. For quick recovery of storage server failures, Mem-Cube [1] exploits the proximity of the BCube network to limit the recovery traffic to the recovery servers' 1-hop neighborhood. However, previous design is applicable only to BCube, and has suboptimal recovery performance due to congestion and contention. To address these problems, in this paper we propose CubeX, which generalizes the “1-hop” principle of MemCube for all cube-based networks, and improves the throughput and recovery performance of RAM-based key-value (KV) store via cross-layer optimizations. At the core of CubeX is to leverage the glocality (= globality + locality) of cube-based networks: it scatters backup data across a large number of disks globally distributed throughout the cube, and restricts all recovery traffic within the small local range of each server node. Our evaluation shows that CubeX efficiently supports RAM-based KV store for cube-based networks, and CubeX remarkably outperforms MemCube in both throughput and recovery time.},
keywords={computer centres;hypercube networks;optimisation;random-access storage;storage management;telecommunication traffic;data center networks;storage server failures;BCube network;recovery traffic;recovery servers;CubeX;cube-based networks;Mem-Cube;RAM-based key-value store;cross-layer optimizations;glocality;Servers;Random access memory;Throughput;Hypercubes;Bandwidth;Conferences},
doi={10.1109/INFOCOM.2017.8057151},
ISSN={},
month={May},}
@INPROCEEDINGS{8057152,
author={F. Wang and L. Gao and S. Xiaozhe and H. Harai and K. Fujikawa},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Towards reliable and lightweight source switching for datacenter networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={A low-latency and reliable message switching network is critical for constructing high-speed datacenter networks. In this paper, we present the design, implementation, and evaluation of a novel Location basEd Source Switching (LESS) for datacenter networks. LESS enables lightweight source switching through a location-based addressing scheme. Each switch and host can independently derive a source route to reach a destination without requiring the full knowledge of the network topology. We demonstrate that using location-based source routes as forwarding labels allows LESS to eliminate the need for routing tables and integrate with minimum required functionality for packet forwarding. Moreover, we propose a fast rerouting solution to address the issue of fault tolerance in source routing. Each switch can locally derive an alternative source route during a failure. The paper evaluates the performance of LESS. Our evaluation results suggest that LESS improves the performance of datacenter networks in terms of latency, throughput, and reliability.},
keywords={computer centres;fault tolerance;telecommunication network routing;telecommunication network topology;telecommunication switching;fault tolerance;packet forwarding;LESS;lightweight source switching;alternative source route;source routing;routing tables;location-based source routes;network topology;novel Location basEd Source Switching;high-speed datacenter networks;reliable message switching network;Switches;Ports (Computers);Topology;Network topology;Routing;Reliability;IP networks},
doi={10.1109/INFOCOM.2017.8057152},
ISSN={},
month={May},}
@INPROCEEDINGS{8057153,
author={J. Fan and C. Guan and Y. Zhao and C. Qiao},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Availability-aware mapping of service function chains},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Network Function Virtualization (NFV) is a promising technique to greatly improve the effectiveness and flexibility of network services through a process named Service Function Chain (SFC) mapping, with which different network services are deployed over virtualized and shared platforms in data centers. However, such an evolution towards software-defined network functions introduces new challenges to network services which require high availability. One effective way of protecting the network services is to use sufficient redundancy. By doing so, however, the efficiency of physical resources may be greatly decreased. To address such an issue, this paper defines an optimal availability-aware SFC mapping problem and presents a novel online algorithm that can minimize the physical resources consumption while guaranteeing the required high availability within a polynomial time. Simulation results show that our proposed algorithm can significantly improve SFC mapping request acceptance ratio and reduce resource consumption.},
keywords={computer centres;computer network management;virtualisation;availability-aware SFC mapping problem;data centers;resource consumption;Service Function Chain mapping;Network Function Virtualization;service function chains;availability-aware mapping;SFC mapping request acceptance ratio;software-defined network functions;virtualized shared platforms;Redundancy;Delays;Computational modeling;Bandwidth;Conferences;Approximation algorithms},
doi={10.1109/INFOCOM.2017.8057153},
ISSN={},
month={May},}
@INPROCEEDINGS{8057154,
author={H. Saito and H. Honda and R. Kawahara},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Disaster avoidance control against heavy rainfall},
year={2017},
volume={},
number={},
pages={1-9},
abstract={This paper proposes a disaster avoidance control method for use against heavy rainfall and discusses its effectiveness through actual weather data. The proposed control method uses geographical information data including weather data, hazard area data, and physical network data. By applying technologies related to meteorology, erosion control, and civil engineering to such data, the proposed method can evaluate the risk of a physical network being disconnected. On the basis of the evaluated risk, the proposed method reconfigures a logical network to reduce service disruption. The proposed method is applied to a cloud computing service network where, in addition to route changes, the relocation of virtual machines is possible, increasing its effectiveness. By using empirical data, we show that the proposed method reduces the probability of service disconnection to almost zero even for heavy rainfall causing landslides. Finally, an experimental system of the proposed method was implemented through software defined network technology and successfully controlled the experimental network.},
keywords={cloud computing;disasters;emergency management;erosion;geographic information systems;geomorphology;rain;software defined networking;virtual machines;erosion control;civil engineering;logical network;service disruption;service disconnection;software defined network technology;disaster avoidance control method;geographical information data;hazard area data;physical network data;heavy rainfall;weather data;landslides;meteorology;virtual machines relocation;cloud computing service network;Hazards;Measurement;Rain;Control systems;Conferences;Terrain factors},
doi={10.1109/INFOCOM.2017.8057154},
ISSN={},
month={May},}
@INPROCEEDINGS{8057155,
author={M. Ashour and J. Wang and C. Lagoa and N. Aybat and H. Che},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Non-concave network utility maximization: A distributed optimization approach},
year={2017},
volume={},
number={},
pages={1-9},
abstract={This paper proposes an algorithm for optimal decentralized traffic engineering in communication networks. We aim at distributing the traffic among the available routes such that the network utility is maximized. In some practical applications, modeling network utility using non-concave functions is of particular interest, e.g., video streaming. Therefore, we tackle the problem of optimizing a generalized class of non-concave utility functions. The approach used to solve the resulting non-convex network utility maximization (NUM) problem relies on designing a sequence of convex relaxations whose solutions converge to that of the original problem. A distributed algorithm is proposed for the solution of the convex relaxation. Each user independently controls its traffic in a way that drives the overall network traffic allocation to an optimal operating point subject to network capacity constraints. All computations required by the algorithm are performed independently and locally at each user using local information and minimal communication overhead. The only non-local information needed is binary feedback from congested links. The robustness of the algorithm is demonstrated, where the traffic is shown to be automatically rerouted in case of a link failure or having new users joining the network. Numerical simulation results are presented to validate our findings.},
keywords={concave programming;convex programming;distributed algorithms;radio networks;telecommunication network routing;telecommunication traffic;nonconcave network utility maximization;distributed optimization approach;optimal decentralized traffic engineering;communication networks;nonconcave utility functions;distributed algorithm;convex relaxation;network traffic allocation;network capacity constraints;nonconvex network utility maximization problem;convex relaxations;binary feedback;Resource management;Optimization;Quality of service;Distributed algorithms;Conferences;Communication networks;Streaming media;Distributed optimization;non-concave utility maximization;traffic engineering},
doi={10.1109/INFOCOM.2017.8057155},
ISSN={},
month={May},}
@INPROCEEDINGS{8057156,
author={P. Wan and F. Al-dhelaan and H. Yuan and S. Ji},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Fractional wireless link scheduling and polynomial approximate capacity regions of wireless networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Fractional Link scheduling is one of the most fundamental problems in wireless networks. The prevailing approach for shortest fractional link scheduling is based on a reduction to the maximum-weighted independent set problem, which itself may not admit efficient approximation algorithms. In addition, except for the wireless networks under the protocol interference model, none of the existing scheduling algorithms can produce a link schedule with explicit upper bounds on its length in terms of the link demands. As the result, the polynomial approximate capacity regions in these networks remain blank. This paper develops a purely combinatorial paradigm for fractional link scheduling in wireless networks. In addition to the superior efficiency, it is able to provide explicit upper bounds on the lengths of the produced link schedule. By exploiting these upper bounds, polynomial approximate capacity regions are derived. The effectiveness of this new paradigm is demonstrated by its applications in wireless networks under the physical interference model and wireless MIMO networks under the protocol interference model.},
keywords={approximation theory;computational complexity;MIMO communication;polynomial approximation;protocols;radio links;radiofrequency interference;scheduling;set theory;telecommunication scheduling;fractional wireless link scheduling;polynomial approximate capacity regions;shortest fractional link scheduling;maximum-weighted independent set problem;protocol interference model;explicit upper bounds;wireless MIMO networks;combinatorial paradigm;approximation algorithms;Wireless networks;Interference;Schedules;Games;Approximation algorithms;Protocols;Upper bound},
doi={10.1109/INFOCOM.2017.8057156},
ISSN={},
month={May},}
@INPROCEEDINGS{8057157,
author={H. Yu and M. J. Neely},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A new backpressure algorithm for joint rate control and routing with vanishing utility optimality gaps and finite queue lengths},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The backpressure algorithm has been widely used as a distributed solution to the problem of joint rate control and routing in multi-hop data networks. By controlling a parameter V in the algorithm, the backpressure algorithm can achieve an arbitrarily small utility optimality gap. However, this in turn brings in a large queue length at each node and hence causes large network delay. This phenomenon is known as the fundamental utility-delay tradeoff. The best known utility-delay tradeoff for general networks is [O(1/V), O(V)] and is attained by a backpressure algorithm based on a drift-pluspenalty technique. This may suggest that to achieve an arbitrarily small utility optimality gap, the existing backpressure algorithms necessarily yield an arbitrarily large queue length. However, this paper proposes a new backpressure algorithm that has a vanishing utility optimality gap, so utility converges to exact optimality as the algorithm keeps running, while queue lengths are bounded throughout by a finite constant. The technique uses backpressure and drift concepts with a new method for convex programming.},
keywords={data communication;delays;distributed algorithms;optimisation;queueing theory;telecommunication network routing;backpressure algorithm;finite queue lengths;multihop data networks;queue length;joint rate control and routing;utility-delay tradeoff;small utility optimality gap;network delay;drift-pluspenalty technique;convex programming;Routing;Delays;Heuristic algorithms;Approximation algorithms;Conferences;Optimization;Network topology},
doi={10.1109/INFOCOM.2017.8057157},
ISSN={},
month={May},}
@INPROCEEDINGS{8057158,
author={S. Vargaftik and I. Keslassy and A. Orda},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Stable user-defined priorities},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Network providers now want to enable users to define their own flow priorities, and commercial devices already implement this ability. However, it has been shown that directly applying arbitrary user-defined priorities can fundamentally destabilize a network. In this paper, we show that it is possible to apply user-defined priorities while keeping the network stable. We introduce U-BP, a scalable approach that extends backpressure-based scheduling techniques to service user-defined flow priorities and rates while maintaining throughput optimality and strong network performance. We explain how our approach relies on a dual-layer scheme with an exponential convergence to requested priorities. We further prove analytically the network stability of our solution, and show how it achieves a strong performance for high-priority flows.},
keywords={quality of service;radio networks;telecommunication scheduling;network providers;commercial devices;arbitrary user-defined priorities;network stable;scalable approach;user-defined flow priorities;requested priorities;high-priority flows;network performance;backpressure-based scheduling techniques;U-BP;Stability analysis;Heuristic algorithms;Standards;Topology;Delays;Conferences;Throughput},
doi={10.1109/INFOCOM.2017.8057158},
ISSN={},
month={May},}
@INPROCEEDINGS{8057159,
author={J. Liu and M. Chen and S. Chen and Q. Pan and L. Chen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Tag-compass: Determining the spatial direction of an object with small dimensions},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Identifying an object's spatial direction (or orientation) plays a fundamental role in a variety of applications, such as automatic assembly, indoor navigation, and robot driving. In this paper, we design a fine-grained direction finding system called Tag-Compass that attaches a single tag to an object (whose size may be small) and identifies the tagged object's orientation by determining the spatial direction of the tag. We exploit the polarization properties of the RF waves used in the communications between an RFID reader and the tag on the object. Polarization mismatch between the tag and the reader's antenna affects the received signal strength at the reader. From the measured signal strength values, we are able to deduce the tag's direction through a series of transformations and deviation minimization. We propose a system design for Tag-Compass and implement a prototype. We evaluate the performance of TagCompass through extensive experiments using the prototype. The experimental results show that Tag-Compass provides accurate direction estimates with a median error of just 2.5° when the tag's position is known and a median error of 3.8° when the tag's position is unknown.},
keywords={electromagnetic wave polarisation;radiofrequency identification;RSSI;fine-grained direction finding system;Tag-compass;Tag-Compass;spatial direction;tagged object;Robots;Radiofrequency identification;Receivers;Transmitters;Object recognition;Directive antennas;Radio frequency},
doi={10.1109/INFOCOM.2017.8057159},
ISSN={},
month={May},}
@INPROCEEDINGS{8057160,
author={S. Zhang and X. Liu and J. Wang and J. Cao},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Tag size profiling in multiple reader RFID systems},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In this paper, we study the tag size profiling (TSP) problem in RFID systems with multiple readers, which is to estimate the number of tags in every subregion in the system covered by different set of readers. The TSF problem is vitally important to reader scheduling and many other related operations in large scale multi-reader RFID systems. To our knowledge, however, it is not well solved in previous researches. We propose a novel approach to the TSF problem. The key idea is to treat the size of subregions as variables and construct a linear system in these variables to solve them. We theoretically prove that for any multi-reader RFID system, a linear system that can be used to uniquely solve the variables corresponding to the subregion sizes can always be constructed. We then propose a time-efficient algorithm that uses two heuristics to quickly find enough linearly independent equations to construct the linear system. Extensive simulation results show that the proposed approach achieves very high accuracy. When the estimation results of individual readers contain 5% errors, our approach achieves median estimation error of smaller than 0.02 and 90-percentile estimation error of smaller than 0.04 in large systems containing more than one hundred readers.},
keywords={radiofrequency identification;telecommunication scheduling;multiple reader RFID systems;tag size profiling problem;multiple readers;TSF problem;reader scheduling;scale multireader RFID systems;linear system;multireader RFID system;subregion sizes;linearly independent equations;individual readers;TSP;Conferences;Radio Frequency Identification;Multiple Readers;Reader Scheduling;RFID Estimation},
doi={10.1109/INFOCOM.2017.8057160},
ISSN={},
month={May},}
@INPROCEEDINGS{8057161,
author={C. Duan and X. Rao and L. Yang and Y. Liu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Fusing RFID and computer vision for fine-grained object tracking},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In recent years, both the RFID and computer vision technologies have been widely employed in indoor scenarios aimed at different goals while faced with respective limitations. For example, the RFID-based EAS system is useful in quickly identifying tagged objects but the accompanying false alarm problem is troublesome and hard to tackle with except that the accurate trajectory of the target tag can be easily acquired. On the other side, the CV system performs fairly well in tracking multiple moving objects precisely while finding it difficult to screen out the specific target among them. To overcome the above limitations, we present TagVision, a hybrid RFID and computer vision system for fine-grained localization and tracking of tagged objects. A fusion algorithm is proposed to organically combine the position information given by the CV subsystem, and phase data output by the RFID subsystem. In addition, we employ the probabilistic model to eliminate the measurement error caused by thermal noise and device diversity. We have implemented TagVision with COTS camera and RFID devices and evaluated it extensively in our lab environment. Experimental results show that TagVision can achieve 98% blob matching accuracy and 10.33mm location tracking precision.},
keywords={computer vision;image fusion;motion estimation;object detection;object tracking;probability;radiofrequency identification;RFID-based EAS system;hybrid RFID system fusion;computer vision system fusion;probabilistic model;thermal noise;device diversity;COTS camera;blob matching accuracy;fine-grained localization;TagVision;multiple moving objects;CV system;tagged objects;indoor scenarios;fine-grained object tracking;Cameras;Radiofrequency identification;Target tracking;Computer vision;Trajectory;Optical imaging;RFID;computer vision;tracking;TagVision},
doi={10.1109/INFOCOM.2017.8057161},
ISSN={},
month={May},}
@INPROCEEDINGS{8057162,
author={C. Yang and J. Gummeson and A. Sample},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Riding the airways: Ultra-wideband ambient backscatter via commercial broadcast systems},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Communication costs dominate the energy consumption, and ultimately limit the utility, of low power devices and sensor nodes. Backscatter communication based on deliberate and ambient sources has the potential to radically alter this paradigm by offering two to three orders of magnitude better communication efficiency (in terms of nJ/Bit) then conventional radio architectures. Initial work on ambient backscatter shows promising results but has focused on narrow band operation in well controlled laboratory settings. The goal of this work is to enable the ubiquitous deployment of ultra-low power nodes that communicate via ambient backscatter to wired Universal Backscatter Readers, in real-world environments. This is accomplished through ultra-wideband backscatter techniques that leverage the breath of commercial broadcast signals in the 80 MHz to 900 MHz range from FM radios, digital TVs, and cellular networks. Additionally the use of powered Universal Backscatter Readers allows a network of ultra-low power nodes to operate on ambient carriers as low as -80 dBm, which is typical for indoor home and office environments. For the first time we demonstrate the simultaneous use of 17 ambient signal sources to achieve node-to-reader communication distances of 50 meters, with data rates up to 1 kbps.},
keywords={backscatter;cellular radio;digital television;radio broadcasting;Ultra-wideband ambient backscatter;commercial broadcast systems;communication costs;energy consumption;low power devices;sensor nodes;Backscatter communication;ambient sources;initial work;narrow band operation;ultra-low power nodes;wired Universal Backscatter Readers;ultra-wideband backscatter techniques;commercial broadcast signals;ambient carriers;node-to-reader communication distances;radio architectures;universal backscatter readers;ambient signal sources;FM radios;digital TV;cellular networks;frequency 80.0 MHz to 900.0 MHz;Backscatter;Poles and towers;Radio frequency;Frequency modulation;Ultra wideband technology;Digital TV;Ultra-Wideband;Ambient Backscatter Communication;Sensor Node;Energy Harvesting},
doi={10.1109/INFOCOM.2017.8057162},
ISSN={},
month={May},}
@INPROCEEDINGS{8057163,
author={C. Wu and Y. Zhang and L. Zhang and B. Yang and X. Chen and W. Zhu and L. Qiu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={ButterFly: Mobile collaborative rendering over GPU workload migration},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The ever increasing of display resolution on mobile devices raises high demand for GPU rendering details. However, the challenge of poor hardware support but fine-grained rendering details often makes user unsatisfied especially in calling for high frame rate scenarios, e.g., game. To resolve such issue, we propose BUTTERFLY, a novel system which collaboratively utilizes mobile GPUs to process high-quality rendering details for on-the-go mobile users. In particular, ButterFly achieves two technical contributions for the collaborative design: (1) a mobile device can migrate GPU workloads in buffer queue to peers, and (2) the collaborative rendering mechanism benefits user high quality details while significant power saving performance. Both techniques are compatible with the OpenGL ES standards. Furthermore, a 40-person survey perceives that ButterFly can provide excellent user experience of both rendering details and frame rate over Wi-Fi network. In addition, our comprehensive trace-driven experiments on Android prototype reveal the benefits of Butterfly have more superior performance over state-of-the-art systems, which achieves more than 28.3% power saving.},
keywords={Android (operating system);graphics processing units;mobile computing;rendering (computer graphics);screens (display);ButterFly;mobile collaborative rendering;GPU workload migration;display resolution;mobile device;BUTTERFLY;Butterfly;Android prototype;GPU rendering;mobile GPU;Rendering (computer graphics);Graphics processing units;Mobile communication;Collaboration;Androids;Humanoid robots;Hardware},
doi={10.1109/INFOCOM.2017.8057163},
ISSN={},
month={May},}
@INPROCEEDINGS{8057164,
author={C. Pei and Z. Wang and Y. Zhao and Z. Wang and Y. Meng and D. Pei and Y. Peng and W. Tang and X. Qu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Why it takes so long to connect to a WiFi access point},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Today's WiFi networks deliver a large fraction of traffic. However, the performance and quality of WiFi networks are still far from satisfactory. Among many popular quality metrics (throughput, latency), the probability of successfully connecting to WiFi APs and the time cost of the WiFi connection set-up process are the two of the most critical metrics that affect WiFi users' experience. To understand the WiFi connection set-up process in real-world settings, we carry out measurement studies on 5 million mobile users from 4 representative cities associating with 7 million APs in 0.4 billion WiFi sessions, collected from a mobile “WiFi Manager” App that tops the Android/iOS App market. To the best of our knowledge, we are the first to do such large scale study on: how large the WiFi connection set-up time cost is, what factors affect the WiFi connection set-up process, and what can be done to reduce the WiFi connection set-up time cost. Based on our data-driven measurement and analysis, we reveal the insights as follows: (1) Connection set-up failure and large connection set-up time cost are common in today's WiFi use. As large as 45% of the users suffer connection set-up failures, and 15% (5%) of them have large connection set-up time costs over 5 seconds (10 seconds). (2) Contrary to the state-of-the-art work, scan, one of the subphase of four phases in the connection set-up process, contributes the most (47%) to the overall connection set-up time cost. (3) Mobile device model and AP model can greatly help us to predict the connection set-up time cost if we can make good use of the hidden information. Based on the measurement analysis, we develop a machine learning based AP selection strategy that can significantly improve WiFi connection set-up performance, against the conventional strategy purely based on signal strength, by reducing the connection set-up failures from 33% to 3.6% and reducing 80% time costs of the connection set-up processes by more than 10 times.},
keywords={learning (artificial intelligence);mobile radio;wireless LAN;WiFi access point;WiFi networks;mobile WiFi Manager App;WiFi users experience;WiFi connection set-up process;WiFi connection set-up time cost;connection set-up failures;mobile device model;AP model;machine learning based AP selection strategy;time 5.0 s;time 10.0 s;Wireless fidelity;Mobile handsets;IP networks;Authentication;Mobile communication;Switches;Measurement},
doi={10.1109/INFOCOM.2017.8057164},
ISSN={},
month={May},}
@INPROCEEDINGS{8057165,
author={F. Ahmed and J. Erman and Z. Ge and A. X. Liu and J. Wang and H. Yan},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Monitoring quality-of-experience for operational cellular networks using machine-to-machine traffic},
year={2017},
volume={},
number={},
pages={1-9},
abstract={It is crucial for cellular data network operators to understand the service quality perceived by its customers. The state-of-art systems deployed in cellular networks mostly report service quality aggregated on cell site level, which is typically an aggregation of tens or hundreds of customers depending on the locations of the cell sites. In this paper, we propose to enhance the measurement of customer-perceived service quality by leveraging M2M devices as sensors in the field, which provide an unprecedented opportunity for cellular network operators to measure what end-users experience with better accuracy and coverage. Our approach is to identify a set of M2M devices which are stationary and communicate continuously over the cellular network over an indefinite period of time. We use these M2M devices to estimate the customer-perceived service quality during cell site outages. We implement our methodology as a system called M2MScan and evaluate M2MScan with both synthetic outages and real outages from a large-scale operational cellular network. To the best of our knowledge, this is the first work that employs M2M devices to measure the service quality perceived by customers in operational cellular networks at a large scale.},
keywords={cellular radio;customer services;machine-to-machine communication;quality of experience;quality of service;telecommunication traffic;service quality;cell site outages;M2MScan;large-scale operational cellular network;quality-of-experience;machine-to-machine traffic;cellular data network operators;cell site level;M2M devices;Machine-to-machine communications;Cellular networks;Monitoring;Object recognition;Performance evaluation;Sensors;Level measurement},
doi={10.1109/INFOCOM.2017.8057165},
ISSN={},
month={May},}
@INPROCEEDINGS{8057166,
author={L. Xue and X. Ma and X. Luo and L. Yu and S. Wang and T. Chen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Is what you measure what you expect? Factors affecting smartphone-based mobile network measurement},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Many apps have been developed to measure the performance of mobile networks. Unfortunately, their measurement results may not be what users expect, because the results could be biased by various factors and the apps' descriptions may confuse users. Although a few recent studies pointed out several factors, they missed other important factors and lacked of finegrained analysis on the factors and measurement apps. Moreover, none has studied whether or not the descriptions of such apps will mislead users. In this paper, we conduct the first systematic study of the factors that could bias the result from measurement apps and their descriptions. We identify new factors, revisit known factors, and propose a novel approach with new tools to discover these factors in proprietary apps. We also develop a new measurement app named MobiScope for demonstrating how to mitigate the negative effects of these factors. Furthermore, we construct enhanced descriptions for measurement apps to provide users more information about what is measured. The extensive experimental results illustrate the negative effects of various factors, the improvement in performance measurement brought by MobiScope, and the clarity of the enhanced descriptions.},
keywords={smart phones;measurement app;performance measurement;mobile network measurement;MobiScope;Androids;Humanoid robots;Runtime;Delays;Mobile communication;Protocols;Smart phones},
doi={10.1109/INFOCOM.2017.8057166},
ISSN={},
month={May},}
@INPROCEEDINGS{8057167,
author={Z. Wang and Y. Zhang and Y. Li and Q. Wang and F. Xia},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Exploiting social influence for context-aware event recommendation in event-based social networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Event-based Social Networks (EBSNs) which bridge the gap between online and offline interactions among users have received increasing popularity. The unique cold-start nature makes event recommendation more challenging than traditional recommendation problems, since even for two events with the same content, they may not happen at the same time, the same location, or be organized by the same host. Existing event recommendation algorithms mainly exploit the basic context information (e.g., location, time and content), while the social influence of event hosts and group members have been ignored. In this paper, we propose a Social Information Augmented Recommender System (SIARS), which fully exploits the social influence of event hosts and group members together with basic context information for event recommendation. In particular, we combine the information of EBSNs and other social networks to characterize the social influence of event hosts, and take interactions between group members into consideration for event recommendation. In addition, we propose a new content-aware recommendation model using the topic model to find the most similar topic the event belongs to, and a new location-aware recommendation model integrating location popularity with location distribution for event recommendation. Extensive experiments on real-world datasets demonstrate that SIARS outperforms other recommendation algorithms.},
keywords={mobile computing;recommender systems;social networking (online);event hosts;group members;content-aware recommendation model;location-aware recommendation model;social influence;context-aware event recommendation;social networks;event recommendation algorithms;Social Information Augmented Recommender System;context information;recommendation problems;event-based social networks;EBSN;SIARS;Recommender systems;Conferences;Facebook;Twitter;Computer security;Collaboration},
doi={10.1109/INFOCOM.2017.8057167},
ISSN={},
month={May},}
@INPROCEEDINGS{8057168,
author={B. Samanta and A. De and N. Ganguly},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={STRM: A sister tweet reinforcement process for modeling hashtag popularity},
year={2017},
volume={},
number={},
pages={1-9},
abstract={With social media platform such as Twitter becoming the de facto destination for users' views and opinions, it is of great importance to forecast an information outbreak. In Twitter, tweets are often annotated with hashtags to help its users to quickly extract their contents. The existing approaches for modeling the dynamics of tweet-messages are usually limited to individual or simple aggregates of tweets rather than the underlying hashtags. In this paper, we develop, STRM, a novel point process driven model that considers the effect of cross-tweet impact in hashtag popularity. STRM, by assuming hashtag to be a heterogeneous collection of tweet-chains. Through extensive experimentation, we find that our algorithm - STRM, shows consistent performance boosts with six diverse real datasets against several strong baselines. Moreover surprisingly, it also offers significant accuracy gains in popularity-prediction for individual tweets as compared with the existing paradigms.},
keywords={information retrieval;social networking (online);cross-tweet impact;tweet-chains;popularity-prediction;STRM process;tweet-messages;information outbreak;Twitter;social media platform;hashtag popularity;sister tweet reinforcement process;Twitter;Tagging;Computational modeling;Predictive models;Data models;Proposals;History},
doi={10.1109/INFOCOM.2017.8057168},
ISSN={},
month={May},}
@INPROCEEDINGS{8057169,
author={X. Xu and C. Lee and D. Y. Eun},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Challenging the limits: Sampling online social networks with cost constraints},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Graph sampling techniques via random walk crawling have been popular for analyzing statistical characteristics of large online social networks due to simple implementation and provable guarantees on unbiased estimates. Despite the growing popularity, the `cost' of sampling and its true impact on the accuracy of estimates still have not been carefully studied. In addition, the random walk-based methods inherently suffer from the sluggish nature of random walks and the `slow-mixing' structure of social graphs, thereby leading to high correlation in the samples obtained. With these in mind, in this paper, we develop a mathematical framework such that the cost of sampling is properly taken into account, which in turn re-defines a widely used asymptotic variance into a cost-based asymptotic variance. Our new metric enables us to compare a class of sampling policies under the same cost constraint, integrating “random skipping” (bypassing nodes without sampling) into the random walk-based sampling. We obtain an optimal policy striking the right balance between sampling quality (less correlation) and sampling quantity (higher cost per sample), which greatly improves over the usual skip-free crawling-based samplers. We further extend our framework, enabling one to design more sophisticated sampling strategies with an array of control knobs, which all produce unbiased estimates under the same cost constraint.},
keywords={graph theory;information retrieval;network theory (graphs);random processes;sampling methods;social networking (online);sampling strategies;cost-based asymptotic variance;random walk-based sampling;sampling quantity;skip-free crawling-bsaed samplers;online social networks sampling;provable guarantees;simple implementation;statistical characteristics;random walk crawling;graph sampling;sampling quality;random skipping;cost constraint;sampling policies;social graphs;slow-mixing structure;unbiased estimates;Web pages;Crawlers;Social network services;Conferences;Correlation;Measurement;Sampling methods},
doi={10.1109/INFOCOM.2017.8057169},
ISSN={},
month={May},}
@INPROCEEDINGS{8057170,
author={Q. Lin and L. Yang and Y. Liu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={TagScreen: Synchronizing social televisions through hidden sound markers},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Millions of people nowadays share their television (TV) experience with other people through social media like Twitter or Facebook with mobile devices. It is generally believed that TV has been repurposed for social networks, called as social television. A key functionality of social television is that it allows the viewers to interchange their comments through mobile devices, thus creating the impression of watching TV like alongside a group of friends. To do so, mobile devices have to be aware of the current media context (identifier and progress) of what the TV is playing, known as synchronizing context from TVs to mobile devices. Unfortunately, most legacy systems are not able to track the playing progresses or need to upgrade TV devices. To address the issue, we design a purely software-based solution, called as TagScreen, which inserts a series of hidden sound markers into the audio of the content. TagScreen supports longrange or multipath-resistant synchronization at second-level, being independent of device diversity over severely frequency-selective acoustic channels. We implement TagScreen by using COTS TVs and mobile devices. The system has been extensively tested on 150 movies and 150 TV series across five different environments. Results show that TagScreen has a mean recognition accuracy of 98% up to 35m, and a mean tracking accuracy of 97%.},
keywords={mobile handsets;mobile television;social networking (online);social networks;social television;mobile devices;TV devices;TagScreen;hidden sound markers;device diversity;television experience;social media;TV series;synchronizing context;multipath-resistant synchronization;frequency-selective acoustic channels;COTS TVs;TV;Mobile handsets;Media;Social network services;Synchronization;Motion pictures;Frequency modulation},
doi={10.1109/INFOCOM.2017.8057170},
ISSN={},
month={May},}
@INPROCEEDINGS{8057171,
author={Q. Liang and E. Modiano},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Coflow scheduling in input-queued switches: Optimal delay scaling and algorithms},
year={2017},
volume={},
number={},
pages={1-9},
abstract={A coflow is a collection of parallel flows belonging to the same job. It has the all-or-nothing property: a coflow is not complete until the completion of all its constituent flows. In this paper, we focus on optimizing coflow-level delay, i.e., the time to complete all the flows in a coflow, in the context of an N × N input-queued switch. In particular, we develop a throughput-optimal scheduling policy that achieves the best scaling of coflow-level delay as N → ∞. We first derive lower bounds on the coflow-level delay that can be achieved by any scheduling policy. It is observed that these lower bounds critically depend on the variability of flow sizes. Then we analyze the coflow-level performance of some existing coflow-agnostic scheduling policies and show that none of them achieves provably optimal performance with respect to coflow-level delay. Finally, we propose the Coflow-Aware Batching (CAB) policy which achieves the optimal scaling of coflow-level delay under some mild assumptions.},
keywords={computer networks;delays;optimisation;queueing theory;scheduling;telecommunication scheduling;throughput-optimal scheduling policy;coflow-level delay;coflow-level performance;Coflow-Aware Batching policy;Optimal delay scaling;coflow-agnostic scheduling policies;CAB policy;Delays;Scheduling;Optimal scheduling;Processor scheduling;Ports (Computers)},
doi={10.1109/INFOCOM.2017.8057171},
ISSN={},
month={May},}
@INPROCEEDINGS{8057172,
author={W. Wang and S. Ma and B. Li and B. Li},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Coflex: Navigating the fairness-efficiency tradeoff for coflow scheduling},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Fair and efficient coflow scheduling improves application-level networking performance in today's datacenters. Ideally, a coflow scheduler should provide isolation guarantees on the minimum coflow progress to achieve predictable networking performance. Network operators, on the other hand, strive to decrease the average coflow completion time (CCT). Unfortunately, optimal isolation guarantees and minimum average CCT are conflicting objectives and cannot be achieved at the same time. Existing coflow schedulers either optimize isolation guarantees at the expense of long CCTs (e.g., HUG [1]), or decrease the average CCT without performance isolation (e.g., Varys and Aalo [2], [3]). The lack of a smooth tradeoff in between poses a dilemma between low efficiency and no performance isolation. To bridge this gap, we develop a new coflow scheduler, Coflex, to navigate this tradeoff. Coflex allows network operators to specify the desired level of isolation guarantee using a tunable fairness knob, while at the same time decreasing the average CCT. Both our real-world deployments and trace-driven simulations have shown that Coflex offers a smooth tradeoff between fairness and efficiency. At an appropriate tradeoff level, Coflex outperforms fair schedulers by 2 × in minimizing the average CCT.},
keywords={computer centres;computer networks;telecommunication scheduling;Coflex;fairness-efficiency tradeoff;application-level networking performance;coflow scheduler;minimum coflow progress;predictable networking performance;network operators;average coflow completion time;optimal isolation guarantees;minimum average CCT;performance isolation;coflow scheduling;Bandwidth;Resource management;Fabrics;Navigation;Production;Silicon;Conferences},
doi={10.1109/INFOCOM.2017.8057172},
ISSN={},
month={May},}
@INPROCEEDINGS{8057173,
author={C. Wang and S. T. Maguluri and T. Javidi},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Heavy traffic queue length behavior in switches with reconfiguration delay},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Optical switches have been drawing attention due to their large data bandwidth and low power consumption. However, scheduling policies need to account for the schedule reconfiguration delay of optical switches to achieve good performance. The Adaptive MaxWeight policy achieves optimal throughput for switches with nonzero reconfiguration delay, and has been shown in simulation to have good delay performance. In this paper, we analyze the queue length behavior of a switch with nonzero reconfiguration delay operating under the Adaptive MaxWeight. We first show that the Adaptive MaxWeight policy exhibits a weak state space collapse behavior in steady-state, which could be viewed as an inheritance of the MaxWeight policy in a switch with zero reconfiguration delay. We then use the weak state space collapse result to obtain a steady state delay bound under the Adaptive MaxWeight algorithm in heavy traffic by applying a recently developed drift technique. The resulting delay bound is dependent on the expected schedule duration. We then derive the relation between the expected schedule duration and the steady state queue length through drift analysis, and obtain asymptotically tight queue length bounds in the heavy traffic regime.},
keywords={delays;optical switches;queueing theory;telecommunication scheduling;telecommunication traffic;Adaptive MaxWeight policy;weak state space collapse behavior;drift analysis;schedule reconfiguration delay;scheduling policies;optical switches;heavy traffic regime;asymptotically tight queue length;steady state queue length;expected schedule duration;Adaptive MaxWeight algorithm;steady state delay;weak state space collapse result;nonzero reconfiguration delay;queue length behavior;Schedules;Delays;Optical switches;Steady-state;Throughput;Ports (Computers);Queueing analysis},
doi={10.1109/INFOCOM.2017.8057173},
ISSN={},
month={May},}
@INPROCEEDINGS{8057174,
author={S. Yang and B. Lin and P. Tune and J. J. Xu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A simple re-sequencing load-balanced switch based on analytical packet reordering bounds},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Chang et al. proposed the load-balanced switch in their seminal work [1], which has received wide attention due to its inherent scalability properties in both size and speed. These scalability properties continue to be of significant interest due to the relentless exponential growth in Internet traffic. The main drawback of the load-balanced switch is that packets can depart out-of-order from the switch, which can significantly degrade network performance by negatively interacting with TCP congestion control. Hence, a large body of subsequent work has proposed a variety of modifications for ensuring packet ordering, but all the proposed approaches tend to increase packet delay significantly in comparison to the basic load-balanced switch. In this paper, we show that the amount of packet reordering that can occur with the load-balanced switch is actually quite limited, which means that packet reordering can simply be rectified by employing reordering buffers at the switch outputs. In particular, we formally bound the worst-case amount of time that a packet has to wait in these output reordering buffers before it is guaranteed to be ready for in-order departure with high probability, and we prove that this bound is linear with respect to the switch size. This linear bound is significant because previous approaches can add quadratic or cubic delays to the load-balanced switch. In addition, we use a hash-grouping method that further reduces resequencing delays significantly. Although simple and intuitive, our experimental results show that our output packet reordering approach substantially outperforms existing load-balanced switch architectures.},
keywords={Internet;packet switching;probability;resource allocation;telecommunication congestion control;telecommunication traffic;transport protocols;switch size;load-balanced switch architectures;analytical packet;basic load-balanced switch;analytical packet reordering bounds;simple resequencing load-balanced switch;Internet traffic;TCP congestion control;hash-grouping method;Ports (Computers);Optical switches;Delays;Out of order;Semantics;Computer architecture},
doi={10.1109/INFOCOM.2017.8057174},
ISSN={},
month={May},}
@INPROCEEDINGS{8057175,
author={J. Lin and M. Li and D. Yang and G. Xue and J. Tang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Sybil-proof incentive mechanisms for crowdsensing},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The rapid growth of sensor-embedded smartphones has led to a new data sensing and collecting paradigm, known as crowdsensing. Many auction-based incentive mechanisms have been proposed to stimulate smartphone users to participate in crowdsensing. However, none of them have taken into consideration the Sybil attack where a user illegitimately pretends multiple identities to gain benefits. This attack may undermine existing inventive mechanisms. To deter the Sybil attack, we design Sybil-proof auction-based incentive mechanisms for crowdsensing in this paper. We investigate both the single-minded and multi-minded cases and propose SPIM-S and SPIM-M, respectively. SPIM-S achieves computational efficiency, individual rationality, truthfulness, and Sybil-proofness. SPIM-M achieves individual rationality, truthfulness, and Sybil-proofness. We evaluate the performance and validate the desired properties of SPIM-S and SPIM-M through extensive simulations.},
keywords={incentive schemes;mobile computing;smart phones;Sybil-proofness;SPIM-S;SPIM-M;Sybil-proof incentive mechanisms;crowdsensing;smartphone users;Sybil attack;inventive mechanisms;sensor-embedded smart phones;data sensing paradigm;data collecting paradigm;Sybil-proof auction-based incentive mechanisms;Sensors;Smart phones;Cost function;Conferences;Electronic mail;Computational modeling},
doi={10.1109/INFOCOM.2017.8057175},
ISSN={},
month={May},}
@INPROCEEDINGS{8057176,
author={C. Liu and S. Wang and L. Ma and X. Cheng and R. Bie and J. Yu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Mechanism design games for thwarting malicious behavior in crowdsourcing applications},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Crowdsourcing applications are vulnerable to malicious behaviors, posing serious threats to their adoption and large deployment. Based on the notion that the requestor (i.e., the crowdsourcer) can block malicious behaviors via leveraging the market power through task allocation and pricing, we propose two novel frameworks based on the mechanism design game theory (i.e., the reverse game theory). To the best of our knowledge, we are the first to exploit the market power and to apply the mechanism design game theory in thwarting malicious behaviors in crowdsourcing. The first proposed framework is built on a requestor-dominant mechanism design game (Rd-MDG), where the game rule is determined solely by the requestor. The second proposed framework is based on the worker-assisted mechanism design game (WaMDG), where the worker (i.e., the contributor) can assist the requestor to determine the game rules by offering advices. These two frameworks have the following salient features: i) neither of them requires the workers to reveal their private information; ii) the game rules of each framework are designed to be able to force the workers to calculate their best strategies based on their actual private information; iii) our theoretical analysis shows that equilibriums exist for both frameworks; and iv) our extensive simulation results demonstrate that these two frameworks can thwart malicious behaviors by driving the workers with a higher attack intent into obtaining lower utilities.},
keywords={crowdsourcing;data privacy;game theory;security of data;malicious behavior;crowdsourcing applications;reverse game theory;requestor-dominant mechanism design game;game rule;Rd-MDG;worker-assisted mechanism design game;WaMDG;private information;Games;Crowdsourcing;Game theory;Resource management;Electronic mail;Conferences},
doi={10.1109/INFOCOM.2017.8057176},
ISSN={},
month={May},}
@INPROCEEDINGS{8057177,
author={F. Qiu and Z. He and L. Kong and F. Wu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={MAGIK: An efficient key extraction mechanism based on dynamic geomagnetic field},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Secret key establishment is a fundamental requirement for private communication between two wireless entities. An intriguing solution is to extract secret keys from the inherent randomness shared between them. Although several works have been done to extract secret keys from different kinds of mediums (e.g., RSSI, CSI, CIR), the efficiency and security problems are not fully solved. In this paper, we consider the problem of secret key establishment for wireless devices, and propose MAGIK, a secure and efficient scheme based on dynamic geoMAGnetic field in Indoor environment for Key establishment. We carefully study the feasibility of utilizing indoor geomagnetic field for key extraction through extensive measurements. Our results demonstrate that geomagnetic field has several dynamic properties, including space-varying, time-varying, sensitive to measurement device, and correlative between two observed points in proximity. We also optimize the key extraction process and present two rotation-angle-based quantification methods, which can achieve faster key generation rates and lower bit mismatching ratios. Besides, we build a prototype on commodity mobile devices, and evaluate its performance by conducting real-word experiments in indoor scenarios. The experiment results confirm that our system is efficient, in terms of key extraction rate, and robust in secret key establishment without requiring additional overhead on mobile devices.},
keywords={private key cryptography;quantum cryptography;telecommunication security;wireless channels;MAGIK;key extraction mechanism;key generation rates;secure scheme;secret keys;dynamic geomagnetic field;secret key establishment;key extraction rate;indoor geomagnetic field;Mobile handsets;Wireless communication;Communication system security;Magnetometers;Wireless sensor networks;Security;Extraterrestrial measurements},
doi={10.1109/INFOCOM.2017.8057177},
ISSN={},
month={May},}
@INPROCEEDINGS{8057178,
author={B. Wang and W. Song and W. Lou and Y. T. Hou},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Privacy-preserving pattern matching over encrypted genetic data in cloud computing},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Personalized medicine performs diagnoses and treatments according to the DNA information of the patients. The new paradigm will change the health care model in the future. A doctor will perform the DNA sequence matching instead of the regular clinical laboratory tests to diagnose and medicate the diseases. Additionally, with the help of the affordable personal genomics services such as 23andMe, personalized medicine will be applied to a great population. Cloud computing will be the perfect computing model as the volume of the DNA data and the computation over it are often immense. However, due to the sensitivity, the DNA data should be encrypted before being outsourced into the cloud. In this paper, we start from a practical system model of the personalize medicine and present a solution for the secure DNA sequence matching problem in cloud computing. Comparing with the existing solutions, our scheme protects the DNA data privacy as well as the search pattern to provide a better privacy guarantee. We have proved that our scheme is secure under the well-defined cryptographic assumption, i.e., the sub-group decision assumption over a bilinear group. Unlike the existing interactive schemes, our scheme requires only one round of communication, which is critical in practical application scenarios. We also carry out a simulation study using the real-world DNA data to evaluate the performance of our scheme. The simulation results show that the computation overhead for real world problems is practical, and the communication cost is small. Furthermore, our scheme is not limited to the genome matching problem but it applies to general privacy preserving pattern matching problems which is widely used in real world.},
keywords={biology computing;cloud computing;cryptography;data privacy;diseases;DNA;genetics;genomics;health care;pattern matching;genome matching problem;privacy-preserving pattern matching;encrypted genetic data;cloud computing;personalized medicine;DNA information;health care model;regular clinical laboratory tests;secure DNA sequence matching problem;DNA data privacy;privacy guarantee;DNA data;personal genomics services;DNA;Cloud computing;Testing;Computational modeling;Encryption},
doi={10.1109/INFOCOM.2017.8057178},
ISSN={},
month={May},}
@INPROCEEDINGS{8057179,
author={Q. Yin and J. Kaur and F. D. Smith},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={TCP Rapid: From theory to practice},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Delay and rate-based alternatives to TCP congestion-control have been around for nearly three decades and have seen a recent surge in interest. However, such designs have faced significant resistance in being deployed on a wide-scale across the Internet - this has been mostly due to serious concerns about noise in delay measurements, pacing inter-packet gaps, and/or required changes to the standard TCP stack/headers. With the advent of high-speed networking, some of these concerns become even more significant. In this paper, we consider Rapid, a recent proposal for ultra-high speed congestion control, which perhaps stretches each of these challenges to the greatest extent. Rapid adopts a framework of continuous fine-scale bandwidth probing, which requires a potentially different and finely-controlled gap for every packet, high-precision timestamping of received packets, and reliance on fine-scale changes in inter-packet gaps. While simulation-based evaluations of Rapid show that it has outstanding performance gains along several important dimensions, these will not translate to the real-world unless the above challenges are addressed. We design a Linux implementation of Rapid after carefully considering each of these challenges. Our evaluations on a 10Gbps testbed confirm that the implementation can indeed achieve the claimed performance gains, and that it would not have been possible unless each of the above challenges was addressed.},
keywords={Internet;Linux;telecommunication congestion control;transport protocols;TCP Rapid;TCP congestion-control;delay measurements;required changes;standard TCP stack/headers;high-speed networking;greatest extent;continuous fine-scale bandwidth probing;finely-controlled gap;high-precision timestamping;fine-scale changes;Internet;interpacket gaps;ultrahigh speed congestion control;Linux;bit rate 10 Gbit/s;Bandwidth;Protocols;Probes;Noise measurement;Delays;Performance gain;Loss measurement},
doi={10.1109/INFOCOM.2017.8057179},
ISSN={},
month={May},}
@INPROCEEDINGS{8057180,
author={T. Bonald and J. Roberts and C. Vitale},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Convergence to multi-resource fairness under end-to-end window control},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The paper relates to multi-resource sharing between flows with heterogeneous requirements as arises in networks with wireless links or software routers implementing network function virtualization. Bottleneck max fairness (BMF) is a sharing objective in this context with good performance. The paper shows that BMF results when local fairness is imposed at each resource while flow rates are controlled by an end-to-end window. We analytically prove convergence to BMF under a fluid model when flows share a network limited to 2 resources while numerical results confirm BMF convergence for larger networks. Simulation results illustrate the impact of packetized transmission.},
keywords={Internet;radio links;telecommunication network routing;virtualisation;end-to-end window control;multiresource fairness;bottleneck max fairness;network function virtualization;software routers;wireless links;multiresource sharing;Resource management;Convergence;Mathematical model;Wireless communication;Bit rate},
doi={10.1109/INFOCOM.2017.8057180},
ISSN={},
month={May},}
@INPROCEEDINGS{8057181,
author={D. Shan and F. Ren},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Improving ECN marking scheme with micro-burst traffic in data center networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In data centers, micro-burst is a common traffic pattern. The packet dropping caused by it usually leads to serious performance degradations. Therefore, much attention has been paid to avoiding buffer overflow caused by micro-burst traffic. In particular, ECN is widely used in data centers to keep persistent queue occupancy low, so that enough buffer space can be available as headroom to absorb micro-burst traffic. However, we find that instantaneous-queue-length-based ECN may cause problems in another direction - buffer underflow. Specifically, current ECN marking scheme in data centers is easy to trigger spurious congestion signals, which may result in overreaction of senders and queue length oscillations in switches. Since ECN threshold is low, the buffer may underflow and link capacity is not fully used. In this paper, we reveal this problem by experiments and simulations. Besides, we theoretically deduce the amplitude of queue length oscillations. The analysis result shows that overreaction of senders is caused by ECN mis-marking. Therefore, we propose Combined Enqueue and Dequeue Marking (CEDM), which can mark packets more accurately. Through simulations, we show that CEDM can greatly reduce throughput loss and improve flow completion time.},
keywords={computer centres;computer networks;queueing theory;telecommunication congestion control;telecommunication switching;telecommunication traffic;transport protocols;microburst traffic;data center networks;common traffic pattern;buffer space;instantaneous-queue-length;buffer underflow;queue length oscillations;ECN threshold;ECN mis-marking;buffer avoidance;low persistent queue occupancy;ECN marking scheme;combined enqueue and dequeue marking scheme;CEDM scheme;Throughput;Oscillators;Integrated circuits;Conferences;Proposals;Delays;ECN marking;Micro-burst traffic;Large Segment Offload;Interrupt Coalescing},
doi={10.1109/INFOCOM.2017.8057181},
ISSN={},
month={May},}
@INPROCEEDINGS{8057182,
author={J. Zhao and J. Liu and H. Wang and C. Xu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Multipath TCP for datacenters: From energy efficiency perspective},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Nearly 50% of the energy overhead in today's datacenters comes from host-to-host data transfers, which largely depend on the transport layer performance. Multipath TCP (MPTCP) has recently been suggested as a promising transport protocol to improve datacenter network throughput, yet it also increases the host CPU power consumption. It remains unclear whether datacenters can indeed benefit from using MPTCP from the perspective of energy efficiency. By analyzing the performance of MPTCP, we show that (1) despite consuming higher host CPU power than TCP, MPTCP can largely reduce the long flow completion time and thus save the aggregated energy; (2) link-sharing subflows in MPTCP not only has negative impact on both throughput-sensitive long flows and latency-sensitive short flows, but also noticeably increases the host CPU power, especially for short flows. We present MPTCP-D, an energy-efficient variant of multipath TCP for datacenters. MPTCP-D incorporates a novel congestion control algorithm that can provide energy efficiency by minimizing the flow completion time, and an extra subflow elimination mechanism that can preclude link-sharing subflows from increasing the host CPU power. We implement MPTCP-D in the Linux kernel and study its performance by experiments on Amazon EC2. Our results show that, without degrading the performance of the long flow throughput and short flow completion time, MPTCP-D reduces the long flow energy consumption by up to 72% compared to DCTCP for data transfers, and reduces the short flow power consumption by up to 46% compared to MPTCP with linksharing subflows.},
keywords={computer centres;computer networks;energy conservation;Linux;power consumption;telecommunication congestion control;telecommunication power management;transport protocols;Linux kernel;transport protocol;short flow power consumption;long flow energy consumption;short flow completion time;long flow throughput;MPTCP-D;energy-efficient variant;latency-sensitive short flows;throughput-sensitive long flows;link-sharing subflows;aggregated energy;long flow completion time;datacenter network throughput;transport layer performance;host-to-host data transfers;energy overhead;energy efficiency perspective;multipath TCP;Throughput;Energy consumption;Data transfer;Servers;Power demand;Transport protocols;Interference},
doi={10.1109/INFOCOM.2017.8057182},
ISSN={},
month={May},}
@INPROCEEDINGS{8057183,
author={J. Palacios and P. Casari and J. Widmer},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={JADE: Zero-knowledge device localization and environment mapping for millimeter wave systems},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Device localization is a highly important functionality for a range of applications. It is particularly beneficial in mmWave networks, where it can be used to reduce the beam training overhead and anticipate handovers between access points. In this paper, we present JADE, an algorithm that estimates the location of a mobile user in an indoor space without any knowledge about the surrounding environment (floor plan, location of walls and presence of reflective surfaces) or about the location and number of access points available therein. JADE leverages the beam procedure used in pre-standard and commercial mmWave equipment to estimate the angle-of-arrival of multipath components of the signal sent by visible access points. This information is then employed to localize the mobile user, estimate the position of access points and finally form a map of the environment. No radar-like ranging operations are required for this. Our results demonstrate that JADE can localize a user with sub-meter accuracy in the broad majority of the cases, and that it even outperforms localization algorithms that require full knowledge of the environment and access point positions.},
keywords={array signal processing;direction-of-arrival estimation;indoor radio;mobility management (mobile radio);multi-access systems;multipath channels;environment mapping;millimeter wave systems;mmWave networks;beam training;JADE;mobile user;indoor space;beam procedure;visible access points;radar-like ranging operations;Zero-knowledge device localization;multipath components;angle-of-arrival;joint anchor and device location estimation;Signal processing algorithms;Phased arrays;Mobile communication;Simultaneous localization and mapping;Estimation;Algorithm design and analysis;Millimeter wave;localization;indoor;virtual anchors;mobility;simulation},
doi={10.1109/INFOCOM.2017.8057183},
ISSN={},
month={May},}
@INPROCEEDINGS{8057184,
author={Z. Zhao and J. Wang and X. Zhao and C. Peng and Q. Guo and B. Wu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={NaviLight: Indoor localization and navigation under arbitrary lights},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Thanks to the highly-dense lighting infrastructure in public areas, visible light emerges as a promising means to indoor localization and navigation. State-of-the-art techniques generally require customized hardware (sensing boards), and mainly work with one single light source (e.g., customized LEDs). This greatly limits their application scope. In this paper, we propose NaviLight, a generic indoor localization and navigation framework based on existing lighting infrastructure with any unmodified light sources (e.g., LED, fluorescent, and incandescent lights). NaviLight simply adopts commercial off-the-shelf mobile phones as receivers, and light intensity values as location signatures. Unlike existing WiFi systems, a single light intensity value is not discriminative enough over space though the light intensity field does vary, which makes our design more challenging. We thus propose a LightPrint as a location signature using a vector of multiple light intensity values obtained during user's walks. Such LightPrints are created by leveraging any user movement (of varying distance and direction) in order to minimize user efforts. A set of techniques are proposed to achieve quick LightPrint matching, which includes a coarse-grained classification and a fine-grained matching over dynamic time warping. We have implemented NaviLight to provide real-time service on Android phones in three typical indoor environments, covering a total area size over 1000m<sup>2</sup>. Our experiments show that NaviLight can achieve sub-meter localization accuracy to meet practical engineering requirements.},
keywords={indoor communication;indoor radio;light sources;lighting;navigation;smart phones;wireless LAN;light intensity field;location signature;multiple light intensity;NaviLight;highly-dense lighting infrastructure;visible light;sensing boards;generic indoor localization;navigation framework;incandescent lights;indoor environments;light source;lighting infrastructure;mobile phones;WiFi systems;light intensity value;LightPrint matching;LED;size 1000.0 m;Light emitting diodes;Navigation;Indoor environments;Light sources;High intensity discharge lamps;Sensors},
doi={10.1109/INFOCOM.2017.8057184},
ISSN={},
month={May},}
@INPROCEEDINGS{8057185,
author={X. Chen and C. Ma and M. Allegue and X. Liu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Taming the inconsistency of Wi-Fi fingerprints for device-free passive indoor localization},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Device-free Passive (DfP) indoor localization releases the users from the burden of wearing sensors or carrying smartphones. Instead of locating devices, DfP technology directly locates human bodies. This promising technology upgrades and even redefines many services, such as intruder alarm, fire rescue, fall detection, baby monitoring, etc. Using Wi-Fi based fingerprints, DfP approaches can achieve a nearly perfect accuracy with a resolution less than one meter. However, Wi-Fi localization profiles may easily drift with a minor environment change, resulting in an inconsistency between fingerprints and new profiles. This inconsistency issue could lead to large errors, and may quickly ruin the whole system. To address this issue, we propose a approach named AutoFi to automatically calibrate the localization profiles in an unsupervised manner. AutoFi embraces a new technique that online estimates and cancels profile contaminants introduced by environment changes. It applies an autoencoder to preserve critical features of fingerprints, and reproduces them later in new localization profiles. Experiment results demonstrate that AutoFi indeed rescues the Wi-Fi fingerprints from variations in the surrounding. The localization accuracy is improved from 18.8% (before auto-calibration) to 84.9% (after auto-calibration).},
keywords={fingerprint identification;indoor radio;telecommunication security;wireless LAN;profile contaminants;AutoFi;inconsistency issue;environment change;Wi-Fi localization profiles;Wi-Fi based fingerprints;fire rescue;intruder alarm;DfP technology;device-free passive indoor localization;Wi-Fi fingerprints;localization accuracy;baby monitoring;direct human bodies localization;Wireless fidelity;Antenna arrays;Databases;Wireless communication;Signal resolution;OFDM;Array signal processing},
doi={10.1109/INFOCOM.2017.8057185},
ISSN={},
month={May},}
@INPROCEEDINGS{8057186,
author={R. Gao and B. Zhou and F. Ye and Y. Wang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Knitter: Fast, resilient single-user indoor floor plan construction},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Lacking of floor plans is a fundamental obstacle to ubiquitous indoor location-based services. Recent work have made significant progress to accuracy, but they largely rely on slow crowdsensing that may take weeks or even months to collect enough data. In this paper, we propose Knitter that can generate accurate floor maps by a single random user's one hour data collection efforts. Knitter extracts high quality floor layout information from single images, calibrates user trajectories and filters outliers. It uses a multi-hypothesis map fusion framework that updates landmark positions/orientations and accessible areas incrementally according to evidences from each measurement. Our experiments on 3 different large buildings and 30+ users show that Knitter produces correct map topology, and 90-percentile landmark location and orientation errors of 3 ~ 5m and 4 ~ 6°, comparable to the state-of-the-art at more than 20× speed up: data collection can finish in about one hour even by a novice user trained just a few minutes.},
keywords={calibration;construction;floors;planning;buildings;crowdsensing;floor maps;ubiquitous indoor location-based services;floor plans;single-user indoor floor plan construction;data collection;correct map topology;multihypothesis map fusion framework;filters outliers;high quality floor layout information;Knitter;time 1.0 hour;size 5.0 m;Trajectory;Calibration;Geometry;Gyroscopes;Layout;Data collection;Cleaning},
doi={10.1109/INFOCOM.2017.8057186},
ISSN={},
month={May},}
@INPROCEEDINGS{8057187,
author={R. Naves and H. Khalifé and G. Jakllari and V. Conan and A. Beylot},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A framework for evaluating physical-layer network coding gains in multi-hop wireless networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We investigate the potential gains of Physical-Layer Network Coding (PLNC) in multi-hop wireless networks. Physical-Layer Network Coding was first introduced as a solution to increase the throughput of a two-way relay channel communication. Unlike most wireless communications techniques which try to avoid collisions, PLNC allows two simultaneous transmissions to a common receiver. Such transmitted messages are summed at signal level and then decoded at packet level. In basic topologies, Physical-Layer Network Coding has been shown to significantly enhance the throughput performance compared to classical communications. However, the impact of PLNC in large multi-hop networks remains an open question. We therefore exploit Linear Programming to evaluate the impact of this paradigm in large realistic radio deployments. Our numerical results show that PLNC can increase the throughput in large multi-hop topologies by 30%. Such gains set theoretical benchmarks for designing new access methods and routing protocols to efficiently exploit the Physical-Layer Network Coding concept.},
keywords={linear programming;network coding;relay networks (telecommunication);routing protocols;telecommunication network topology;wireless channels;multihop wireless networks;PLNC;wireless communications techniques;multihop topologies;Physical-Layer Network Coding concept;two-way relay channel communication;throughput performance enhancement;linear programming;large realistic radio deployments;routing protocols;Network coding;Throughput;Network topology;Topology;Spread spectrum communication;Wireless communication;Computational modeling},
doi={10.1109/INFOCOM.2017.8057187},
ISSN={},
month={May},}
@INPROCEEDINGS{8057188,
author={A. Zhou and X. Zhang and H. Ma},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Beam-forecast: Facilitating mobile 60 GHz networks via model-driven beam steering},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Low robustness under mobility is the Achilles' heel of the emerging 60 GHz networking technology. Instead of using omni-directional antennas as in existing Wi-Fi/cellular networks, 60 GHz radios communicate via highly-directional links formed by phased-array beam-forming, so as to upgrade wireless link throughput to multi-Gbps. However, user motion causes misalignment between the Tx's and Rx's beam directions, and often leads to link outage. Legacy 60 GHz protocols realign the beams by scanning alternative Tx/Rx beams. But unfortunately this tedious process can easily overwhelm the useful channel time, leaving the Tx/Rx in misalignment most of the time during mobility. In this paper, we propose Beam-forecast, a novel model-driven beam steering approach that can sustain high performance for mobile 60 GHz links. Beam-forecast is built on the observation that 60 GHz channel profiles at nearby locations are highly-correlated. By exploiting this correlation, Beam-forecast can reconstruct the channel profile as the Tx/Rx moves, without explicit channel scanning. In this way, it can predict new optimal beams and realign links for mobile users with minimal overhead. We evaluate Beam-forecast using a reconfigurable 60 GHz testbed along with a trace-driven simulator. Our experiments demonstrate multi-fold throughput gain compared with state-of-the-art under various practical scenarios.},
keywords={array signal processing;beam steering;cellular radio;directive antennas;mobile radio;protocols;radio links;wireless LAN;channel scanning;wireless link;omnidirectional antennas;cellular networks;mobile links;model-driven beam steering;Wi-Fi;networking technology;channel profiles;beam directions;highly-directional links;mobile 60 GHz networks;optimal beams;beam steering approach;Beam-forecast;alternative Tx/Rx beams;legacy 60 GHz protocols;phased-array beam-forming;frequency 60.0 GHz;Correlation;Mobile communication;Horn antennas;Array signal processing;Antenna measurements;Beam steering;Wireless communication},
doi={10.1109/INFOCOM.2017.8057188},
ISSN={},
month={May},}
@INPROCEEDINGS{8057189,
author={Q. Chen and H. Gao and Y. Li and S. Cheng and J. Li},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Edge-based beaconing schedule in duty-cycled multihop wireless networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Beaconing is a fundamental networking service where each node broadcasts a packet to all its neighbors locally. Unfortunately, the problem Minimum Latency Beaconing Schedule (MLBS) in duty-cycled scenarios is not well studied. Existing works always have rigid assumption that each node is only active once per working cycle. Aiming at making the work more practical and general, MLBS problem in duty-cycled network where each node is allowed to active multiple times in each working cycle (MLBSDCA for short) is investigated in this paper. Firstly, a modified first-fit coloring based algorithm is proposed for MLBSDCA under protocol interference model. After that, a (ρ + 1)2*|W|-approximation algorithm is proposed to further reduce the beaconing latency, where ρ denotes the interference radius, and |W| is the maximum number of active time slots per working cycle. When ρ and |W| is equal to 1, the approximation ratio is only 4, which is better than the one (i.e., 10) in existing works. Furthermore, two approximation algorithms for MLBSDCA under physical interference model are also investigated. The theoretical analysis and experimental results demonstrate the efficiency of the proposed algorithms in term of latency.},
keywords={approximation theory;graph colouring;protocols;radiofrequency interference;telecommunication scheduling;wireless sensor networks;multihop wireless networks;fundamental networking service;duty-cycled scenarios;duty-cycled network;active multiple times;MLBSDCA;first-fit coloring based algorithm;active time slots;edge-based beaconing schedule;Minimum Latency Beaconing Schedule;protocol interference model;physical interference model;Interference;Approximation algorithms;Protocols;Schedules;Scheduling algorithms;Algorithm design and analysis;Scheduling},
doi={10.1109/INFOCOM.2017.8057189},
ISSN={},
month={May},}
@INPROCEEDINGS{8057190,
author={S. Saha and M. C. Chan},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Design and application of a many-to-one communication protocol},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In this paper, we address the fundamental problem of improving the performance of many-to-one and many-to-many communications. Our approach is Time Division Multiple Access (TDMA) based but addresses the limitations of existing TDMA implementations in a novel way. In a nutshell, we combine packets from many senders into a single large packet transmission by exploiting capture effect achieved through fine grained power control at the level of segments within a single packet. We applied our technique to the design of a one-hop, many-to-one communication protocol, SyncMerge, and a multi-hop, many-to-many communication protocol, ByteCast. Our evaluation shows that SyncMerge is able to achieve 2 to 15 times improvement over traditional many-to-one communication schemes. In addition, ByteCast is able to disseminate 1 byte of data from every node to all other nodes in about 600 ms with 99.5% reliability on a 90 node testbed. Compared to the state-of-the-art protocols such as LWB and Chaos, ByteCast is able to reduce the radio-on time by up to 90% while achieving similar reliability.},
keywords={power control;telecommunication network topology;time division multiple access;state-of-the-art protocols;Time Division Multiple Access;single large packet transmission;fine grained power control;communication schemes;many-to-one communication protocol;many-to-many communication;TDMA implementations;SyncMerge protocol;ByteCast protocol;memory size 1.0 Byte;Protocols;Time division multiple access;Power control;Synchronization;Reliability;Conferences;Chaos},
doi={10.1109/INFOCOM.2017.8057190},
ISSN={},
month={May},}
@INPROCEEDINGS{8057191,
author={H. Dai and B. Liu and H. Yuan and P. Crowley and J. Lu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Analysis of tandem PIT and CS with non-zero download delay},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Collapsed forwarding has long been used in cache systems to reduce the load on servers by aggregating requests for the same content. Named Data Networking (NDN) as a future Internet architecture incorporates this technique through a data structure called Pending Interest Table (PIT). The request aggregation feature suggests that PIT can be viewed as a nonreset time-to-live (TTL) based cache. The Content Store (CS) is a content cache placed in front of the PIT on the NDN forwarding path, so they make up a tandem cache network. To investigate the metrics of interest in this network, like the hit probability for the PIT and the CS, the expected PIT size, non-zero download delay (non-ZDD) should be taken into consideration. Caching policies usually assume zero download delay (ZDD), i.e., request and object arrive simultaneously, and numerous analytical methods have been proposed to study the ZDD caching policies. In this paper, after dissecting the LRU policy, we for the first time propose two LRU variants considering non-ZDD by defining separate operations for the request and object arrivals. When CS adopts the proposed LRU variants, the analysis of the CS-PIT network can still take advantage of the existing models, so the metrics of interest can be computed. Especially, the distribution for the “inter-miss” time of this network can be derived, which has not been achieved by prior works. Finally, the analytical results are verified through simulations.},
keywords={cache storage;data structures;Internet;future Internet architecture;data structure;Pending Interest Table;request aggregation feature;Content Store;content cache;NDN forwarding path;tandem cache network;nonzero download delay;nonZDD;zero download delay;ZDD caching policies;LRU policy;LRU variants;CS-PIT network;cache systems;Named Data Networking;nonreset time-to-live cache;TTL based cache;inter-miss time;Delays;Computational modeling;Random variables;Conferences;Data structures;Analytical models},
doi={10.1109/INFOCOM.2017.8057191},
ISSN={},
month={May},}
@INPROCEEDINGS{8057192,
author={G. Neglia and D. Carra and P. Michiardi},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Cache policies for linear utility maximization},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Cache policies to minimize the content retrieval cost have been studied through competitive analysis when the miss costs are additive and the sequence of content requests is arbitrary. More recently, a cache utility maximization problem has been introduced, where contents have stationary popularities and utilities are strictly concave in the hit rates. This paper bridges the two formulations, considering linear costs and content popularities. We show that minimizing the retrieval cost corresponds to solving an online knapsack problem, and we propose new dynamic policies inspired by simulated annealing, including DynqLRU, a variant of qLRU. For such policies we prove asymptotic convergence to the optimum under the characteristic time approximation. In a real scenario, popularities vary over time and their estimation is very difficult. DynqLRU does not require popularity estimation, and our realistic, trace-driven evaluation shows that it significantly outperforms state-of-the-art policies, with up to 45% cost reduction.},
keywords={cache storage;information retrieval;knapsack problems;simulated annealing;cache utility maximization problem;linear costs;online knapsack problem;DynqLRU;cost reduction;retrieval cost;trace-driven evaluation;simulated annealing;content requests;miss costs;content retrieval cost;linear utility maximization;cache policies;Simulated annealing;Estimation;Approximation algorithms;Space exploration;Detectors;Conferences;Heuristic algorithms},
doi={10.1109/INFOCOM.2017.8057192},
ISSN={},
month={May},}
@INPROCEEDINGS{8057193,
author={M. Zhang and V. Lehman and L. Wang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Scalable name-based data synchronization for named data networking},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In Named Data Networking (NDN), data synchronization plays an important role similar to transport protocols in IP. Many distributed applications, including pub-sub applications such as news and weather services, require a synchronization protocol where each consumer can subscribe to a different subset of a producer's data streams. However, existing Sync protocols support only full-data synchronization, which is a special case of this problem. We propose PSync to efficiently address different types of data synchronization. Names are used in PSync messages to carry producers' latest namespace information and each consumer's subscription information, which allows producers to maintain a single state for all consumers and enables consumers to synchronize with any producer that replicates the same data. We have implemented PSync in the NDN codebase and used it to develop a prototype pub-sub module for building management. Our experimental results show that PSync scales well as the number of consumers, subscriptions, and data streams increases and it outperforms the state-of-the-art Sync protocol in supporting full-data synchronization.},
keywords={Internet;synchronisation;telecommunication traffic;transport protocols;named data networking;synchronization protocol;Sync protocols support;full-data synchronization;data streams increases;state-of-the-art Sync protocol;scalable name-based data synchronization;transport protocols;PSync messages;NDN codebase;Synchronization;Protocols;Arrays;Distributed databases;Conferences;Buildings},
doi={10.1109/INFOCOM.2017.8057193},
ISSN={},
month={May},}
@INPROCEEDINGS{8057194,
author={J. Choi and S. Moon and J. Woo and K. Son and J. Shin and Y. Yi},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Rumor source detection under querying with untruthful answers},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Social networks are the major routes for most individuals to exchange their opinions about new products, social trends and political issues via their interactions. It is often of significant importance to figure out who initially diffuses the information, i.e., finding a rumor source or a trend setter. It is known that such a task is highly challenging and the source detection probability cannot be beyond 31% for regular trees, if we just estimate the source from a given diffusion snapshot. In practice, finding the source often entails the process of querying that asks “Are you the rumor source?” or “Who tells you the rumor?” that would increase the chance of detecting the source. In this paper, we consider two kinds of querying: (a) simple batch querying and (b) interactive querying with direction under the assumption that queriees can be untruthful with some probability. We propose estimation algorithms for those queries, and quantify their detection performance and the amount of extra budget due to untruthfulness, analytically showing that querying significantly improves the detection performance. We perform extensive simulations to validate our theoretical findings over synthetic and real-world social network topologies.},
keywords={probability;query processing;social networking (online);social sciences computing;social trends;political issues;source detection probability;simple batch querying;detection performance;rumor source detection;untruthful answers;social networks;diffusion snapshot;social network topologies;Network topology;Topology;Conferences;Computational modeling;Internet;Maximum likelihood estimation},
doi={10.1109/INFOCOM.2017.8057194},
ISSN={},
month={May},}
@INPROCEEDINGS{8057195,
author={S. Achleitner and T. L. Porta and P. McDaniel and S. V. Krishnamurthy and A. Poylisher and C. Serban},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Stealth migration: Hiding virtual machines on the network},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Live virtual machine (VM) migration is commonly used for enabling dynamic resource or fault management, or for load balancing in datacenters or cloud platforms. A service hosted by a VM may also be migrated to prevent its visibility to an external adversary who may seek to disrupt its operation by launching a DDoS attack against it. We first show that current systems cannot adequately hide a VM migration from an external adversary. The key reason for this is that a migration typically manifests a traffic pattern with distinguishable statistical properties. We introduce two new attacks that can allow an adversary to effectively track a migration in progress, by leveraging observations of these properties. As our primary contribution, we design and implement a stealth migration framework that causes migration traffic to be indistinguishable from regular Internet traffic, with a negligible latency overhead of approximately 0.37 seconds, on average.},
keywords={cloud computing;computer centres;computer network security;resource allocation;telecommunication traffic;virtual machines;live virtual machine migration;dynamic resource;live VM migration;statistical properties;fault management;datacenters;traffic pattern;latency overhead;migration traffic;stealth migration framework;DDoS attack;external adversary;cloud platforms;load balancing;Virtual machining;Virtual machine monitors;Hardware;Timing;Conferences;Cloud computing},
doi={10.1109/INFOCOM.2017.8057195},
ISSN={},
month={May},}
@INPROCEEDINGS{8057196,
author={Y. Xiao and M. Krunz},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={QoE and power efficiency tradeoff for fog computing networks with fog node cooperation},
year={2017},
volume={},
number={},
pages={1-9},
abstract={This paper studies the workload offloading problem for fog computing networks in which a set of fog nodes can offload part or all the workload originally targeted to the cloud data centers to further improve the quality-of-experience (QoE) of users. We investigate two performance metrics for fog computing networks: users' QoE and fog nodes' power efficiency. We observe a fundamental tradeoff between these two metrics for fog computing networks. We then consider cooperative fog computing networks in which multiple fog nodes can help each other to jointly offload workload from cloud data centers. We propose a novel cooperation strategy referred to as offload forwarding, in which each fog node, instead of always relying on cloud data centers to process its unprocessed workload, can also forward part or all of its unprocessed workload to its neighboring fog nodes to further improve the QoE of its users. A distributed optimization algorithm based on distributed alternating direction method of multipliers (ADMM) via variable splitting is proposed to achieve the optimal workload allocation solution that maximizes users' QoE under the given power efficiency. We consider a fog computing platform that is supported by a wireless infrastructure as a case study to verify the performance of our proposed framework. Numerical results show that our proposed approach significantly improves the performance of fog computing networks.},
keywords={cloud computing;evolutionary computation;resource allocation;fog computing networks;fog node cooperation;cloud data centers;fog computing platform;distributed alternating direction method of multipliers;distributed optimization algorithm;ADMM;variable splitting;user QoE;quality of experience;power efficiency;Edge computing;Cloud computing;Resource management;Power demand;Measurement;Fog computing;response-time analysis;power efficiency;offload forwarding;ADMM},
doi={10.1109/INFOCOM.2017.8057196},
ISSN={},
month={May},}
@INPROCEEDINGS{8057197,
author={R. D. Yates and M. Tavan and Y. Hu and D. Raychaudhuri},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Timely cloud gaming},
year={2017},
volume={},
number={},
pages={1-9},
abstract={This work introduces a new model for cloud gaming systems aimed at optimizing the timeliness of video frames based on an age of information (AoI) metric. Mobile clients submit actions through an access network to a game server. The game server generates video frames at a constant frame rate. At the mobile device, the display of these frames represent game status updates. We develop a Markov model to characterize the frame delivery process in low-latency edge cloud gaming systems. Based on this model, we derive a simple formula for the average status age of a tightly synchronized low-latency mobile gaming system in which the inter-frame period is a significant contributor to the system latency. We validate the model by ns-3 simulation of a low-latency edge cloud gaming system. Our evaluation scenarios included single-player games as well as multi-player games in which the game processing was conducted by a combination of a centralized game server and edge cloud renderers.},
keywords={cloud computing;computer games;Markov processes;rendering (computer graphics);edge cloud renderers;timely cloud gaming;cloud gaming systems;video frames;game status updates;Markov model;frame delivery process;low-latency edge cloud gaming system;low-latency mobile gaming system;inter-frame period;single-player games;multiplayer games;game processing;centralized game server;Servers;Mobile communication;Delays;Cloud gaming;Streaming media;Mobile computing},
doi={10.1109/INFOCOM.2017.8057197},
ISSN={},
month={May},}
@INPROCEEDINGS{8057198,
author={X. Wang and X. Chen and W. Wu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Towards truthful auction mechanisms for task assignment in mobile device clouds},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Despite the increased capabilities of mobile devices, resource-demanded mobile applications still transcend what can be accomplished on a single device. As such, mobile device cloud (MDC), an environment that enables computation-intensive tasks to be performed among a set of nearby mobile devices, offers a promising architecture to support real-time mobile applications. To stimulate mobile devices to execute tasks for others, it is essential to design an incentive mechanism that appropriately charges the owners of the tasks, acted as the buyers, and rewards the mobile devices, acted as the sellers. In this paper, we propose two truthful auction mechanisms for two different task models, heterogeneous and homogeneous task models, which assume the different and the same resource requirements of the tasks, respectively. Specifically, for heterogeneous task model, we propose an efficient heuristic winning bids determination algorithm to allocate the tasks, and decide the payment of each seller for its winning bids. For homogeneous task model, we design an optimal winning bid determination algorithm, and propose a Vickrey-Clarke-Groves (VCG) based auction mechanism to determine the payment of each bid. Both theoretical analysis and simulations show that the proposed auction mechanisms achieve several desirable properties such as individual rationality, truthfulness and computational efficiency.},
keywords={cloud computing;mobile computing;resource allocation;mobile device cloud;computation-intensive tasks;real-time mobile applications;truthful auction mechanisms;heterogeneous task models;homogeneous task models;Vickrey-Clarke-Groves based auction mechanism;task assignment;mobile devices;VCG based auction mechanism;winning bid determination algorithm;Mobile handsets;Computational modeling;Cloud computing;Mobile communication;Resource management;Batteries},
doi={10.1109/INFOCOM.2017.8057198},
ISSN={},
month={May},}
@INPROCEEDINGS{8057199,
author={C. Hu and A. Alhothaily and A. Alrawais and X. Cheng and C. Sturtivant and H. Liu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A secure and verifiable outsourcing scheme for matrix inverse computation},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Matrix inverse computation is one of the most fundamental mathematical problems in large-scale data analytics and computing. It is often too expensive to be solved in resource-constrained devices such as sensors. Outsourcing the computation task to a cloud server or a fog server is a potential approach as the server is able to perform large-scale scientific computations on behalf of resource-constrained users with special software. However, outsourcing brings in new security concerns and challenges such as data privacy violations and result invalidation. In this paper, we propose a secure and verifiable outsourcing scheme to compute the matrix inverse in a server. In our scheme, the client generates two secret key sets based on two chaotic systems, which are utilized to create two sparse matrices whose permuted versions are used for matrix encryption and decryption to protect input and output privacy. The server computes the inverse over the ciphertext matrix and returns the result to the client who can verify the validity of the inverse. We analyze the proposed scheme in terms of correctness, security, verifiability, and attack resistance, and compare its performance (computation, storage, and communication overheads) with those of the state-of-the-art. Our theoretical results and comparison study demonstrate that the proposed scheme provides a secure and efficient outsourcing mechanism for matrix inverse computation.},
keywords={cryptography;data privacy;matrix inversion;computation task;cloud server;fog server;large-scale scientific computations;resource-constrained users;verifiable outsourcing scheme;matrix encryption;ciphertext matrix;secure outsourcing mechanism;matrix inverse computation;large-scale data analytics;resource-constrained devices;secure outsourcing scheme;matrix decryption;Servers;Outsourcing;Sparse matrices;Computational modeling;Encryption;Iterative methods;Matrix inversion;cloud/fog computing;secure outsourcing;verification;data privacy;chaotic systems},
doi={10.1109/INFOCOM.2017.8057199},
ISSN={},
month={May},}
@INPROCEEDINGS{8057200,
author={X. Feng and Z. Zheng and D. Cansever and A. Swami and P. Mohapatra},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={A signaling game model for moving target defense},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Incentive-driven advanced attacks have become a major concern to cyber-security. Traditional defense techniques that adopt a passive and static approach by assuming a fixed attack type are insufficient in the face of highly adaptive and stealthy attacks. In particular, a passive defense approach often creates information asymmetry where the attacker knows more about the defender. To this end, moving target defense (MTD) has emerged as a promising way to reverse this information asymmetry. The main idea of MTD is to (continuously) change certain aspects of the system under control to increase the attacker's uncertainty, which in turn increases attack cost/complexity and reduces the chance of a successful exploit in a given amount of time. In this paper, we go one step beyond and show that MTD can be further improved when combined with information disclosure. In particular, we consider that the defender adopts a MTD strategy to protect a critical resource across a network of nodes, and propose a Bayesian Stackelberg game model with the defender as the leader and the attacker as the follower. After fully characterizing the defender's optimal migration strategies, we show that the defender can design a signaling scheme to exploit the uncertainty created by MTD to further affect the attacker's behavior for its own advantage. We obtain conditions under which signaling is useful, and show that strategic information disclosure can be a promising way to further reverse the information asymmetry and achieve more efficient active defense.},
keywords={Bayes methods;game theory;security of data;signaling game model;cyber-security;traditional defense techniques;static approach;fixed attack type;highly adaptive attacks;stealthy attacks;passive defense approach;information asymmetry;MTD strategy;Bayesian Stackelberg game model;signaling scheme;strategic information disclosure;efficient active defense;moving target defense;incentive-driven advanced attacks;attack cost-complexity;Games;Computational modeling;Bayes methods;Uncertainty;Face;Servers;Conferences},
doi={10.1109/INFOCOM.2017.8057200},
ISSN={},
month={May},}
@INPROCEEDINGS{8057201,
author={J. Chen and S. Yao and Q. Yuan and R. Du and G. Xue},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Checks and balances: A tripartite public key infrastructure for secure web-based connections},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Recent real-world attacks against Certification Authorities (CAs) and fraudulently issued certificates arouse the public to rethink the security of public key infrastructure for web-based connections. To distribute the trust of CAs, notaries, as an independent party, are introduced to record certificates, and a client can request an audit proof of certificates from notaries directly. However, there are two challenges. On one hand, existing works consider the security of notaries insufficiently. Due to lack of systematic mutual verification, notaries might bring safety bottlenecks. On the other hand, the service of these works is not sustainable, when any party leaks its private key or fails. In this paper, we propose a Tripartite Public Key Infrastructure (TriPKI), using Certificates Authorities, Integrity Log Servers, and Domain Name Servers, to provide a basis for establishing secure SSL/TLS connections. Specifically, we apply checks-and balances among those three parties in the structure to make them verify mutually, which avoids any single party compromise. Furthermore, we design a collaborative certificate management scheme to provide sustainable services. The security analysis and experiment results demonstrate that our scheme is suitable for practical usage with moderate overhead.},
keywords={authorisation;certification;Internet;public key cryptography;balances;tripartite public key infrastructure;Certification Authorities;CAs;fraudulently issued certificates;notaries;independent party;record certificates;systematic mutual verification;party leaks;private key;Certificates Authorities;secure SSL/TLS;checks;single party compromise;collaborative certificate management scheme;security analysis;secure Web-based connections;Public key;Servers;Collaboration;Electronic mail;Conferences;Authentication;Public Key Infrastructure;DNS-based;Mutual Verification},
doi={10.1109/INFOCOM.2017.8057201},
ISSN={},
month={May},}
@INPROCEEDINGS{8057202,
author={A. Klein and H. Shulman and M. Waidner},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Internet-wide study of DNS cache injections},
year={2017},
volume={},
number={},
pages={1-9},
abstract={DNS caches are an extremely important tool, providing services for DNS as well as for a multitude of applications, systems and security mechanisms, such as anti-spam defences, routing security (e.g., RPKI), firewalls. Subverting the security of DNS is detrimental to the stability and security of the clients and services, and can facilitate attacks, circumventing even cryptographic mechanisms. We study the caching component of DNS resolution platforms in diverse networks in the Internet, and evaluate injection vulnerabilities allowing cache poisoning attacks. Our evaluation includes networks of leading Internet Service Providers and enterprises, and professionally managed open DNS resolvers. We test injection vulnerabilities against known payloads as well as a new class of indirect attacks that we define in this work. Our Internet evaluation indicates that more than 92% of the Internet's DNS resolution platforms are vulnerable to records injection and can be persistently poisoned.},
keywords={cache storage;computer network security;Internet;DNS cache injections;security mechanisms;cryptographic mechanisms;caching component;injection vulnerabilities;cache poisoning attacks;open DNS resolvers;indirect attacks;Internet evaluation;DNS resolution platforms;antispam defences;routing security;firewalls;Internet service providers;Electronic mail;IP networks;Servers;Payloads;Internet;Computer crime},
doi={10.1109/INFOCOM.2017.8057202},
ISSN={},
month={May},}
@INPROCEEDINGS{8057203,
author={Y. Cheng and H. Jiang and F. Wang and Y. Hua and D. Feng},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={BlitzG: Exploiting high-bandwidth networks for fast graph processing},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Nowadays, high-bandwidth networks are easily accessible in data centers. However, existing distributed graph-processing frameworks fail to efficiently utilize the additional bandwidth capacity in these networks for higher performance, due to their inefficient computation and communication models, leading to very long waiting times experienced by users for the graph-computing results. The root cause lies in the fact that the computation and communication models of these frameworks generate, send and receive messages so slowly that only a small fraction of the available network bandwidth is utilized. In this paper, we propose a high-performance distributed graph-processing framework, called BlitzG, to address this problem. This framework fully exploits the available network bandwidth capacity for fast graph processing. Our approach aims at significant reduction in (i) the computation workload of each vertex for fast message generation by using a new slimmed-down vertex-centric computation model and (ii) the average message overhead for fast message delivery by designing a lightweight message-centric communication model. Evaluation on a 40Gbps Ethernet, driven by real-world graph datasets, shows that BlitzG outperforms the state-of-the-art distributed graph-processing frameworks by up to 27x with an average of 20.7x.},
keywords={computer centres;distributed processing;graph theory;local area networks;network theory (graphs);communication models;long waiting times;high-performance distributed graph-processing framework;fast graph processing;computation workload;fast message generation;vertex-centric computation model;fast message delivery;lightweight message-centric communication model;real-world graph datasets;high-bandwidth networks;BlitzG;network bandwidth capacity;data centers;slimmed-down vertex-centric computation model;Ethernet;bit rate 40 Gbit/s;Computational modeling;Message systems;Kernel;Conferences;Bandwidth;Receivers;Scalability},
doi={10.1109/INFOCOM.2017.8057203},
ISSN={},
month={May},}
@INPROCEEDINGS{8057204,
author={T. Shu and C. Q. Wu},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Performance optimization of Hadoop workflows in public clouds through adaptive task partitioning},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Cloud computing provides a cost-effective computing platform for big data workflows where moldable parallel computing models such as MapReduce are widely applied to meet stringent performance requirements. The granularity of task partitioning in each moldable job has a significant impact on workflow completion time and financial cost. We investigate the properties of moldable jobs and design a big-data workflow mapping model, based on which, we formulate a workflow mapping problem to minimize workflow makespan under a budget constraint in public clouds. We show this problem to be strongly NP-complete and design i) a fully polynomial-time approximation scheme (FPTAS) for a special case with a pipeline-structured workflow executed on virtual machines in a single class, and ii) a heuristic for a generalized problem with an arbitrary directed acyclic graph-structured workflow executed on virtual machines in multiple classes. The performance superiority of the proposed solution is illustrated by extensive simulation-based results in Hadoop/YARN in comparison with existing workflow mapping models and algorithms.},
keywords={Big Data;cloud computing;computational complexity;directed graphs;optimisation;parallel processing;scheduling;performance optimization;Hadoop workflows;public clouds;cloud computing;cost-effective computing platform;moldable parallel computing models;task partitioning;workflow completion time;financial cost;big-data workflow mapping model;workflow mapping problem;virtual machines;generalized problem;performance superiority;Hadoop/YARN;workflow mapping models;adaptive task partitioning;Big Data workflow;NP-complete design;fully polynomial-time approximation scheme;arbitrary directed acyclic graph-structured workflow;Cloud computing;Computational modeling;Big Data;Optimization;Scheduling;Program processors},
doi={10.1109/INFOCOM.2017.8057204},
ISSN={},
month={May},}
@INPROCEEDINGS{8057205,
author={X. Mei and X. Chu and H. Liu and Y. Leung and Z. Li},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Energy efficient real-time task scheduling on CPU-GPU hybrid clusters},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Conserving the energy consumption of large data centers is of critical significance, where a few percent in consumption reduction translates into millions-dollar savings. This work studies energy conservation on emerging CPU-GPU hybrid clusters through dynamic voltage and frequency scaling (DVFS). We aim at minimizing the total energy consumption of processing a sequence of real-time tasks under deadline constraints. We compute the appropriate voltage/frequency setting for each task through mathematical optimization, and assign multiple tasks to the cluster with heuristic scheduling algorithms. In performance evaluation driven by real-world power measurement traces, our scheduling algorithm shows comparable energy savings to the theoretical upper bound. With a GPU scaling interval where analytically at most 38% of energy can be saved, we record 30-36% of energy savings. Our results are applicable to energy management on modern heterogeneous clusters. In particular, our model stresses the nonlinear relationship between task execution time and processor speed for GPU-accelerated applications, for more accurately capturing real-world GPU energy consumption.},
keywords={graphics processing units;multiprocessing systems;performance evaluation;power aware computing;processor scheduling;upper bound;GPU energy consumption;total energy consumption reduction;dynamic voltage and frequency scaling;GPU-accelerated applications;task execution time;GPU scaling interval;real-world power measurement traces;heuristic scheduling algorithms;data centers;CPU-GPU hybrid clusters;energy efficient real-time task scheduling;Graphics processing units;Energy consumption;Servers;Runtime;Power demand;Scheduling algorithms},
doi={10.1109/INFOCOM.2017.8057205},
ISSN={},
month={May},}
@INPROCEEDINGS{8057206,
author={D. Cheng and Y. Chen and X. Zhou and D. Gmach and D. Milojicic},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Adaptive scheduling of parallel jobs in spark streaming},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Streaming data analytics has become increasingly vital in many applications such as dynamic content delivery (e.g., advertisements), Twitter sentiment analysis, and security event processing (e.g., intrusion detection systems, and spam filters). Emerging stream processing systems, such as Spark Streaming, treat the continuous stream as a series of micro-batches of data and continuously process these micro-batch jobs. Such micro-batch based stream processing provides several advantages over traditional stream processing systems, which process streaming data one record at a time, including fast recovery from failures, better load balancing and scalability. However, efficient scheduling of micro-batch jobs to achieve high throughput and low latency is very challenging due to the complex data dependency and dynamism inherent in streaming workloads. In this paper, we propose A-scheduler, an adaptive scheduling approach that dynamically schedules parallel micro-batch jobs in Spark Streaming and automatically adjusts scheduling parameters to improve performance and resource efficiency. Specifically, A-scheduler dynamically schedules multiple jobs concurrently using different policies based on their data dependencies and automatically adjusts the level of job parallelism and resource shares among jobs based on workload properties. We implemented A-scheduler and evaluated it with a real-time security event processing workload. Our experimental results show that A-scheduler can reduce end-to-end latency by 42% and improve workload throughput and energy efficiency by 21% and 13%, respectively, compared to the default Spark Streaming scheduler.},
keywords={adaptive scheduling;data analysis;dynamic scheduling;parallel processing;resource allocation;scheduling;security of data;parallel jobs;data analytics;dynamic content delivery;intrusion detection systems;microbatch jobs;microbatch based stream processing;traditional stream processing systems;adaptive scheduling approach;data dependencies;job parallelism;resource shares;real-time security event processing workload;streaming data;data dependency;A-scheduler;Spark Streaming scheduler;Sparks;Parallel processing;Resource management;Throughput;Schedules;Real-time systems},
doi={10.1109/INFOCOM.2017.8057206},
ISSN={},
month={May},}
@INPROCEEDINGS{8057207,
author={W. Wang and Y. Chen and L. Yang and Q. Zhang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Detecting on-body devices through creeping wave propagation},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The ability to detect which wearables and smartphones are on the same body has the potential to support a wealth of applications, including user authentication, automatic data synchronization, and personalized profile loading. This paper brings this feature to commercial off-the-shelf (COTS) wearables and smartphones, by creating a virtual “on-body detection sensor” based on devices' inherent wireless capabilities. We investigate using the peculiar propagation characteristics of creeping waves to discern on-body wearables. To this end, we decompose signals into multiple independent components to exploit the variation features of creeping waves. We implement our system on COTS wearables and a smartphone. Extensive experiments are conducted in a lab, apartments, malls, and outdoor areas, involving 12 volunteer subjects of different age groups, to demonstrate the robustness of our system. Results show that our system can identify on-body devices at 92.3% average true positive rate and 5% average false positive rate.},
keywords={body sensor networks;electromagnetic wave propagation;object detection;smart phones;on-body devices;smartphones;user authentication;automatic data synchronization;virtual on-body detection sensor;on-body wearables;COTS wearables;smartphone;creeping wave propagation;personalized profile loading;commercial off-the-shelf wearables;Smart phones;Feature extraction;Radio propagation;Wireless sensor networks;Wireless communication;Dynamics;Biomedical monitoring;Creeping waves;wearables;on-body detection},
doi={10.1109/INFOCOM.2017.8057207},
ISSN={},
month={May},}
@INPROCEEDINGS{8057208,
author={X. Guo and J. Liu and Y. Chen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={FitCoach: Virtual fitness coach empowered by wearable mobile devices},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Acknowledging the powerful sensors on wearables and smartphones enabling various applications to improve users' life styles and qualities (e.g., sleep monitoring and running rhythm tracking), this paper takes one step forward developing FitCoach, a virtual fitness coach leveraging users' wearable mobile devices (including wrist-worn wearables and arm-mounted smartphones) to assess dynamic postures (movement patterns &amp; positions) in workouts. FitCoach aims to help the user to achieve effective workout and prevent injury by dynamically depicting the short-term and long-term picture of a user's workout based on various sensors in wearable mobile devices. In particular, FitCoach recognizes different types of exercises and interprets fine-grained fitness data (i.e., motion strength and speed) to an easy-to-understand exercise review score, which provides a comprehensive workout performance evaluation and recommendation. FitCoach has the ability to align the sensor readings from wearable devices to the human coordinate system, ensuring the accuracy and robustness of the system. Extensive experiments with over 5000 repetitions of 12 types of exercises involve 12 participants doing both anaerobic and aerobic exercises in indoors as well as outdoors. Our results demonstrate that FitCoach can provide meaningful review and recommendations to users by accurately measure their workout performance and achieve 93% accuracy for workout analysis.},
keywords={accelerometers;assisted living;biomechanics;body sensor networks;medical computing;mobile computing;patient rehabilitation;sleep;smart phones;FitCoach;wearable mobile devices;smartphones;sleep monitoring;running rhythm tracking;dynamic postures;fine-grained fitness data;exercise review score;comprehensive workout performance evaluation;sensor readings;wearable devices;workout analysis;virtual fitness coach;wrist-worn wearables;arm-mounted smart phones;movement pattern-and-positions;Biomedical monitoring;Performance evaluation;Wearable sensors;Monitoring;Smart phones},
doi={10.1109/INFOCOM.2017.8057208},
ISSN={},
month={May},}
@INPROCEEDINGS{8057209,
author={L. Xie and X. Dong and W. Wang and D. Huang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Meta-activity recognition: A wearable approach for logic cognition-based activity sensing},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Activity sensing has become a key technology for many ubiquitous applications, such as exercise monitoring and elder care. Most traditional approaches track the human motions and perform activity recognition based on the waveform matching schemes in the raw data representation level. In regard to the complex activities with relatively large moving range, they usually fail to accurately recognize these activities, due to the inherent variations in human activities. In this paper, we propose a wearable approach for logic cognition-based activity sensing scheme in the logical representation level, by leveraging the meta-activity recognition. Our solution extracts the angle profiles from the raw inertial measurements, to depict the angle variation of limb movement in regard to the consistent body coordinate system. It further extracts the meta-activity profiles to depict the sequence of small-range activity units in the complex activity. By leveraging the least edit distance-based matching scheme, our solution is able to accurately perform the activity sensing. Based on the logic cognition-based activity sensing, our solution achieves lightweight-training recognition, which requires a small quantity of training samples to build the templates, and user-independent recognition, which requires no training from the specific user. The experiment results in real settings shows that our meta-activity recognition achieves an average accuracy of 92% for user-independent activity sensing.},
keywords={cognition;feature extraction;geriatrics;medical signal processing;patient monitoring;ubiquitous computing;wearable computers;meta-activity recognition;wearable approach;raw data representation level;complex activity;human activities;logical representation level;meta-activity profiles;small-range activity units;user-independent activity sensing;logic cognition-based activity sensing;ubiquitous applications;exercise monitoring;elder care;human motions;waveform matching;least edit distance-based matching scheme;limb movement;Coordinate measuring machines;Motion measurement;Training;Activity recognition;Transforms;Optical wavelength conversion},
doi={10.1109/INFOCOM.2017.8057209},
ISSN={},
month={May},}
@INPROCEEDINGS{8057210,
author={X. Liang and T. Yun and R. Peterson and D. Kotz},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={LightTouch: Securely connecting wearables to ambient displays with user intent},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Wearables are small and have limited user interfaces, so they often wirelessly interface with a personal smartphone/computer to relay information from the wearable for display or other interactions. In this paper, we envision a new method, LightTouch, by which a wearable can establish a secure connection to an ambient display, such as a television or a computer monitor, while ensuring the user's intention to connect to the display. LightTouch uses standard RF methods (like Bluetooth) for communicating the data to display, securely bootstrapped via the visible-light communication (the brightness channel) from the display to the low-cost, low-power, ambient light sensor of a wearable. A screen `touch' gesture is adopted by users to ensure that the modulation of screen brightness can be securely captured by the ambient light sensor with minimized noise. Wireless coordination with the processor driving the display establishes a shared secret based on the brightness channel information. We further propose novel onscreen localization and correlation algorithms to improve security and reliability. Through experiments and a preliminary user study we demonstrate that LightTouch is compatible with current display and wearable designs, is easy to use (about 6 seconds to connect), is reliable (up to 98% success connection ratio), and is secure against attacks.},
keywords={interactive devices;mobile handsets;security of data;smart phones;user interfaces;LightTouch;ambient display;user intent;user interfaces;standard RF methods;visible-light communication;ambient light sensor;screen touch gesture;screen brightness;wireless coordination;brightness channel information;preliminary user study;wearable designs;personal smartphone;personal computer;connection ratio;Brightness;Biomedical monitoring;Radio frequency;Security;Monitoring;Correlation;Light sources},
doi={10.1109/INFOCOM.2017.8057210},
ISSN={},
month={May},}
@INPROCEEDINGS{8057211,
author={Z. Li and K. G. Shin and L. Zhen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={When and how much to neutralize interference?},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Interference management (IM) is essential to wireless communication networks, but interference suppression, a key component of IM, is known to degrade users' achievable spectral efficiency (SE). It is thus important to select an appropriate IM method with optimal operating parameters according to diverse network deployments, transmit power differences of various communication equipments, and dynamically changing channel conditions so as to balance the benefits brought by and the cost of IM. Interference neutralization (IN) has recently been receiving considerable attention, with which a duplicate of interference of the same strength and opposite phase w.r.t. the original interfering signal is generated to neutralize the disturbance at the intended receiver. However, to the best of our knowledge, all existing IN schemes assume that interference is completely neutralized without accounting for their power consumption. To remedy this deficiency, we propose a novel scheme, called dynamic interference neutralization (DIN). By intelligently determining the appropriate portion of interference to be neutralized, we balance the transmitter's power used for IN and the desired signal's transmission. We then present a new way to adaptively select one of DIN and other IM methods by taking into account the cost of multiple IM methods and their benefits. Our analysis has shown that DIN can include complete IN and non-interference management (non-IM) as special cases. The proposed strategy is shown via simulation to be able to make better use of the transmit power than existing IM methods, hence enhancing users' SE.},
keywords={interference suppression;radio networks;radiofrequency interference;interference management;wireless communication networks;interference suppression;optimal operating parameters;diverse network deployments;transmit power differences;communication equipments;channel conditions;opposite phase w.r.t;original interfering signal;intended receiver;dynamic interference neutralization;nonIM;noninterference management;multiple IM methods;transmitter;DIN;power consumption;Interference;Transmitters;Receivers;Relays;Macrocell networks;Conferences;Array signal processing},
doi={10.1109/INFOCOM.2017.8057211},
ISSN={},
month={May},}
@INPROCEEDINGS{8057212,
author={K. Hsu and K. C. Lin and H. Wei},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Inter-client interference cancellation for full-duplex networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Recent studies have experimentally shown the gains of full-duplex radios. However, due to its relatively higher cost and complexity, we can envision a more practical step in the network evolution is to have a full-duplex access point (AP) but keep the clients half-duplex. Unfortunately, the full-duplex gains can hardly be extracted in practice as the uplink transmission from a half-duplex client introduces inter-client interference to another downlink client. To address this issue, we present the design and implementation of IC2 (Inter-Client Interference Cancellation), the first physical layer solution that exploits the AP's full-duplex capability to actively cancel the interference at the downlink client. Such active cancellation not only improves the achievable capacity, but also better tolerates imperfect user pairing, simplifying the MAC design as a result. We build a prototype of IC2 on USRP-N200 and evaluate its performance via both testbed experiments and large-scale trace-driven simulations. The results show that, without IC2, about 60% of client pairs produce no gain from full-duplex transmissions, while, with IC2 the median throughput gain over conventional half-duplex networks can be 1.65× even when clients are simply paired randomly.},
keywords={access protocols;interference suppression;radio networks;radiofrequency interference;software radio;wireless LAN;active cancellation;client pairs;full-duplex transmissions;Inter-client interference cancellation;full-duplex networks;full-duplex radios;full-duplex access point;downlink client;AP's full-duplex capability;Inter-Client Interference Cancellation;IC2;Downlink;Uplink;Interference cancellation;Throughput;Hardware;Relays},
doi={10.1109/INFOCOM.2017.8057212},
ISSN={},
month={May},}
@INPROCEEDINGS{8057213,
author={T. Vermeulen and M. Laghate and G. Hattab and D. Cabric and S. Pollin},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Towards instantaneous collision and interference detection using in-band full duplex},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Wireless devices are ubiquitous nowadays and, since most of them use the same unlicensed frequency bands, the high number of packet losses due to interference and collisions degrade performance. Reliability, energy consumption, and latency are key challenges for future dense networks. Allowing the transmitter to take action, i.e., vacating the channel, as soon as a collision or interference is detected is crucial in improving these metrics. In-band full duplex radios enable the transmitter to simultaneously transmit packets and sense the spectrum for collisions and interference. This paper studies two important questions regarding transmitter-based collision and interference detection: (1) from an overall system perspective, does such detection outperform receiver-based detection and (2) which test statistic is the most accurate and sensitive at detecting collisions and interference. First, NS-3 simulations are used to show that transmitter-based detection reduces the energy consumption while improving the throughput in a typical star topology network. Next, we present a measurement-based study of four different techniques for transmitter-based collision and interference detection. In particular, we compare the energy detector with three goodness-of-fit tests in terms of probability of detection and false alarm. Our analysis shows that transmitter-based detection can detect between 80% to 100% of the collisions and interference occurring at the receiver, depending on the distance between the transmitter and the receiver. Of those detectable by the transmitter, our measurement results show that goodness-of-fit tests can detect nearly 100% of the collisions and have at least 10 dB better sensitivity as compared to the commonly proposed energy detection test. In general, the proposed techniques can detect interfering signals that are up to 25 dB below the remaining self-interference power.},
keywords={energy consumption;interference suppression;probability;radio receivers;radio transmitters;radiofrequency interference;signal detection;telecommunication network reliability;telecommunication network topology;wireless channels;interference detection;unlicensed frequency bands;collision;In-band full duplex radios;goodness-of-fit tests;remaining self-interference power;instantaneous collision;energy detection test;latency;transmitter-based collision;NS-3 simulations;energy consumption reduction;star topology network;false alarm;probability of detection;receiver;interfering signal detection;Interference;Receivers;Radio transmitters;Silicon;Reliability;Energy consumption},
doi={10.1109/INFOCOM.2017.8057213},
ISSN={},
month={May},}
@INPROCEEDINGS{8057214,
author={G. Naik and J. Liu and J. J. Park},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Coexistence of Dedicated Short Range Communications (DSRC) and Wi-Fi: Implications to Wi-Fi performance},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The 5.9 GHz band is being actively explored for possible spectrum sharing opportunities between Dedicated Short Range Communications (DSRC) and IEEE 802.11ac networks in order to address the increasing demand for bandwidth-intensive Wi-Fi applications. In this paper, we study the implications of this spectrum sharing to the performance of Wi-Fi systems. Through experiments performed on our testbed, we first investigate band sharing options available for Wi-Fi devices. Using experimental results, we show the need for using conservative Wi-Fi transmission parameters to enable harmonious coexistence between DSRC and Wi-Fi. Moreover, we show that under the current 802.11ac standard, certain channelization options, particularly the high bandwidth ones, cannot be used by Wi-Fi devices without causing interference to the DSRC nodes. Under these constraints, we propose a Real-time Channelization Algorithm (RCA) for Wi-Fi Access Points (APs) operating in the shared spectrum. Evaluation of the proposed algorithm using a prototype implementation on commodity hardware as well as via simulations show that informed channelization decisions can significantly increase Wi-Fi throughput compared to static channelization schemes.},
keywords={radio spectrum management;real-time systems;wireless LAN;real-time channelization algorithms;spectrum sharing;conservative Wi-Fi transmission parameters;band sharing options;Wi-Fi systems;bandwidth-intensive Wi-Fi applications;IEEE 802.11ac networks;Wi-Fi performance;DSRC;dedicated short range communications;shared spectrum;Wi-Fi Access Points;Wi-Fi devices;frequency 5.9 GHz;Wireless fidelity;Transmitters;IEEE 802.11 Standard;Bandwidth;Hardware;Performance evaluation},
doi={10.1109/INFOCOM.2017.8057214},
ISSN={},
month={May},}
@INPROCEEDINGS{8057215,
author={R. Ben Basat and G. Einziger and R. Friedman and Y. Kassner},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Randomized admission policy for efficient top-k and frequency estimation},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Network management protocols often require timely and meaningful insight about per flow network traffic. This paper introduces Randomized Admission Policy (RAP) - a novel algorithm for the frequency and top-k estimation problems, which are fundamental in network monitoring. We demonstrate space reductions compared to the alternatives by a factor of up to 32 on real packet traces and up to 128 on heavy-tailed workloads. For top-k identification, RAP exhibits memory savings by a factor of between 4 and 64 depending on the workloads' skewness. These empirical results are backed by formal analysis, indicating the asymptotic space improvement of our probabilistic admission approach. Additionally, we present d-Way RAP, a hardware friendly variant of RAP that empirically maintains its space and accuracy benefits.},
keywords={frequency estimation;protocols;telecommunication network management;telecommunication traffic;top-k estimation problems;Randomized admission policy;frequency estimation;network management protocols;flow network traffic;Randomized Admission Policy;Radiation detectors;Frequency estimation;Monitoring;Random access memory;Probabilistic logic;Algorithm design and analysis;Estimation},
doi={10.1109/INFOCOM.2017.8057215},
ISSN={},
month={May},}
@INPROCEEDINGS{8057216,
author={R. Ben Basat and G. Einziger and R. Friedman and Y. Kassner},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Optimal elephant flow detection},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Monitoring the traffic volumes of elephant flows, including the total byte count per flow, is a fundamental capability for online network measurements. We present an asymptotically optimal algorithm for solving this problem in terms of both space and time complexity. This improves on previous approaches, which can only count the number of packets in constant time. We evaluate our work on real packet traces, demonstrating an up to X2.5 speedup compared to the best alternative.},
keywords={computational complexity;optimisation;telecommunication network management;telecommunication traffic;optimal elephant flow detection;traffic volumes;fundamental capability;online network measurements;asymptotically optimal algorithm;time complexity;constant time;byte count;space complexity;Radiation detectors;Maintenance engineering;Monitoring;Data structures;Software algorithms;Runtime;Real-time systems},
doi={10.1109/INFOCOM.2017.8057216},
ISSN={},
month={May},}
@INPROCEEDINGS{8057217,
author={G. Xie and K. Xie and J. Huang and X. Wang and Y. Chen and J. Wen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Fast low-rank matrix approximation with locality sensitive hashing for quick anomaly detection},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Detecting anomalous traffic is a critical task for advanced Internet management. The traditional approaches based on Principal Component Analysis (PCA) are effective only when the corruption is caused by small additive i.i.d. Gaussian noise. The recent Direct Robust Matrix Factorization (DRMF) is proven to be more robust and accurate in anomaly detection, but it incurs a high computation cost due to its need of singular value decomposition (SVD) for low-rank matrix approximation and the iterative use of SVD execution to find the final solution. To enable the anomaly detection for large traffic matrix with the use of DRMF, we formulate the low-rank matrix approximation problem as a problem of searching for the subspace to project the traffic matrix with the minimum error. We propose a novel approach, LSH-subspace, for fast low-rank matrix approximation. To facilitate the matrix partition for the quick search of the subspace, we propose several novel techniques: a multi-layer locality sensitive hashing (LSH) table to reorder the OD pairs based on LSH function, a partition principle to guide the partition to minimize the projection error, and a lightweight algorithm to exploit the sparsity of the outlier matrix to update the LSH table at low overhead. Our extensive simulations based on real trace data demonstrate that our LSH-subspace is 3 times faster than DRMF with high anomaly detection accuracy.},
keywords={approximation theory;Internet;matrix decomposition;security of data;singular value decomposition;fast low-rank matrix approximation;quick anomaly detection;DRMF;traffic matrix;low-rank matrix approximation problem;LSH-subspace;matrix partition;multilayer locality sensitive hashing table;outlier matrix;high anomaly detection accuracy;anomalous traffic detection;direct robust matrix factorization;SVD execution;singular value decomposition;multilayer locality sensitive hashing;partition principle;Anomaly detection;Principal component analysis;Matrix decomposition;Robustness;Approximation algorithms;Noise measurement;Sparse matrices;Low-Rank Matrix Approximation;Anomaly Detection},
doi={10.1109/INFOCOM.2017.8057217},
ISSN={},
month={May},}
@INPROCEEDINGS{8057218,
author={K. Xie and C. Peng and X. Wang and G. Xie and J. Wen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Accurate recovery of internet traffic data under dynamic measurements},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The inference of the network traffic matrix from partial measurement data becomes increasingly critical for various network engineering tasks, such as capacity planning, load balancing, path setup, network provisioning, anomaly detection, and failure recovery. The recent study shows it is promising to more accurately interpolate the missing data with a three-dimensional tensor as compared to interpolation methods based on two-dimensional matrix. Despite the potential, it is difficult to form a tensor with measurements taken at varying rate in a practical network. To address the issues, we propose Reshape-Align scheme to form the regular tensor with data from dynamic measurements, and introduce user-domain and temporal-domain factor matrices which takes full advantage of features from both domains to translate the matrix completion problem to the tensor completion problem based on CP decomposition for more accurate missing data recovery. Our performance results demonstrate that our Reshape-Align scheme can achieve significantly better performance in terms of two metrics: error ratio and mean absolute error (MAE).},
keywords={Internet;interpolation;matrix algebra;telecommunication traffic;tensors;internet traffic data;dynamic measurements;mean absolute error;capacity planning;data recovery;tensor completion problem;matrix completion problem;temporal-domain factor matrices;user-domain;regular tensor;Reshape-Align scheme;practical network;two-dimensional matrix;interpolation methods;three-dimensional tensor;failure recovery;anomaly detection;network provisioning;path setup;load balancing;network engineering tasks;partial measurement data;network traffic matrix;Tensile stress;Matrix decomposition;Monitoring;Correlation;Indexes;Interpolation;Computational modeling;Internet traffic data recovery;Matrix completion;Tensor completion},
doi={10.1109/INFOCOM.2017.8057218},
ISSN={},
month={May},}
@INPROCEEDINGS{8057219,
author={F. Dang and P. Zhou and Z. Li and E. Zhai and A. Mohaisen and Q. Wen and M. Li},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Large-scale invisible attack on AFC systems with NFC-equipped smartphones},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Automated Fare Collection (AFC) systems have been globally deployed for decades, particularly in public transportation. Although the transaction messages of AFC systems are mostly transferred in plaintext, which is obviously insecure, system operators do not need to pay much attention to this issue, since the AFC network is well isolated from public network (e.g., the Internet). Nevertheless, in recent years, the advent of Near Field Communication (NFC)-equipped smartphones has bridged the gap between the AFC network and the Internet through Host-based Card Emulation (HCE). Motivated by this fact, we design and practice a novel paradigm of attack on modern distance-based pricing AFC systems, enabling users to pay much less than actually required. Our constructed attack has two important properties: 1) it is invisible to AFC system operators because the attack never causes any inconsistency in the backend database of the operators; and 2) it can be scalable to large number of users (e.g., 10,000) by maintaining a moderate-sized AFC card pool (e.g., containing 150 cards). Based upon this constructed attack, we developed an HCE app, named LessPay. Our real-world experiments on LessPay demonstrate not only the feasibility of our attack (with 97.6% success rate), but also its low-overhead in terms of bandwidth and computation.},
keywords={Internet;mobile computing;near-field communication;security of data;smart cards;smart phones;large-scale invisible attack;NFC-equipped smartphones;automated fare collection systems;host-based card emulation;HCE;distance-based pricing AFC systems;backend database;LessPay;moderate-sized AFC card pool;Near Field Communication;public network;AFC network;transaction messages;public transportation;Protocols;Authentication;Smart phones;Conferences;Urban areas;Web servers},
doi={10.1109/INFOCOM.2017.8057219},
ISSN={},
month={May},}
@INPROCEEDINGS{8057220,
author={Y. Chen and J. Sun and X. Jin and T. Li and R. Zhang and Y. Zhang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Your face your heart: Secure mobile face authentication with photoplethysmograms},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Face authentication emerges as a powerful method for preventing unauthorized access to mobile devices. It is, however, vulnerable to photo-based forgery attacks (PFA) and videobased forgery attacks (VFA), in which the adversary exploits a photo or video containing the user's frontal face. Effective defenses against PFA and VFA often rely on liveness detection, which seeks to find a live indicator that the submitted face photo or video of the legitimate user is indeed captured in real time. In this paper, we propose FaceHeart, a novel and practical face authentication system for mobile devices. FaceHeart simultaneously takes a face video with the front camera and a fingertip video with the rear camera on COTS mobile devices. It then achieves liveness detection by comparing the two photoplethysmograms independently extracted from the face and fingertip videos, which should be highly consistent if the two videos are for the same live person and taken at the same time. As photoplethysmograms are closely tied to human cardiac activity and almost impossible to forge or control, FaceHeart is strongly resilient to PFA and VFA. Extensive user experiments on Samsung Galaxy S5 have confirmed the high efficacy and efficiency of FaceHeart.},
keywords={face recognition;feature extraction;mobile computing;security of data;smart phones;video cameras;video signal processing;photoplethysmograms;unauthorized access;PFA;videobased forgery attacks;VFA;liveness detection;live indicator;legitimate user;FaceHeart;face video;fingertip video;rear camera;COTS mobile devices;fingertip videos;live person;mobile face authentication security;photo-based forgery attack;face photo;face authentication system;Samsung Galaxy S5;extensive user experiments;Face;Authentication;Mobile handsets;Cameras;Streaming media;Feature extraction;Mobile communication},
doi={10.1109/INFOCOM.2017.8057220},
ISSN={},
month={May},}
@INPROCEEDINGS{8057221,
author={H. Fu and Z. Zheng and S. Bose and M. Bishop and P. Mohapatra},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={LeakSemantic: Identifying abnormal sensitive network transmissions in mobile applications},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Mobile applications (apps) often transmit sensitive data through network with various intentions. Some transmissions are needed to fulfill the app's functionalities. However, transmissions with malicious receivers may lead to privacy leakage and tend to behave stealthily to evade detection. The problem is twofold: how does one unveil sensitive transmissions in mobile apps, and given a sensitive transmission, how does one determine if it is legitimate? In this paper, we propose LeakSemantic, a framework that can automatically locate abnormal sensitive network transmissions from mobile apps. LeakSemantic consists of a hybrid program analysis component and a machine learning component. Our program analysis component combines static analysis and dynamic analysis to precisely identify sensitive transmissions. Compared to existing taint analysis approaches, LeakSemantic achieves better accuracy with fewer false positives and is able to collect runtime data such as network traffic for each transmission. Based on features derived from the runtime data, machine learning classifiers are built to further differentiate between the legal and illegal disclosures. Experiments show that LeakSemantic achieves 91% accuracy on 2279 sensitive connections from 1404 apps.},
keywords={data privacy;learning (artificial intelligence);mobile computing;pattern classification;program diagnostics;security of data;system monitoring;runtime data;LeakSemantic;mobile applications;hybrid program analysis component;network traffic;abnormal sensitive network transmission identification;sensitive data transmission;dynamic analysis;machine learning classifiers;legal disclosures;illegal disclosures;Runtime;Mobile communication;Androids;Humanoid robots;Conferences;Analytical models;Privacy},
doi={10.1109/INFOCOM.2017.8057221},
ISSN={},
month={May},}
@INPROCEEDINGS{8057222,
author={Y. Yang and J. Sun},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Energy-efficient W-layer for behavior-based implicit authentication on mobile devices},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Motivated by the great potential of implicit and seamless user authentication, we attempt to build an efficient middle layer running on mobile devices to support implicit authentication (IA) systems with adaptive sampling. Various activities, such as user location, application usage, user motion, and battery usage have been popular choices to generate behaviors, the soft biometrics, for implicit authentication. Unlike password-based or hard biometric-based authentication, implicit authentication does not require explicit user action or expensive hardware. However, user behaviors can change unpredictably which renders it more challenging to develop systems that depend on them. Various machine learning algorithms have been used to address this challenge. The expensive training process is usually outsourced to the remote server but this can potentially increase the chance of data leakage. In addition, mobile devices may not always have reliable network connections to send real-time data to the server for training. Motivated by these limitations, we propose a W-layer, an overlay that provides an energy-efficient solution for real-time implicit authentication on mobile devices. The size of the data the system needs to collect at different times depends on the legitimacy of the user. This in turn affects how the sampling rate is adjusted which can reduce energy consumption. To evaluate our method, we conducted several experiments on both synthetic and real datasets. The average accuracy of identifying legitimate users is 96.73% using the synthetic dataset and 96.70% using the real dataset. Furthermore, we tested the power consumption on a low-end Nexus S smartphone to obtain a more pessimistic result. We found that our method consumed 14.5% of the device's total battery usage. The power consumption performance is expected to improve significantly on high-end mobile devices.},
keywords={biometrics (access control);energy conservation;learning (artificial intelligence);message authentication;mobile computing;mobile handsets;power aware computing;smart phones;implicit authentication systems;user location;user motion;user behaviors;energy-efficient solution;real-time implicit authentication;legitimate users;high-end mobile devices;energy-efficient W-layer;implicit user authentication;seamless user authentication;user action;Nexus S smartphone;power consumption;energy consumption reduction;battery usage;Authentication;Sensors;Real-time systems;Mobile handsets;Biometrics (access control);Servers;Training},
doi={10.1109/INFOCOM.2017.8057222},
ISSN={},
month={May},}
@INPROCEEDINGS{8057223,
author={R. Vaze},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Online knapsack problem and budgeted truthful bipartite matching},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Two related online problems: knapsack and truthful bipartite matching are considered. For these two problems, the common theme is how to `match' an arriving left vertex in an online fashion with any of the available right vertices, if at all, so as to maximize the sum of the value of the matched edges, subject to satisfying a sum-weight constraint on the matched left vertices. Assuming that the left vertices arrive in an uniformly random order (secretary model), two almost similar algorithms are proposed for the two problems, that are 2e competitive and 24 competitive, respectively. The proposed online bipartite matching algorithm is also shown to be truthful: there is no incentive for any left vertex to misreport its bid/weight. Direct applications of these problems include job allocation with load balancing, generalized adwords, crowdsourcing auctions, and matching wireless users to cooperative relays in device-to-device communication enabled cellular network.},
keywords={combinatorial mathematics;knapsack problems;sum-weight constraint;bipartite matching algorithm;online knapsack problem;truthful bipartite matching;left vertex;job allocation;load balancing;generalized adwords;crowdsourcing auctions;wireless users;cooperative relays;device-to-device communication enabled cellular network;Device-to-device communication;Impedance matching;Conferences;Resource management;Load management;Crowdsourcing;Wireless communication},
doi={10.1109/INFOCOM.2017.8057223},
ISSN={},
month={May},}
@INPROCEEDINGS{8057224,
author={J. Zhang and E. Modiano},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Robust routing in interdependent networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={We consider a model of two interdependent networks, where every node in one network depends on one or more supply nodes in the other network and a node fails if it loses all of its supply nodes. We develop algorithms to compute the failure probability of a path, and obtain the most reliable path between a pair of nodes in a network, under the condition that each supply node fails independently with a given probability. Our work generalizes the classical shared risk group model, by considering multiple risks associated with a node and letting a node fail if all the risks occur. Moreover, we study the diverse routing problem by considering two paths between a pair of nodes. We define two paths to be d-failure resilient if at least one path survives after removing d or fewer supply nodes, which generalizes the concept of disjoint paths in a single network, and risk-disjoint paths in a classical shared risk group model. We compute the probability that both paths fail, and develop algorithms to compute the most reliable pair of paths.},
keywords={probability;telecommunication network reliability;telecommunication network routing;interdependent networks;supply node;risk-disjoint paths;shared risk group model;failure probability;diverse routing problem;Routing;Computer network reliability;Robustness;Computational modeling;Approximation algorithms;Algorithm design and analysis},
doi={10.1109/INFOCOM.2017.8057224},
ISSN={},
month={May},}
@INPROCEEDINGS{8057225,
author={J. Yu and X. Ning and Y. Sun and S. Wang and Y. Wang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Constructing a self-stabilizing CDS with bounded diameter in wireless networks under SINR},
year={2017},
volume={},
number={},
pages={1-9},
abstract={As a virtual backbone structure, connected dominating sets (CDSs) play an important role in topology control for wireless networks. In this paper, we develop a distributed self-stabilizing CDS construction algorithm under the SINR model (also known as the physical interference model), a more practical yet more challenging interference model for distributed algorithm design. Specifically, we propose a randomized distributed algorithm that can construct a CDS in O (log n) timeslots with a high probability, where n is the total number of nodes in the network. The constructed CDS achieves constant approximation in both density and diameter. To the best of our knowledge, this is the first known asymptotically optimal self-stabilizing result in terms of both density and diameter for distributed CDS construction under the practical SINR model.},
keywords={approximation theory;distributed algorithms;probability;radiofrequency interference;set theory;telecommunication network topology;wireless sensor networks;bounded diameter;wireless networks;virtual backbone structure;topology control;distributed self-stabilizing CDS construction algorithm;physical interference model;distributed algorithm design;randomized distributed algorithm;practical SINR model;asymptotically optimal self-stabilizing result;Interference;Signal to noise ratio;Algorithm design and analysis;Approximation algorithms;Wireless networks;Wireless sensor networks},
doi={10.1109/INFOCOM.2017.8057225},
ISSN={},
month={May},}
@INPROCEEDINGS{8057226,
author={W. Li and J. Zhang and Y. Zhao},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Conflict graph embedding for wireless network optimization},
year={2017},
volume={},
number={},
pages={1-9},
abstract={With the dense deployment of wireless infrastructure such as radio towers and WiFi access points, wireless network optimization becomes very important for improving the network capacity and enhancing the communication quality of wireless links. Most optimization algorithms rely on the conflict graph to describe the interference situation. However, building a conflict graph requires exhaustive measurements of the whole network and the existing estimation approaches are static and inaccurate. In this paper, we propose a conflict graph embedding approach to assess network interference situations by representing the wireless nodes with low-dimensional vectors while preserving their conflict relationships. Specifically, our approach introduces a sliding-window based partial measurement method to sample the interference graph in the network, then adopts a learning algorithm to obtain the vector representation of the nodes, and then infers the interference situations by exploring the feature vectors. The proposed approach has been proved to be low measurement overhead, low computational cost, and self-adaptive, which is suitable for large-scale dynamic wireless networks. We illustrate that conflict graph embedding can be used for interference-aware wireless network optimizations. We conduct extensive experiments based on real wireless network datasets, which show the efficiency of the proposed approach.},
keywords={graph theory;optimisation;radio networks;radiofrequency interference;sliding-window based partial measurement method;large-scale dynamic wireless networks;conflict graph embedding;interference-aware wireless network optimizations;wireless network datasets;interference situation;optimization algorithms;wireless links;network capacity;wireless network optimization;WiFi access points;Interference;Wireless networks;Optimization;Extraterrestrial measurements;Time measurement},
doi={10.1109/INFOCOM.2017.8057226},
ISSN={},
month={May},}
@INPROCEEDINGS{8057227,
author={S. Lin and I. F. Akyildiz},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Dynamic base station formation for solving NLOS problem in 5G millimeter-wave communication},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Millimeter-wave communication is one of the enabling technologies to meet high data-rate requirements of 5G wireless systems. Millimeter-wave systems due large available bandwidth enable gigabit-per-second data rates for line-of-sight (LOS) transmissions in short distances. However, for non-line-of-sight (NLOS) transmissions, millimeter-wave systems suffers performance degradation because the received signal strengths at user equipments (UEs) are not satisfactory. In this paper, the NLOS problem in millimeter-wave systems is treated from SoftAir (a wireless software-defined networking architecture) perspective. In particular, a so-called dynamic base station (BS) formation is introduced, which adaptively coordinates BSs and their multiple antennas to always satisfy UEs' quality-of-service (QoS) requirements in NLOS cases. First, the architecture for software-defined millimeter-wave system is introduced, where remote radio heads (RRHs) coordination is explained and millimeter-wave channel model between RRHs and UEs is analyzed. A ubiquitous millimeter-wave coverage problem is formulated, which jointly optimizes RRH-UE associations and beamforming weights of RRHs to maximize the UE sum-rate while guaranteeing QoS and system-level constraints. After proving the np-hardness of the coverage optimization problem with non-convex constraints, an iterative algorithm is developed for dynamic BS formation that achieves ubiquitous coverage with high data rates in LOS and NLOS cases. Through successive convex approximations, the proposed dynamic BS formation algorithm transforms the original mixed-integer nonlinear programming into a mixed-integer second-order cone programming, which is efficiently solved by convex tools. Simulations validate the efficacy of our solution that completely solves NLOS problem by facilitating ubiquitous coverage in 5G millimeter-wave systems.},
keywords={5G mobile communication;approximation theory;array signal processing;concave programming;convex programming;integer programming;iterative methods;quality of service;RSSI;software defined networking;5G millimeter-wave communication;line-of-sight transmission;received signal strengths;user equipments;SoftAir;wireless software-defined networking architecture;multiple antennas;quality-of-service;software-defined millimeter-wave system;remote radio heads;system-level constraints;coverage optimization problem;nonconvex constraints;beamforming weights;5G wireless systems;mixed-integer second-order cone programming;mixed-integer nonlinear programming;convex approximations;iterative algorithm;nonline-of-sight transmissions;NLOS problem;ubiquitous millimeter-wave coverage problem;millimeter-wave channel model;NLOS cases;dynamic base station formation;Millimeter wave communication;Nonlinear optics;5G mobile communication;Array signal processing;Computer architecture;Antennas},
doi={10.1109/INFOCOM.2017.8057227},
ISSN={},
month={May},}
@INPROCEEDINGS{8057228,
author={S. Borst and A. Ö. Kaya and D. Calin and H. Viswanathan},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Dynamic path selection in 5G multi-RAT wireless networks},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Emerging 5G networks will not only offer higher link rates, but also integrate a variety of Radio Access Technologies (RATs) in order to provide ultra-reliable broadband access to a wide range of applications with high throughput and low latency requirements. SDN-enabled dynamic path selection is of critical importance in exploiting the collective transmission resources in such heterogeneous multi-RAT environments and delivering excellent user performance. In the present paper we propose the `best-rate' path selection algorithm for multi-RAT networks with various types of traffic flows. The best-rate algorithm accounts for the radio conditions and performance requirements of individual flows as well as the load conditions at the various access points. We analytically establish that the rates received by the various flows under the best-rate path selection, in conjunction with local fair resource sharing at the individual access points, are close to globally Proportional Fair. Detailed simulation experiments demonstrate that the best-rate algorithm achieves significant gains in terms of user-perceived throughput performance over various baseline policies.},
keywords={5G mobile communication;radio access networks;resource allocation;software defined networking;traffic flows;best-rate algorithm accounts;radio conditions;individual access points;Radio Access Technologies;ultra-reliable broadband access;collective transmission resources;heterogeneous multiRAT environments;best-rate path selection algorithm;5G multi-RAT wireless networks;local fair resource sharing;SDN-enabled dynamic path selection;Resource management;Throughput;Heuristic algorithms;Streaming media;Sociology;Statistics;Load management},
doi={10.1109/INFOCOM.2017.8057228},
ISSN={},
month={May},}
@INPROCEEDINGS{8057229,
author={J. Kuo and S. Shen and H. Kang and D. Yang and M. Tsai and W. Chen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Service chain embedding with maximum flow in software defined network and application to the next-generation cellular network architecture},
year={2017},
volume={},
number={},
pages={1-9},
abstract={With software-defined network (SDN) and network function virtualization (NFV) techniques, we can embed the service chain consisting of a sequence of virtualized network functions (VNFs), i.e., we can determine the flow path and deploy the VNFs contained in the service chain at any place on the path. In the literature, the methods of service chain embedding bound the number of VNFs at a node, whereas the link capacities are disregarded and the amount of flows is not considered, which could cause serious congestion. In addition, according to our experiment, the process overhead on a computation node is linear to the total amount of flows processed. In this paper, we propose a method of service chain embedding to maximize the total amount of flows while bounding the process overhead of the flows on a node by its computation capability and the total amount of flows on an link by its bandwidth capacity. To our knowledge, our method is the first approximation algorithm of service chain embedding with considering flow in the literature. Simulations show our algorithm has good performance in terms of the total amount of flows.},
keywords={cellular radio;embedded systems;mobile computing;software defined networking;virtualisation;software defined network;next-generation cellular network architecture;virtualized network functions;flow path;VNF;SDN;service chain embedding;Routing;Encapsulation;Approximation algorithms;Bandwidth;Servers;Software;Network function virtualization},
doi={10.1109/INFOCOM.2017.8057229},
ISSN={},
month={May},}
@INPROCEEDINGS{8057230,
author={V. Sciancalepore and K. Samdanis and X. Costa-Perez and D. Bega and M. Gramaglia and A. Banchs},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Mobile traffic forecasting for maximizing 5G network slicing resource utilization},
year={2017},
volume={},
number={},
pages={1-9},
abstract={The emerging network slicing paradigm for 5G provides new business opportunities by enabling multi-tenancy support. At the same time, new technical challenges are introduced, as novel resource allocation algorithms are required to accommodate different business models. In particular, infrastructure providers need to implement radically new admission control policies to decide on network slices requests depending on their Service Level Agreements (SLA). When implementing such admission control policies, infrastructure providers may apply forecasting techniques in order to adjust the allocated slice resources so as to optimize the network utilization while meeting network slices' SLAs. This paper focuses on the design of three key network slicing building blocks responsible for (i) traffic analysis and prediction per network slice, (ii) admission control decisions for network slice requests, and (iii) adaptive correction of the forecasted load based on measured deviations. Our results show very substantial potential gains in terms of system utilization as well as a trade-off between conservative forecasting configurations versus more aggressive ones (higher gains, SLA risk).},
keywords={5G mobile communication;contracts;quality of service;resource allocation;telecommunication congestion control;telecommunication traffic;different business models;infrastructure providers;admission control policies;allocated slice resources;network utilization;traffic analysis;admission control decisions;network slice requests;forecasted load;trade-off between conservative forecasting configurations;mobile traffic forecasting;resource utilization;emerging network slicing paradigm;business opportunities;multitenancy support;resource allocation algorithms;5G network slicing resource utilization;service level agreements;SLA;slicing building blocks;Forecasting;Admission control;5G mobile communication;3GPP;Prediction algorithms;Training;Resource management},
doi={10.1109/INFOCOM.2017.8057230},
ISSN={},
month={May},}
@INPROCEEDINGS{8057231,
author={S. Pradhan and L. Qiu and A. Parate and K. Kim},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={Understanding and managing notifications},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In today's always-connected world, we receive a large number of notifications on our mobile devices. These notifications cause interruptions, stress, and even impact users' lifestyle. To understand how users respond to notifications, we develop an application that monitors various features (e.g., importance) of the notifications, users' actions, and the level of users' engagement with the notifications. We recruit 30 users to use the application and monitor over 30 days, and subsequently find that 20% to 50% of the notifications generally get ignored by the users. In addition, we also solicit explicit feedback about the importance of notifications from 12 users over 14 days and identify the relation between perceived importance and users' engagement level. Based on this study, we identify the key characteristics of notifications and users' engagement, which is further substantiated by an onfine survey of 400+ users. In addition, we develop a notification manager that includes a machine learning based prediction model and that shows only the important notifications and delays the unimportant notifications. Our experimental results show that our notification manager automatically assesses the importance of notifications with more than 87% accuracy. We believe this work is a promising step toward intelligent personal assistant that manages notifications.},
keywords={learning (artificial intelligence);mobile computing;user interfaces;notification manager;important notifications;notification understanding;notification management;always-connected world;mobile devices;user engagement;machine learning based prediction model;Electronic mail;Monitoring;Vibrations;Delays;Conferences;Mobile communication;Operating systems},
doi={10.1109/INFOCOM.2017.8057231},
ISSN={},
month={May},}
@INPROCEEDINGS{8057232,
author={Y. Chen and X. Jin and J. Sun and R. Zhang and Y. Zhang},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={POWERFUL: Mobile app fingerprinting via power analysis},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Which apps a mobile user has and how they are used can disclose significant private information about the user. In this paper, we present the design and evaluation of POWERFUL, a new attack which can fingerprint sensitive mobile apps (or infer sensitive app usage) by analyzing the power consumption profiles on Android devices. POWERFUL works on the observation that distinct apps and their different usage patterns all lead to distinguishable power consumption profiles. Since the power profiles on Android devices require no permission to access, POWERFUL is very difficult to detect and can pose a serious threat against user privacy. Extensive experiments involving popular and sensitive apps in Google Play Store show that POWERFUL can identify the app used at any particular time with accuracy up to 92.9%, demonstrating the feasibility of POWERFUL.},
keywords={data privacy;mobile computing;power aware computing;security of data;smart phones;power analysis;mobile user;POWERFUL;sensitive app usage;Android devices;power profiles;sensitive mobile app fingerprinting;power consumption profiles;usage patterns;Google Play Store;user privacy;Androids;Humanoid robots;Mobile communication;Feature extraction;Power demand;Google;Mobile handsets},
doi={10.1109/INFOCOM.2017.8057232},
ISSN={},
month={May},}
@INPROCEEDINGS{8057233,
author={Z. Li and M. Li and P. Mohapatra and J. Han and S. Chen},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={iType: Using eye gaze to enhance typing privacy},
year={2017},
volume={},
number={},
pages={1-9},
abstract={This paper presents iType, a system that uses eye gaze for typing private information on commodity mobile platforms. The design combats three primary challenges: 1) relatively low accuracy of mobile gaze tracking; 2) difficulties in correcting input errors due to lacking the comparison with the true text-entry value; and 3) device motions and other noises that may interfere gaze tracking accuracy and thus the iType performance. We devise a set of effective techniques, including leveraging a collective behavior of the gaze tracking results, unique correlation of the typing error spatial distributions, and motion sensor hints from mobile devices, to address above challenges. A set of enhancement techniques are applied to further improve iType's robustness and reliability. We consolidate above designs and implement iType on iOS platform. Evaluations show that iType achieves high keystroke detection accuracy for the secure typing within a reasonable short latency.},
keywords={data privacy;eye;gaze tracking;mobile computing;iOS platform;high keystroke detection accuracy;secure typing;eye gaze;typing privacy;private information;commodity mobile platforms;mobile gaze tracking;input errors;text-entry value;gaze tracking accuracy;iType performance;leveraging a collective behavior;typing error spatial distributions;motion sensor hints;mobile devices;enhancement techniques;reliability;Gaze tracking;Mobile handsets;Cameras;Keyboards;Engines;Mobile communication;Privacy},
doi={10.1109/INFOCOM.2017.8057233},
ISSN={},
month={May},}
@INPROCEEDINGS{8057234,
author={P. Y. Cao and G. Li and A. C. Champion and D. Xuan and S. Romig and W. Zhao},
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
title={On human mobility predictability via WLAN logs},
year={2017},
volume={},
number={},
pages={1-9},
abstract={In this research, we conduct a comprehensive measurement study on the predictability of human mobility with respect to demographic differences. We leverage an extensive WLAN dataset collected on a large university campus. Specifically, our dataset includes over 41 million WLAN entries gathered from over 5,000 students (with demographic information) during a four-month period in 2015. We observed surprising patterns on large increases of long-term mobility entropy by age, and the impact of academic majors on students long-term mobility entropy. The distribution of long-term entropy follows a bimodal distribution, which is different from previous studies. We also find that the predictability of students' short-term (daily or weekly) mobility varies on different days of the week and with student gender. Because of the large campus size, our results can mimic people's mobility patterns in metropolitan areas. We also anticipate that our results will provide insight that guides academic administrators' decisions regarding facilities planning, emergency management, etc. on campus.},
keywords={demography;educational institutions;entropy;Internet;mobile computing;social sciences computing;wireless LAN;human mobility predictability;WLAN logs;comprehensive measurement study;demographic differences;university campus;demographic information;academic majors;WLAN dataset;student long-term mobility entropy;metropolitan area;campus size;student gender;bimodal distribution;Entropy;Wireless LAN;Mobile handsets;Buildings;Mobile communication;Global Positioning System;Conferences},
doi={10.1109/INFOCOM.2017.8057234},
ISSN={},
month={May},}

