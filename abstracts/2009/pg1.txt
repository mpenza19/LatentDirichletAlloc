@INPROCEEDINGS{5061900,
author={V. Konda and J. Kaur},
booktitle={IEEE INFOCOM 2009},
title={RAPID: Shrinking the Congestion-Control Timescale},
year={2009},
volume={},
number={},
pages={1-9},
abstract={TCP congestion-control is fairly inefficient in achieving high throughput in high-speed and dynamic-bandwidth environments. The main culprit is the slow bandwidth-search process used by TCP, which may take up to several thousands of round-trip times (RTTs) in searching for and acquiring the end-to-end spare bandwidth. Even the recently-proposed "highspeed" transport protocols may take hundreds of RTTs for this. In this paper, we design a new approach for congestion-control that allows TCP connections to boldly search for, and adapt to, the available bandwidth within a single RTT. Our approach relies on carefully orchestrated packet sending times and estimates the available bandwidth based on the delays experienced by these. We instantiate our new protocol, referred to as RAPID, using mechanisms that promote efficiency, queue-friendliness, and fairness. Our experimental evaluations on gigabit networks indicate that RAPID: (i) converges to an updated value of bandwidth within 1-4 RTTs; (ii) helps maintain fairly small queues; (iii) has negligible impact on regular TCP traffic; and (iv) exhibits excellent intra-protocol fairness among co-existing RAPID transfers. The rate-based design allows RAPID to be truly RTT-fair.},
keywords={queueing theory;telecommunication congestion control;telecommunication traffic;transport protocols;TCP congestion-control;round-trip time;RTT;gigabit network;RAPID scheme;TCP traffic;dynamic-bandwidth environment;bandwidth estimation;queue-friendliness;Bandwidth;Delay estimation;Transport protocols;Probes;Feedback;Telecommunication traffic;Traffic control;Switches;Communications Society;Throughput},
doi={10.1109/INFCOM.2009.5061900},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061901,
author={I. A. Qazi and L. L. H. Andrew and T. Znati},
booktitle={IEEE INFOCOM 2009},
title={Congestion Control using Efficient Explicit Feedback},
year={2009},
volume={},
number={},
pages={10-18},
abstract={This paper proposes a framework for congestion control, called binary marking congestion control (BMCC) for high bandwidth-delay product networks. The basic components of BMCC are i) a packet marking scheme for obtaining high resolution congestion estimates using the existing bits available in the IP header for explicit congestion notification (ECN) and ii) a set of load-dependent control laws that use these congestion estimates to achieve efficient and fair bandwidth allocations on high bandwidth-delay product networks, while maintaining a low persistent queue length and negligible packet loss rate. We present analytical models that predict and provide insights into the convergence properties of the protocol. Using extensive packet-level simulations, we assess the efficacy of BMCC and perform comparisons with several proposed schemes. BMCC outperforms VCP, MLCP, XCP, SACK+RED/ECN and in some cases RCP, in terms of average flow completion times for typical Internet flow sizes.},
keywords={bandwidth allocation;convergence;feedback;Internet;queueing theory;telecommunication congestion control;transport protocols;binary marking congestion control;high bandwidth-delay product network;packet marking scheme;IP header;explicit congestion notification;load-dependent control law;bandwidth allocation;queue length;packet loss rate;convergence;protocol;packet-level simulation;Internet;explicit feedback;Feedback;Convergence;Protocols;Internet;Channel allocation;Signal resolution;Communications Society;Communication system control;Computer science;USA Councils},
doi={10.1109/INFCOM.2009.5061901},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061902,
author={F. Baccelli and G. Carofiglio and M. Piancino},
booktitle={IEEE INFOCOM 2009},
title={Stochastic Analysis of Scalable TCP},
year={2009},
volume={},
number={},
pages={19-27},
abstract={The unsatisfactory performance of TCP in high speed wide area networks has led to several versions of TCP- like H-TCP, Fast TCP, Scalable TCP, Compound or CUBIC, all aimed at speeding up the window update algorithm. In this paper we focus on Scalable TCP (STCP), a TCP version which belongs to the class of Multiplicative Increase Multiplicative Decrease (MIMD) congestion protocols. We present a new stochastic model for the evolution of the instantaneous throughput of a single STCP flow in the Congestion Avoidance phase, under the assumption of a constant per-packet loss probability. This model allows one to derive several closed-form expressions for the key stationary distributions associated with this protocol: we characterize the throughput obtained by the flow, the time separating Multiplicative Decrease events, the number of bits transmitted over certain time intervals and the size of rate decrease. Several applications leveraging these closed form expressions are considered with a particular emphasis on QoS guarantees in the context of dimensioning. A set of ns2 simulations highlights the model accuracy.},
keywords={partial differential equations;probability;quality of service;stochastic processes;telecommunication congestion control;transport protocols;stochastic analysis;scalable TCP flow;multiplicative increase multiplicative decrease congestion protocol;congestion avoidance phase;constant per-packet loss probability;QoS;partial differential equation;Stochastic processes;Throughput;Algorithm design and analysis;Protocols;Performance analysis;Closed-form solution;Event detection;Communications Society;Wide area networks;Differential equations},
doi={10.1109/INFCOM.2009.5061902},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061903,
author={T. Bonald and M. Feuillet and A. Proutiere},
booktitle={IEEE INFOCOM 2009},
title={Is the ''Law of the Jungle'' Sustainable for the Internet?},
year={2009},
volume={},
number={},
pages={28-36},
abstract={In this paper we seek to characterize the behavior of the Internet in the absence of congestion control. More specifically, we assume all sources transmit at their maximum rate and recover from packet loss by the use of some ideal erasure coding scheme. We estimate the efficiency of resource utilization in terms of the maximum load the network can sustain, accounting for the random nature of traffic. Contrary to common belief, there is generally no congestion collapse. Efficiency remains higher than 90% for most network topologies as long as maximum source rates are less than link capacity by one or two orders of magnitude. Moreover, a simple fair drop policy enforcing fair sharing at flow level is sufficient to guarantee 100% efficiency in all cases.},
keywords={Internet;resource allocation;Internet;congestion control;erasure coding scheme;resource utilization;network topologies;fair sharing;fair drop policy;Internet;Stability;Bandwidth;Fluid flow control;Telecommunication traffic;Streaming media;Communications Society;Telecommunication congestion control;Telecommunication control;Propagation losses},
doi={10.1109/INFCOM.2009.5061903},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061904,
author={R. Laufer and H. Dubois-Ferriere and L. Kleinrock},
booktitle={IEEE INFOCOM 2009},
title={Multirate Anypath Routing in Wireless Mesh Networks},
year={2009},
volume={},
number={},
pages={37-45},
abstract={In this paper, we present a new routing paradigm that generalizes opportunistic routing in wireless mesh networks. In multirate anypath routing, each node uses both a set of next hops and a selected transmission rate to reach a destination. Using this rate, a packet is broadcast to the nodes in the set and one of them forwards the packet on to the destination. To date, there is no theory capable of jointly optimizing both the set of next hops and the transmission rate used by each node. We bridge this gap by introducing a polynomial-time algorithm to this problem and provide the proof of its optimality. The proposed algorithm runs in the same running time as regular shortest-path algorithms and is therefore suitable for deployment in link-state routing protocols. We conducted experiments in a 802.11b testbed network, and our results show that multirate anypath routing performs on average 80% and up to 6.4 times better than anypath routing with a fixed rate of 11 Mbps. If the rate is fixed at 1 Mbps instead, performance improves by up to one order of magnitude.},
keywords={computational complexity;radio networks;routing protocols;telecommunication network topology;multirate anypath routing;wireless mesh network;transmission rate;polynomial-time algorithm;shortest-path algorithm;link-state routing protocol;Wireless mesh networks;Peer to peer computing;Polynomials;Testing;Bit rate;Computer science;Broadcasting;Routing protocols;Costs;Performance gain},
doi={10.1109/INFCOM.2009.5061904},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061905,
author={H. Li and Y. Cheng and C. Zhou and W. Zhuang},
booktitle={IEEE INFOCOM 2009},
title={Minimizing End-to-End Delay: A Novel Routing Metric for Multi-Radio Wireless Mesh Networks},
year={2009},
volume={},
number={},
pages={46-54},
abstract={This paper studies how to select a path with the minimum cost in terms of expected end-to-end delay (EED) in a multi-radio wireless mesh network. Different from the previous efforts, the new EED metric takes the queuing delay into account, since the end-to-end delay consists of not only the transmission delay over the wireless links but also the queuing delay in the buffer. In addition to minimizing the end-to-end delay, the EED metric implies the concept of load balancing. We develop EED- based routing protocols for both single-channel and multi-channel wireless mesh networks. In particular for the multi-radio multichannel case, we develop a generic iterative approach to calculate a multi-radio achievable bandwidth (MRAB) for a path, taking the impacts of inter/intra-flow interference and space/channel diversity into account. The MRAB is then integrated with EED to form the metric of weighted end-to-end delay (WEED). As a byproduct of MRAB, a channel diversity coefficient can be defined to quantitatively represent the channel diversity along a given path. Both numerical analysis and simulation studies are presented to validate the performance of the routing protocol based on the EED/WEED metric, with comparison to some well- known routing metrics.},
keywords={iterative methods;queueing theory;radiofrequency interference;resource allocation;routing protocols;telecommunication network topology;wireless channels;multiradio wireless mesh network;EED/WEED routing metric;weighted end-to-end delay;queuing delay;load balancing;multichannel wireless mesh network;routing protocol;generic iterative approach;multiradio achievable bandwidth;inter/intra-flow interference;space/channel diversity;MRAB network;Wireless mesh networks;Delay;Routing protocols;Costs;Load management;Iterative methods;Bandwidth;Interference;Numerical analysis;Analytical models},
doi={10.1109/INFCOM.2009.5061905},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061906,
author={S. Heimlicher and M. Karaliopoulos and H. Levy and T. Spyropoulos},
booktitle={IEEE INFOCOM 2009},
title={On Leveraging Partial Paths in Partially-Connected Networks},
year={2009},
volume={},
number={},
pages={55-63},
abstract={Mobile wireless network research focuses on scenarios at the extremes of the network connectivity continuum where the probability of all nodes being connected is either close to unity, assuming connected paths between all nodes (mobile ad hoc networks), or it is close to zero, assuming no multi-hop paths exist at all (delay-tolerant networks). In this paper, we argue that a sizable fraction of networks lies between these extremes and is characterized by the existence of partial paths, i.e., multi-hop path segments that allow forwarding data closer to the destination even when no end-to-end path is available. A fundamental issue in such networks is dealing with disruptions of end-to-end paths. Under a stochastic model, we compare the performance of the established end-to-end retransmission (ignoring partial paths), against a forwarding mechanism that leverages partial paths to forward data closer to the destination even during disruption periods. Perhaps surprisingly, the alternative mechanism is not necessarily superior. However, under a stochastic monotonicity condition between current vs. future path length, which we demonstrate to hold in typical network models, we manage to prove superiority of the alternative mechanism in stochastic dominance terms. We believe that this study could serve as a foundation to design more efficient data transfer protocols for partially-connected networks, which could potentially help reducing the gap between applications that can be supported over disconnected networks and those requiring full connectivity.},
keywords={mobile radio;probability;protocols;stochastic processes;partially-connected network;mobile wireless network;network connectivity continuum;probability;partial path;multihop path segment;stochastic model;data transfer protocol;Mobile ad hoc networks;Disruption tolerant networking;Stochastic processes;Peer to peer computing;Spread spectrum communication;Wireless application protocol;Communications Society;Computer science;Computer networks;Laboratories},
doi={10.1109/INFCOM.2009.5061906},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061907,
author={Y. Li and Y. Yang and X. Lu},
booktitle={IEEE INFOCOM 2009},
title={Routing Metric Designs for Greedy, Face and Combined-Greedy-Face Routing},
year={2009},
volume={},
number={},
pages={64-72},
abstract={Different geographic routing protocols have different requirements on routing metric designs to ensure proper operation. Combining a wrong type of routing metric with a geographic routing protocol may produce unexpected results, such as geographic routing loops and unreachable nodes. In this paper, we propose a novel routing algebra system to investigate the compatibilities between routing metrics and three geographic routing protocols including greedy, face and combined-greedy- face routing. Four important algebraic properties, respectively named odd symmetry, transitivity, source independence and local minimum freeness, are defined in this algebra system. Based on these algebraic properties, the necessary and sufficient conditions for loop-free and delivery guaranteed routing are derived when greedy, face and combined-greedy-face routing serve as packet forwarding schemes or as path discovery algorithms respectively. Our work provides essential criterions for evaluating and designing geographic routing protocols.},
keywords={greedy algorithms;routing protocols;unreachable nodes;geographic routing protocols;combined-Greedy-face routing;routing metric designs;Routing protocols;Algebra;Wireless networks;Design engineering;Global Positioning System;Switches;Communications Society;Computer science;USA Councils;Peer to peer computing},
doi={10.1109/INFCOM.2009.5061907},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061908,
author={D. Wu and Y. Liu and K. Ross},
booktitle={IEEE INFOCOM 2009},
title={Queuing Network Models for Multi-Channel P2P Live Streaming Systems},
year={2009},
volume={},
number={},
pages={73-81},
abstract={In recent years there have been several large-scale deployments of P2P live video systems. Existing and future P2P live video systems will offer a large number of channels, with users switching frequently among the channels. In this paper, we develop infinite-server queueing network models to analytically study the performance of multi-channel P2P streaming systems. Our models capture essential aspects of multi-channel video systems, including peer channel switching, peer churn, peer bandwidth heterogeneity, and Zipf-like channel popularity. We apply the queueing network models to two P2P streaming designs: the isolated channel design (ISO) and the View-Upload Decoupling (VUD) design. For both of these designs, we develop efficient algorithms to calculate critical performance measures, develop an asymptotic theory to provide closed-form results when the number of peers approaches infinity, and derive near- optimal provisioning rules for assigning peers to groups in VUD. We use the analytical results to compare VUD with ISO. We show that VUD design generally performs significantly better, particularly for systems with heterogeneous channel popularities and streaming rates.},
keywords={peer-to-peer computing;queueing theory;video streaming;multichannel P2P live streaming systems;queuing network models;multichannel video systems;isolated channel design;view-upload decoupling design;peer-to-peer computing;Streaming media;ISO;Switches;Computer networks;Large-scale systems;Bandwidth;Delay;Communications Society;Information science;USA Councils},
doi={10.1109/INFCOM.2009.5061908},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061909,
author={Z. Liu and C. Wu and B. Li and S. Zhao},
booktitle={IEEE INFOCOM 2009},
title={Distilling Superior Peers in Large-Scale P2P Streaming Systems},
year={2009},
volume={},
number={},
pages={82-90},
abstract={In large-scale peer-to-peer (P2P) live streaming systems with a limited supply of server bandwidth, increasing the amount of upload bandwidth supplied by peers becomes critically important to the "well being" of streaming sessions in live channels. Intuitively, two types of peers are preferred to be kept up in a live session: peers that contribute a higher percentage of their upload capacities, and peers that are stable for a long period of time. The fundamental challenge is to identify, and satisfy the needs of, these types of "superior" peers in a live session, and to achieve this goal with minimum disruption to the traditional pull-based protocols that real-world live streaming protocols use. In this paper, we conduct a comprehensive and in-depth statistical analysis based on more than 130 GB worth of runtime traces from hundreds of streaming channels in a large- scale real-world live streaming system, UUSee (among the top three commercial systems in popularity in mainland China). Our objective is to discover critical factors that may influence the longevity and bandwidth contribution ratio of peers, using survival analysis techniques such as the Cox proportional hazards model and the Mantel-Haenszel test. Once these influential factors are found, they can be used to form a superiority index to distill superior peers from the general peer population. The index can be used in any way to favor superior peers, and we simulate the use of a simple ranking mechanism in a natural selection algorithm to show the effectiveness of the index, based on a replay of real-world traces from UUSee.},
keywords={computer network reliability;large-scale systems;media streaming;peer-to-peer computing;statistical analysis;telecommunication channels;large-scale P2P live streaming systems;upload bandwidth;live channels;pull-based protocols;real-world live streaming protocols;statistical analysis;streaming channels;UUSee;China;bandwidth contribution ratio;survival analysis techniques;Cox proportional hazards model;Mantel-Haenszel test;superiority index;natural selection algorithm;Large-scale systems;Bandwidth;Protocols;Streaming media;Runtime;Statistical analysis;Hazards;Testing;Multimedia systems;Stability},
doi={10.1109/INFCOM.2009.5061909},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061910,
author={V. Gopalakrishnan and B. Bhattacharjee and K. K. Ramakrishnan and R. Jana and D. Srivastava},
booktitle={IEEE INFOCOM 2009},
title={CPM: Adaptive Video-on-Demand with Cooperative Peer Assists and Multicast},
year={2009},
volume={},
number={},
pages={91-99},
abstract={We present CPM, a unified approach that exploits server multicast, assisted by peer downloads, to provide efficient video-on-demand (VoD) in a service provider environment. We describe our architecture and show how CPM is designed to dynamically adapt to a wide range of situations including highly different peer-upload bandwidths, content popularity, user request arrival patterns, video library size, and subscriber population. We demonstrate the effectiveness of CPM using simulations (based on an actual implementation codebase) across the range of situations described above and show that CPM does significantly better than traditional unicast, different forms of multicast, as well as peer-to-peer schemes. Along with synthetic parameters, we augment our experiments using data from a deployed VoD service to evaluate the performance of CPM.},
keywords={multicast communication;peer-to-peer computing;video on demand;video-on-demand;cooperative peer assists;multicast;CPM;peer-to-peer schemes;synthetic parameters;Unicast;Peer to peer computing;Network servers;Bandwidth;Delay;Libraries;Web and internet services;Streaming media;Costs;Network topology},
doi={10.1109/INFCOM.2009.5061910},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061911,
author={E. Alessandria and M. Gallo and E. Leonardi and M. Mellia and M. Meo},
booktitle={IEEE INFOCOM 2009},
title={P2P-TV Systems under Adverse Network Conditions: A Measurement Study},
year={2009},
volume={},
number={},
pages={100-108},
abstract={In this paper we define a simple experimental setup to analyze the behavior of commercial P2P-TV applications under adverse network conditions. Our goal is to reveal the ability of different P2P-TV applications to adapt to dynamically changing conditions, such as delay, loss and available capacity, e.g., checking whether such systems implement some form of congestion control. We apply our methodology to four popular commercial P2P-TV applications: PPLive, SOPCast, TVants and TVUPlayer. Our results show that all the considered applications are in general capable to cope with packet losses and to react to congestion arising in the network core. Indeed, all applications keep trying to download data by avoiding bad paths and carefully selecting good peers. However, when the bottleneck affects all peers, e.g., it is at the access link, their behavior results rather aggressive, and potentially harmful for both other applications and the network.},
keywords={digital television;peer-to-peer computing;ubiquitous computing;video streaming;P2P-TV system;adverse network condition;commercial P2P-TV application;congestion control;PPLive;SOPCast;TVants;TVUPlayer;Streaming media;Internet;Costs;Bandwidth;Delay;Peer to peer computing;Network servers;Web server;Telecommunication traffic;Testing},
doi={10.1109/INFCOM.2009.5061911},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061912,
author={M. -. Zhao and J. Lei and M. -. Wu and Y. Liu and W. Shu},
booktitle={IEEE INFOCOM 2009},
title={Surface Coverage in Wireless Sensor Networks},
year={2009},
volume={},
number={},
pages={109-117},
abstract={Coverage is a fundamental problem in Wireless Sensor Networks (WSNs). Existing studies on this topic focus on 2D ideal plane coverage and 3D full space coverage. The 3D surface of a targeted Field of Interest is complex in many real world applications; and yet, existing studies on coverage do not produce practical results. In this paper, we propose a new coverage model called surface coverage. In surface coverage, the targeted Field of Interest is a complex surface in 3D space and sensors can be deployed only on the surface. We show that existing 2D plane coverage is merely a special case of surface coverage. Simulations point out that existing sensor deployment schemes for a 2D plane cannot be directly applied to surface coverage cases. In this paper, we target two problems assuming cases of surface coverage to be true. One, under stochastic deployment, how many sensors are needed to reach a certain expected coverage ratio? Two, if sensor deployment can be planned, what is the optimal deployment strategy with guaranteed full coverage with the least number of sensors? We show that the latter problem is NP-complete and propose three approximation algorithms. We further prove that these algorithms have a provable approximation ratio. We also conduct comprehensive simulations to evaluate the performance of the proposed algorithms.},
keywords={computational complexity;optimisation;wireless sensor networks;surface coverage;wireless sensor networks;NP-complete;stochastic deployment;optimal deployment strategy;approximation algorithms;Wireless sensor networks;Volcanoes;Stochastic processes;Approximation algorithms;Space technology;Monitoring;Communications Society;USA Councils;Robustness;Base stations},
doi={10.1109/INFCOM.2009.5061912},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061913,
author={J. Luo and D. Wang and Q. Zhang},
booktitle={IEEE INFOCOM 2009},
title={Double Mobility: Coverage of the Sea Surface with Mobile Sensor Networks},
year={2009},
volume={},
number={},
pages={118-126},
abstract={We are interested in the sensor networks for scientific applications to cover and measure statistics on the sea surface. Due to flows and waves, the sensor nodes may gradually lose their positions; leaving the points of interest uncovered. Manual readjustment is costly and cannot be performed in time. We argue that a network of mobile sensor nodes which can perform self-adjustment is the best candidate to maintain the coverage of the surface area. In our application, we face a unique double mobility coverage problem. That is, there is an uncontrollable mobility, U-Mobility, by the flows which breaks the coverage of the sensor network. Moreover, there is also a controllable mobility, C-Mobility, by the mobile nodes which we can utilize to reinstall the coverage. Our objective is to build an energy efficient scheme for the sensor network coverage issue with this double mobility behavior. A key observation of our scheme is that the motion of the flow is not only a curse but should also be considered as a fortune. The sensor nodes can be pushed by free to some locations that potentially help to improve the overall coverage. With that taken into consideration, more efficient movement decision can be made. To this end, we present a dominating set maintenance scheme to maximally exploit the U-Mobility and balance the energy consumption among all the sensor nodes. We prove that the coverage is guaranteed in our scheme. We further propose a fully distributed protocol that addresses a set of practical issues. Through extensive simulation, we demonstrate that the network lifetime can be significantly extended, compared to a straight forward back-to-original reposition scheme.},
keywords={mobility management (mobile radio);wireless sensor networks;sea surface coverage;mobile sensor networks;double mobility coverage problem;U-Mobility;controllable mobility;uncontrollable mobility;C-Mobility;dominating set maintenance scheme;energy consumption;fully distributed protocol;Sea surface;Monitoring;Ocean temperature;Peer to peer computing;Mechanical sensors;Sea measurements;Energy consumption;Batteries;Sensor systems;Communications Society},
doi={10.1109/INFCOM.2009.5061913},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061914,
author={A. Saipulla and C. Westphal and B. Liu and J. Wang},
booktitle={IEEE INFOCOM 2009},
title={Barrier Coverage of Line-Based Deployed Wireless Sensor Networks},
year={2009},
volume={},
number={},
pages={127-135},
abstract={Barrier coverage of wireless sensor networks has been studied intensively in recent years under the assumption that sensors are deployed uniformly at random in a large area (Poisson point process model). However, when sensors are deployed along a line (e.g., sensors are dropped from an aircraft along a given path), they would be distributed along the line with random offsets due to wind and other environmental factors. It is important to study the barrier coverage of such line- based deployment strategy as it represents a more realistic sensor placement model than the Poisson point process model. This paper presents the first set of results in this direction. In particular, we establish a tight lower-bound for the existence of barrier coverage under line-based deployments. Our results show that the barrier coverage of the line-based deployments significantly outperforms that of the Poisson model when the random offsets are relatively small compared to the sensor's sensing range. We then study sensor deployments along multiple lines and show how barrier coverage is affected by the distance between adjacent lines and the random offsets of sensors. These results demonstrate that sensor deployment strategies have direct impact on the barrier coverage of wireless sensor networks. Different deployment strategies may result in significantly different barrier coverage. Therefore, in the planning and deployment of wireless sensor networks, the coverage goal and possible sensor deployment strategies must be carefully and jointly considered. The results obtained in this paper will provide important guidelines to the deployment and performance of wireless sensor networks for barrier coverage.},
keywords={random processes;stochastic processes;telecommunication network planning;wireless sensor networks;barrier coverage;line-based deployed wireless sensor network planning;Poisson point process model;random offset;environmental factor;Wireless sensor networks;Aircraft;Sensor phenomena and characterization;Communications Society;Environmental factors;Strategic planning;Guidelines;Mechanical sensors;Computer science;USA Councils},
doi={10.1109/INFCOM.2009.5061914},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061915,
author={P. Balister and Z. Zheng and S. Kumar and P. Sinha},
booktitle={IEEE INFOCOM 2009},
title={Trap Coverage: Allowing Coverage Holes of Bounded Diameter in Wireless Sensor Networks},
year={2009},
volume={},
number={},
pages={136-144},
abstract={Tracking of movements such as that of people, animals, vehicles, or of phenomena such as fire, can be achieved by deploying a wireless sensor network. So far only prototype systems have been deployed and hence the issue of scale has not become critical. Real-life deployments, however, will be at large scale and achieving this scale will become prohibitively expensive if we require every point in the region to be covered (i.e., full coverage), as has been the case in prototype deployments. In this paper we therefore propose a new model of coverage, called trap coverage, that scales well with large deployment regions. A sensor network providing trap coverage guarantees that any moving object or phenomena can move at most a (known) displacement before it is guaranteed to be detected by the network, for any trajectory and speed. Applications aside, trap coverage generalizes the de-facto model of full coverage by allowing holes of a given maximum diameter. From a probabilistic analysis perspective, the trap coverage model explains the continuum between percolation (when coverage holes become finite) and full coverage (when coverage holes cease to exist). We take first steps toward establishing a strong foundation for this new model of coverage. We derive reliable, explicit estimates for the density needed to achieve trap coverage with a given diameter when sensors are deployed randomly. Our density estimates are more accurate than those obtained using asymptotic critical conditions. We show by simulation that our analytical predictions of density are quite accurate even for small networks. We then propose polynomial-time algorithms to determine the level of trap coverage achieved once sensors are deployed on the ground. Finally, we point out several new research problems that arise by the introduction of the trap coverage model.},
keywords={probability;wireless sensor networks;coverage hole;wireless sensor network;trap coverage model;probabilistic analysis;polynomial-time algorithm;Wireless sensor networks;Prototypes;Tracking;Animals;Vehicles;Fires;Large-scale systems;Sensor phenomena and characterization;Object detection;Analytical models},
doi={10.1109/INFCOM.2009.5061915},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061916,
author={N. Buchbinder and L. Lewin-Eytan and I. Menache and J. Naor and A. Orda},
booktitle={IEEE INFOCOM 2009},
title={Dynamic Power Allocation Under Arbitrary Varying Channels - An Online Approach},
year={2009},
volume={},
number={},
pages={145-153},
abstract={A major problem in wireless networks is coping with limited resources, such as bandwidth and energy. These issues become a major algorithmic challenge in view of the dynamic nature of the wireless domain. We consider in this paper the single-transmitter power assignment problem under time-varying channels, with the objective of maximizing the data throughput. It is assumed that the transmitter has a limited power budget, to be sequentially divided during the lifetime of the battery. We deviate from the classic work in this area, which leads to explicit "water-filling" solutions, by considering a realistic scenario where the channel state quality changes arbitrarily from one transmission to the other. The problem is accordingly tackled within the framework of competitive analysis, which allows for worst case performance guarantees in setups with arbitrarily varying channel conditions. We address both a "discrete" case, where the transmitter can transmit only at a fixed power level, and a "continuous" case, where the transmitter can choose any power level out of a bounded interval. For both cases, we propose online power-allocation algorithms with proven worst-case performance bounds. In addition, we establish lower bounds on the worst-case performance of any online algorithm, and show that our proposed algorithms are optimal.},
keywords={channel allocation;radio networks;radio transmitters;time-varying channels;wireless channels;wireless network;single-transmitter power assignment problem;time-varying channel;battery lifetime;online power-allocation algorithm;Transmitters;Batteries;Resource management;Wireless networks;Bandwidth;Throughput;Communications Society;Laboratories;Computer science;Time-varying channels},
doi={10.1109/INFCOM.2009.5061916},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061917,
author={J. Gummeson and D. Ganesan and M. D. Corner and P. Shenoy},
booktitle={IEEE INFOCOM 2009},
title={An Adaptive Link Layer for Range Diversity in Multi-Radio Mobile Sensor Networks},
year={2009},
volume={},
number={},
pages={154-162},
abstract={An important challenge in mobile sensor networks is to enable energy-efficient communication over a diversity of distances while being robust to wireless effects caused by node mobility. In this paper, we argue that the pairing of two complementary radios with heterogeneous range characteristics enables greater range diversity at lower energy cost than a single radio. We make three contributions towards the design of such multi-radio mobile sensor systems. First, we present the design of a novel reinforcement learning-based link layer algorithm that continually learns channel characteristics and dynamically decides when to switch between radios. Second, we describe a simple protocol that translates the benefits of the adaptive link layer into practice in an energy-efficient manner. Third, we present the design of Arthropod, a mote-class sensor platform that combines two such heterogeneous radios (XE1205 and CC2420) and our implementation of the Q-learning based switching protocol in TinyOS 2.0. Using experiments conducted in a variety of urban and forested environments, we show that our system achieves up to 52% energy gains over a single radio system.},
keywords={learning (artificial intelligence);mobile radio;protocols;wireless sensor networks;multi radio mobile sensor networks;adaptive link layer;range diversity;reinforcement learning-based link layer algorithm;mote-class sensor platform;Arthropod;Q-learning based switching protocol;TinyOS 2.0;Sensor phenomena and characterization;Wireless sensor networks;Energy efficiency;Switches;Protocols;Mobile communication;Robustness;Costs;Sensor systems;Algorithm design and analysis},
doi={10.1109/INFCOM.2009.5061917},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061918,
author={H. Shpungin and M. Segal},
booktitle={IEEE INFOCOM 2009},
title={Near Optimal Multicriteria Spanner Constructions in Wireless Ad-Hoc Networks},
year={2009},
volume={},
number={},
pages={163-171},
abstract={This paper studies asymmetric power assignments for which the induced communication graph is a good spanner of the Euclidean graph, induced on the wireless nodes V, while the total energy is minimized. We propose two spanner models: distance and energy. We consider a random wireless ad-hoc network with |V| = n nodes distributed uniformly and independently in a unit square. For the first model, we propose an approximation algorithm, which constructs a power assignment so that the induced network is an O ((1 + alpha)(n-m)/m log n + alpha)-spanner of G<sub>v</sub> with high probability, such that the total energy is at most betaldrm+2 times the optimum, for any alpha &gt; 1, beta ges 1 + 2/(alpha-1), and any positive integer m les n in O(mn<sup>2</sup>) time. For the second model, we develop a power assignment such that the resulting network is a 2-spanner with a total energy of at most O(log n) times the optimum in O(n<sup>4</sup> log n) time. We also analyze a power assignment developed and show it is a low cost good k-fault resistant spanner. To the best of our knowledge, these are the first provable theoretic results for low cost spanners in wireless ad-hoc networks.},
keywords={ad hoc networks;graph theory;minimisation;probability;radio networks;optimal multicriteria spanner construction;wireless ad-hoc network;asymmetric power assignment;Euclidean graph;approximation algorithm;probability;k-fault resistant spanner;Ad hoc networks;Costs;Network topology;Computer science;Peer to peer computing;Relays;Tellurium;Communications Society;Communication systems;Power engineering and energy},
doi={10.1109/INFCOM.2009.5061918},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061919,
author={L. P. Qian and Y. J. Zhang},
booktitle={IEEE INFOCOM 2009},
title={Monotonic Optimization for Non-Concave Power Control in Multiuser Multicarrier Network Systems},
year={2009},
volume={},
number={},
pages={172-180},
abstract={Maximizing system utility corresponding to different performance measures through power control has been a long standing open problem in interference-limited multiuser multicarrier wireless networks. The complicated coupling between the mutual interference of links on each subcarrier gives rise to a series of non-convex power control optimization problems, for which the global optimal solution is hard to obtain. This paper proposes a novel algorithm, MARL, to efficiently solve the non-convex power control problem in multiuser multicarrier wireless networks. The algorithm is guaranteed to converge to a global optimal solution, as long as the utility function of each link is monotonically increasing with its data rate. The MARL algorithm is designed based on three key observations of the power control problems considered in this paper: (1) the objective function is increasing in (1+SINR) (SINR: signal to interference- plus-noise ratio); (2) the feasible set of the corresponding equivalent reformulated problem is always "normal", although not necessarily convex; and (3) the two former observations imply that the power control problem can be transformed into a monotonic optimization (MO) problem, where the optimal solution always occurs at the upper boundary of the feasible (1+SINR) region. The MARL algorithm finds the desired optimal power control solution by constructing a series of polyblocks that approximate the feasible (1+SINR) region with an increasing precision. Furthermore, by tuning the error tolerance in MARL, we could engineer a desirable tradeoff between optimality and convergence time. MARL provides an important benchmark for performance evaluation of other heuristic algorithms targeting the same problem. With the help of MARL, we evaluate the performance of a state-of-the-art algorithm through extensive simulations.},
keywords={optimal control;optimisation;power control;radio networks;telecommunication control;monotonic optimization;nonconcave power control;multiuser multicarrier network systems;system utility;interference-limited wireless networks;nonconvex optimization problems;signal to interference- plus-noise ratio;optimal power control solution;Power control;Interference;Wireless networks;Power measurement;Mutual coupling;Signal design;Algorithm design and analysis;Signal to noise ratio;Design optimization;Power engineering and energy},
doi={10.1109/INFCOM.2009.5061919},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061920,
author={J. Zhang and Q. Zhang},
booktitle={IEEE INFOCOM 2009},
title={Cooperative Network Coding-Aware Routing for Multi-Rate Wireless Networks},
year={2009},
volume={},
number={},
pages={181-189},
abstract={Recent research has proven that network coding has great potential to improve network throughput in wireless networks. To fully exploit the performance gain brought by network coding, coding-aware routing has been studied to proactively change route of flows for creating more coding opportunities. However, in today's multi-rate wireless networks, coding may not be a wise decision as the lowest rate has to be used for coded information broadcasting, which causes significant resource waste for the high-rate links. In this paper, we propose the idea of cooperative network coding (CNC) to exploit spatial diversity for improving coding opportunity. We provide a theoretical formulation for calculating the maximal throughput of unicast traffic that can be achieved with CNC in multi-rate wireless networks. CNC-aware routing under both Alice-Bob and X-structure are discussed in this paper. The performance evaluation demonstrates that a CNC-aware route selection scheme that leverages cooperative communication to improve coding opportunity leads to higher end-to-end throughput comparing with the coding-oblivious and traditional coding-aware schemes.},
keywords={broadcasting;diversity reception;encoding;radio networks;telecommunication network routing;telecommunication traffic;multirate wireless network;cooperative network coding-aware routing;information broadcasting;network traffic;Alice-Bob structure;X-structure;spatial diversity;Routing;Wireless networks;Network coding;Throughput;Performance gain;Broadcasting;Contracts;Computer numerical control;Unicast;Telecommunication traffic},
doi={10.1109/INFCOM.2009.5061920},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061921,
author={W. -. Yeow and A. T. Hoang and C. -. Tham},
booktitle={IEEE INFOCOM 2009},
title={Minimizing Delay for Multicast-Streaming in Wireless Networks with Network Coding},
year={2009},
volume={},
number={},
pages={190-198},
abstract={Network coding is a method that promises to achieve the min-cut capacity in multicasts. However, pushing towards this gain in throughput comes with two sacrifices. Delay suffers as the decoding procedure requires buffering and is performed in batches of coded packets, and unfairness prevails in terms of delay increases between receivers with worse channel conditions and those with better channel conditions. In this paper, we focus on optimizing the delay performance in reliably multicasting a data stream to a set of one-hop receivers from the receiver perspective. We analyze the system based on queueing theory using semi-Markov chains from both the system-wide and receiver perspectives. We find that the average delay per received packet at the receivers' end can be minimized by appropriate scheduling of data packets and appropriate size of the coding buffer, which depends on the rate of incoming data stream and capacities of the receivers. To circumvent unduly computational complexities, we design a heuristic scheme which can achieve significant performance gain when compared to an existing method. Our scheme readily adapts the coding size to the dynamics of the system, and schedules data packets to be coded via some strict priority measure for optimized delay performance. We show through extensive simulations that our scheme gives low average delay at high streaming rates and narrows the performance gap between receivers with bad and good channel conditions.},
keywords={multicast communication;radio networks;telecommunication channels;delay;multicast-streaming;wireless networks;network coding;coding buffer;data packets;Wireless networks;Network coding;Delay;Processor scheduling;Throughput;Decoding;Queueing analysis;Computational complexity;Performance gain;Dynamic scheduling},
doi={10.1109/INFCOM.2009.5061921},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061922,
author={C. Zhang and Y. Fang and X. Zhu},
booktitle={IEEE INFOCOM 2009},
title={Throughput-Delay Tradeoffs in Large-Scale MANETs with Network Coding},
year={2009},
volume={},
number={},
pages={199-207},
abstract={This paper characterizes the throughput-delay tradeoffs in mobile ad hoc networks (MANETs) with network coding, and compares results in the situation where only replication and forwarding are allowed in each node. The schemes/protocols achieving those tradeoffs in an effective and decentralized way are proposed and the optimality of those tradeoffs is established. The scenarios in which network coding can provide significant improvement on network performance are identified under different node mobility patterns (fast and slow mobility). The insights on when and how information mixing is beneficial for MANETs with multiple unicast and multicast sessions are provided. As far as we know, this is the first work characterizing scaling laws of throughput and delay of MANETs with network coding.},
keywords={ad hoc networks;delays;encoding;mobile radio;protocols;throughput-delay;large-scale MANET;network coding;mobile ad hoc networks;protocols;node mobility patterns;Large-scale systems;Network coding;Throughput;Relays;Wireless networks;Delay;Mobile ad hoc networks;Peer to peer computing;Communications Society;Large scale integration},
doi={10.1109/INFCOM.2009.5061922},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061923,
author={J. Barros and R. A. Costa and D. Munaretto and J. Widmer},
booktitle={IEEE INFOCOM 2009},
title={Effective Delay Control in Online Network Coding},
year={2009},
volume={},
number={},
pages={208-216},
abstract={Motivated by streaming applications with stringent delay constraints, we consider the design of online network coding algorithms with timely delivery guarantees. Assuming that the sender is providing the same data to multiple receivers over independent packet erasure channels, we focus on the case of perfect feedback and heterogeneous erasure probabilities. Based on a general analytical framework for evaluating the decoding delay, we show that existing ARQ schemes fail to ensure that receivers with weak channels are able to recover from packet losses within reasonable time. To overcome this problem, we re-define the encoding rules in order to break the chains of linear combinations that cannot be decoded after one of the packets is lost. Our results show that sending uncoded packets at key times ensures that all the receivers are able to meet specific delay requirements with very high probability.},
keywords={automatic repeat request;delays;encoding;feedback;probability;radio receivers;delay control;online network coding;multiple receivers;independent packet erasure channels;feedback;heterogeneous erasure probability;decoding delay;ARQ scheme;probability;Delay effects;Network coding;Feedback;Decoding;Throughput;Algorithm design and analysis;Transport protocols;Performance analysis;Communications Society;Communication system control},
doi={10.1109/INFCOM.2009.5061923},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061924,
author={H. Zhu and Y. Zhu and M. Li and L. M. Ni},
booktitle={IEEE INFOCOM 2009},
title={SEER: Metropolitan-Scale Traffic Perception Based on Lossy Sensory Data},
year={2009},
volume={},
number={},
pages={217-225},
abstract={Intelligent transportation systems have become increasingly important for the public transportation in Shanghai. In response, Shanghai Grid (SG) aims to provide abundant intelligent transportation services to improve the traffic condition. A challenging service in SG is to estimate the real-time traffic condition on surface streets. In this paper, we present an innovative approach SEER to tackle this problem. In SEER, we deploy a cost-effective system of taxi traffic sensors. These taxi sensory data are found to be noisy and very lossy in both time and space. By intensively mining the spatio-temporal correlations along with the evolution of traffic condition, SEER provides wealthy knowledge to setup statistical models for inferring traffic condition when they cannot be directly calculated. As an example, we demonstrate utilizing multichannel singular spectrum analysis (MSSA) to iteratively produce estimates of traffic condition in a metropolitan scale. The optimal window width of MSSA is determined with the basic periodicity found in traffic condition. Moreover, we minimize the number of channels required by MSSA to estimate traffic condition at any location. Given a desired estimation granularity, we optimize the MSSA parameters to minimize the estimation error.},
keywords={automated highways;correlation methods;data mining;Global Positioning System;inference mechanisms;mobile radio;road traffic;spatiotemporal phenomena;statistical analysis;wireless sensor networks;metropolitan-scale traffic perception;lossy sensory data;intelligent transportation system;public transportation;Shanghai grid;surface street;taxi traffic sensor;spatio-temporal correlation mining;statistical model;traffic condition inference;GPS;mobile sensor network;Telecommunication traffic;Traffic control;Intelligent transportation systems;Sensor systems;Intelligent sensors;Global Positioning System;Road transportation;Vehicle detection;Cities and towns;Communications Society},
doi={10.1109/INFCOM.2009.5061924},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061925,
author={F. Farnoud Hassanzadeh and S. Valaee},
booktitle={IEEE INFOCOM 2009},
title={Reliable Broadcast of Safety Messages in Vehicular Ad Hoc Networks},
year={2009},
volume={},
number={},
pages={226-234},
abstract={Broadcast communications is critically important in vehicular networks. Many safety applications need safety warning messages to be broadcast to all vehicles present in an area. Design of a medium access control (MAC) protocol for vehicular networks is an interesting problem because of challenges posed by broadcast traffic, high mobility, high reliability and low delay requirements of these networks. In this article, we propose a topology-transparent broadcast protocol and present a detailed mathematical analysis for obtaining the probability of success and the average delay. We show, by analysis and simulations, that the proposed protocol outperforms two existing protocols for vehicular networks with topology-transparent properties and provides reliable broadcast communications for delivering safety messages under load conditions deemed to be common in vehicular environments.},
keywords={access protocols;ad hoc networks;broadcasting;mobile radio;probability;road accidents;road safety;telecommunication network reliability;telecommunication network topology;telecommunication traffic;vehicular ad hoc network;safety warning message;reliable broadcast communication;medium access control;MAC protocol;broadcast traffic;mobility;topology-transparent broadcast protocol;mathematical analysis;probability;road accident;Broadcasting;Ad hoc networks;Telecommunication network reliability;Vehicle safety;Media Access Protocol;Access protocols;Communication system traffic control;Traffic control;Mathematical analysis;Delay},
doi={10.1109/INFCOM.2009.5061925},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061926,
author={M. Fiore and F. Mininni and C. Casetti and C. -. Chiasserini},
booktitle={IEEE INFOCOM 2009},
title={To Cache or Not To Cache?},
year={2009},
volume={},
number={},
pages={235-243},
abstract={We address cooperative caching in mobile ad hoc networks where information is exchanged in a peer-to-peer fashion among the network nodes. Our objective is to devise a fully-distributed caching strategy whereby nodes, independently of each other, decide whether to cache or not some content, and for how long. Each node takes this decision according to its perception of what nearby users may be storing in their caches and with the aim to differentiate its own cache content from the others'. We aptly named such algorithm "Hamlet". The result is the creation of a content diversity within the nodes neighborhood, so that a requesting user likely finds the desired information nearby. We simulate our caching algorithm in an urban scenario, featuring vehicular mobility, as well as in a mall scenario with pedestrians carrying mobile devices. Comparison with other caching schemes under different forwarding strategies confirms that Hamlet succeeds in creating the desired content diversity thus leading to a resource-efficient information access.},
keywords={ad hoc networks;cache storage;distributed processing;mobile radio;peer-to-peer computing;cooperative caching;mobile ad hoc network;peer-to-peer computing;Hamlet;Peer to peer computing;Telecommunication traffic;Cooperative caching;Vehicles;Weather forecasting;Broadcasting;Ad hoc networks;Cache storage;Communications Society;Mobile ad hoc networks},
doi={10.1109/INFCOM.2009.5061926},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061927,
author={P. Jacquet and B. Mans and G. Rodolakis},
booktitle={IEEE INFOCOM 2009},
title={Information Propagation Speed in Mobile and Delay Tolerant Networks},
year={2009},
volume={},
number={},
pages={244-252},
abstract={The goal of this paper is to increase our understanding of the fundamental performance limits of mobile and delay tolerant networks (DTNs), where end-to-end multi-hop paths may not exist and communication routes may only be available through time and mobility. We use analytical tools to derive generic theoretical upper bounds for the information propagation speed in large scale mobile and intermittently connected networks. In other words, we upper-bound the optimal performance, in terms of delay, that can be achieved using any routing algorithm. We then show how our analysis can be applied to specific mobility and graph models to obtain specific analytical estimates. In particular, when nodes move at speed v and their density v is small (the network is sparse and surely disconnected), we prove that the information propagation speed is upper bounded by (1 + O(v<sup>2</sup>))v in the random way-point model, while it is upper bounded by O(radic(vv)v) for other mobility models (random walk, Brownian motion). We also present simulations that confirm the validity of the bounds in these scenarios.},
keywords={ad hoc networks;delays;graph theory;mobility management (mobile radio);telecommunication network routing;information propagation speed;delay tolerant network;mobility;large scale mobile networks;routing algorithm;graph models;random way-point model;mobile ad hoc networks;Propagation delay;Disruption tolerant networking;Upper bound;Routing;Wireless networks;Delay effects;Spread spectrum communication;Mobile communication;Mobile ad hoc networks;Communications Society},
doi={10.1109/INFCOM.2009.5061927},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061928,
author={R. Li and L. Ying and A. Eryilmaz and N. Shroff},
booktitle={IEEE INFOCOM 2009},
title={A Unified Approach to Optimizing Performance in Networks Serving Heterogeneous Flows},
year={2009},
volume={},
number={},
pages={253-261},
abstract={In this work, we study the control of communication networks in the presence of both inelastic and elastic traffic flows. The characteristics of these two types of traffic differ significantly. Hence, earlier approaches that focus on homogeneous scenarios with a single traffic type are not directly applicable. We formulate a new network optimization problem that incorporates the performance requirements of inelastic and elastic traffic flows. The solution of this problem provides us with a new queueing architecture, and distributed load balancing and congestion control algorithm with provably optimal performance. In particular, we show that our algorithm achieves the dual goal of maximizing the aggregate utility gained by the elastic flows while satisfying the demands of inelastic flows. Our base optimal algorithm is extended to provide better delay performance for both types of traffic with minimal degradation in throughput. It is also extended to the practically relevant case of dynamic arrivals and departures. Our solution allows for a controlled interaction between the performance of inelastic and elastic traffic flows. This performance can be tuned to achieve the appropriate design tradeoff. The network performance is studied both theoretically and through extensive simulations.},
keywords={delays;optimisation;queueing theory;telecommunication congestion control;telecommunication network management;telecommunication traffic;communication networks;inelastic traffic flows;elastic traffic flows;network optimization problem;queueing architecture;distributed load balancing;congestion control algorithm;aggregate utility;optimal algorithm;delay performance;Communication system traffic control;Traffic control;Resource management;Communication system control;Load management;Throughput;Communication networks;Communications Society;Intelligent networks;Telecommunication traffic},
doi={10.1109/INFCOM.2009.5061928},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061929,
author={A. Warrier and S. Janakiraman and S. Ha and I. Rhee},
booktitle={IEEE INFOCOM 2009},
title={DiffQ: Practical Differential Backlog Congestion Control for Wireless Networks},
year={2009},
volume={},
number={},
pages={262-270},
abstract={Congestion control in wireless multi-hop networks is challenging and complicated because of two reasons. First, interference is ubiquitous and causes loss in the shared medium. Second, wireless multihop networks are characterized by the use of diverse and dynamically changing routing paths. Traditional end point based congestion control protocols are ineffective in such a setting resulting in unfairness and starvation. This paper adapts the optimal theoretical work of Tassiulas and Ephremedes on cross-layer optimization of wireless networks involving congestion control, routing and scheduling, for practical solutions to congestion control in multi-hop wireless networks. This work is the first that implements in real off-shelf radios, a differential backlog based MAC scheduling and router-assisted backpressure congestion control for multi-hop wireless networks. Our adaptation, called DiffQ, is implemented between transport and IP and supports legacy TCP and UDP applications. In a network of 46 IEEE 802.11 wireless nodes, we demonstrate that DiffQ far outperforms many previously proposed "practical" solutions for congestion control.},
keywords={optimisation;radio networks;scheduling;telecommunication congestion control;telecommunication network routing;differential backlog congestion control;wireless multihop network;routing path;optimal theory;cross-layer optimization;scheduling;DiffQ;Wireless networks;Spread spectrum communication;Routing;Interference;Communication system control;Optimal control;TCPIP;Delay estimation;Communications Society;Computer science},
doi={10.1109/INFCOM.2009.5061929},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061930,
author={K. Jagannathan and E. Modiano and L. Zheng},
booktitle={IEEE INFOCOM 2009},
title={On the Trade-Off Between Control Rate and Congestion in Single Server Systems},
year={2009},
volume={},
number={},
pages={271-279},
abstract={The goal of this paper is to characterize the tradeoff between the rate of control and network congestion for flow control policies. We consider a simple model of a single server queue with congestion-based flow control. The input rate at any instant is decided by a flow control policy, based on the queue occupancy. We identify a simple 'two threshold' control policy, which achieves the best possible congestion probability, for any rate of control. We show that in the absence of control channel errors, the control rate needed to ensure the optimal decay exponent for the congestion probability can be made arbitrarily small. However, if control channel errors occur probabilistically, we show the existence of a critical error probability threshold beyond which the congestion probability undergoes a drastic increase due to the frequent loss of control packets. Finally, we determine the optimal amount of error protection to apply to the control signals by using a simple bandwidth sharing model.},
keywords={error statistics;flow control;network servers;probability;queueing theory;telecommunication congestion control;network congestion control;congestion-based flow control;control channel errors;error probability threshold;bandwidth sharing model;queueing system;Control systems;Optimal control;Communication system control;Error correction;Resource management;Protocols;Network servers;Costs;Communications Society;Error probability},
doi={10.1109/INFCOM.2009.5061930},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061931,
author={J. K. Sundararajan and D. Shah and M. Medard and M. Mitzenmacher and J. Barros},
booktitle={IEEE INFOCOM 2009},
title={Network Coding Meets TCP},
year={2009},
volume={},
number={},
pages={280-288},
abstract={We propose a mechanism that incorporates network coding into TCP with only minor changes to the protocol stack, thereby allowing incremental deployment. In our scheme, the source transmits random linear combinations of packets currently in the congestion window. At the heart of our scheme is a new interpretation of ACKs - the sink acknowledges every degree of freedom (i.e., a linear combination that reveals one unit of new information) even if it does not reveal an original packet immediately. Such ACKs enable a TCP-compatible sliding-window approach to network coding. Our scheme has the nice property that packet losses are essentially masked from the congestion control algorithm. Our algorithm therefore reacts to packet drops in a smooth manner, resulting in a novel and effective approach for congestion control over networks involving lossy links such as wireless links. Our scheme also allows intermediate nodes to perform re-encoding of the data packets. Our simulations show that our algorithm, with or without re-encoding inside the network, achieves much higher throughput compared to TCP over lossy wireless links. We also establish the soundness and fairness properties of our algorithm. Finally, we present queuing analysis for the case of intermediate node re-encoding.},
keywords={encoding;protocols;queueing theory;radio networks;network coding;TCP;protocol stack;packets;data packets;lossy wireless links;queuing analysis;Network coding;Protocols;Size control;Decoding;USA Councils;Wireless networks;Subcontracting;Communications Society;Telecommunications;Computer networks},
doi={10.1109/INFCOM.2009.5061931},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061932,
author={S. Raza and Y. Zhu and C. -. Chuah},
booktitle={IEEE INFOCOM 2009},
title={Graceful Network Operations},
year={2009},
volume={},
number={},
pages={289-297},
abstract={A significant fraction of network events (such as topology or route changes) and the resulting performance degradation stem from premeditated network management and operational tasks. This paper introduces a general class of graceful network operation (GNO) problems, where the goal is to discover the optimal sequence of operations that progressively transition the network from its initial to a desired final state while minimizing the overall performance disruption. We investigate two specific GNO problems: (a) link weight reassignment scheduling (LWRS) studies the optimal ordering of link weight updates to migrate from an existing to a new link weight assignment, and (b) link maintenance scheduling (LMS) looks at how to schedule link deactivations and subsequent reactivations for maintenance purposes. LWRS and LMS are both combinatorial optimization problems. We use dynamic programming to find the optimal solutions when the problem size is small, and leverage ant colony optimization to get near-optimal solutions for large problem sizes. Our simulation study reveals that judiciously ordering network operations can achieve significant performance gains. Our GNO solution framework is generic and applies to similar problems with different operational contexts, underlying network protocols or mechanisms, and performance metrics.},
keywords={combinatorial mathematics;computer network management;dynamic programming;Internet;maintenance engineering;performance degradation;network management;network operational tasks;graceful network operation problem;link weight reassignment scheduling;link maintenance scheduling;link deactivation scheduling;combinatorial optimization problems;dynamic programming;ant colony optimization;Internet;Telecommunication traffic;Communication system traffic control;Quality of service;Network topology;Least squares approximation;Ant colony optimization;Protocols;Web and internet services;Streaming media;Communications Society},
doi={10.1109/INFCOM.2009.5061932},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061933,
author={Y. Ohara and S. Imahori and R. Van Meter},
booktitle={IEEE INFOCOM 2009},
title={MARA: Maximum Alternative Routing Algorithm},
year={2009},
volume={},
number={},
pages={298-306},
abstract={In hop-by-hop networks, provision of multipath routes for all nodes can improve fault tolerance and performance. In this paper we study the multipath route calculation by constructing a directed acyclic graph (DAG) which includes all edges in the network. We define new DAG construction problems with the objectives of 1) maximizing the minimum connectivity, 2) maximizing the minimum max-flow, and 3) maximizing the minimum max-flow as an extension of shortest path routing. A family of new algorithms called Maximum Alternative Routing Algorithms (MARAs) is described, proven formally to solve the problems optimally, and contrasted with existing multipath algorithms. MARAs are evaluated for the number of paths, the length of paths, the computational complexity, and the computation time, using simulations based on several real Internet Autonomous System (AS) network topologies. We show that MARAs run in sub-second times on moderate-speed processors and achieve a significant increase in the number of paths compared to existing multipath routing algorithms. These results should help further the process of deploying multipath routing in real-world networks.},
keywords={computational complexity;computer network reliability;directed graphs;fault tolerance;Internet;optimisation;telecommunication network routing;telecommunication network topology;maximum alternative routing algorithm;hop-by-hop network;fault tolerance;multipath route calculation;directed acyclic graph;DAG;shortest path routing;computational complexity;Internet autonomous system;network topology;optimisation;Routing;Internet;Costs;IP networks;Communications Society;Peer to peer computing;Fault tolerance;Computational complexity;Computer networks;Computational modeling},
doi={10.1109/INFCOM.2009.5061933},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061934,
author={F. Wang and L. Gao},
booktitle={IEEE INFOCOM 2009},
title={Path Diversity Aware Interdomain Routing},
year={2009},
volume={},
number={},
pages={307-315},
abstract={As the Internet becomes the critical information infrastructure for both personal and business applications, fast and reliable routing protocols need to be designed to maintain the performance of those applications in the presence of failures. Today's interdomain routing protocol, BGP, is known to be slow in reacting and recovering from network failures. Increasing path diversity by advertising multiple paths is an effective strategy to achieve reliable interdomain routing. However, designing a scalable and efficient multiple path advertisement to enhance the reliability of interdomain routing is still challenging. In this paper, we propose two path diversity aware routing protocols, D-BGP and B-BGP, to improve the resilience of interdomain routing. The two protocols can establish multiple paths with low routing overhead by exploiting the path diversity existing in the underlying network infrastructure. We evaluate the reliability improvements by simulation.},
keywords={computer network reliability;Internet;routing protocols;path diversity aware interdomain routing protocol;Internet;network failure recovery;network reliability;D-BGP;B-BGP;Routing protocols;Internet;Advertising;Redundancy;Application software;Topology;Communications Society;Cultural differences;Maintenance engineering;Reliability engineering},
doi={10.1109/INFCOM.2009.5061934},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061935,
author={S. Balon and G. Leduc},
booktitle={IEEE INFOCOM 2009},
title={BGP-Aware IGP Link Weight Optimization in Presence of Route Reflectors},
year={2009},
volume={},
number={},
pages={316-324},
abstract={The first generation of IGP Link Weight Optimizers (LWOs) was based on presumably invariant intra-domain traffic matrices only, ignoring the fact that updating link weights had a side effect on these traffic matrices due to hot-potato routing, thus resulting in suboptimal link weight settings, and sometimes to very bad performance. The second generation of IGP LWOs, referred to as BGP-aware LWOs, has been able to optimize link weights while taking hot-potato effects into account. However, these tools relied on the complete visibility assumption fulfilled by e.g. a full-mesh iBGP configuration. This paper proposes a third generation LWO, still BGP-aware, but also able to work with iBGP configurations based on route reflectors, which usually hide some reachability information from routers. This partial visibility may cause various problems, including path deflections (i.e., the actual egress router is not the expected one), which may in turn create forwarding loops. Our LWO embeds a BGP routing solver which can always predict the actual egress router, even when route reflectors are used. It can also forbid solutions leading to path deflection. Its efficiency is evaluated on a real dataset, and compared to other LWOs.},
keywords={internetworking;reachability analysis;routing protocols;telecommunication traffic;BGP-aware IGP link weight optimization;route reflectors;invariant intra-domain traffic matrices;hot-potato routing;reachability information;path deflection;Interior Gateway Protocol;Border Gateway Protocol;Telecommunication traffic;Communications Society;Routing protocols;Intersymbol interference;Costs;FETs;Optimization methods;IEEE news;Scalability},
doi={10.1109/INFCOM.2009.5061935},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061936,
author={B. Y. Zhao and J. C. S. Lui and D. -. Chiu},
booktitle={IEEE INFOCOM 2009},
title={Analysis of Adaptive Incentive Protocols for P2P Networks},
year={2009},
volume={},
number={},
pages={325-333},
abstract={Incentive protocols play a crucial role to encourage cooperation among nodes in networking applications. The aim of this paper is to provide a general analytical framework to analyze and design a large family of incentive protocols. We consider a class of incentive protocols wherein peers can distributively learn and adapt their actions. Using our analytical framework, one can evaluate the expected performance gain and system robustness of a given incentive protocol. To illustrate the framework, we present three incentive policies and two learning (or adaptive) models. We show under what conditions the network may collapse (e.g., no cooperation in the system) or the incentive protocol can guarantee a high degree of cooperation. In particular, we formally show the connection between evaluating incentive protocols and evolutionary game theory so to identify robustness characteristics of an incentive policy.},
keywords={evolutionary computation;game theory;peer-to-peer computing;protocols;P2P networks;adaptive incentive protocols;networking applications;incentive policies;evolutionary game theory;Protocols;Robustness;Peer to peer computing;Mathematical model;Performance gain;Game theory;Network servers;Communications Society;Bridges;Performance analysis},
doi={10.1109/INFCOM.2009.5061936},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061937,
author={X. Zhang and B. Li},
booktitle={IEEE INFOCOM 2009},
title={On the Market Power of Network Coding in P2P Content Distribution Systems},
year={2009},
volume={},
number={},
pages={334-342},
abstract={Network coding is emerging as a promising alternative to traditional content distribution approaches in P2P networks. By allowing information mixture in peers, it simplifies the block scheduling problem, resulting in more efficient data delivery. Existing protocols have validated such advantages assuming altruistic and obedient peers. In this paper, we develop an analytical framework that characterizes a coding based P2P content distribution market where peers selfishly seek for individual payoff maximization. Through virtual monetary exchanges, agents in the market buy the coded blocks from others and resell their possessions to those in need. We model such transactions as decentralized strategic bargaining games, and derive the equilibrium prices between arbitrary pairs of agents when the market enters the steady state. We identify the traditional P2P content distribution approach as a special case of network coding, and characterize the relations between coding complexity and market performance metrics, including agents' entry price and expected payoff, thus providing operation guidelines for a real P2P market. Our analysis reveals that the major power of network coding lies in its ability to maintain stability of the market with impatient and selfish agents, and to incentivize agents with lower price and higher payoff, at the cost of reasonable coding complexity.},
keywords={channel coding;peer-to-peer computing;scheduling;market power;network coding;P2P content distribution systems;block scheduling problem;virtual monetary exchanges;decentralized strategic bargaining games;coding complexity;market performance metrics;Network coding;Protocols;Steady-state;Equations;Network servers;Communications Society;Processor scheduling;Measurement;Guidelines;Stability analysis},
doi={10.1109/INFCOM.2009.5061937},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061938,
author={R. Landa and D. Griffin and R. G. Clegg and E. Mykoniati and M. Rio},
booktitle={IEEE INFOCOM 2009},
title={A Sybilproof Indirect Reciprocity Mechanism for Peer-to-Peer Networks},
year={2009},
volume={},
number={},
pages={343-351},
abstract={Although direct reciprocity (Tit-for-Tai) contribution systems have been successful in reducing freeloading in peer- to-peer overlays, it has been shown that, unless the contribution network is dense, they tend to be slow (or may even fail) to converge. On the other hand, current indirect reciprocity mechanisms based on reputation systems tend to be susceptible to sybil attacks, peer slander and whitewashing. In this paper we present PledgeRoute, an accounting mechanism for peer contributions that is based on social capital. This mechanism allows peers to contribute resources to one set of peers and use this contribution to obtain services from a different set of peers, at a different time. PledgeRoute is completely decentralised, can be implemented in both structured and unstructured peer-to-peer systems, and it is resistant to the three kinds of attacks mentioned above. To achieve this, we model contribution transitivity as a routing problem in the contribution network of the peer-to-peer overlay, and we present arguments for the routing behaviour and the sybilproofness of our contribution transfer procedures on this basis. Additionally, we present mechanisms for the seeding of the contribution network, and a combination of incentive mechanisms and reciprocation policies that motivate peers to adhere to the protocol and maximise their service contributions to the overlay.},
keywords={cryptographic protocols;peer-to-peer computing;routing protocols;telecommunication security;sybilproof indirect reciprocity mechanism;peer-to-peer network;direct reciprocity contribution system;reputation system;PledgeRoute accounting mechanism;social capital;routing problem;incentive mechanism;cryptography protocol technique;Peer to peer computing;Routing;Communications Society;Educational institutions;Costs;Forgery;Public key;Public key cryptography;Convergence;Protocols},
doi={10.1109/INFCOM.2009.5061938},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061939,
author={Y. Hu and M. Feng and L. N. Bhuyan and V. Kalogeraki},
booktitle={IEEE INFOCOM 2009},
title={Budget-Based Self-Optimized Incentive Search in Unstructured P2P Networks},
year={2009},
volume={},
number={},
pages={352-360},
abstract={Distributed object search is the primary function of peer-to-peer (P2P) file sharing system to locate and transfer the file. The predominant search schemes in unstructured P2P systems have their problems: flooding creates excessive traffic overhead and random walk prolongs search delay. Moreover, both use uniform time-to-live (TTL) control for all users, which makes them vulnerable to selfish user attacks, and results in the "free-riding" and "tragedy of the commons" problems. In this paper, we propose a budget-based self-optimized incentive search (BuSIS) protocol for unstructured P2P file sharing systems, which is robust to and restricts selfish user behaviors. Furthermore, our protocol lowers the search overhead while keeping high hit rate. BuSIS provides differentiated search service for selfish users and ties a user's contribution to its service level. We present the analytical models on expected search performance, associated search cost and the user satisfaction level. Extensive emulations have been conducted at large scale network scenarios to compare performance of BuSIS with flooding and random walk searches with and without selfish user behaviors. The experimental results show that BuSIS always has the lowest search overhead without sacrificing the hit rate. When serving selfish users, flooding and random walk performance degrade dramatically, while BuSIS gracefully keeps the hit rate only with 20% overhead of flooding and 25% of random walk.},
keywords={peer-to-peer computing;protocols;budget-based self-optimized incentive search protocols;unstructured P2P networks;distributed object search;peer-to-peer file sharing system;uniform time-to-live control;Peer to peer computing;Floods;Protocols;Communication system traffic control;Delay;Robustness;Analytical models;Costs;Emulation;Large-scale systems},
doi={10.1109/INFCOM.2009.5061939},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061940,
author={X. Chu and H. Sethu},
booktitle={IEEE INFOCOM 2009},
title={A New Distributed Algorithm for Even Coverage and Improved Lifetime in a Sensor Network},
year={2009},
volume={},
number={},
pages={361-369},
abstract={In "area-sensing" applications of sensor networks, such as surveillance or target tracking, each sensor node has a sensing radius within which it can monitor events. Coverage problems in sensor networks have largely focused on such applications, where the goal of good coverage is one of ensuring that each point in the region of interest is within the sensing radius of at least one node. On the other hand, in "spot-sensing" applications, each node makes a measurement (such as temperature or humidity) at the precise location of the node and there is no concept of a sensing radius. In this paper, we introduce a new coverage problem that is more meaningful to spot-sensing applications. In such cases, good coverage usually implies even coverage across points in the region. We borrow from the field of economics and adapt a well-accepted measure of inequality, the Gini index, to develop a metric for the evenness of coverage by a sensor network. Based on mathematical results on the expected distances between neighboring nodes, we present a new distributed algorithm, called EvenCover, for each node to determine if and when it should sleep or sense. We prove that the expected Gini index is 1 - 1radic2 ap 0.293 when the spatial distribution of sensing nodes is given by a Poisson random process. On the other hand, when the sensing nodes are perfectly evenly distributed, we show that the Gini index has a lower bound of 0.2. These two results serve as points of reference to evaluate the coverage achieved by the EvenCover algorithm. We present a thorough simulation-based comparison of EvenCover against other distributed algorithms showing that it achieves better evenness and significantly increased lifetime. In addition, we discover that evenness of coverage permits a graceful degradation of the network as nodes exhaust their energy resources.},
keywords={distributed sensors;random processes;stochastic processes;target tracking;distributed algorithm;sensor network;area-sensing applications;target tracking;spot-sensing applications;Gini index;EvenCover;spatial distribution;Poisson random process;energy resources;Distributed algorithms;Temperature measurement;Surveillance;Target tracking;Monitoring;Humidity measurement;Temperature sensors;Power generation economics;Sleep;Random processes},
doi={10.1109/INFCOM.2009.5061940},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061941,
author={M. Khan and V. S. A. Kumar and M. V. Marathe and G. Pandurangan and S. S. Ravi},
booktitle={IEEE INFOCOM 2009},
title={Bi-Criteria Approximation Algorithms for Power-Efficient and Low-Interference Topology Control in Unreliable Ad Hoc Networks},
year={2009},
volume={},
number={},
pages={370-378},
abstract={Topology control in ad hoc networks is a multi-criteria optimization problem involving (contradictory) objectives of connectivity, interference, and power minimization. Additionally, nodes can be unreliable, which adds another dimension to an already challenging problem. In this paper, we study topology control problems in ad hoc networks under node failures for arbitrary node distributions. We consider a simple and natural stochastic failure model, in which each node can fail independently with a given probability. The topology control problem under stochastic failures is to choose a power level for each node and a subset of edges such that the residual graph (i.e., the graph formed by the nodes which have not failed) is connected and can be scheduled efficiently, with high probability. We develop provably efficient bi-criteria approximation algorithms for this problem that simultaneously minimize power, reduce interference, and ensure that the surviving graph is connected with high probability. Our algorithms can be implemented efficiently in a distributed manner.},
keywords={ad hoc networks;interference suppression;probability;stochastic processes;telecommunication congestion control;telecommunication network topology;bicriteria approximation algorithms;low-interference topology control;ad hoc networks;multicriteria optimization problem;power minimization;interference;natural stochastic failure model;stochastic failures;Approximation algorithms;Network topology;Ad hoc networks;Stochastic processes;Peer to peer computing;Interference;Computer science;Protocols;Bioinformatics;Communications Society},
doi={10.1109/INFCOM.2009.5061941},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061942,
author={M. P. Johnson and D. Sarioz and A. Bar-Noy and T. Brown and D. Verma and C. W. Wu},
booktitle={IEEE INFOCOM 2009},
title={More is More: The Benefits of Denser Sensor Deployment},
year={2009},
volume={},
number={},
pages={379-387},
abstract={Positioning disk-shaped sensors to optimize certain coverage parameters is a fundamental problem in ad-hoc sensor networks. The hexagon grid lattice is known to be optimally efficient, but the 20.9% of the area covered by two sensors may be considered a waste. Furthermore, any movement of a sensor from its designated grid position or sensor failure, due to placement error or obstacle avoidance, leaves some region uncovered, as would the failure of any one sensor. We explore how shrinking the grid can help to remedy these shortcomings. First, shrinking to obtain a denser hexagonal lattice allows all sensors to move about their intended positions independently while nonetheless guaranteeing full coverage. Second, sufficiently increasing the lattice density will naturally yield k-coverage for k &gt; 1. Moreover, we show that a density increase tantamount to fc copies of the lattice can yield k' -coverage, for k<sup>j</sup> &gt; k (e.g. k = 11, k<sup>j</sup> = 12), through the exploitation of the double-coverage regions. Our examples' savings provably converge in the limit to the ap 20.9% maximum. We also provide analogous results for the square lattice and its ap 57% inefficiency, including k = 3, k<sup>j</sup> = 4, k = 5,k<sup>j</sup> = 7, indicating that for multi-coverage, the square lattice can actually be more efficient than the hexagon lattice. All these efficiency gains can be used to provide 1-coverage or fc-coverage even in the face of probabilistic sensor failure. We conclude by construing the shrinking factor as a budget to be divided among these three benefits.},
keywords={ad hoc networks;telecommunication network reliability;wireless sensor networks;disk-shaped sensors position;ad-hoc sensor networks;hexagon grid lattice;grid position;placement error;obstacle avoidance;grid shrinking;lattice density;double-coverage regions;square lattice;probabilistic sensor failure;shrinking factor;Lattices;US Government;Robustness;Communications Society;Computer science;Face detection;Sensor fusion;Sensor phenomena and characterization;Computer vision;Performance evaluation},
doi={10.1109/INFCOM.2009.5061942},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061943,
author={X. Bai and C. Zhang and D. Xuan and W. Jia},
booktitle={IEEE INFOCOM 2009},
title={Full-Coverage and k-Connectivity (k=14,6) Three Dimensional Networks},
year={2009},
volume={},
number={},
pages={388-396},
abstract={In this paper, we study the problem of constructing full-coverage three dimensional networks with multiple connectivity. We design a set of patterns for full coverage and two representative connectivity requirements, i.e. 14- and 6-connectivity. We prove their optimality under any ratio of the communication range over the sensing range among regular lattice deployment patterns. We also conduct a study on the proposed patterns under practical settings. To our knowledge, our work is the first one that provides deployment patterns with proven optimality that achieve both coverage and connectivity in three dimensional networks.},
keywords={wireless sensor networks;full-coverage three dimensional network;WSN;wireless sensor network;connectivity requirement;regular lattice deployment pattern;Wireless sensor networks;Application software;Kelvin;Computer science;Lattices;Intelligent sensors;Communications Society;USA Councils;Mathematics;Intelligent systems},
doi={10.1109/INFCOM.2009.5061943},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061944,
author={A. Bremler-Barr and Y. Koral},
booktitle={IEEE INFOCOM 2009},
title={Accelerating Multi-Patterns Matching on Compressed HTTP Traffic},
year={2009},
volume={},
number={},
pages={397-405},
abstract={One of the fundamental technique which is used today by network security tools to detect malicious activities is 'signature based' detection. Today, the performance of the security tools is dominated by the speed of the string-matching algorithms that detect these signatures. Currently these security tools do not deal with compressed traffic, which becomes more and more common in HTTP. HTTP protocol uses the GZIP compression, which first requires some kind of decompression phase before performing the multi-patterns matching task. Thus, there is a high performance penalty in pattern matching on compressed data. In this paper we present a novel algorithm, Aho-Corasick-based algorithm for compressed HTTP (ACCH) that takes advantage of information gathered by the decompression phase in order to accelerate the commonly used Aho-Corasick pattern matching algorithm. We show by analyzing real HTTP traffic and real WAF signatures patterns, that we can skip scanning up to 75% of the data. Surprisingly, we show that in some situations, it is faster to do pattern matching on the compressed data, with the penalty of decompression, than doing pattern matching on regular traffic. As far as we know we are the first paper, that analyzes the problem of 'on-the-fly' multi-patterns matching algorithms on compressed HTTP traffic and suggest a solution.},
keywords={data compression;digital signatures;Internet;string matching;telecommunication security;telecommunication traffic;transport protocols;Aho-Corasick multipattern matching algorithm;compressed HTTP traffic;HTTP protocol;network security tool;malicious activity detection;signature-based detection;string-matching algorithm;GZIP compression;decompression phase;Web traffic;Acceleration;Pattern matching;Telecommunication traffic;Intrusion detection;Web server;Computer science;Data security;Algorithm design and analysis;Encoding;Compression algorithms},
doi={10.1109/INFCOM.2009.5061944},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061945,
author={Z. Yu and Y. Wei and B. Ramkumar and Y. Guan},
booktitle={IEEE INFOCOM 2009},
title={An Efficient Scheme for Securing XOR Network Coding against Pollution Attacks},
year={2009},
volume={},
number={},
pages={406-414},
abstract={Network coding is promising to maximize throughput in various networking systems. Compared to normal network coding operated over large finite fields, XOR network coding has gained an increasing number of applications for its simplicity, especially in wireless networks. However, both types of network coding systems are vulnerable to pollution attacks in which the compromised forwarders inject polluted messages into the systems. Existing solutions to pollution attacks can protect only the normal network coding, but none of them is able to secure XOR network coding. In this paper, we propose an efficient scheme for securing XOR network coding against pollution attacks. Our scheme exploits probabilistic key pre-distribution and message authentication codes (MACs). In our scheme, the source appends multiple MACs to each message, where each MAC can authenticate only a part of the message and the parts authenticated by different MACs are overlapped. Thus, multiple forwarders can collaboratively verify different parts of messages using the MACs with their own shared keys. By carefully controlling the overlapping between the parts authenticated by different MACs, our scheme can filter polluted messages in a few hops with a high probability. To the best of our knowledge, this is the first solution to pollution attacks for XOR network coding. Experimental results show that it is 200 to 1000 times faster than existing ones, hence, it is particularly suitable for resource-constrained wireless networks.},
keywords={cryptography;encoding;message authentication;probability;radio networks;telecommunication security;pollution attack;secure XOR network coding;probabilistic key pre-distribution;message authentication code;resource-constrained wireless network;Network coding;Pollution;Filters;Filtering;Wireless networks;Wireless sensor networks;Message authentication;Collaboration;Throughput;Galois fields},
doi={10.1109/INFCOM.2009.5061945},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061946,
author={N. Hua and H. Song and T. V. Lakshman},
booktitle={IEEE INFOCOM 2009},
title={Variable-Stride Multi-Pattern Matching For Scalable Deep Packet Inspection},
year={2009},
volume={},
number={},
pages={415-423},
abstract={Accelerating multi-pattern matching is a critical issue in building high-performance deep packet inspection systems. Achieving high-throughputs while reducing both memory-usage and memory-bandwidth needs is inherently difficult. In this paper, we propose a pattern (string) matching algorithm that achieves high throughput while limiting both memory-usage and memory-bandwidth. We achieve this by moving away from a byte-oriented processing of patterns to a block-oriented scheme. However, different from previous block-oriented approaches, our scheme uses variable-stride blocks. These blocks can be uniquely identified in both the pattern and the input stream, hence avoiding the multiplied memory costs which is intrinsic in previous approaches. We present the algorithm, tradeoffs, optimizations, and implementation details. Performance evaluation is done using the Snort and ClamAV pattern sets. Using our algorithm, the throughput of a single search engine can easily have a many-fold increase at a small storage cost, typically less than three bytes per pattern character.},
keywords={content management;pattern matching;search engines;variable-stride multipattern matching;scalable deep packet inspection;memory-usage;memory-bandwidth;block-oriented processing;Snort;ClamAV;search engine;content inspection systems;Inspection;Throughput;Pattern matching;Costs;Engines;Intrusion detection;Pipeline processing;Automata;Communications Society;Educational institutions},
doi={10.1109/INFCOM.2009.5061946},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061947,
author={N. Duffield and P. Haffner and B. Krishnamurthy and H. Ringberg},
booktitle={IEEE INFOCOM 2009},
title={Rule-Based Anomaly Detection on IP Flows},
year={2009},
volume={},
number={},
pages={424-432},
abstract={Rule-based packet classification is a powerful method for identifying traffic anomalies, with network security as a key application area. While popular systems like Snort are used in many network locations, comprehensive deployment across Tier-1 service provider networks is costly due to the need for high-speed monitors at many network ingress points. Since ISPs already collect flow statistics ubiquitously, can we use it for detecting the same anomalies as the packet based rules in spite of aggregation and absence of payload information? We exploit correlations between packet and flow level information via a machine learning (ML) approach to associate packet level alarms with a feature vector derived from flow records on the same traffic. We describe a system architecture for network-wide flow- alarming and describe the steps required to establish a proof- of-concept. We evaluate prediction accuracy of candidate ML algorithms on actual packet traces. The duration of prediction effectiveness is an issue for ML approaches and more so in resource intensive network applications. Initial results show little impairment of performance over periods of one or two weeks.},
keywords={IP networks;learning (artificial intelligence);security of data;telecommunication traffic;rule-based anomaly detection;packet classification;network traffic;network security;Tier-1 service provider network;IP flow;machine learning approach;network-wide flow-alarming architecture;Telecommunication traffic;Payloads;Inspection;Monitoring;Statistics;Costs;Intrusion detection;Computer worms;Buffer overflow;Communications Society},
doi={10.1109/INFCOM.2009.5061947},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061948,
author={Y. Choi and P. Momcilovic},
booktitle={IEEE INFOCOM 2009},
title={On Effectiveness of Application-Layer Coding},
year={2009},
volume={},
number={},
pages={433-441},
abstract={The effectiveness of application-layer coding in a system with a large number of users is considered. The end users encode data packets before transmitting them. The effect of additional packets on the system performance is twofold: (i) additional packets increase the offered load, which results in higher drop probability, and (ii) some of dropped packets can be recovered at the receivers after decoding. The paper establishes an asymptotic regime in which systems with and without coding have the same performance. The space of all systems is partitioned into two regions where coding is beneficial and detrimental, respectively. Informally, it is argued that application-layer coding improves the performance only in systems with low loss probabilities (without coding), and employing such coding in systems with high loss probabilities only degrades the performance.},
keywords={encoding;application-layer coding;drop probability;decoding;packet networks;Decoding;System performance;Performance loss;Communications Society;Degradation;Buffer overflow;Capacity planning;Performance analysis},
doi={10.1109/INFCOM.2009.5061948},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061949,
author={S. Dumitrescu and M. Shao and X. Wu},
booktitle={IEEE INFOCOM 2009},
title={Layered Multicast with Inter-Layer Network Coding},
year={2009},
volume={},
number={},
pages={442-449},
abstract={Multirate multicast is a powerful methodology of multimedia communication in heterogenous networks. A variant of multirate multicast motivated by scalable multimedia streaming is layered multicast, where the transmitted signal is presented in successive data layers. With recent advances of network coding theory, many layered multicast schemes using network coding have been proposed to improve the performance of traditional routing based layered multicast. They divide the network into different layers and construct a unirate multicast network code for each layer. However, these schemes do not perform network coding between data layers, and consequently cannot realize the full potential of network coding. In this paper, we propose a novel approach to layered multicast that allows network coding of data in different layers. This relaxation lends the proposed scheme greater flexibility in optimizing the data flow than previous layered solutions, and thus achieves higher throughput.},
keywords={encoding;media streaming;multicast communication;layered multicast;inter-layer network coding;multimedia communication;heterogenous networks;scalable multimedia streaming;Network coding;Streaming media;Routing;Throughput;Bandwidth;Communications Society;Multimedia communication;Decoding;Tree graphs;Multicast algorithms},
doi={10.1109/INFCOM.2009.5061949},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061950,
author={M. Kim and M. Medard and U. -. O'Reilly and D. Traskov},
booktitle={IEEE INFOCOM 2009},
title={An Evolutionary Approach To Inter-Session Network Coding},
year={2009},
volume={},
number={},
pages={450-458},
abstract={Whereas the theory and application of optimal network coding are well studied for the single-session multicast scenario, there is no known optimal network coding strategy for a more general connection problem where there are more than one session and receivers may demand different sets of information. Though there have been a number of recent studies that demonstrate various utilities of network coding in the multi- session scenario, they rely on very restricted classes of codes in terms of the coding operations allowed and/or the location of decoding. In this paper, we propose a novel inter-session network coding strategy for a general connection problem. Our coding strategy allows fairly general random linear coding over a large finite field, in which decoding is done at receivers and the mixture of information at interior nodes is controlled by evolutionary mechanisms. We demonstrate how our coding strategy may surpass existing end-to-end pairwise XOR coding schemes in terms of effectiveness and practicality.},
keywords={decoding;evolutionary computation;linear codes;multicast communication;random codes;optimal inter-session network coding;single-session multicast scenario;evolutionary approach;random linear coding;finite field;decoding receiver;Network coding;Decoding;Unicast;Laboratories;Communications Society;Galois fields;Peer to peer computing;Scalability;Linear code;Polynomials},
doi={10.1109/INFCOM.2009.5061950},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061951,
author={H. Shojania and B. Li and X. Wang},
booktitle={IEEE INFOCOM 2009},
title={Nuclei: GPU-Accelerated Many-Core Network Coding},
year={2009},
volume={},
number={},
pages={459-467},
abstract={While it is a well known result that network coding achieves optimal flow rates in multicast sessions, its potential for practical use has remained to be a question, due to its high computational complexity. Our previous work has attempted to design a hardware-accelerated and multi-threaded implementation of network coding to fully utilize multi-core CPUs, as well as SSE2 and AltiVec SIMD vector instructions on X86 and PowerPC processors. This paper represents another step forward, and presents the first attempt in the literature to maximize the performance of network coding by taking advantage of not only multi-core CPUs, but also potentially hundreds of computing cores in commodity off-the-shelf graphics processing units (GPU). With GPU computing gaining momentum as a result of increased hardware capabilities and improved programmability, our work shows how the GPU, with a design involving thousands of lightweight threads, can boost network coding performance significantly. Many-core GPUs can be deployed as an attractive alternative and complementary solution to multi-core servers, by offering a better price/performance advantage. In fact, multi-core CPUs and many-core GPUs can be deployed and used to perform network coding simultaneously, potentially useful in media streaming servers where hundreds of peers are served concurrently by these dedicated servers. In this paper, we present Nuclei, the design and implementation of GPU-based network coding. With Nuclei, only one mainstream NVidia 8800 GT GPU outperforms an 8-core Intel Xeon server in most test cases. A combined CPU-GPU encoding scenario achieves coding rates of up to 116 MB/second for a variety of coding settings, which is sufficient to saturate a Gigabit Ethernet interface.},
keywords={computer graphic equipment;encoding;local area networks;media streaming;multicast communication;multimedia servers;multi-threading;telecommunication traffic;Nuclei GPU-accelerated many-core network coding;optimal flow rate;multicast session;computational complexity;multithreaded implementation;multicore CPU;SSE2 SIMD vector instruction;AltiVec SIMD vector instruction;X86 processor;PowerPC processor;off-the-shelf graphics processing unit;multicore server;media streaming server;NVidia 8800 GT GPU;8-core Intel Xeon server;CPU-GPU encoding scenario;Gigabit Ethernet interface;Network coding;Network servers;Computer networks;Streaming media;Computational complexity;Graphics;Hardware;Yarn;Testing;Encoding},
doi={10.1109/INFCOM.2009.5061951},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061952,
author={F. Wang and J. Liu},
booktitle={IEEE INFOCOM 2009},
title={Duty-Cycle-Aware Broadcast in Wireless Sensor Networks},
year={2009},
volume={},
number={},
pages={468-476},
abstract={Broadcast is one of the most fundamental services in wireless sensor networks (WSNs). It facilitates sensor nodes to propagate messages across the whole network, serving a wide range of higher-level operations and thus being critical to the overall network design. A distinct feature of WSNs is that many nodes alternate between active and dormant states, so as to conserve energy and extend the network lifetime. Unfortunately, the impact of such cycles has been largely ignored in existing broadcast implementations that adopt the common assumption of all nodes being active all over the time. In this paper, we revisit the broadcast problem with active/dormant cycles. We show strong evidence that conventional broadcast approaches will suffer from severe performance degradation, and, under low duty-cycles, they could easily fail to cover the whole network in an acceptable timeframe. To this end, we remodel the broadcast problem in this new context, seeking a balance between efficiency and latency with coverage guarantees. We demonstrate that this problem can be translated into a graph equivalence, and develop a centralized optimal solution. It provides a valuable benchmark for assessing diverse duty-cycle-aware broadcast strategies. We then extend it to an efficient and scalable distributed implementation, which relies on local information and operations only, with built-in loss compensation mechanisms. The performance of our solution is evaluated under diverse network configurations. The results suggest that our distributed solution is close to the lower bounds of both time and forwarding costs, and it well resists to the network size and wireless loss increases. In addition, it enables flexible control toward the quality of broadcast coverage.},
keywords={broadcasting;quality of service;wireless sensor networks;duty-cycle-aware broadcast strategy;wireless sensor networks;sensor nodes;loss compensation mechanisms;network design;Broadcasting;Wireless sensor networks;Peer to peer computing;Delay;Floods;Communications Society;Computer networks;Degradation;Costs;Resists},
doi={10.1109/INFCOM.2009.5061952},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061953,
author={Q. Du and X. Zhang},
booktitle={IEEE INFOCOM 2009},
title={Statistical QoS Provisionings for Wireless Unicast/Multicast of Layered Video Streams},
year={2009},
volume={},
number={},
pages={477-485},
abstract={Due to the highly-varying wireless channels, deterministic quality of service (QoS) is usually difficult to guarantee for real-time multi-layer video transmissions in wireless networks. Consequently, statistical QoS guarantees have become an important alternative in supporting real-time video transmissions. In this paper, we propose an efficient framework to model the statistical delay QoS guarantees, in terms of QoS exponent, effective bandwidth/capacity, and delay-bound violation probability, for multi-layer video transmission over wireless fading channels. In particular, a separate queue is maintained for each video layer, and the same delay bound and corresponding violation probability threshold are set up for all layers. Applying the effective bandwidth/capacity analyses on the incoming video stream, we obtain a set of QoS exponents for all video layers to effectively characterize this delay QoS requirement. We then develop a set of optimal adaptive transmission schemes to minimize the resource consumption while fulfilling the diverse QoS requirements under various scenarios, including video unicast/multicast with and/or without loss tolerance. Simulation results are also presented to demonstrate the impact of statistical QoS provisionings on resource allocations of our proposed adaptive transmission schemes.},
keywords={delays;fading channels;mobile radio;multicast communication;probability;quality of service;radio networks;video coding;video streaming;statistical QoS provisionings;wireless unicast-multicast;layered video streams;quality of service;real-time multilayer video transmissions;wireless network;statistical delay;delay-bound violation probability;wireless fading channel;optimal adaptive transmission scheme;video unicast-multicast;resource allocation;Unicast;Streaming media;Quality of service;Delay effects;Bandwidth;Probability;Wireless networks;Capacity planning;Fading;Propagation losses},
doi={10.1109/INFCOM.2009.5061953},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061954,
author={I. -. Hou and V. Borkar and P. R. Kumar},
booktitle={IEEE INFOCOM 2009},
title={A Theory of QoS for Wireless},
year={2009},
volume={},
number={},
pages={486-494},
abstract={Wireless networks are increasingly used to carry applications with QoS constraints. Two problems arise when dealing with traffic with QoS constraints. One is admission control, which consists of determining whether it is possible to fulfill the demands of a set of clients. The other is finding an optimal scheduling policy to meet the demands of all clients. In this paper, we propose a framework for jointly addressing three QoS criteria: delay, delivery ratio, and channel reliability. We analytically prove the necessary and sufficient condition for a set of clients to be feasible with respect to the above three criteria. We then establish an efficient algorithm for admission control to decide whether a set of clients is feasible. We further propose two scheduling policies and prove that they are feasibility optimal in the sense that they can meet the demands of every feasible set of clients. In addition, we show that these policies are easily implementable on the IEEE 802.11 mechanisms. We also present the results of simulation studies that appear to confirm the theoretical studies and suggest that the proposed policies outperform others tested under a variety of settings.},
keywords={quality of service;radio networks;scheduling;telecommunication congestion control;telecommunication network reliability;QoS;wireless networks;admission control;optimal scheduling policy;channel reliability;IEEE 802.11 mechanisms;quality of service;Quality of service;Admission control;Wireless networks;USA Councils;Optimal scheduling;Delay;Sufficient conditions;Testing;Contracts;Communications Society},
doi={10.1109/INFCOM.2009.5061954},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061955,
author={A. Farbod and B. Liang},
booktitle={IEEE INFOCOM 2009},
title={Structured Admission Control Policy in Heterogeneous Wireless Networks with Mesh Underlay},
year={2009},
volume={},
number={},
pages={495-503},
abstract={In this paper, we investigate into optimal admission control policies for Heterogeneous Wireless Networks (HWN), considering an integration of wireless mesh networks with an overlaying cellular infrastructure. In order to characterize the overflow traffic from the underlaying mesh to the overlay, a Partially-Observable Markov-Modulated Poisson Process (PO-MMPP) traffic model is developed. This model captures the burstiness of the overflow traffic under the imperfect observability of the mesh network states. Then, by modeling the overlay network as a controlled PO-MMPP/M/C/C queueing system and obtaining structured decision theoretic results, it is shown that the optimal control policies for this class of HWNs can be characterized as monotonic threshold curves. Further, these results are used to design a computationally efficient algorithm to determine the optimal policy in terms of thresholds. Numerical observations suggest that the proposed algorithm is efficient in terms of time-complexity and can drastically reduce the cost of dropped and blocked calls.},
keywords={cellular radio;computer networks;decision theory;Markov processes;optimal control;queueing theory;radio networks;telecommunication congestion control;telecommunication network topology;telecommunication traffic;structured optimal admission control policy;heterogeneous wireless mesh underlay network;overlaying cellular infrastructure;partially-observable Markov-modulated Poisson process overflow traffic model;PO-MMPP traffic model;overlay network;controlled PO-MMPP/M/C/C queueing system;decision theory;monotonic threshold curve;time complexity;Admission control;Wireless mesh networks;Communication system traffic control;Traffic control;Optimal control;Cellular networks;Observability;Mesh networks;Queueing analysis;Algorithm design and analysis},
doi={10.1109/INFCOM.2009.5061955},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061956,
author={M. Yoon and T. Li and S. Chen and J. -. Peir},
booktitle={IEEE INFOCOM 2009},
title={Fit a Spread Estimator in Small Memory},
year={2009},
volume={},
number={},
pages={504-512},
abstract={The spread of a source host is the number of distinct destinations that it has sent packets to during a measurement period. A spread estimator is a software/hardware module on a router that inspects the arrival packets and estimates the spread of each source. It has important applications in detecting port scans and DDoS attacks, measuring the infection rate of a worm, assisting resource allocation in a server farm, determining popular Web contents for caching, to name a few. The main technical challenge is to fit a spread estimator in a fast but small memory (such as SRAM) in order to operate it at the line speed in a high-speed network. In this paper, we design a new spread estimator that delivers good performance in tight memory space where all existing estimators no longer work. The new estimator not only achieves space compactness but operates more efficiently than the existing ones. Its accuracy and efficiency come from a new method for data storage, called virtual vectors, which allow us to measure and remove the errors in spread estimation. We perform experiments on real Internet traces to verify the effectiveness of the new estimator.},
keywords={Internet;invasive software;resource allocation;telecommunication network routing;telecommunication security;virtual storage;worm spread estimator;software/hardware module;network router;port scan detection;DDoS attack;resource allocation;server farm;Web content caching;small SRAM memory;high-speed network;virtual vector data storage;Internet trace;Random access memory;Internet;Hardware;Computer crime;Resource management;Network servers;High-speed networks;Monitoring;Web server;Communications Society},
doi={10.1109/INFCOM.2009.5061956},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061957,
author={F. Hao and M. Kodialam and T. V. Lakshman and H. Song},
booktitle={IEEE INFOCOM 2009},
title={Fast Multiset Membership Testing Using Combinatorial Bloom Filters},
year={2009},
volume={},
number={},
pages={513-521},
abstract={In this paper we consider the problem of designing a data structure that can perform fast multiset membership testing in deterministic time. Our primary goal is to develop a hardware implementation of the data structure which uses only embedded memory blocks. Prior efforts to solve this problem involve hashing into multiple bloom filters. Such approach needs a priori knowledge of the number of elements in each set in order to size the bloom filter. We use a single bloom filter based approach and use multiple sets of hash functions to code for the set (group) id. Since a single bloom filter is used, it does not need a priori knowledge of the distribution of the elements across the different sets. We show how to improve the performance of the data structure by using constant weight error correcting codes for coding the group id. Using error correcting codes improves the performance of these data structures especially when there are large number of sets. We also outline an efficient hardware based approach to generate the the large number of hash functions that we need for this data structure. The resulting data structure, COMB, is amenable to a variety of time-critical network applications.},
keywords={cryptography;data structures;error correction codes;information filtering;fast dynamic multiset membership testing;combinatorial bloom filters;data structure;hash functions;constant weight error correcting codes;efficient hardware based approach;time-critical network applications;Testing;Filters;Data structures;Table lookup;Hardware;Pattern matching;Switches;Error correction codes;Energy consumption;Throughput},
doi={10.1109/INFCOM.2009.5061957},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061958,
author={Y. Lu and B. Prabhakar},
booktitle={IEEE INFOCOM 2009},
title={Robust Counting Via Counter Braids: An Error-Resilient Network Measurement Architecture},
year={2009},
volume={},
number={},
pages={522-530},
abstract={A novel counter architecture, called counter braids, has recently been proposed for accurate per-flow measurement on high-speed links. Inspired by sparse random graph codes, counter braids solves two central problems of per-flow measurement: one-to-one flow-to-counter association and large amount of unused counter space. It eliminates the one-to-one association by randomly hashing a flow label to multiple counters and minimizes counter space by incrementally compressing counts as they accumulate. The random hash values are reproduced offline from a list of flow labels, with which flow sizes are decoded using a fast message passing algorithm. The decoding of counter braids introduces the problem of collecting flow labels active in a measurement epoch. An exact solution to this problem is expensive. This paper complements the previous proposal with an approximate flow label collection scheme and a novel error-resilient decoder that decodes despite missing flow labels. The approximate flow label collection detects new flows with variable-length signature counting Bloom filters in SRAM, and stores flow labels in high-density DRAM. It provides a good trade-off between space and accuracy: more than 99 percent of the flows are captured with very little SRAM space. The decoding challenge posed by missing flow labels calls for a new algorithm as the original message passing decoder becomes error-prone. In terms of sparse random graph codes, the problem is equivalent to decoding with graph deficiency, a scenario beyond coding theory. The error-resilient decoder employs a new message passing algorithm that recovers most flow sizes exactly despite graph deficiency. Together, our solution achieves a 10-fold reduction in SRAM space compared to hash-table based implementations, as demonstrated with Internet trace evaluations.},
keywords={decoding;DRAM chips;filters;graph theory;message passing;SRAM chips;variable length codes;robust counting;counter braids;error-resilient network measurement;high-speed links;sparse random graph codes;per-flow measurement;one-to-one flow-to-counter association;randomly hashing;message passing algorithm;flow label collection scheme;error-resilient decoder;variable-length signature;Bloom filters;SRAM;DRAM;graph deficiency;scenario beyond coding theory;Internet trace evaluations;Robustness;Counting circuits;Decoding;Random access memory;Message passing;Extraterrestrial measurements;Fluid flow measurement;Proposals;Filters;Codes},
doi={10.1109/INFCOM.2009.5061958},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061959,
author={Y. Zhao and Z. Zhu and Y. Chen and D. Pei and J. Wang},
booktitle={IEEE INFOCOM 2009},
title={Towards Efficient Large-Scale VPN Monitoring and Diagnosis under Operational Constraints},
year={2009},
volume={},
number={},
pages={531-539},
abstract={Continuous monitoring and diagnosis of network performance are of crucial importance for the Internet access service and virtual private network (VPN) service providers. Various operational constraints, which are crucial to the practice, are largely ignored in previous monitoring system designs, or are simply replaced with load balancing problems which do not work for real heterogeneous networks. Given these real-world challenges, in this paper, we design a VScope monitoring system with the following contributions. First, we design a greedy-assisted linear programming algorithm to select as few monitors as possible that can monitor the whole network under the operational constraints. Secondly, VScope takes a multi-round measurement approach to further reduce monitors deployment/management cost, by scheduling the path measurements in different rounds under the operational constraints. Evaluations based on several real VPN topologies from a tier-1 ISP as well as some other synthetic topologies demonstrate that VScope is promising to solve the aforementioned challenges.},
keywords={greedy algorithms;Internet;linear programming;monitoring;scheduling;virtual private networks;virtual private network;VPN operational constraint;continuous network performance monitoring;network performance diagnosis;Internet access service;VScope monitoring system;greedy-assisted linear programming algorithm;path measurement scheduling;Large-scale systems;Virtual private networks;Monitoring;Network topology;IP networks;Web and internet services;Load management;Algorithm design and analysis;Linear programming;Costs},
doi={10.1109/INFCOM.2009.5061959},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061960,
author={D. DiPalantino and R. Johari},
booktitle={IEEE INFOCOM 2009},
title={Traffic Engineering vs. Content Distribution: A Game Theoretic Perspective},
year={2009},
volume={},
number={},
pages={540-548},
abstract={In this paper we explore the interaction between content distribution and traffic engineering. Because a traffic engineer may be unaware of the structure of content distribution systems or overlay networks, this management of the network does not fully anticipate how traffic might change as a result of his actions. Content distribution systems that assign servers at the application level can respond very rapidly to changes in the routing of the network. Consequently, the traffic engineer's decisions may almost never be applied to the intended traffic. We use a game-theoretic framework in which infinitesimal users of a network select the source of content, and the traffic engineer decides how the traffic will route through the network. We formulate a game and prove the existence of equilibria. Additionally, we present a setting in which equilibria are socially optimal, essentially unique, and stable. Conditions under which efficiency loss may be bounded are presented, and the results are extended to the cases of general overlay networks and multiple autonomous systems.},
keywords={game theory;Internet;telecommunication network routing;telecommunication traffic;network traffic engineering;content distribution system;game theoretic perspective;overlay network routing;ISP;Internet service provider;Game theory;Telecommunication traffic;Routing;Traffic control;Network servers;Communication system traffic control;Content management;Engineering management;Signal generators;Protocols},
doi={10.1109/INFCOM.2009.5061960},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061961,
author={R. Sami and M. Schapira and A. Zohar},
booktitle={IEEE INFOCOM 2009},
title={Searching for Stability in Interdomain Routing},
year={2009},
volume={},
number={},
pages={549-557},
abstract={The border gateway protocol (BGP) handles the task of establishing routes between the autonomous systems (ASes) that make up the Internet. It is known that it is possible for a group of ASes to define local BGP policies that lead to global BGP protocol oscillations. We close a long standing open question by showing that, for any network, if two stable routing outcomes exist then persistent BGP route oscillations are possible. This is the first non-trivial necessary condition for BGP safety. It shows that BGP safety must always come at the price of severe restrictions on ASes' expressiveness in their choice of routing policies. The technical tools used in our proof may be helpful in the detection of potential route oscillations and their debugging. We also address the question of how long it takes BGP to converge to a stable routing outcome. We analyze a formal measure of the convergence time of BGP for the policy class defined by Gao and Rexford, which is said to accurately depict the business structure underlying the Internet. We prove that, even for this restricted class of preferences, the convergence time might be linear in the size of the network. However, we show a much more reasonable bound if the network structure is similar to the current Internet: we prove that the number of phases required for convergence is bounded by approximately twice the depth of the customer-provider hierarchy.},
keywords={Internet;internetworking;routing protocols;stability;interdomain routing stability;border gateway protocol;autonomous systems;Internet;network structure;customer-provider hierarchy;Stability;Safety;Internet;Convergence;Computer science;Routing protocols;Debugging;Sufficient conditions;Communications Society;USA Councils},
doi={10.1109/INFCOM.2009.5061961},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061962,
author={S. Misra and G. Xue and D. Yang},
booktitle={IEEE INFOCOM 2009},
title={Polynomial Time Approximations for Multi-Path Routing with Bandwidth and Delay Constraints},
year={2009},
volume={},
number={},
pages={558-566},
abstract={In this paper, we study the problem of multi-path routing with bandwidth and delay constraints, which arises in applications for video delivery over bandwidth limited networks. Assume that each link in the network has a bandwidth and a delay. For a given source-destination pair and a bandwidth requirement, we want to find a set of source to destination paths such that the delay of the longest path is minimized while the aggregated bandwidth of the set of paths meets the bandwidth requirement. This problem is NP-hard, and the state of the art is a maximum flow based heuristic. We first construct a class of examples showing that the maximum flow based heuristic could have very bad performance. We then present a fully polynomial time approximation scheme that can compute a (1 + epsiv) -approximation with running time bounded by a polynomial in 1/epsiv and the input size of the instance. Given the NP-hardness of the problem, our approximation scheme is the best possible. We also present numerical results confirming the advantage of our scheme over the current state of the art.},
keywords={bandwidth allocation;communication complexity;Internet;telecommunication network routing;video communication;polynomial time approximation;multipath routing;bandwidth limited network;delay constraint;video delivery;NP-hard;maximum flow based heuristic;Internet;source-to-destination path;Polynomials;Routing;Bandwidth;Delay effects;Quality of service;Video on demand;Streaming media;Communications Society;Time factors;Internet},
doi={10.1109/INFCOM.2009.5061962},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061963,
author={A. Majumder and N. Shrivastava and R. Rastogi and A. Srinivasan},
booktitle={IEEE INFOCOM 2009},
title={Scalable Content-Based Routing in Pub/Sub Systems},
year={2009},
volume={},
number={},
pages={567-575},
abstract={In this paper, we develop a framework for achieving scalable and communication-efficient dissemination of content in pub/sub systems. To maximize communication sharing across subscriptions, our routing framework groups subscriptions based on similarity, and transmits content matching one or more subscriptions in a group over a single dissemination tree for the group. We develop a cost model that uses published content samples in conjunction with the knowledge of consumer subscriptions to estimate the communication cost of a set of routing trees for subscription groups. The problem of computing a communication-optimal set of routing trees is then formulated as an optimization problem that seeks to find trees with the minimum cost. It turns out that the problem of computing a minimum-cost tree for a subscription group is a new generalization of the well-known Steiner tree problem, and an interesting problem in its own right. We develop an approximation algorithm that uses low-stretch spanning trees to compute a tree whose communication cost is within a polylogarithmic factor of the optimum. We use this to compute trees for various subscription- grouping configurations generated using a greedy clustering strategy, and select the one with the lowest cost. Our experimental study demonstrates the effectiveness of our content-aware routing approach compared to traditional routing based on content oblivious spanning trees.},
keywords={approximation theory;greedy algorithms;message passing;middleware;tree data structures;scalable content-based routing;pub/sub systems;communication sharing;content matching;dissemination tree;routing trees;approximation algorithm;greedy clustering strategy;content-aware routing;Routing;Subscriptions;Uniform resource locators;Network servers;Cost function;Communications Society;Prototypes;IP networks;Peer to peer computing;Scalability},
doi={10.1109/INFCOM.2009.5061963},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061964,
author={S. Ioannidis and P. Marbach},
booktitle={IEEE INFOCOM 2009},
title={Absence of Evidence as Evidence of Absence: A Simple Mechanism for Scalable P2P Search},
year={2009},
volume={},
number={},
pages={576-584},
abstract={We propose a novel search mechanism for unstructured p2p networks, and show that it is both scalable, i.e., it leads to a bounded query traffic load per peer as the peer population grows, and reliable, i.e., it successfully locates all files that are (sufficiently often) brought into the system. To the best of our knowledge, this is the first time that a search mechanism for unstructured p2p networks has been shown to be both scalable and reliable. We provide both a formal analysis and a numerical case study to illustrate this result. Our analysis is based on a random graph model for the overlay graph topology and uses a mean-field approximation to characterize the evolution of how files are replicated in the network.},
keywords={peer-to-peer computing;P2P search;bounded query traffic load;p2p networks;overlay graph topology;mean-field approximation;Telecommunication traffic;Peer to peer computing;Traffic control;Communications Society;Network topology},
doi={10.1109/INFCOM.2009.5061964},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061965,
author={A. L. H. Chow and L. Golubchik and V. Misra},
booktitle={IEEE INFOCOM 2009},
title={BitTorrent: An Extensible Heterogeneous Model},
year={2009},
volume={},
number={},
pages={585-593},
abstract={Peer-to-peer (P2P) systems in general, and BitTorrent (BT) specifically, have been of significant interest to researchers and Internet users alike. Existing models of BT abstract away certain characteristics of the protocol that are important, which we address in this work. We present a simple yet accurate and easily extensible model of BT. The model's accuracy is validated through a rigorous simulation-based study and its extensibility is illustrated by incorporating recently proposed approaches to protocol changes in BT.},
keywords={Internet;peer-to-peer computing;BitTorrent;extensible heterogeneous model;peer-to-peer systems;Internet;Peer to peer computing;Steady-state;Internet;Thin film transistors;Protocols;Bandwidth;Communications Society;Context modeling;Robustness;Ecosystems},
doi={10.1109/INFCOM.2009.5061965},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061966,
author={G. M. Ezovski and A. Tang and L. L. H. Andrew},
booktitle={IEEE INFOCOM 2009},
title={Minimizing Average Finish Time in P2P Networks},
year={2009},
volume={},
number={},
pages={594-602},
abstract={Peer-to-peer (P2P) file distribution is a scalable way to disseminate content to a wide audience. For a P2P network, one fundamental performance metric is the average time needed to deliver a certain file to all peers, which in general depends on the topology of the network and the scheduling of transmissions. Despite its apparent importance, how to minimize average finish time remains an open question even for a fully- connected network. This is mainly due to the analytical challenges that come with the combinatorial structures of the problem. In this paper, by using the water-filling technique, we determine how each peer should use its capacity to sequentially minimize the file download times in an upload-constrained P2P network. Furthermore, it is argued that this scheduling also potentially minimizes average finish time for the network. This result not only provides fundamental insight to scheduling in such P2P systems, but also can serve as a benchmark to evaluate practical algorithms and illustrate the scalability of P2P networks.},
keywords={minimisation;peer-to-peer computing;scheduling;telecommunication network topology;peer-to-peer network topology;average finish time minimization;peer-to-peer file distribution;transmission scheduling;combinatorial structure;water-filling technique;Peer to peer computing;Optimal scheduling;Scheduling algorithm;Scalability;IP networks;Algorithm design and analysis;Communications Society;Computer networks;Distributed computing;Measurement},
doi={10.1109/INFCOM.2009.5061966},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061967,
author={X. Xiao and Y. Shi and Y. Gao and Q. Zhang},
booktitle={IEEE INFOCOM 2009},
title={LayerP2P: A New Data Scheduling Approach for Layered Streaming in Heterogeneous Networks},
year={2009},
volume={},
number={},
pages={603-611},
abstract={Although layered streaming in heterogeneous peer-to-peer networks has drawn great interest in recent years, there's still a lack of systematical studies on its data scheduling issue. In this paper, we propose a new scheduling approach for layered video streaming, called LayerP2P. The key idea and main contributions of LayerP2P come in two-fold: 1) According to the characteristics caused by layered coding, we propose four objectives that should be achieved by data scheduling: high throughput, high layer delivery ratio, low useless packets ratio, and low subscription jitter; 2) We design a 3-stage scheduling mechanism to request absent blocks, where the min-cost flow model, probability decision mechanism and multi-window remedy mechanism are employed in Free Stage, Decision Stage and Remedy Stage, respectively. Each stage has different scheduling objective while collaborates with each other, to achieve the above four objectives. Experimental results indicate that our approach outperforms other schemes in simulation environment. Besides, LayerP2P is implemented in the PDEPS Project in China, which is expected to be the first practical layered streaming system for education in peer-to-peer networks.},
keywords={data compression;decision theory;peer-to-peer computing;probability;scheduling;video coding;video streaming;layerP2P-layered video streaming;data scheduling approach;heterogeneous peer-to-peer network;layered videocoding;min-cost flow model;probability decision mechanism;multiwindow remedy mechanism;video compression;Streaming media;Throughput;Peer to peer computing;Bandwidth;Processor scheduling;Jitter;Delay;Computer science;Encoding;Video compression},
doi={10.1109/INFCOM.2009.5061967},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061968,
author={J. Kim and X. Lin and N. Shroff},
booktitle={IEEE INFOCOM 2009},
title={Optimal Anycast Technique for Delay-Sensitive Energy-Constrained Asynchronous Sensor Networks},
year={2009},
volume={},
number={},
pages={612-620},
abstract={In wireless sensor networks, asynchronous sleep-wake scheduling protocols can significantly reduce energy consumption without incurring the communication overhead for clock synchronization used in typical sleep-wake scheduling protocols. However, the savings could come at a significant cost in delay performance. Recently, researchers have attempted to exploit the inherent broadcast nature of the wireless medium to reduce this delay with virtually no additional energy cost. These schemes are called "anycasting," where each sensor node forwards the packet to the first node that wakes up among a set of candidate next-hop nodes. In this paper, we develop a delay-optimal anycasting scheme under periodic sleep-wake patterns. Our solution is computationally simple and fully distributed. We show that periodic sleep-wake patterns result in the smallest delay among all wake-up patterns under given energy constraints. Simulation results illustrate the benefit of our proposed schemes over the state-of-the art.},
keywords={delays;protocols;scheduling;synchronisation;wireless sensor networks;delay-sensitive energy-constrained asynchronous sensor networks;optimal anycast technique;wireless sensor networks;asynchronous sleep-wake scheduling protocols;energy consumption;communication overhead;clock synchronization;sensor node;delay-optimal anycasting scheme;Delay;Wireless sensor networks;Wireless application protocol;Costs;Energy consumption;Clocks;Synchronization;Broadcasting;Distributed computing;Computational modeling},
doi={10.1109/INFCOM.2009.5061968},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061969,
author={M. Z. Siam and M. Krunz and O. Younis},
booktitle={IEEE INFOCOM 2009},
title={Energy-Efficient Clustering/Routing for Cooperative MIMO Operation in Sensor Networks},
year={2009},
volume={},
number={},
pages={621-629},
abstract={Employing multi-input multi-output (MIMO) links can improve energy efficiency in wireless sensor networks (WSNs). Although a sensor node is likely to be equipped with only one antenna, it is possible to group several sensors to form a virtual MIMO link. Such grouping can be formed by means of clustering. In this paper, we propose a distributed MIMO-adaptive energy-efficient clustering/routing scheme, coined cooperative MIMO (CMIMO), which aims at reducing energy consumption in multi- hop WSNs. In CMIMO, each cluster has up to two cluster heads (CHs), which are responsible for routing traffic between clusters (i.e., inter-cluster communications). CMIMO has the ability to adapt the transmission mode and transmission power on a per-packet basis. The transmission mode can be one of four transmit/receive configurations: 1 times 1 (SISO), 2 times 1 (MISO), 1 times 2 (SIMO), and 2 times 2 (MIMO). We study the performance of CMIMO via simulations. Results indicate that our proposed scheme achieves a significant reduction in energy consumption, compared to non-adaptive clustered WSNs.},
keywords={MIMO communication;telecommunication network routing;wireless sensor networks;energy-efficient clustering/routing;cooperative MIMO operation;sensor networks;multi-input multi-output links;wireless sensor networks;energy consumption;Energy efficiency;Routing;MIMO;Wireless sensor networks;Energy consumption;Circuits;Diversity methods;Bit error rate;Fading;Spread spectrum communication},
doi={10.1109/INFCOM.2009.5061969},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061970,
author={J. Ma and W. Lou and Y. Wu and X. -. Li and G. Chen},
booktitle={IEEE INFOCOM 2009},
title={Energy Efficient TDMA Sleep Scheduling in Wireless Sensor Networks},
year={2009},
volume={},
number={},
pages={630-638},
abstract={Sleep scheduling is a widely used mechanism in wireless sensor networks (WSNs) to reduce the energy consumption since it can save the energy wastage caused by the idle listening state. In a traditional sleep scheduling, however, sensors have to start up numerous times in a period, and thus consume extra energy due to the state transitions. The objective of this paper is to design an energy efficient sleep scheduling for low data-rate WSNs, where sensors not only consume different amounts of energy in different states (transmit, receive, idle and sleep), but also consume energy for state transitions. We use TDMA as the MAC layer protocol, because it has the advantages of avoiding collisions, idle listening and overhearing. We first propose a novel interference-free TDMA sleep scheduling problem called contiguous link scheduling, which assigns sensors with consecutive time slots to reduce the frequency of state transitions. To tackle this problem, we then present efficient centralized and distributed algorithms that use time slots at most a constant factor of the optimum. The simulation studies corroborate the theoretical results, and show the efficiency of our proposed algorithms.},
keywords={access protocols;distributed algorithms;electromagnetic wave interference;scheduling;time division multiple access;wireless sensor networks;time division multiple access;TDMA sleep scheduling;wireless sensor network;energy consumption;MAC layer protocol;contiguous link scheduling;distributed algorithm;Energy efficiency;Time division multiple access;Sleep;Wireless sensor networks;Interference;Energy consumption;Delay;Processor scheduling;Media Access Protocol;Frequency},
doi={10.1109/INFCOM.2009.5061970},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061971,
author={R. K. Panta and S. Bagchi},
booktitle={IEEE INFOCOM 2009},
title={Hermes: Fast and Energy Efficient Incremental Code Updates for Wireless Sensor Networks},
year={2009},
volume={},
number={},
pages={639-647},
abstract={Wireless reprogramming of sensor nodes is a requirement for long-lived networks due to changes in the functionality of the software running on the nodes. The amount of information that needs to be wirelessly transmitted during reprogramming should be minimized to reduce reprogramming time and energy. In this paper, we present a multi-hop incremental reprogramming protocol called Hermes that transfers over the network the delta between the old and new software and lets the sensor nodes rebuild the new software using the received delta and the old software. It reduces the delta by using techniques to mitigate the effects of function and global variable shifts caused by the software modifications. Then it compares the binary images at the byte level with a method to create small delta. For a wide range of software change scenarios that we experimented with, we find that Hermes transfers up to 201 times less information than Deluge, the standard reprogramming protocol for TinyOS and 64 times less than an existing incremental reprogramming protocol by Jeong and Culler.},
keywords={programming;protocols;telecommunication computing;wireless sensor networks;wireless sensor network;energy efficient incremental code;Hermes-multihop incremental reprogramming protocol;software reprogramming;Energy efficiency;Wireless sensor networks;Peer to peer computing;Joining processes;Protocols;Delay;Computer networks;Operating systems;Clocks;Energy consumption},
doi={10.1109/INFCOM.2009.5061971},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061972,
author={Y. Qi and L. Xu and B. Yang and Y. Xue and J. Li},
booktitle={IEEE INFOCOM 2009},
title={Packet Classification Algorithms: From Theory to Practice},
year={2009},
volume={},
number={},
pages={648-656},
abstract={During the past decade, the packet classification problem has been widely studied to accelerate network applications such as access control, traffic engineering and intrusion detection. In our research, we found that although a great number of packet classification algorithms have been proposed in recent years, unfortunately most of them stagnate in mathematical analysis or software simulation stages and few of them have been implemented in commercial products as a generic solution. To fill the gap between theory and practice, in this paper, we propose a novel packet classification algorithm named HyperSplit. Compared to the well-known HiCuts and HSM algorithms, HyperSplit achieves superior performance in terms of classification speed, memory usage and preprocessing time. The practicability of the proposed algorithm is manifested by two facts in our test: HyperSplit is the only algorithm that can successfully handle all the rule sets; HyperSplit is also the only algorithm that reaches more than 6Gbps throughput on the Octeon3860 multi-core platform when tested with 64-byte Ethernet packets against 10K ACL rules.},
keywords={Internet;pattern classification;security of data;packet classification algorithms;mathematical analysis;software simulation;HyperSplit;HiCuts;HSM algorithms;classification speed;memory usage;preprocessing time;Classification algorithms;Testing;Acceleration;Application software;Access control;Communication system traffic control;Traffic control;Intrusion detection;Software algorithms;Mathematical analysis},
doi={10.1109/INFCOM.2009.5061972},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061973,
author={K. Pelechrinis and G. Yan and S. Eidenbenz and S. V. Krishnamurthy},
booktitle={IEEE INFOCOM 2009},
title={Detecting Selfish Exploitation of Carrier Sensing in 802.11 Networks},
year={2009},
volume={},
number={},
pages={657-665},
abstract={Recently, tuning the clear channel assessment (CCA) threshold in conjunction with power control has been considered for improving the performance of Wireless LANs. However, CCA tuning can be exploited by selfish nodes in order to obtain an unfair share of the available bandwidth. In particular, by increasing the CCA threshold, a selfish client can manipulate the carrier sensing mechanism to ignore the presence of other transmissions on the medium; consequently, it increases the probability of accessing the medium and therefore obtains a higher, unfair share of the available bandwidth. In this paper, we propose a novel approach to detect this misbehavior in WLANs. A key insight that leads to our approach is that a misbehaving node that has increased its CCA is unlikely to recognize low power receptions as legitimate packets; by intelligently sending low power probe messages, an AP can detect a misbehaving node with high probability. In a nutshell, our contributions are as follows: (a) We are the first to quantify the impact of selfish CCA tuning via extensive experimentation (b) We propose a novel lightweight scheme for detecting selfish nodes that inappropriately increase their CCA thresholds; we call our scheme CMD (for carrier sensing misbehavior detection) (c) We perform extensive evaluations on an indoor 802.11 WLAN testbed to demonstrate that CMD detects misbehaving users with very high accuracy (approximately 95 % of the time). Furthermore, it only incurs a false positive rate of less than 5 %.},
keywords={bandwidth allocation;probability;wireless channels;wireless LAN;carrier sensing selfish exploitation detection;802.11 network;clear channel assessment threshold;power control;wireless LAN performance;available bandwidth;probability;carrier sensing misbehavior detection;Probes;Throughput;Wireless LAN;Peer to peer computing;Bandwidth;Performance evaluation;Decoding;Communications Society;Laboratories;Power control},
doi={10.1109/INFCOM.2009.5061973},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061974,
author={J. Yang and Y. Chen and W. Trappe and J. Cheng},
booktitle={IEEE INFOCOM 2009},
title={Determining the Number of Attackers and Localizing Multiple Adversaries in Wireless Spoofing Attacks},
year={2009},
volume={},
number={},
pages={666-674},
abstract={Wireless spoofing attacks are easy to launch and can significantly impact the performance of networks. Although the identity of a node can be verified through cryptographic authentication, conventional security approaches are not always desirable because of their overhead requirements. In this paper, we propose to use location information, a physical property associated with each node, hard to falsify, and not reliant on cryptography, as the basis for (1) detecting spoofing attacks; (2) determining the number of attackers when multiple adversaries masquerading as a same node identity; and (3) localizing multiple adversaries. We formulate the problem of determining the number of attackers as a multi-class detection problem. We first propose two cluster-based mechanisms to determine the number of attackers. We then develop SILENCE that employs the minimum distance testing of RSS values in addition to cluster analysis and can achieve better accuracy than other methods under study that merely use cluster analysis alone. We further developed an integrated detection and localization system that can localize the positions of multiple attackers. We evaluated our techniques through two testbeds using both an 802.11 (WiFi) network and an 802.15.4 (ZigBee) network in two real office buildings. Our experimental results show that SILENCE can achieve over 90% Hit Rate and Precision when determining the number of attackers. Additionally, our localization results using a representative set of algorithms provide strong evidence of high accuracy of localizing multiple adversaries.},
keywords={cryptography;pattern clustering;radio networks;telecommunication security;wireless spoofing attacks;cryptographic authentication;multiclass detection problem;cluster analysis;localization system;ZigBee network;WiFi network;Cryptography;Peer to peer computing;Testing;Communications Society;Statistics;Collaboration;Computer crime;Costs;Signal detection;Signal analysis},
doi={10.1109/INFCOM.2009.5061974},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061975,
author={S. Liu and Y. Chen and W. Trappe and L. J. Greenstein},
booktitle={IEEE INFOCOM 2009},
title={ALDO: An Anomaly Detection Framework for Dynamic Spectrum Access Networks},
year={2009},
volume={},
number={},
pages={675-683},
abstract={Dynamic spectrum access has been proposed as a means to share scarce radio resources, and requires devices to follow protocols that use resources in a proper, disciplined manner. For a cognitive radio network to achieve this goal, spectrum policies and the ability to enforce them are necessary. Detection of an unauthorized (anomalous) usage is one of the critical issues in spectrum etiquette enforcement. In this paper, we present a network structure for dynamic spectrum access and formulate the anomalous usage detection problem using statistical significance testing. The detection problem is classified into two subproblems. For the case where no authorized signal is present, we describe the existing cooperative sensing schemes and investigate the impact of signal path loss on their performance. For the case where an authorized signal is present, we propose three methods that detect anomalous transmissions by making use of the characteristics of radio propagation. Analytical models are formulated for two special cases and, due to the intractability of the general problem, we present an algorithm using machine learning techniques to solve the general case. Our simulation results show that our approaches can effectively detect unauthorized spectrum usage with high detection rate and low false positive rate.},
keywords={authorisation;cognitive radio;learning (artificial intelligence);protocols;radio access networks;radiowave propagation;statistical testing;telecommunication computing;telecommunication security;anomaly detection framework;dynamic spectrum access network;scarce radio resource sharing;protocol;cognitive radio network;statistical significance testing;machine learning;authorized transmitter;cooperative sensing scheme;radio propagation;Radio transmitters;Power measurement;Access protocols;Chromium;Radio propagation;Signal detection;Cognitive radio;Signal processing;Communications Society;TV},
doi={10.1109/INFCOM.2009.5061975},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061976,
author={D. Nechay and Y. Pointurier and M. Coates},
booktitle={IEEE INFOCOM 2009},
title={Controlling False Alarm/Discovery Rates in Online Internet Traffic Flow Classification},
year={2009},
volume={},
number={},
pages={684-692},
abstract={Existing Internet traffic classification techniques achieve impressively low misclassification rates, but do not provide performance guarantees for particular classes of interest. In this paper, we propose two novel online traffic classifiers - one based on Neyman-Pearson classification and one based on the Learning Satisfiability (LSAT) framework - that can provide class-specific performance guarantees on the false alarm and false discovery rates, respectively. We also present a preprocessor for our classifiers that predicts, after the reception of only a small number of packets, whether a flow will be 'large' (as defined by a network operator). Only these resource-intensive flows are passed to the classifier, greatly reducing the computation burden imposed. We validate our methodology by testing our approaches using traffic data provided by an ISP.},
keywords={Internet;pattern classification;quality of service;telecommunication traffic;online Internet traffic flow classification;Neyman-Pearson classification;learning satisfiability framework;false discovery rates;false alarm rates;quality-of-service;Internet;Telecommunication traffic;Communication system traffic control;Peer to peer computing;Statistics;Measurement;Communications Society;Testing;Quality of service;Teleconferencing},
doi={10.1109/INFCOM.2009.5061976},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061977,
author={D. Choffnes and F. E. Bustamante},
booktitle={IEEE INFOCOM 2009},
title={On the Effectiveness of Measurement Reuse for Performance-Based Detouring},
year={2009},
volume={},
number={},
pages={693-701},
abstract={For both technological and economic reasons, the default path between two end systems in the wide-area Internet can be suboptimal. This observation has motivated a number of systems that attempt to improve reliability and performance by routing over one or more hops in an overlay. Most of the proposed solutions, however, fall at an extreme in the cost-performance trade-off. While some provide near-optimal performance with an un-scalable measurement overhead, others avoid measurement when selecting routes around network failures but make no attempt to optimize performance. This paper presents an experimental evaluation of an alternative approach to scalable, performance detouring based on the strategic reuse of measurements from other large distributed systems, namely content distribution networks (CDNs). By relying on CDN redirections as hints on network conditions, higher performance paths are readily found with little overhead and no active network measurement. We report results from a study of more than 13,700 paths between 170 widely-distributed hosts over a three-week period, showing the benefits of this approach. We demonstrate that it is practical by implementing an FTP suite that uses our publicly available Side Step library to take advantage of these alternative Internet routes.},
keywords={computer network reliability;Internet;telecommunication network routing;measurement reuse;performance-based detouring;wide-area Internet;reliability;routing;network failures;large distributed systems;content distribution networks;SideStep library;Performance evaluation;Internet;Routing;Libraries;Network servers;Communications Society;Electric variables measurement;Computer science;Computerized monitoring;Delay},
doi={10.1109/INFCOM.2009.5061977},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061978,
author={K. -. Chen and C. -. Tu and W. -. Xiao},
booktitle={IEEE INFOCOM 2009},
title={OneClick: A Framework for Measuring Network Quality of Experience},
year={2009},
volume={},
number={},
pages={702-710},
abstract={As the service requirements of network applications shift from high throughput to high media quality, interactivity, and responsiveness, the definition of QoE (Quality of Experience) has become multidimensional. Although it may not be difficult to measure individual dimensions of the QoE, how to capture users' overall perceptions when they are using network applications remains an open question. In this paper, we propose a framework called OneClick to capture users' perceptions when they are using network applications. The framework only requires a subject to click a dedicated key whenever he/she feels dissatisfied with the quality of the application in use. OneClick is particularly effective because it is intuitive, lightweight, efficient, time-aware, and application-independent. We use two objective quality assessment methods, PESQ and VQM, to validate OneClick's ability to evaluate the quality of audio and video clips. To demonstrate the proposed framework's efficiency and effectiveness in assessing user experiences, we implement it on two applications, one for instant messaging applications, and the other for first- person shooter games. A Flash implementation of the proposed framework is also presented.},
keywords={computer games;electronic messaging;quality of service;OneClick;network experience quality;high throughput;high media quality;interactivity;responsiveness;QoE;quality of experience;objective quality assessment methods;PESQ;VQM;instant messaging applications;first-person shooter games;Flash;perceptual evaluation of speech quality model;video quality measurement model;Testing;Delay;Throughput;Multidimensional systems;Feedback;Communications Society;Information science;Computer science;Application software;Quality assessment},
doi={10.1109/INFCOM.2009.5061978},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061979,
author={P. Borgnat and G. Dewaele and K. Fukuda and P. Abry and K. Cho},
booktitle={IEEE INFOCOM 2009},
title={Seven Years and One Day: Sketching the Evolution of Internet Traffic},
year={2009},
volume={},
number={},
pages={711-719},
abstract={This contribution aims at performing a longitudinal study of the evolution of the traffic collected every day for seven years on a trans-Pacific backbone link (the MAWI dataset). Long term characteristics are investigated both at TCP/IP layers (packet and flow attributes) and application usages. The analysis of this unique dataset provides new insights into changes in traffic statistics, notably on the persistence of Long Range Dependence, induced by the on-going increase in link bandwidth. Traffic in the MAWI dataset is subject to bandwidth changes, to congestions, and to a variety of anomalies. This allows the comparison of their impacts on the traffic statistics but at the same time significantly impairs long term evolution characterizations. To account for this difficulty, we show and explain how and why random projection (sketch) based analysis procedures provide practitioners with an efficient and robust tool to disentangle actual long term evolutions from time localized events such as anomalies and link congestions. Our central results consist in showing a strong and persistent long range dependence controlling jointly byte and packet counts. An additional study of a 24-hour trace complements the long-term results with the analysis of intraday variabilities.},
keywords={estimation theory;Internet;telecommunication traffic;transport protocols;Internet traffic;TCP/IP layers;longitudinal study;long range dependence;MAWI dataset;robust estimation;Internet;Robustness;Statistics;Statistical analysis;Bandwidth;Long Term Evolution;Traffic control;Telecommunication traffic;Spine;TCPIP},
doi={10.1109/INFCOM.2009.5061979},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061980,
author={D. -. Lee and K. -. Hsu and C. -. Chang and J. Cheng},
booktitle={IEEE INFOCOM 2009},
title={Emulation and Approximation of a Flexible Delay Line by Parallel Non-Overtaking Delay Lines},
year={2009},
volume={},
number={},
pages={720-728},
abstract={In this paper we propose to construct an flexible delay line with maximum delay d by parallel non-overtaking delay lines. We show that for a fixed number of non-overtaking delay lines, an optimal policy to minimize packet losses is to assign arriving packets to the non-overtaking delay line that has the largest residual service time while maintaining the FIFO order for each non-overtaking delay lines. Based on this optimal policy we show that to exactly emulate an flexible delay line, one needs [(d + 1)/2] non-overtaking delay lines. We also show that if one can tolerate a small packet loss probability, one just needs O(radic(d)) non-overtaking delay lines. In this case, we show that the residual service times of the non-overtaking delay lines behaved as if they followed the order statistics of uniform random variables.},
keywords={optical delay lines;optical fibre networks;probability;random processes;statistical analysis;parallel nonovertaking delay line;optimal policy;minimize packet loss probability;residual service time;FIFO order;uniform random variable;statistical analysis;optical fibre network;Delay lines;Emulation;Optical buffering;Optical switches;Optical packet switching;Statistics;Routing;Probability;Random variables;Optical fiber networks},
doi={10.1109/INFCOM.2009.5061980},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061981,
author={Y. Kanizo and D. Hay and I. Keslassy},
booktitle={IEEE INFOCOM 2009},
title={The Crosspoint-Queued Switch},
year={2009},
volume={},
number={},
pages={729-737},
abstract={This paper calls for rethinking packet-switch architectures by cutting all dependencies between the switch fabric and the linecards. Most single-stage packet-switch architectures rely on an instantaneous communication between the switch fabric and the linecards. Today, however, this assumption is breaking down, because effective propagation times are too high and keep increasing with the line rates. In this paper, we argue for a self-sufficient switch fabric by moving all the buffering from the linecards to the switch fabric. We introduce the crosspoint-queued (CQ) switch, a new buffered-crossbar switch architecture with large crosspoint buffers and no input queues, and show how it can be readily implemented in a single SRAM-based chip using current technology. For a crosspoint buffer size of one, we provide a closed-form throughput formula for all work-conserving schedules under uniform Bernoulli i.i.d. arrivals. Furthermore, we study the performance of the switch for larger buffer sizes and show that it nearly behaves as an ideal output-queued switch. Finally, we confirm our results using synthetic as well as trace-based simulations.},
keywords={packet switching;queueing theory;SRAM chips;crosspoint-queued switch;single-stage packet-switch architectures;linecards;switch fabric;buffered- crossbar;single SRAM-based chip;uniform Bernoulli i.i.d. arrivals;Switches;Communication switching;Packet switching;Fabrics;Computer architecture;Optical propagation;Communications Society;Computer science;Throughput;Internet},
doi={10.1109/INFCOM.2009.5061981},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061982,
author={C. -. Chang and J. Cheng and D. -. Lee},
booktitle={IEEE INFOCOM 2009},
title={SDL Constructions of FIFO, LIFO and Absolute Contractors},
year={2009},
volume={},
number={},
pages={738-746},
abstract={Despite all the recent advances in the mathematical theories for constructing optical queues by optical Switches and fiber Delay Lines (SDL), there are still many problems that need to be resolved. In this paper, we tackle the following problems: (i) is it possible to construct optical queues with switches of arbitrary sizes? (ii) is there a general theory that unifies many constructions of optical queues with known packet delays? and (iii) under what conditions can a concatenation of optical queues allow overtaking? For the first problem, we propose a new class of optical memory cells that can be made by switches of arbitrary sizes. Moreover, we propose the generalized C -transform for routing packets through such optical memory cells. For the second problem, we introduce a new class of optical queues, including FIFO, LIFO and absolute contractors. We show that both linear compressors in [13] and FIFO multiplexers (with multiple inputs) in [5], [7] are special cases of contractors. An interesting finding is that overtaking can occur in LIFO and absolute contractors.},
keywords={optical delay lines;optical fibres;optical storage;optical switches;SDL constructions;optical switches;fiber delay lines;FIFO;first-in first-out multiplexers;LIFO;last-in first-out queues;absolute contractors;optical memory cells;generalized C-transform;routing packets;linear compressors;Optical packet switching;Optical buffering;Optical network units;Optical switches;Queueing analysis;Delay lines;Routing;Compressors;Multiplexing;Optical fiber networks},
doi={10.1109/INFCOM.2009.5061982},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061983,
author={C. -. Chang and Y. -. Hsu and J. Cheng and D. -. Lee},
booktitle={IEEE INFOCOM 2009},
title={A Dynamic Frame Sizing Algorithm for CICQ Switches with 100% Throughput},
year={2009},
volume={},
number={},
pages={747-755},
abstract={A Combined Input and Crosspoint Queueing (CICQ) switch is a switch that has both buffers at the crosspoints of the switch fabric and buffers at the inputs. Inspired by the fixed frame based algorithm for an input-buffered switch and the smooth scheduling algorithm for a CICQ switch, in this paper we propose using a dynamic frame sizing algorithm for a CICQ switch. It is formally shown that such a CICQ switch indeed achieves 100% throughput for certain Poisson-like traffic models. This is done without using the framed Birkhoff-von Neumann decomposition needed. Moreover, such a CICQ switch only requires a two-cell buffer at each crosspoint when there is only unicast traffic. Unlike input-buffered switches, the dynamic frame sizing algorithm also achieves 100% throughput in the setting of multicast traffic. This is done at the cost of increasing the buffer size at each crosspoint.},
keywords={computational complexity;queueing theory;scheduling;stochastic processes;switches;telecommunication traffic;dynamic frame sizing algorithm;combined input and crosspoint queueing switch;input-buffered switch;fixed frame based algorithm;scheduling algorithm;Poisson-like traffic models;framed Birkhoff-von Neumann decomposition;unicast traffic;multicast traffic;Heuristic algorithms;Switches;Throughput;Traffic control;Communication switching;Scheduling algorithm;Fabrics;Communications Society;Unicast;Multicast algorithms},
doi={10.1109/INFCOM.2009.5061983},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061984,
author={W. Rao and L. Chen and A. W. -. Fu and H. Chen and F. Zou},
booktitle={IEEE INFOCOM 2009},
title={On Efficient Content Matching in Distributed Pub/Sub Systems},
year={2009},
volume={},
number={},
pages={756-764},
abstract={The efficiency of matching structures is the key issue for content publish/subscribe systems. In this paper, we propose an efficient matching tree structure, named CobasTree, for a distributed environment. Particularly, we model a predicate in each subscription filter as an interval and published content value as a data point. The CobasTree is designed to index all subscription intervals and a matching algorithm is proposed to match the data points to these indexed intervals. Through a set of techniques including selective multicast by bounding intervals, cost model-based interval division, and CobasTree merging, CobasTree can match the published contents against subscription filters with a high efficiency. We call the whole framework including CobasTree and the associated techniques as COBAS. The performance evaluation in simulation environment and PlanetLab environment shows COBAS significantly outperforms two counterparts with low cost and fast forwarding.},
keywords={content management;information filtering;message passing;middleware;tree data structures;distributed pub/sub system;content matching;publish/subscribe system;CobasTree tree structure matching;subscription filtering;subscription interval index;cost model-based interval division;bounding interval;Matched filters;Subscriptions;Delay;Tree data structures;Costs;Communications Society;Distributed computing;Information security;Algorithm design and analysis;Multicast algorithms},
doi={10.1109/INFCOM.2009.5061984},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061985,
author={H. Ballani and P. Francis},
booktitle={IEEE INFOCOM 2009},
title={Fault Management Using the CONMan Abstraction},
year={2009},
volume={},
number={},
pages={765-773},
abstract={Fault management in networks is difficult. We argue that a major contributor to the difficulty of debugging network faults is the sheer volume of semantically anemic details exposed by protocols. Unlike past approaches that try to cope with the deluge of information exposed, in this paper we explore how to reduce and structure the management information exposed by data-plane protocols and devices to make them more amenable to fault management. To this effect, we delineate two conditions that the management interface of data-plane protocols should satisfy: it should provide a structured description of protocol reality and it should support what we call a "conservation of bytes" invariant. Based on this, we propose an architecture wherein data- plane protocols expose management information satisfying these conditions. This allows management applications to detect, localize and (possibly) resolve faults in a structured fashion. We discuss the detection of a representative set of real-world faults to illustrate our approach. We implemented these fault management features into three protocols and built a management application that uses the features to debug faults. Apart from serving as a proof of concept, this exercise indicates that our proposal does indeed simplify debugging of a large fraction of network faults.},
keywords={fault diagnosis;protocols;telecommunication network management;telecommunication network reliability;fault management;CONMan abstraction;data-plane protocols;network fault debugging;protocol reality;management information;Protocols;Counting circuits;Debugging;Information management;Fault detection;Proposals;Power system management;Humans;Impedance;Packaging},
doi={10.1109/INFCOM.2009.5061985},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061986,
author={H. Q. Ngo and T. -. Nguyen and D. Xu},
booktitle={IEEE INFOCOM 2009},
title={Hardness and Approximation of the Survivable Multi-Level Fat Tree Problem},
year={2009},
volume={},
number={},
pages={774-782},
abstract={With the explosive deployment of "triple play" (voice, video and data services) over the same access network, guaranteeing a certain-level of survivability for the access network is becoming critical for service providers. The problem of economically provisioning survivable access networks has given rise to a new class of network design problems, including the so-called survivable multi-level fat tree problem (SMFT). We show that two special cases of SMFT are polynomial- time solvable, and present two approximation algorithms for the general case. The first is a combinatorial algorithm with approximation ratio min{[~L/2] + 1, 2log<sub>2</sub> n} where L is the longest Steiner path length between two terminals, and n is the number of nodes. The second is a primal-dual (2Delta<sub>s</sub> + 2)- approximation algorithm where Delta<sub>s</sub> is the maximum Sterner degree of terminals in the access network. We then show that approximating SMFT to within a certain constant c &gt; 1 is NP- hard, even when all edge-weights of G are 1, L les 10, and Delta<sub>s</sub> les 3. Finally, we experimentally show that the approximation algorithms perform extremely well on random instances of the problem.},
keywords={approximation theory;computational complexity;Internet;optimisation;subscriber loops;trees (mathematics);survivable multilevel fat tree problem;access network;graph theory;Internet service provider;network design problem;polynomial-time;approximation algorithm;combinatorial algorithm;Steiner path length;NP-hard problem;SMFT problem;Approximation algorithms;Tree graphs;Network topology;Computer science;Data engineering;Telecommunication traffic;Cable TV;Central office;Steiner trees;Communications Society},
doi={10.1109/INFCOM.2009.5061986},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061987,
author={N. M. M. K. Chowdhury and M. R. Rahman and R. Boutaba},
booktitle={IEEE INFOCOM 2009},
title={Virtual Network Embedding with Coordinated Node and Link Mapping},
year={2009},
volume={},
number={},
pages={783-791},
abstract={Recently network virtualization has been proposed as a promising way to overcome the current ossification of the Internet by allowing multiple heterogeneous virtual networks (VNs) to coexist on a shared infrastructure. A major challenge in this respect is the VN embedding problem that deals with efficient mapping of virtual nodes and virtual links onto the substrate network resources. Since this problem is known to be NP-hard, previous research focused on designing heuristic-based algorithms which had clear separation between the node mapping and the link mapping phases. This paper proposes VN embedding algorithms with better coordination between the two phases. We formulate the VN embedding problem as a mixed integer program through substrate network augmentation. We then relax the integer constraints to obtain a linear program, and devise two VN embedding algorithms D-ViNE and R-ViNE using deterministic and randomized rounding techniques, respectively. Simulation experiments show that the proposed algorithms increase the acceptance ratio and the revenue while decreasing the cost incurred by the substrate network in the long run.},
keywords={embedded systems;Internet;virtual machines;virtual network embedding;coordinated node;link mapping;network virtualization;Internet;multiple heterogeneous virtual networks;heuristic-based algorithms;mixed integer program;substrate network augmentation;Peer to peer computing;Computer science;IP networks;Heuristic algorithms;Indium phosphide;Bandwidth;Communications Society;Algorithm design and analysis;Costs;Resource virtualization},
doi={10.1109/INFCOM.2009.5061987},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061988,
author={Y. Shavitt and U. Weinsberg},
booktitle={IEEE INFOCOM 2009},
title={Quantifying the Importance of Vantage Points Distribution in Internet Topology Measurements},
year={2009},
volume={},
number={},
pages={792-800},
abstract={The topology of the Internet has been extensively studied in recent years, driving a need for increasingly complex measurement infrastructures. These measurements have produced detailed topologies with steadily increasing temporal resolution, but concerns exist about the ability of active measurement to measure the true Internet topology. Difficulties in ensuring the accuracy of every individual measurement when millions of measurements are made daily, and concerns about the bias that might result from measurement along the tree of routes from each vantage point to the wider reaches of the Internet must be addressed. However, early discussions of these concerns were based mostly on synthetic data, oversimplified models or data with limited or biased observer distributions. In this paper, we show the importance that extensive sampling from a broad distribution of vantage points has on the resulting topology and bias. We present two methods for designing and analyzing the topology coverage by vantage points: one, when system-wide knowledge exists, provides a near-optimal assignment of measurements to vantage points; while the second one is suitable for an oblivious system and is purely probabilistic. The majority of the paper is devoted to a first look at the importance of the distribution's quality. We show that diversity in the locations and types of vantage points is required for obtaining an unbiased topology. We analyze the effect that broad distribution has over the convergence of various autonomous systems topology characteristics. We show that although diverse and broad distribution is not required for all inspected properties, it is required for some. Finally, some recent bias claims that were made against active traceroute sampling are revisited, and we empirically show that diverse and broad distribution can question their conclusions.},
keywords={Internet;sampling methods;statistical distributions;telecommunication network topology;vantage point distribution;Internet topology measurement;temporal resolution;near-optimal assignment;oblivious system;probability;convergence;traceroute sampling;Internet;Extraterrestrial measurements;Network topology;Electric variables measurement;Sampling methods;Force measurement;Software agents;Particle measurements;Time measurement;Communications Society},
doi={10.1109/INFCOM.2009.5061988},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061989,
author={B. Eriksson and P. Barford and R. Nowak},
booktitle={IEEE INFOCOM 2009},
title={Estimating Hop Distance Between Arbitrary Host Pairs},
year={2009},
volume={},
number={},
pages={801-809},
abstract={Establishing a clear and timely picture of Internet topology is complicated by many factors including the vast size and dynamic nature of the infrastructure. In this paper, we describe a methodology for estimating an important characteristic of Internet topology - the hop distance between arbitrary pairs of end hosts. Our goal is to develop an approach to pairwise hop distance estimation that is accurate, scalable, timely and does not require a significant measurement infrastructure. Our methodology is based on deploying a small set of landmark nodes that use trace route-like probes between each other to establish a set of accurate pairwise hop distances. The landmark nodes are also configured to collect source IP addresses and TTL values from passively monitored network packet traffic. We develop a novel multidimensional scaling algorithm that can be applied to both the passive and active measurements to generate pairwise hop distance estimates for all of the observed source host addresses. The basic algorithm is then enhanced to consider the autonomous system membership of source hosts via BGP routing information. We investigate the capabilities of our estimation algorithms using a set of synthetic network topologies. The results show that our method can generate highly accurate pairwise hop distance estimates over a range of network sizes and configurations, and landmark infrastructure sizes.},
keywords={Internet;IP networks;telecommunication network routing;telecommunication network topology;telecommunication traffic;of Internet topology;pairwise hop distance estimation;trace route-like probes;IP addresses;network packet traffic;multidimensional scaling algorithm;synthetic network topologies;Internet;Network topology;Telecommunication traffic;Robustness;IP networks;Peer to peer computing;Communications Society;Probes;Monitoring;Multidimensional systems},
doi={10.1109/INFCOM.2009.5061989},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061990,
author={J. Cao and Y. Jin and A. Chen and T. Bu and Z. -. Zhang},
booktitle={IEEE INFOCOM 2009},
title={Identifying High Cardinality Internet Hosts},
year={2009},
volume={},
number={},
pages={810-818},
abstract={The Internet host cardinality, defined as the number of distinct peers that an Internet host communicates with, is an important metric for profiling Internet hosts. Some example applications include behavior based network intrusion detection, p2p hosts identification, and server identification. However, due to the tremendous number of hosts in the Internet and high speed links, tracking the exact cardinality of each host is not feasible due to the limited memory and computation resource. Existing approaches on host cardinality counting have primarily focused on hosts of extremely high cardinalities. These methods do not work well with hosts of moderately large cardinalities that are needed for certain host behavior profiling such as detection of p2p hosts or port scanners. In this paper, we propose an online sampling approach for identifying hosts whose cardinality exceeds some moderate prescribed threshold, e.g. 50, or within specific ranges. The main advantage of our approach is that it can filter out the majority of low cardinality hosts while preserving the hosts of interest, and hence minimize the memory resources wasted by tracking irrelevant hosts. Our approach consists of three components: 1) two-phase filtering for eliminating low cardinality hosts, 2) thresholded bitmap for counting cardinalities, and 3) bias correction. Through both theoretical analysis and experiments using real Internet traces, we demonstrate that our approach requires much less memory than existing approaches do whereas yields more accurate estimates.},
keywords={Internet;peer-to-peer computing;Internet hosts;p2p;online sampling approach;prescribed threshold;cardinality;peer to peer;Internet;Sampling methods;Filtering;Telecommunication traffic;Intrusion detection;Statistics;Communications Society;Computer science;Application software;Network servers},
doi={10.1109/INFCOM.2009.5061990},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061991,
author={A. Chen and L. E. Li and J. Cao},
booktitle={IEEE INFOCOM 2009},
title={Tracking Cardinality Distributions in Network Traffic},
year={2009},
volume={},
number={},
pages={819-827},
abstract={Understanding the aggregate behavior of network host connectivities is important for network monitoring and traffic engineering. One characterization of such an aggregate behavior is the host distributions of distinct communicating peers or flows. For example, during the worm outbreak, the port scanning activities would cause many hosts with increasing number of (one-way) peers (or flows), and hence a change in the host distributions of distinct communicating peers or flows. In this paper, we develop an efficient streaming algorithm for tracking these host distributions of distinct elements, also called cardinality distributions, for a high speed network with a large number of hosts. Our approach utilizes the continuous Flajolet-Martin sketches, which is the minimal order statistics of hashed values, as a compact data summary and develops maximum likelihood estimates of these distributions. By leveraging the aggregation of many hosts, we are able to obtain very accurate estimates of the cardinality distributions by maintaining a compact statistical summary that is as small as one number (at most 32 bits) per host. Extensive experimental studies are carried out to demonstrate their excellent performance.},
keywords={IP networks;maximum likelihood estimation;peer-to-peer computing;telecommunication traffic;tracking;tracking cardinality distribution;network traffic engineering;network monitoring;distinct communicating peer;data streaming algorithm;continuous Flajolet-Martin sketch;minimal order statistics;maximum likelihood estimation;IP address;Telecommunication traffic;Monitoring;Traffic control;Entropy;Aggregates;High-speed networks;Communications Society;USA Councils;Statistical distributions;Maximum likelihood detection},
doi={10.1109/INFCOM.2009.5061991},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061992,
author={C. -. Chau and P. Basu},
booktitle={IEEE INFOCOM 2009},
title={Exact Analysis of Latency of Stateless Opportunistic Forwarding},
year={2009},
volume={},
number={},
pages={828-836},
abstract={Stateless opportunistic forwarding is a simple fault- tolerant distributed approach for data delivery and information querying in wireless ad hoc networks, where packets are forwarded to the next available neighbors in a "random walk" fashion, until they reach the destinations or expire. This approach is robust against ad hoc topology changes and is amenable to computation/bandwidth/energy-constrained devices; however, it is generally difficult to predict the end-to-end latency suffered by such a random walk in a given network. In this paper, we make several contributions on this topic. First, by using spectral graph theory we derive a general formula for computing the exact hitting and commute times of weighted random walks on a finite graph with heterogeneous sojourn times at relaying nodes. Such sojourn times can model heterogeneous duty cycling rates in sensor networks, or heterogeneous delivery times in delay tolerant networks. Second, we study a common class of distance-regular networks with varying numbers of geographical neighbors, and obtain simple estimate-formulas of hitting times by numerical analysis. Third, we study the more sophisticated settings of random geographical locations and distance-dependent sojourn times through simulations. Finally, we discuss the implications of this on the optimization of latency-overhead trade-off.},
keywords={ad hoc networks;fault tolerance;graph theory;telecommunication network topology;wireless sensor networks;stateless opportunistic forwarding latency;fault-tolerant distributed approach;data delivery;information querying;wireless ad hoc networks;ad hoc topology changes;end-to-end latency;spectral graph theory;weighted random walks;finite graph;heterogeneous sojourn times;heterogeneous duty cycling rates;sensor networks;heterogeneous delivery times;delay tolerant networks;distance-regular networks;distance-dependent sojourn times;latency-overhead trade-off;Delay;Mobile ad hoc networks;Robustness;Network topology;Computer networks;Bandwidth;Graph theory;Relays;Disruption tolerant networking;Numerical analysis},
doi={10.1109/INFCOM.2009.5061992},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061993,
author={J. M. Pujol and A. Lopez Toledo and P. Rodriguez},
booktitle={IEEE INFOCOM 2009},
title={Fair Routing in Delay Tolerant Networks},
year={2009},
volume={},
number={},
pages={837-845},
abstract={The typical state-of-the-art routing algorithms for delay tolerant networks are based on best next hop hill-climbing heuristics in order to achieve throughput and efficiency. The combination of these heuristics and the social network structure leads the routing to direct most of the traffic through a small subset of good users. For instance, in the SimBet algorithm, the top 10% of users carry out 54% of all the forwards and 85% of all the handovers. This unfair load distribution is not sustainable as it can quickly deplete constraint resources in heavily utilized mobile devices (e.g. storage, battery, budget, etc.). Moreover, because a small number of users carry a significant amount of the traffic, the system is not robust to random failures and attacks. To overcome these inefficiencies, this paper introduces Fair-Route, a routing algorithm for delay tolerant networks inspired by the social processes of perceived interaction strength, where messages are preferably forwarded to users that have a stronger social relation with the target of the message; and assortativity, that limits the exchange of messages to those users with similar "social status". We compare the performance of FairRoute to the state-of-the-art algorithms by extensive simulations on the MIT reality mining dataset. The results show that our algorithm outperforms existing algorithms in the de facto benchmark of throughput vs. forwards. Furthermore, it distributes better the load; the top 10% carry out 26% of the forwards and 28% of the handovers without any loss in performance.},
keywords={telecommunication network routing;fair routing;delay tolerant networks;hop hill-climbing heuristics;traffic;SimBet algorithm;Routing;Disruption tolerant networking;Telecommunication traffic;Throughput;Social network services;Electronic mail;Humans;Communications Society;Batteries;Robustness},
doi={10.1109/INFCOM.2009.5061993},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061994,
author={S. C. Nelson and M. Bakht and R. Kravets},
booktitle={IEEE INFOCOM 2009},
title={Encounter-Based Routing in DTNs},
year={2009},
volume={},
number={},
pages={846-854},
abstract={Current work in routing protocols for delay and disruption tolerant networks leverage epidemic-style algorithms that trade off injecting many copies of messages into the network for increased probability of message delivery. However, such techniques can cause a large amount of contention in the network, increase overall delays, and drain each mobile node's limited battery supply. We present a new DTN routing algorithm, called Encounter-Based Routing (EBR), which maximizes delivery ratios while minimizing overhead and delay. Furthermore, we present a means of securing EBR against black hole denial- of-service attacks. EBR achieves up to a 40% improvement in message delivery over the current state-of-the-art, as well as achieving up to a 145% increase in goodput. Also, we further show how EBR outperforms other protocols by introduce three new composite metrics that better characterize DTN routing performance.},
keywords={probability;routing protocols;encounter-based routing;disruption tolerant network;routing protocol;epidemic-style algorithm;message delivery;probability;delay;Disruption tolerant networking;Routing protocols;Peer to peer computing;Delay;Computer crime;Bandwidth;Taxonomy;Communications Society;Computer science;Batteries},
doi={10.1109/INFCOM.2009.5061994},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061995,
author={K. Lee and S. Hong and S. J. Kim and I. Rhee and S. Chong},
booktitle={IEEE INFOCOM 2009},
title={SLAW: A New Mobility Model for Human Walks},
year={2009},
volume={},
number={},
pages={855-863},
abstract={Simulating human mobility is important in mobile networks because many mobile devices are either attached to or controlled by humans and it is very hard to deploy real mobile networks whose size is controllably scalable for performance evaluation. Lately various measurement studies of human walk traces have discovered several significant statistical patterns of human mobility. Namely these include truncated power-law distributions of flights, pause-times and inter-contact times, fractal way-points, and heterogeneously defined areas of individual mobility. Unfortunately, none of existing mobility models effectively captures all of these features. This paper presents a new mobility model called SLAW (self-similar least action walk) that can produce synthetic walk traces containing all these features. This is by far the first such model. Our performance study using using SLAW generated traces indicates that SLAW is effective in representing social contexts present among people sharing common interests or those in a single community such as university campus, companies and theme parks. The social contexts are typically common gathering places where most people visit during their daily lives such as student unions, dormitory, street malls and restaurants. SLAW expresses the mobility patterns involving these contexts by fractal way points and heavy-tail flights on top of the way points. We verify through simulation that SLAW brings out the unique performance features of various mobile network routing protocols.},
keywords={mobile radio;radio networks;routing protocols;statistical distributions;SLAW human walk mobility model;mobile network routing protocol;mobile device;performance evaluation;statistical pattern;power-law distribution;self-similar least action walk;social context;heavy-tail flight;Humans;Fractals;Size control;Global Positioning System;Disruption tolerant networking;Delay;Communications Society;Anthropometry;Routing protocols;Probability distribution},
doi={10.1109/INFCOM.2009.5061995},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061996,
author={C. Dale and J. Liu},
booktitle={IEEE INFOCOM 2009},
title={apt-p2p: A Peer-to-Peer Distribution System for Software Package Releases and Updates},
year={2009},
volume={},
number={},
pages={864-872},
abstract={The Internet has become a cost-effective vehicle for software development and release, particular in the free software community. Given the free nature of this software, there are often a number of users motivated by altruism to help out with the distribution, so as to promote the healthy development of this voluntary society. It is thus naturally expected that a peer-to- peer distribution can be implemented, which will scale well with large user bases, and can easily explore the network resources made available by the volunteers. Unfortunately, this application scenario has many unique characteristics, which make a straightforward adoption of existing peer-to-peer systems for file sharing (such as BitTorrent) suboptimal. In particular, a software release often consists of a large number of packages, which are difficult to distribute individually, but the archive is too large to be distributed in its entirety. The packages are also being constantly updated by the loosely-managed developers, and the interest in a particular version of a package can be very limited depending on the computer platforms and operating systems used. In this paper, we propose a novel peer-to-peer assisted distribution system design that addresses the above challenges. It enhances the existing distribution systems by providing compatible and yet more efficient downloading and updating services for software packages. Our design leads to apt-p2p, a practical implementation that extends the popular apt distributor. apt-p2p has been used in conjunction with Debian-based distribution of Linux software packages and is also available in the latest release of Ubuntu. We have addressed the key design issues in apt-p2p, including indexing table customization, response time reduction, and multi-value extension. They together ensure that the altruistic users' resources are effectively utilized and thus significantly reduces the currently large bandwidth requirements of hosting the software, as confirmed by our existing real user statistics gathered over the Internet.},
keywords={client-server systems;configuration management;Internet;Linux;peer-to-peer computing;public domain software;software maintenance;software packages;peer-to-peer distribution system;software package release;Internet;software development;free software community;network resource;file sharing;package version;operating system;updating service;apt p2p;Linux;table customization;response time reduction;multi-value extension;client-server system;Peer to peer computing;Software packages;Packaging;Internet;Vehicles;Programming;Application software;Operating systems;Lead;Linux},
doi={10.1109/INFCOM.2009.5061996},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061997,
author={Ye Sun and Fangming Liu and Bo Li and Baochun Li and Xinyan Zhang},
booktitle={IEEE INFOCOM 2009},
title={FS2You: Peer-Assisted Semi-Persistent Online Storage at a Large Scale},
year={2009},
volume={},
number={},
pages={873-881},
abstract={It has been widely acknowledged that online storage systems within the "cloud" of the Internet provide services of a substantial value to end users who wish to share files of any sizes within a group. Such online storage services are typically provided by dedicated servers, either in content distribution networks (CDNs) or large data centers. Server bandwidth costs, however, are prohibitive in these cases, especially when serving large volumes of files to a large number of users. Though it seems intuitive to take advantage of peer upload bandwidth to mitigate such server bandwidth costs in a complementary fashion, it is not trivial to design and fine-tune important aspects of such peer-assisted online storage in a real-world large-scale deployment. This paper presents FS2You, a large-scale and real-world online storage system with peer assistance and semi-persistent file availability, in order to dramatically mitigate server bandwidth costs. In this paper, we show a number of challenges involved in such a design objective, our architectural and protocol design in response to these challenges, as well as an extensive measurement study at a large scale to demonstrate the effectiveness of our design, using real-world traces that we have collected. To our knowledge, this paper represents the first attempt to design, implement, and evaluate a new peer-assisted semi-persistent online storage system at a realistic scale. Since the launch of FS2You, it has quickly become one of the most popular online storage systems in mainland China, and a favorite in many online forums across the country.},
keywords={Internet;peer-to-peer computing;protocols;storage management;FS2You;peer-assisted semipersistent online storage system;Internet;file sharing;content distribution network;large data center;server bandwidth cost;peer upload bandwidth;file availability;protocol design;Large-scale systems;Bandwidth;Costs;File servers;Peer to peer computing;Network servers;Web server;Protocols;Contracts;Clouds},
doi={10.1109/INFCOM.2009.5061997},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061998,
author={M. Onus and A. W. Richa},
booktitle={IEEE INFOCOM 2009},
title={Minimum Maximum Degree Publish-Subscribe Overlay Network Design},
year={2009},
volume={},
number={},
pages={882-890},
abstract={Designing an overlay network for publish/subscribe communication in a system where nodes may subscribe to many different topics of interest is of fundamental importance. For scalability and efficiency, it is important to keep the degree of the nodes in the publish/subscribe system low. It is only natural then to formalize the following problem: Given a collection of nodes and their topic subscriptions connect the nodes into a graph which has least possible maximum degree and in such a way that for each topic t, the graph induced by the nodes interested in t is connected. We present the first polynomial time logarithmic approximation algorithm for this problem and prove an almost tight lower bound on the approximation ratio. Our experimental results show that our algorithm drastically improves the maximum degree of publish/subscribe overlay systems. We also propose a variation of the problem by enforcing that each topic-connected overlay network be of constant diameter, while keeping the average degree low. We present a heuristic for this problem which guarantees that each topic-connected overlay network will be of diameter 2 and which aims at keeping the overall average node degree low. Our experimental results validate our algorithm showing that our algorithm is able to achieve very low diameter without increasing the average degree by much.},
keywords={computational complexity;computer networks;graph theory;message passing;middleware;minimax techniques;network theory (graphs);publish-subscribe overlay network design;minimum maximum degree;graph theory;polynomial time logarithmic approximation algorithm;Publish-subscribe;Peer to peer computing;Subscriptions;Computer science;Design engineering;Broadcasting;Communications Society;Scalability;Polynomials;Approximation algorithms},
doi={10.1109/INFCOM.2009.5061998},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5061999,
author={Chen Feng and Baochun Li and Bo Li},
booktitle={IEEE INFOCOM 2009},
title={Understanding the Performance Gap Between Pull-Based Mesh Streaming Protocols and Fundamental Limits},
year={2009},
volume={},
number={},
pages={891-899},
abstract={Pull-based mesh streaming protocols have recently received much research attention, with successful commercial systems showing their viability in the Internet. Despite the remarkable popularity in real-world systems, the fundamental properties and limitations of pull-based protocols are not yet well understood from a theoretical perspective, as there exists no prior work that studies the performance gap between the fundamental limits and the actual performance. In this paper, we develop a unified framework based on trellis graph techniques to mathematically analyze and understand the performance of pull-based mesh streaming protocols, with a particular focus on such a performance gap. We show that there exists a significant performance gap that separates the actual and optimal performance of pull-based mesh protocols. Moreover, periodic buffer map exchanges account for most of this performance gap. Our analytical characterization of the performance gap brings us not only a better understanding of several fundamental tradeoffs in pull-based mesh protocols, but also important insights on the design of practical streaming systems that can achieve high streaming rates and short initial buffering delays.},
keywords={graph theory;Internet;media streaming;protocols;scheduling;telecommunication network topology;performance gap;pull-based mesh streaming protocol;Internet;unified framework;trellis graph technique;periodic buffer map;peer-to-peer streaming;scheduling algorithm;Protocols;Performance analysis;Delay;Internet;Communications Society;Computer science;Appropriate technology;Costs;Network coding;Solids},
doi={10.1109/INFCOM.2009.5061999},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062000,
author={S. C. Geyik and B. K. Szymanski},
booktitle={IEEE INFOCOM 2009},
title={Event Recognition in Sensor Networks by Means of Grammatical Inference},
year={2009},
volume={},
number={},
pages={900-908},
abstract={Modern military and civilian surveillance applications should provide end users with the high level representation of events observed by sensors rather than with the raw data measurements. Hence, there is a need for a system that can infer higher level meaning from collected sensor data. We demonstrate that probabilistic context free grammars (PCFGs) can be used as a basis for such a system. To recognize events from raw sensor network measurements, we use a PCFG inference method based on Stolcke (1994) and Chen(1996). We present a fast algorithm for deriving a concise probabilistic context free grammar from the given observational data. The algorithm uses an evaluation metric based on Bayesian formula for maximizing grammar a posteriori probability given the training data. We also present a real-world scenario of monitoring a parking lot and the simulation based on this scenario. We described the use of PCFGs to recognize events in the results of such a simulation. We finally demonstrate the deployment details of such an event recognition system.},
keywords={belief networks;context-free grammars;learning (artificial intelligence);pattern recognition;telecommunication computing;wireless sensor networks;event recognition;sensor networks;grammatical inference;civilian surveillance;military surveillance;probabilistic context free grammars;raw sensor network measurement;PCFG inference method;Bayesian formula;grammar a posteriori probability;parking lot monitoring;Wireless sensor networks;Inference algorithms;Sensor systems;Training data;Monitoring;Production;Communications Society;Computer science;Pervasive computing;Military computing},
doi={10.1109/INFCOM.2009.5062000},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062001,
author={R. Zheng and A. Pendharkar},
booktitle={IEEE INFOCOM 2009},
title={Obstacle Discovery in Distributed Active Sensor Networks},
year={2009},
volume={},
number={},
pages={909-917},
abstract={Distributed active sensing is a new sensing paradigm, where active sensors as illuminating sources and passive sensors as receivers are distributed in a field, and collaboratively detect objects of interest. Obstacle discovery concerns with the problem of detecting the presence and determining the location of obstacles with many applications in robot navigation, object tracking, and surface and/or structure fatigue testing etc. In this paper, we study the fundamental properties of distributed active sensing networks (DASNs) in detecting and localizing obstacles. A novel notion of "exposure" is defined, which quantifies the dimension limitations in detectability. Using simple geometric constructs, we propose polynomial-time algorithms to compute the exposure and regions where the center of the obstacles may lie.},
keywords={distributed sensors;object detection;polynomials;obstacle discovery;distributed active sensor networks;distributed active sensing;robot navigation;object tracking;fatigue testing;polynomial time algorithms;Actuators;Concrete;Laser radar;Acoustic sensors;Sensor systems;Mechanical sensors;Sensor arrays;Object detection;Robot sensing systems;Navigation},
doi={10.1109/INFCOM.2009.5062001},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062002,
author={G. Yang and D. Qiao},
booktitle={IEEE INFOCOM 2009},
title={Barrier Information Coverage with Wireless Sensors},
year={2009},
volume={},
number={},
pages={918-926},
abstract={Sensor networks have been deployed for many barrier coverage applications such as intrusion detection and border surveillance. In these applications, it is critical to operate a sensor network in an energy-efficient manner so the barrier can be covered with as few active sensors as possible. In this paper, we study barrier information coverage which exploits collaborations and information fusion between neighboring sensors to reduce the number of active sensors needed to cover a barrier and hence to prolong the network lifetime. Moreover, we propose a practical solution to identify the barrier information coverage set which can information-cover the barrier with a small number of active sensors. The effectiveness of the proposed solution is demonstrated by numerical and simulation results.},
keywords={computerised instrumentation;intelligent sensors;security of data;wireless sensor networks;barrier information coverage;wireless sensor network;intrusion detection;border surveillance;active sensor;information fusion;Wireless sensor networks;Sensor fusion;Collaboration;Collaborative work;Intrusion detection;Surveillance;Shape;Object detection;Sensor phenomena and characterization;Communications Society},
doi={10.1109/INFCOM.2009.5062002},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062003,
author={J. Jeong and Y. Gu and T. He and D. Du},
booktitle={IEEE INFOCOM 2009},
title={VISA: Virtual Scanning Algorithm for Dynamic Protection of Road Networks},
year={2009},
volume={},
number={},
pages={927-935},
abstract={This paper proposes a virtual scanning algorithm (VISA), tailored and optimized for road network surveillance. Our design uniquely leverages upon the facts that (i) the movement of targets (e.g., vehicles) is confined within roadways and (ii) the road network maps are normally known. We guarantee the detection of moving targets before they reach designated protection points (such as temporary base camps), while maximizing the lifetime of the sensor network. The main idea of this work is virtual scan - waves of sensing activities scheduled for road network protection. We provide design-space analysis on the performance of virtual scan in terms of lifetime and average detection delay. Importantly, to our knowledge, this is the first work to study how to guarantee target detection while sensor network deteriorates, using a novel hole handling technique. Through theoretical analysis and extensive simulation, it is shown that a surveillance system, using our design, sustains orders-of-magnitude longer lifetime than full coverage algorithms, and as much as ten times longer than legacy duty cycling algorithms.},
keywords={object detection;road safety;surveillance;telecommunication network reliability;wireless sensor networks;virtual scanning algorithm;VISA algorithm;road network surveillance;roadway network map;wireless sensor network;road network protection;average target detection delay;legacy duty cycling algorithm;magnitude longer network lifetime;Heuristic algorithms;Protection;Surveillance;Algorithm design and analysis;Vehicle dynamics;Road vehicles;Performance analysis;Delay;Object detection;Analytical models},
doi={10.1109/INFCOM.2009.5062003},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062004,
author={N. Subramanian and K. Yang and W. Zhang and D. Qiao},
booktitle={IEEE INFOCOM 2009},
title={ElliPS: A Privacy Preserving Scheme for Sensor Data Storage and Query},
year={2009},
volume={},
number={},
pages={936-944},
abstract={With in-network sensor data storage and query, storage nodes are responsible for storing the data collected by sensor nodes and answering queries from users. Thus, without proper protection for data types and user queries, compromise of storage nodes and/or sensor nodes may reveal sensitive information about the sensed environment as well as users' private interests and query patterns. In this paper, we explore trade-offs between privacy, computation overhead, communication overhead, network flexibility and network complexity, and propose ElliPS (Elliptic curve based Privacy Scheme) to provide joint protection on data type privacy and query privacy in the presence of sensor node compromise, storage node compromise, or under collusive attacks by compromised sensor nodes and storage nodes together. Extensive analysis and simulation are conducted to verify the security properties and efficiency of the proposed scheme.},
keywords={data privacy;elliptic equations;query processing;storage management;ElliPS;privacy preserving scheme;in-network sensor data storage;elliptic curve based privacy scheme;data type privacy;query privacy;Data privacy;Memory;Peer to peer computing;Protection;Computer networks;Elliptic curves;Base stations;Query processing;Cryptography;Wireless sensor networks},
doi={10.1109/INFCOM.2009.5062004},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062005,
author={J. Shi and R. Zhang and Y. Zhang},
booktitle={IEEE INFOCOM 2009},
title={Secure Range Queries in Tiered Sensor Networks},
year={2009},
volume={},
number={},
pages={945-953},
abstract={We envision a two-tier sensor network which consists of resource-rich master nodes at the upper tier and resource- poor sensor nodes at the lower tier. Master nodes collect data from sensor nodes and answer the queries from the network owner. The reliance on master nodes for data storage and query processing raises concerns about both data confidentiality and query-result correctness in hostile environments. In particular, a compromised master node may leak hosted sensitive data to the adversary; it may also return juggled or incomplete data in response to a query. This paper presents a novel spatiotemporal crosscheck approach to ensure secure range queries in event- driven two-tier sensor networks. It offers data confidentiality by preventing master nodes from reading hosted data and also enables efficient range-query processing. More importantly, it allows the network owner to verify with very high probability whether a query result is authentic and complete by examining the spatial and temporal relationships among the returned data. The high efficacy and efficiency of our approach are confirmed by detailed performance evaluations.},
keywords={query processing;wireless sensor networks;secure range queries;tiered sensor networks;resource-rich master nodes;resource-poor sensor nodes;data storage;spatiotemporal crosscheck approach;Wireless sensor networks;Peer to peer computing;Query processing;Memory;Computer architecture;Spread spectrum communication;Communications Society;Spatiotemporal phenomena;Large-scale systems;Wireless mesh networks},
doi={10.1109/INFCOM.2009.5062005},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062006,
author={Q. Wang and K. Ren and W. Lou and Y. Zhang},
booktitle={IEEE INFOCOM 2009},
title={Dependable and Secure Sensor Data Storage with Dynamic Integrity Assurance},
year={2009},
volume={},
number={},
pages={954-962},
abstract={Recently, distributed data storage has gained increasing popularity for efficient and robust data management in wireless sensor networks (WSNs). But the distributed architecture also makes it challenging to build a highly secure and dependable yet lightweight data storage system. On the one hand, sensor data are subject to not only Byzantine failures, but also dynamic pollution attacks, as along the time the adversary may modify/pollute the stored data by compromising individual sensors. On the other hand, the resource-constrained nature of WSNs precludes the applicability of heavyweight security designs. To address the challenges, we propose a novel dependable and secure data storage scheme with dynamic integrity assurance in this paper. Based on the principle of secret sharing and erasure coding, we first propose a hybrid share generation and distribution scheme to achieve reliable and fault-tolerant initial data storage by providing redundancy for original data components. To further dynamically ensure the integrity of the distributed data shares, we then propose an efficient data integrity verification scheme exploiting the technique of algebraic signatures. The proposed scheme enables individual sensors to verify in one protocol execution all the pertaining data shares simultaneously in the absence of the original data. Extensive security and performance analysis shows that the proposed schemes have strong resistance against various attacks and are practical for WSNs.},
keywords={security of data;wireless sensor networks;secure sensor data storage;dynamic integrity assurance;wireless sensor networks;robust data management;Byzantine failures;secret sharing;erasure coding;hybrid share generation;Memory;Wireless sensor networks;Pollution;Data security;Robustness;Data storage systems;Cryptography;Hybrid power systems;Fault tolerance;Redundancy},
doi={10.1109/INFCOM.2009.5062006},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062007,
author={S. Yu and K. Ren and W. Lou},
booktitle={IEEE INFOCOM 2009},
title={FDAC: Toward Fine-Grained Distributed Data Access Control in Wireless Sensor Networks},
year={2009},
volume={},
number={},
pages={963-971},
abstract={Distributed sensor data storage and retrieval has gained increasing popularity in recent years for supporting various applications. While distributed architecture enjoys a more robust and fault-tolerant wireless sensor network (WSN), such architecture also poses a number of security challenges especially when applied in mission-critical applications such as battle field and e-healthcare. First, as sensor data are stored and maintained by individual sensors and unattended sensors are easily subject to strong attacks such as physical compromise, it is significantly harder to ensure data security. Second, in many mission-critical applications, fine-grained data access control is a must as illegal access to the sensitive data may cause disastrous result and/or prohibited by the law. Last but not least, sensors usually are resource-scarce, which limits the direct adoption of expensive cryptographic primitives. To address the above challenges, we propose in this paper a distributed data access control scheme that is able to fulfill fine-grained access control over sensor data and is resilient against strong attacks such as sensor compromise and user colluding. The proposed scheme exploits a novel cryptographic primitive called attribute-based encryption (ABE), tailors, and adapts it for WSNs with respect to both performance and security requirements. The feasibility of the scheme is demonstrated by experiments on real sensor platforms. To our best knowledge, this paper is the first to realize distributed fine-grained data access control for WSNs.},
keywords={access control;cryptography;wireless sensor networks;distributed data access control;wireless sensor networks;fault tolerance;battle field;e-healthcare;sensor data;attribute-based encryption;Access control;Wireless sensor networks;Data security;Cryptography;Sensor phenomena and characterization;Mission critical systems;Memory;Information retrieval;Robustness;Fault tolerance},
doi={10.1109/INFCOM.2009.5062007},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062008,
author={T. Moscibroda and S. Schmid},
booktitle={IEEE INFOCOM 2009},
title={On Mechanism Design without Payments for Throughput Maximization},
year={2009},
volume={},
number={},
pages={972-980},
abstract={It is well-known that the overall efficiency of a distributed system can suffer if the participating entities seek to maximize their individual performance. Consequently, mechanisms have been designed that force the participants to behave more cooperatively. Most of these game-theoretic solutions rely on payments between participants. Unfortunately, such payments are often cumbersome to implement in practice, especially in dynamic networks and where transaction costs are high. In this paper, we investigate the potential of mechanisms which work without payments. We consider the problem of throughput maximization in multi-channel environments and shed light onto the throughput increase that can be achieved with and without payments. We introduce and analyze two different concepts: the worst-case <i>leverage</i> where we assume that players end up in the worst rational strategy profile, and the average-case <i>leverage</i> where player select a random non-dominated strategy. Our theoretical insights are complemented by simulations.},
keywords={distributed processing;game theory;systems analysis;mechanism design;throughput maximization;distributed system;game-theoretic solutions;multi-channel environments;Throughput;Communications Society;Algorithm design and analysis;USA Councils;Costs;Government;Finance;Large-scale systems;Spread spectrum communication;Wireless networks},
doi={10.1109/INFCOM.2009.5062008},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062009,
author={Y. Wu and D. H. K. Tsang},
booktitle={IEEE INFOCOM 2009},
title={Distributed Power Allocation Algorithm for Spectrum Sharing Cognitive Radio Networks with QoS Guarantee},
year={2009},
volume={},
number={},
pages={981-989},
abstract={In this paper we study the distributed multi-channel power allocation for spectrum sharing cognitive radio networks with QoS guarantee. We formulate this problem as a non- cooperative game G<sub>MCPA-C</sub> with coupled strategy space to address both the co-channel interference among secondary users and the interference temperature regulation imposed by primary systems. We investigate the properties of Nash equilibrium (N.E.) for our G<sub>MCPA-C</sub>, including the existence and QoS provisioning. Furthermore, we derive a layered structure by applying the Lagrangian dual decomposition to G<sub>MCPA-C</sub> and design a distributed algorithm to find the N.E. via this structure. Simulation results are presented to show both the validity of our game theoretic model and the performance of our proposed algorithm. Finally, we incorporate the Pigouvian taxation into our algorithm to improve the efficiency of N.E. when social optimality is considered.},
keywords={cochannel interference;cognitive radio;game theory;quality of service;resource allocation;distributed power allocation;spectrum sharing;cognitive radio;quality of service;non-cooperative game;Nash equilibrium;Lagrangian dual decomposition;cochannel interference;Cognitive radio;Temperature;Interchannel interference;Frequency;Space technology;Nash equilibrium;Lagrangian functions;Algorithm design and analysis;Distributed algorithms;Energy consumption},
doi={10.1109/INFCOM.2009.5062009},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062010,
author={P. Hande and M. Chiang and R. Calderbank and S. Rangan},
booktitle={IEEE INFOCOM 2009},
title={Network Pricing and Rate Allocation with Content Provider Participation},
year={2009},
volume={},
number={},
pages={990-998},
abstract={Pricing content-providers for connectivity to end- users and setting connection parameters based on the price is an evolving model on the Internet. The implications are heavily debated in telecom policy circles, and some advocates of "Network Neutrality" have opposed price based differentiation in connectivity. However, pricing content providers can possibly subsidize the end-user's cost of connectivity, and the consequent increase in end-user demand can benefit ISPs and content providers. This paper provides a framework to quantify the precise trade-off in the distribution of benefits among ISPs, content-providers, and end-users. The framework generalizes the well-known utility maximization based rate allocation model, which has been extensively studied as an interplay between the ISP and the end-users, to incorporate pricing of content-providers. We derive the resulting equilibrium prices and data rates in two different ISP market conditions: competition and monopoly. Network neutrality based restriction on content-provider pricing is then modeled as a constraint on the maximum price that can be charged to content-providers. We demonstrate that, in addition to gains in total and end- user surplus, content-provider experiences a net surplus from participation in rate allocation under low cost of connectivity. The surplus gains are, however, limited under monopoly conditions in comparison to competition in the ISP market.},
keywords={activity based costing;Internet;pricing;network pricing;rate allocation;Internet;network neutrality;utility maximization;content-provider pricing;Pricing;Costs;Network neutrality;Video sharing;USA Councils;IP networks;Monopoly;Communications Society;Telecommunications;Distributed algorithms},
doi={10.1109/INFCOM.2009.5062010},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062011,
author={X. Zhou and H. Zheng},
booktitle={IEEE INFOCOM 2009},
title={TRUST: A General Framework for Truthful Double Spectrum Auctions},
year={2009},
volume={},
number={},
pages={999-1007},
abstract={We design truthful double spectrum auctions where multiple parties can trade spectrum based on their individual needs. Open, market-based spectrum trading motivates existing spectrum owners (as sellers) to lease their selected idle spectrum to new spectrum users, and provides new users (as buyers) the spectrum they desperately need. The most significant challenge is how to make the auction economic-robust (truthful in particular) while enabling spectrum reuse to improve spectrum utilization. Unfortunately, existing designs either do not consider spectrum reuse or become untruthful when applied to double spectrum auctions. We address this challenge by proposing TRUST, a general framework for truthful double spectrum auctions. TRUST takes as input any reusability-driven spectrum allocation algorithm, and applies a novel winner determination and pricing mechanism to achieve truthfulness and other economic properties while significantly improving spectrum utilization. To our best knowledge, TRUST is the first solution for truthful double spectrum auctions that enable spectrum reuse. Our results show that economic factors introduce a tradeoff between spectrum efficiency and economic robustness. TRUST makes an important contribution on enabling spectrum reuse to minimize such tradeoff.},
keywords={frequency allocation;telecommunication services;TRUST;truthful double spectrum auction;market-based spectrum trading;spectrum reuse;spectrum utilization;reusability-driven spectrum allocation algorithm;economic factor;Economics;Pricing;FCC;Communications Society;Computer science;Mechanical factors;Robustness;Equal opportunities;Supply and demand;Wireless networks},
doi={10.1109/INFCOM.2009.5062011},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062012,
author={J. Tapolcai and B. Wu and P. -. Ho},
booktitle={IEEE INFOCOM 2009},
title={On Monitoring and Failure Localization in Mesh All-Optical Networks},
year={2009},
volume={},
number={},
pages={1008-1016},
abstract={Achieving fast and precise failure localization has long been a highly desired feature in all-optical mesh networks. M-trail (monitoring trail) has been proposed as the most general monitoring structure for achieving unambiguous failure localization (UFL) of any single link failure while effectively reducing the amount of alarm signals flooded in the networks. However, it is critical to come up with a fast and intelligent m-trail design approach for minimizing the number of m-trails and the totally consumed bandwidth, which ubiquitously determines the length of alarm code and bandwidth overhead for the M-trail deployment, respectively. In this paper, the m-trail design problem is investigated. To gain deeper understanding of the problem, we firstly conduct a bound analysis on the minimum length of alarm code required for UFL. Then, a novel algorithm based on random code assignment (RCA) and random code swapping (RCS) is developed for solving the m-trail design problem. The algorithm prototype can be found in. The algorithm is verified by comparing with an integer linear program (ILP), and the results demonstrate its superiority in minimizing the fault management cost and bandwidth consumption while achieving significant reduction in computation time. To investigate the impact of topology diversity, extensive simulation is conducted on thousands of random network topologies with systematically increased network connectivity. Lastly, we provide abundant discussions and interesting conclusive remarks that position our discoveries.},
keywords={bandwidth allocation;communication complexity;integer programming;linear programming;optical fibre networks;random codes;telecommunication network reliability;telecommunication network topology;all-optical mesh network;intelligent m-trail design approach;monitoring trail;unambiguous failure localization;alarm signal;alarm code;random code assignment;random code swapping;integer linear program;fault management cost;bandwidth consumption;random network topology diversity;computation time;RCA algorithm;RCS algorithm;Condition monitoring;All-optical networks;Bandwidth;Network topology;Computerized monitoring;Computer networks;Shape;Algorithm design and analysis;Communications Society;Informatics},
doi={10.1109/INFCOM.2009.5062012},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062013,
author={K. Lee and E. Modiano},
booktitle={IEEE INFOCOM 2009},
title={Cross-Layer Survivability in WDM-Based Networks},
year={2009},
volume={},
number={},
pages={1017-1025},
abstract={In layered networks, a single failure at a lower layer may cause multiple failures in the upper layers. As a result, traditional schemes that protect against single failures may not be effective in cross-layer networks. In this paper, we introduce the problem of maximizing the connectivity of layered networks. We show that connectivity metrics in layered networks have significantly different meaning than their single-layer counterparts. Results that are fundamental to survivable single-layer network design, such as the Max-Flow Min-Cut theorem, are no longer applicable to the layered setting. We propose new metrics to measure connectivity in layered networks and analyze their properties. We use one of the metrics, Min Cross Layer Cut, as the objective for the survivable lightpath routing problem, and develop several algorithms to produce lightpath routings with high survivability. This allows the resulting cross-layer architecture to be resilient to failures.},
keywords={wavelength division multiplexing;cross-layer survivability;WDM;single-layer network design;lightpath routing problem;Network topology;Routing;Protection;Asynchronous transfer mode;Wavelength division multiplexing;Optical fiber networks;Optical packet switching;Switching circuits;Packet switching;Communication switching},
doi={10.1109/INFCOM.2009.5062013},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062014,
author={K. Thulasiraman and M. S. Javed and G. Xue},
booktitle={IEEE INFOCOM 2009},
title={Circuits/Cutsets Duality and a Unified Algorithmic Framework for Survivable Logical Topology Design in IP-over-WDM Optical Networks},
year={2009},
volume={},
number={},
pages={1026-1034},
abstract={Given a logical topology G<sub>L</sub> and a physical topology G, the survivable logical topology design problem in an IP-over- WDM optical network is to map the logical links into lightpaths in G such that G<sub>L</sub> remains connected after the failure of any edge in G. In view of its fundamental nature and its practical importance, this problem has received considerable attention in the literature. The SMART algorithmic framework based on the circuits in G<sub>L</sub> is a novel and very significant contribution to this problem. Taking advantage of the dual relationship between circuits and cutsets in a graph, we first present in this paper the primal algorithm CIRCUIT-SMART (similar to SMART) and algorithm CUTSET-SMART that is dual of CIRCUIT-SMART and proofs of correctness of these algorithms. To guarantee survivability we add additional logical links called protection edges, if necessary. This investigation has provided much insight into the structural properties of solutions to this problem and the structure of survivable logical graphs. Specifically, we present a highly simplified version of CUTSET-SMART that always provides a survivable mapping as long as G is 3-edge connected, and a survivable logical topology structure. We also present algorithm INCIDENCE-SMART that uses incidence sets that are special cases of a cut. Two efficient heuristics, one based on maximum matching theory and the other based on both the primal and dual algorithms are also presented. Simulation results comparing the different algorithms in terms of computational time, protection capacity and survivability success rate are also presented.},
keywords={optical communication;wavelength division multiplexing;unified algorithmic framework;survivable logical topology design;optical networks;WDM;IP;logical graphs;CIRCUIT-SMART;CUTSET-SMART;Network topology;Circuit topology;Optical design;Algorithm design and analysis;Optical fiber networks;Optical fiber cables;Optical fiber devices;Protection;Communication cables;Computer science},
doi={10.1109/INFCOM.2009.5062014},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062015,
author={H. -. Lee and E. Modiano},
booktitle={IEEE INFOCOM 2009},
title={Diverse Routing in Networks with Probabilistic Failures},
year={2009},
volume={},
number={},
pages={1035-1043},
abstract={We develop diverse routing schemes for dealing with multiple, possibly correlated, failures. While disjoint path protection can effectively deal with isolated single link failures, recovering from multiple failures is not guaranteed. In particular, events such as natural disasters or intentional attacks can lead to multiple correlated failures, for which recovery mechanisms are not well understood. We take a probabilistic view of network failures where multiple failure events can occur simultaneously, and develop algorithms for finding diverse routes with minimum joint failure probability. Moreover, we develop a novel Probabilistic Shared Risk Link Group (PSRLG) framework for modeling correlated failures. In this context, we formulate the problem of finding two paths with minimum joint failure probability as an Integer Non-Linear Program (INLP), and develop approximations and linear relaxations that can find nearly optimal solutions in most cases.},
keywords={integer programming;telecommunication network routing;diverse network routing;probabilistic failures;disjoint path protection;natural disasters;intentional attacks;probabilistic shared risk link group;integer nonlinear program;Routing;Protection;Optical fiber communication;EMP radiation effects;Optical fiber networks;Communication networks;Optical fiber cables;Satellites;Communications Society;Isolation technology},
doi={10.1109/INFCOM.2009.5062015},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062016,
author={K. Chaudhuri and C. Daskalakis and R. D. Kleinberg and H. Lin},
booktitle={IEEE INFOCOM 2009},
title={Online Bipartite Perfect Matching With Augmentations},
year={2009},
volume={},
number={},
pages={1044-1052},
abstract={In this paper, we study an online bipartite matching problem, motivated by applications in wireless communication, content delivery, and job scheduling. In our problem, we have a bipartite graph G between n clients and n servers, which represents the servers to which each client can connect. Although the edges of G are unknown at the start, we learn the graph over time, as each client arrives and requests to be matched to a server. As each client arrives, she reveals the servers to which she can connect, and the goal of the algorithm is to maintain a matching between the clients who have arrived and the servers. Assuming that G has a perfect matching which allows all clients to be matched to servers, the goal of the online algorithm is to minimize the switching cost, the total number of times a client needs to switch servers in order to maintain a matching at all times. Although there are no known algorithms which are guaranteed to yield switching cost better than the trivial O(n<sup>2</sup>) in the worst case, we show that the switching cost can be much lower in three natural settings. In our first result, we show that for any arbitrary graph G with a perfect matching, if the clients arrive in random order, then the total switching cost is only O(n log n) with high probability. This bound is tight, as we show an example where the switching cost is Omega(n log n) in expectation. In our second result, we show that if each client has edges to Theta(log n) uniformly random servers, then the total switching cost is even better; in this case, it is only O(n) with high probability, and we also have a lower bound of Omega(n/log n). In terms of the number of edges needed for each client, our result is tight, since Omega(log n) edges are needed to guarantee a perfect matching in G with high probability. In our last result, we derive the first algorithm known to yield total cost O(n log n), given that the underlying graph G is a forest. This is the first result known to match the existing lower bound for forests, which shows that any online algorithm must have switching cost Omega(n log n), even when G is restricted to be a forest.},
keywords={client-server systems;computational complexity;graph theory;online bipartite perfect matching;augmentations;bipartite graph;switching cost;Costs;Web pages;Computer science;Memory;Switches;Communications Society;Information theory;Application software;Data security;Routing},
doi={10.1109/INFCOM.2009.5062016},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062017,
author={M. Uchida and J. Kurose},
booktitle={IEEE INFOCOM 2009},
title={An Information-Theoretic Characterization of Weighted alpha-Proportional Fairness},
year={2009},
volume={},
number={},
pages={1053-1061},
abstract={This paper provides a novel characterization of fairness concepts in network resource allocation problems from the viewpoint of information theory. The fundamental idea adopted in this paper is to characterize the utility functions used in optimization problems, which motivate fairness concepts, based on a trade-off between user and system satisfaction. Here, user satisfaction is evaluated using information divergence measures that were originally used in information theory to evaluate the difference between two probability distributions. In this paper, information divergence measures are applied to evaluate the difference between the implemented resource allocation and a requested resource allocation. The requested resource allocation is assumed to be ideal in some sense from the user's point of view. Also, system satisfaction is evaluated based on the efficiency of the implemented resource utilization, which is defined as the total amount of resources allocated to each user. The results discussed in this paper indicate that the well-known fairness concept called weighted alpha-proportional fairness can be characterized using the alpha-divergence measure, which is a general class of information divergence measures, as an equilibrium of the trade-off described above. In the process of obtaining these results, we also obtained a new utility function that has a parameter to control the trade-off. This new function is then applied to typical examples to solve resource allocation problems in simple network models such as those for two-link networks and wireless LANs.},
keywords={computer network management;information theory;optimisation;resource allocation;statistical distributions;wireless LAN;weighted alpha-proportional fairness;network resource allocation problem;optimization problem;information divergence measures;probability distribution;information theory;fairness concept;wireless LAN;two-link network;Resource management;Information theory;Communications Society;Paper technology;Computer science;USA Councils;Probability distribution;Wireless LAN;Degradation;Throughput},
doi={10.1109/INFCOM.2009.5062017},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062018,
author={F. Li},
booktitle={IEEE INFOCOM 2009},
title={Competitive Scheduling of Packets with Hard Deadlines in a Finite Capacity Queue},
year={2009},
volume={},
number={},
pages={1062-1070},
abstract={Motivated by the quality-of-service (QoS) buffer management problem, we consider online scheduling of packets with hard deadlines in a finite capacity queue. At any time, a queue can store at most <i>b</i> isin Z<sup>+</sup> packets. Packets arrive over time. Each packet is associated with a non-negative value and an integer deadline. In each time step, only one packet is allowed to be sent. Our objective is to maximize the total value gained by the packets sent by their deadlines in an online manner. Due to the Internet traffic's chaotic characteristics, no stochastic assumptions are made on the packet input sequences. This model is called a finite-queue model. We use competitive analysis to measure an online algorithm's performance versus an unrealizable optimal offline algorithm who constructs the worst possible input based on the knowledge of the online algorithm. For the finite-queue model, we first present a deterministic 3-competitive memoryless online algorithm. Then, we give a randomized (Phi<sup>2</sup> = (1+radic(5)/2)<sup>2</sup> ap 2.618)-competitive memoryless online algorithm. The algorithmic framework and its theoretical analysis include several interesting features. First, our algorithms use (possibly) modified characteristics of packets; these characteristics may not be same as those specified in the input sequence. Second, our analysis method is different from the classical potential function approach. We use a simple charging scheme, which depends on a clever modification (during the course of the algorithm) on the packets in the queue of the optimal offline algorithm. We then prove that a set of invariants holds at the end of each time step. Finally, we analyze the two proposed algorithm in a relaxed model, in which packets have no hard deadlines but an order. We conclude that both algorithms have the same competitive ratios in the relaxed model.},
keywords={competitive algorithms;computer network management;Internet;quality of service;queueing theory;scheduling;telecommunication traffic;competitive packet scheduling;hard deadlines;finite capacity queue;quality-of-service buffer management problem;online packets scheduling;integer deadline;Internet traffic chaotic characteristics;competitive analysis;deterministic 3-competitive memoryless online algorithm;randomized competitive memoryless online algorithm;charging scheme;QoS;Quality of service;Optimal scheduling;Algorithm design and analysis;Processor scheduling;Quality management;Internet;Traffic control;Communications Society;Computer science;USA Councils},
doi={10.1109/INFCOM.2009.5062018},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062019,
author={F. Ciucu and J. Liebeherr},
booktitle={IEEE INFOCOM 2009},
title={A Case for Decomposition of FIFO Networks},
year={2009},
volume={},
number={},
pages={1071-1079},
abstract={Recent findings showing that the output of traffic flows at packet switches has similar characteristics as the corresponding input enable a decomposition analysis of a network where nodes can be studied in isolation, thus simplifying an end- to-end analysis of networks. However, network decomposition results currently available are mostly many-sources asymptotics. In this paper we explore the viability of network decomposition in a non-asymptotic regime with a finite number of flows. For traffic with Exponentially Bounded Burstiness (EBB) we derive statistical bounds for the output traffic at a FIFO buffer and compare them with bounds on the input. By evaluating the accuracy of the output bounds with exact results available for special cases and by using numerical examples we find that conditions for network decomposition appear favorable even if the number of flows is relatively small.},
keywords={decomposition;packet switching;decomposition;FIFO networks;traffic flows;packet switches;nodes;end-to-end analysis;exponentially bounded burstiness;Telecommunication traffic;Traffic control;Peer to peer computing;Calculus;Scheduling algorithm;Aggregates;Network topology;Communications Society;Packet switching;Switches},
doi={10.1109/INFCOM.2009.5062019},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062020,
author={M. A. Y. Khan and D. Veitch},
booktitle={IEEE INFOCOM 2009},
title={Isolating Physical PER for Smart Rate Selection in 802.11},
year={2009},
volume={},
number={},
pages={1080-1088},
abstract={Current rate control (selection) algorithms in IEEE 802.11 are not based on accurate measurements of packet errors caused at the physical layer. Instead, algorithms act on measurements which, either implicitly or explicitly, mix physical errors with those arising from contention. In this paper we first illustrate how contention can adversely affect the performance of these algorithms, and point out the potential benefits of an ability to isolate physical packet error rate. We introduce and compare two variants of a single core idea enabling the isolation and accurate measurement of physical packet error, based on exploiting existing features of the MAC standard in a novel way. One is based on the RTS/CTS mechanism, and the other on packet fragmentation. Using proof of concept experimental results from a wireless testbed, we show these mechanisms can be used to improve the performance of two existing algorithms, SampleRate and AMRR, both for individual stations and for the system as a whole, and show how incremental deployment is unproblematic. We discuss how the methodology can be integrated in a modular way into rate control algorithms with acceptable overhead.},
keywords={error statistics;wireless LAN;physical PER;smart rate selection;IEEE 802.11;packet error rate;MAC standard;CTS mechanism;packet fragmentation;incremental deployment;Throughput;Error analysis;Physical layer;Communications Society;Interference;Bit error rate;Error correction;Measurement standards;System testing;Delay},
doi={10.1109/INFCOM.2009.5062020},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062021,
author={P. Serrano and M. Zink and J. Kurose},
booktitle={IEEE INFOCOM 2009},
title={Assessing the Fidelity of COTS 802.11 Sniffers},
year={2009},
volume={},
number={},
pages={1089-1097},
abstract={Recent measurement studies have analyzed WLAN performance by means of wireless sniffers that passively capture transmitted frames. Also, for relatively large (enterprise) WLAN scenarios, previous work has investigated multi-sniffer deployments with devices placed far apart in order to capture all traffic in the network (even frames transmitted simultaneously by different nodes at non-interfering locations). However, for both these single- and multi-sniffer scenarios, little attention has been given to the fidelity of an individual device, i.e., the ability of a given sniffer to capture all frames that could have been captured by a more faithful device. We assess this fidelity (a term we make precise in this paper) by running controlled experiments inside an anechoic chamber and analyzing the similarities and differences between the trace file from the device under study and those of additional "shadow" devices placed in its close proximity. Our results show that fidelity varies significantly across sniffers, both quantitatively and qualitatively, and that performance may also depend on the nature of the experiment under study and on slight changes of the sniffer position.},
keywords={anechoic chambers (electromagnetic);wireless LAN;wireless LAN;802.11 sniffers;sniffer fidelity;sniffer position;anechoic chamber;trace file;Anechoic chambers;Interference;Semiconductor device measurement;Antenna measurements;Time measurement;Wireless LAN;Wireless networks;Communications Society;Computer science;Performance analysis},
doi={10.1109/INFCOM.2009.5062021},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062022,
author={M. Bredel and M. Fidler},
booktitle={IEEE INFOCOM 2009},
title={Understanding Fairness and its Impact on Quality of Service in IEEE 802.11},
year={2009},
volume={},
number={},
pages={1098-1106},
abstract={The distributed coordination function (DCF) aims at fair and efficient medium access in IEEE 802.11. In face of its success, it is remarkable that there is little consensus on the actual degree of fairness achieved, particularly bearing its impact on quality of service in mind. In this paper we provide an accurate model for the fairness of the DCF. Given M greedy stations we assume fairness if a tagged station contributes a share of 1/M to the overall number of packets transmitted. We derive the probability distribution of fairness deviations and support our analytical results by an extensive set of measurements. We find a closed-form expression for the improvement of long-term over short-term fairness. Regarding the random countdown values we quantify the significance of their distribution whereas we discover that fairness is largely insensitive to the distribution parameters. Based on our findings we view the DCF as emulating an ideal fair queuing system to quantify the deviations from a fair rate allocation. We deduce a stochastic service curve model for the DCF to predict packet delays in IEEE 802.11. We show how a station can estimate its fair bandwidth share from passive measurements of its traffic arrivals and departures.},
keywords={quality of service;queueing theory;statistical distributions;stochastic processes;telecommunication traffic;wireless LAN;quality-of-service;IEEE 802.11;distributed coordination function;packets transmission;probability distribution;closed-form expression;queuing system;fair rate allocation;stochastic service curve model;network traffic;Quality of service;Global Positioning System;Stochastic processes;Delay;Throughput;Scheduling algorithm;Calculus;Closed-form solution;Media Access Protocol;Processor scheduling},
doi={10.1109/INFCOM.2009.5062022},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062023,
author={J. Riihijarvi and M. Wellens and P. Mahonen},
booktitle={IEEE INFOCOM 2009},
title={Measuring Complexity and Predictability in Networks with Multiscale Entropy Analysis},
year={2009},
volume={},
number={},
pages={1107-1115},
abstract={We propose to use multiscale entropy analysis in characterisation of network traffic and spectrum usage. We show that with such analysis one can quantify complexity and predictability of measured traces in widely varying timescales. We also explicitly compare the results from entropy analysis to classical characterisations of scaling and self-similarity in time series by means of fractal dimension and the Hurst parameter. Our results show that the used entropy analysis indeed complements these measures, being able to uncover new information from traffic traces and time series models. We illustrate the application of these techniques both on time series models and on measured traffic traces of different types. As potential applications of entropy analysis in the networking area, we highlight and discuss anomaly detection and validation of traffic models. In particular, we show that anomalous network traffic can have significantly lower complexity than ordinary traffic, and that commonly used traffic and time series models have different entropy structures compared to the studied traffic traces. We also show that the entropy metrics can be applied to the analysis of wireless communication and networks. We point out that entropy metrics can improve the understanding of how spectrum usage changes over time and can be used to enhance the efficiency of dynamic spectrum access networks.},
keywords={entropy;fractals;radio networks;radio spectrum management;telecommunication security;telecommunication traffic;time series;complexity measurement;wireless network traffic trace;dynamic spectrum usage;multiscale entropy analysis;fractal dimension;Hurst parameter;time series model;anomaly detection;self similarity;Entropy;Telecommunication traffic;Traffic control;Time series analysis;Fractals;Time measurement;Information analysis;Communications Society;Wireless networks;Wireless communication},
doi={10.1109/INFCOM.2009.5062023},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062024,
author={P. -. Tournoux and J. Leguay and F. Benbadis and V. Conan and M. Dias de Amorim and J. Whitbeck},
booktitle={IEEE INFOCOM 2009},
title={The Accordion Phenomenon: Analysis, Characterization, and Impact on DTN Routing},
year={2009},
volume={},
number={},
pages={1116-1124},
abstract={We analyze the dynamics of a mobility dataset collected in a pipelined disruption-tolerant network (DTN), a particular class of intermittently-connected wireless networks characterized by a one-dimensional topology. First, we collected and investigated traces of contact times among a thousand participants of a rollerblading tour in Paris. The dataset shows extreme dynamics in the mobility pattern of a large number of nodes. Most strikingly, fluctuations in the motion of the rollerbladers cause a typical accordion phenomenon - the topology expands and shrinks with time, thus influencing connection times and opportunities between participants. Second, we show through an analytical model that the accordion phenomenon, through the variation of the average node degree, has a major impact on the performance of epidemic dissemination. Finally, we test epidemic dissemination and other existing forwarding schemes on our traces, and argue that routing should adapt to the varying, though predictable, nature of the network. To this end, we propose DA-SW (density-aware spray-and-wait), a measurement-oriented variant of the spray-and-wait algorithm that tunes, in a dynamic fashion, the number of a message copies disseminated in the network. We show that DA-SW leads to performance results that are close to the best case (obtained with an oracle).},
keywords={mobile radio;telecommunication network routing;telecommunication network topology;accordion phenomenon;DTN routing;mobility dataset;pipelined disruption-tolerant network;intermittently-connected wireless networks;one-dimensional topology;epidemic dissemination;forwarding schemes;density-aware spray-and-wait algorithm;Disruption tolerant networking;Routing;Network topology;Peer to peer computing;Spraying;Bluetooth;Interconnected systems;Stability;Communications Society;Wireless networks},
doi={10.1109/INFCOM.2009.5062024},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062025,
author={U. G. Acer and A. A. Abouzeid and S. Kalyanaraman},
booktitle={IEEE INFOCOM 2009},
title={An Evaluation of Weak State Mechanism Design for Indirection in Dynamic Networks},
year={2009},
volume={},
number={},
pages={1125-1133},
abstract={State signaling and maintenance mechanisms play crucial roles in communication network protocols. State is used to facilitate indirections in protocols such as routing. Design approaches for traditional state signaling mechanisms have been categorized into soft and hard state. In both approaches, the state is deterministic. Hence, we call both as having strong state semantics, or more crisply, refer to them as strong state. If the state tracks entities with dynamic nature, strong state rapidly becomes invalidated and needs to be refreshed explicitly through control packets. In this paper, we evaluate the recently proposed weak state. Weak state is a generalization of soft state that is characterized by probabilistic semantics and local updates. It is interpreted as a probabilistic hint and not absolute truth. Weak state also contains the confidence in the state value, which is a measure of the probability that the state remains valid. The confidence or the state semantics is decayed locally without the need for explicit state update traffic traversing the network. The local updates also help the protocol use better estimates for the state value. We define two metrics, pure distortion and informed distortion, to evaluate the consistency of the weak state paradigm and compare it against strong state. Pure distortion measures the average gap between the actual value of the state and the value maintained at a remote node. On the other hand, the use of confidence increases the protocol's ability to cope with even large pure distortion. The resulting effective distortion is captured by the informed distortion metric. Using mathematical analysis, we compare weak with strong state. Local updates reduce the pure distortion because the protocol uses the best estimate of state value. The informed distortion is also significantly less because the probabilistic confidence value hints the protocol if the state is invalid. The weak state mechanism can be used to build protocols (eg: WSR [1]), which systematically interpret the state information. The state itself can be mostly updated locally, with less frequent explicit update messages over the network (i.e. leading to dramatic reductions in control traffic).},
keywords={routing protocols;signalling protocols;weak state mechanism design;dynamic networks;maintenance mechanisms;communication network protocols;routing;state signaling mechanisms;state semantics;strong state;soft state;probabilistic semantics;pure distortion;informed distortion;state value;probabilistic confidence value;Signal design;Distortion measurement;State estimation;USA Councils;Communications Society;Laboratories;Communication networks;Routing protocols;Telecommunication traffic;Mathematical analysis},
doi={10.1109/INFCOM.2009.5062025},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062026,
author={E. Altman and G. Neglia and F. De Pellegrini and D. Miorandi},
booktitle={IEEE INFOCOM 2009},
title={Decentralized Stochastic Control of Delay Tolerant Networks},
year={2009},
volume={},
number={},
pages={1134-1142},
abstract={We study in this paper optimal stochastic control issues in delay tolerant networks. We first derive the structure of optimal 2-hop forwarding policies. In order to be implemented, such policies require the knowledge of some system parameters such as the number of mobiles or the rate of contacts between mobiles, but these could be unknown at system design time or may change over time. To address this problem, we design adaptive policies combining estimation and control that achieve optimal performance in spite of the lack of information. We then study interactions that may occur in the presence of several competing classes of mobiles and formulate this as a cost-coupled stochastic game. We show that this game has a unique Nash equilibrium such that each class adopts the optimal forwarding policy determined for the single class problem.},
keywords={ad hoc networks;decentralised control;mobile radio;optimal control;stochastic games;stochastic systems;telecommunication control;decentralized optimal stochastic control;delay tolerant network;optimal 2-hop forwarding policy;cost-coupled stochastic game;Nash equilibrium;mobile ad hoc network;Stochastic processes;Disruption tolerant networking;Optimal control;Protocols;Routing;Nash equilibrium;Mobile ad hoc networks;Communications Society;Communication system control;Programmable control},
doi={10.1109/INFCOM.2009.5062026},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062027,
author={A. Jabbar and J. P. Rohrer and A. Oberthaler and E. K. Cetinkaya and V. Frost and J. P. G. Sterbenz},
booktitle={IEEE INFOCOM 2009},
title={Performance Comparison of Weather Disruption-Tolerant Cross-Layer Routing Algorithms},
year={2009},
volume={},
number={},
pages={1143-1151},
abstract={With growing demand for high-speed access to mobile handheld devices, there is a significant cost benefit in deploying fixed wireless-mesh networks for backhaul access. However, enabling reliable broadband access over high-frequency radios (such as millimeter-wave networks) posses a fundamental challenge due to weather disruptions in general and rain attenuation in particular. In this paper, we present an analysis of the impact of precipitation on millimeter-wave mesh networks based on radar measurements of real storms in the Midwest US. Furthermore, we compare two novel algorithms that use physical-layer information to optimize routing at the network layer: P-WARP (Predictive Weather-Assisted Routing Protocol) and XL-OSPF (Cross-Layered Open Shortest Path First). Finally, we present simulation studies to compare the performance of the proposed protocols and evaluate the dependability of the end-user service during weather disruptions.},
keywords={routing protocols;predictive weather-assisted routing protocol;mobile handheld devices;wireless-mesh networks;backhaul access;high-frequency radios;millimeter-wave networks;Routing;Millimeter wave measurements;Millimeter wave radar;Handheld computers;Rain;Attenuation;Information analysis;Algorithm design and analysis;Mesh networks;Radar measurements},
doi={10.1109/INFCOM.2009.5062027},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062028,
author={X. Cheng and J. Liu},
booktitle={IEEE INFOCOM 2009},
title={NetTube: Exploring Social Networks for Peer-to-Peer Short Video Sharing},
year={2009},
volume={},
number={},
pages={1152-1160},
abstract={The recent three years have witnessed an explosion of networked video sharing, represented by YouTube, as a new killer Internet application. Their sustainable development however is severely hindered by the intrinsic limit of their client/server architecture. A shift to the peer-to-peer paradigm has been widely suggested with success already shown in live video streaming and movie-on-demand. Unfortunately, our latest measurement demonstrates that short video clips exhibit drastically different statistics, which would simply render these existing solutions suboptimal, if not entirely inapplicable. Our long-term measurement over five million YouTube videos, on the other hand, reveal interesting social networks with strong clustering among the videos, thus opening new opportunities to explore. In this paper, we present NetTube, a novel peer-to- peer assisted delivering framework that explores the clustering in social networks for short video sharing. We address a series of key design issues to realize the system, including a bi-layer overlay, an efficient indexing scheme and a pre-fetching strategy leveraging social networks. We evaluate NetTube through simulations and prototype experiments, which show that it greatly reduces the server workload, improves the playback quality and scales well.},
keywords={peer-to-peer computing;video streaming;peer-to-peer short video sharing;social networks;client/server architecture;video streaming;movie-on-demand;NetTube;bi-layer overlay;Streaming media;YouTube;Servers;Social network services;Peer to peer computing;Accuracy;Indexing},
doi={10.1109/INFCOM.2009.5062028},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062029,
author={M. Varvello and C. Diot and E. W. Biersack},
booktitle={IEEE INFOCOM 2009},
title={P2P Second Life: Experimental Validation Using Kad},
year={2009},
volume={},
number={},
pages={1161-1169},
abstract={Applications such as Second Life require massive deployment of servers worldwide to support a large number of users. We investigate experimentally how Peer-to-Peer (P2P) communication could help cut the deployment cost and increase the scalability of Social Virtual Worlds such as Second Life. We design and build a communication infrastructure that distributes the management of the virtual world among user resources using a structured P2P network. Our communication infrastructure is implemented on the top of Kad, the P2P network that supports millions of eMule users. We then use avatar and object traces collected on Second Life to perform a realistic emulation of P2P Second Life over the Internet. We show that, despite using a standard P2P solution, P2P Second Life is mostly consistent, persistent and scalable. However, the latency avatars experience to recover from an inconsistent view of the virtual world can become disturbing for very large numbers of participants and objects. We analyze and discuss this limitation and give recommendation on how to design P2P Social Virtual Worlds.},
keywords={avatars;Internet;peer-to-peer computing;P2P Second Life;peer-to-peer communication;social virtual world;Kad P2P network;avatar;Internet;Second Life;Avatars;Scalability;Internet;Delay;Web server;Emulation;Virtual environment;File servers;Communications Society},
doi={10.1109/INFCOM.2009.5062029},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062030,
author={W. H. Tang and H. W. Chan},
booktitle={IEEE INFOCOM 2009},
title={MIX-Crowds, an Anonymity Scheme for File Retrieval Systems},
year={2009},
volume={},
number={},
pages={1170-1178},
abstract={In this paper, we propose an anonymous scheme for file retrieval systems, MIX-Crowds, in which it is harder for an attacker to identify the requester of the file by making use of the idea of MIX [7] and Crowds [20] to establish a path from the requester to the file holder. Result shows that predecessor attack [26] is much more difficult to succeed compared with Crowds [20]. We are able to reduce the estimated number of rounds needed for successful predecessor attack for MIX-Crowds. We also propose a file transfer strategy according to file size. With such strategy, requests for small size files can be completed faster while the downloading time of large size files only increases slightly.},
keywords={data privacy;information retrieval systems;MIX-Crowds;anonymity scheme;file retrieval systems;predecessor attack;file transfer strategy;Peer to peer computing;Internet;Routing;Computer science;Protection;Privacy;Publishing;Communications Society;Electronic mail;Discussion forums},
doi={10.1109/INFCOM.2009.5062030},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062031,
author={T. -. Huang and K. -. Chen and P. Huang},
booktitle={IEEE INFOCOM 2009},
title={Tuning Skype's Redundancy Control Algorithm for User Satisfaction},
year={2009},
volume={},
number={},
pages={1179-1187},
abstract={Determining how to transport delay-sensitive voice data has long been a problem in multimedia networking. The difficulty arises because voice and best-effort data are different by nature. It would not be fair to give priority to voice traffic and starve its best-effort counterpart; however, the voice data delivered might not be perceptible if each voice call is limited to the rate of an average TCP flow. To address the problem, we approach it from a user-centric perspective by tuning the voice data rate based on user satisfaction. Our contribution in this work is threefold. First, we investigate how Skype, the largest and fastest growing VoIP service on the Internet, adapts its voice data rate (i.e., the redundancy ratio) to network conditions. Second, by exploiting implementations of public domain codecs, we discover that Skype's mechanism is not really geared to user satisfaction. Third, based on a set of systematic experiments that quantify user satisfaction under different levels of packet loss and burstiness, we derive a concise model that allows user-centric redundancy control. The model can be easily incorporated into general VoIP services (not only Skype) to ensure consistent user satisfaction.},
keywords={customer satisfaction;Internet telephony;quality of service;Skype redundancy control;user satisfaction;delay-sensitive voice data;multimedia networking;voice over IP;public domain codecs;Codecs;Bit rate;Telecommunication traffic;Web and internet services;IP networks;Quality of service;Automatic control;Communications Society;Communication system control;Information science},
doi={10.1109/INFCOM.2009.5062031},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062032,
author={L. Liu and X. Zhang and H. Ma},
booktitle={IEEE INFOCOM 2009},
title={Dynamic Node Collaboration for Mobile Target Tracking in Wireless Camera Sensor Networks},
year={2009},
volume={},
number={},
pages={1188-1196},
abstract={Compared to the other types of sensor networks, the wireless camera sensor networks can offer much more comprehensive and accurate information in mobile target tracking applications. We propose a dynamic node collaboration scheme for mobile target tracking in wireless camera sensor networks. Unlike the traditional sensing models, we develop a nonlinear localization-oriented sensing model for camera sensors by taking the perspective projection and the observation noises into account. Based on our sensing model, we apply the sequential Monte Carlo (SMC) technique to estimate the belief state of the target location. In order to implement the SMC based tracking mechanism efficiently, we propose a dynamic node collaboration scheme, which can balance the tradeoff between the quality of tracking and the network cost. Our scheme deploys the dynamic cluster architecture which mainly includes the following two components. First, we design a scheme to elect the cluster heads during the tracking process. Second, we develop an optimization-based algorithm to select an optimal subset of camera sensors as the cluster members for estimating the target location cooperatively. Also conducted is a set of extensive simulations to validate and evaluate our proposed schemes.},
keywords={cameras;mobile radio;Monte Carlo methods;optimisation;target tracking;wireless sensor networks;wireless camera sensor network;mobile target tracking application;dynamic node collaboration scheme;nonlinear localization-oriented sensing model;sequential Monte Carlo technique;SMC technique;dynamic cluster architecture;optimization-based algorithm;Wireless sensor networks;Collaboration;Target tracking;Cameras;Intelligent sensors;Vehicle dynamics;Peer to peer computing;Image sensors;Sensor phenomena and characterization;Sliding mode control},
doi={10.1109/INFCOM.2009.5062032},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062033,
author={D. Zhou and J. Gao},
booktitle={IEEE INFOCOM 2009},
title={Opportunistic Processing and Query of Motion Trajectories in Wireless Sensor Networks},
year={2009},
volume={},
number={},
pages={1197-1205},
abstract={We study the problem of in-network processing and queries of trajectories of moving targets in a sensor network. The main idea is to exploit the spatial coherence of target trajectories for opportunistic information dissemination with no or small extra communication cost, as well as for efficient probabilistic queries searching for a given target signature in a real-time manner. Sensors near a moving target are waken up to record information about this target and take the communication opportunities to exchange their knowledge with preceding and descending sensor nodes along the trajectory. Thus a moving target's information is naturally detected, recorded, and disseminated along its trajectory, as well as the motion trajectories that enter the sensor field afterwards. We analyzed and through simulations tested the dissemination cost and query success rate for randomly generated data sets. Trajectories of reasonable length can be discovered by probabilistic in-network queries with high probability. Compared with the scheme without opportunistic dissemination, the in-network processing of trajectories, with modest cost on dissemination, allows substantially reduced query cost and delay.},
keywords={data handling;information dissemination;query processing;wireless sensor networks;opportunistic processing;motion trajectories query;wireless sensor networks;information dissemination;probabilistic in-network queries;Wireless sensor networks;Trajectory;Costs;Event detection;Monitoring;Target tracking;Motion detection;Delay;Base stations;Intelligent sensors},
doi={10.1109/INFCOM.2009.5062033},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062034,
author={Y. Yang and I. -. Hou and J. C. Hou and M. Shankar and N. S. V. Rao},
booktitle={IEEE INFOCOM 2009},
title={Sensor Placement for Detecting Propagative Sources in Populated Environments},
year={2009},
volume={},
number={},
pages={1206-1214},
abstract={We consider the placement of sensors to detect propagative sources where the sensing area of each sensor is anisotropic and arbitrarily-shaped due to the terrain and meteorological conditions. The propagation and detection times are non-negligible due to the propagation of source effects through space at a slow speed. We formulate the problem as placing the minimum number of sensors to ensure a detection time T and the coverage utility C. Both the sensing areas of sensors and the utility function U(ldr) are chosen to capture the environmental factors and the population distribution. We show this problem to be NP-hard, and present heuristic algorithms for 1-coverage and fc-coverage by adopting exiting methods. We evaluate the proposed algorithms in the realistic setting of Port of Memphis where the objective is to protect the population against chemical leaks or attacks. We utilize the SCIPUFF dispersion model to determine the sensing areas by accounting for the terrain and meteorological conditions, and use the real-life population distribution as the utility function. Based on empirical study, we make several important observations.},
keywords={computational complexity;optimisation;wireless sensor networks;sensor placement;anisotropic sensor;detection time;coverage utility;utility function;environmental factors;NP-hard problem;heuristic algorithms;Port of Memphis;SCIPUFF dispersion model;real-life population distribution;Meteorology;Sensor phenomena and characterization;Protection;Chemical sensors;Acoustic propagation;Optical propagation;Anisotropic magnetoresistance;Hardware;Prototypes;Leak detection},
doi={10.1109/INFCOM.2009.5062034},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062035,
author={Z. Zhong and T. Zhu and D. Wang and T. He},
booktitle={IEEE INFOCOM 2009},
title={Tracking with Unreliable Node Sequences},
year={2009},
volume={},
number={},
pages={1215-1223},
abstract={Tracking mobile targets using sensor networks is a challenging task because of the impacts of in-the-flled factors such as environment noise, sensing irregularity and etc. This paper proposes a robust tracking framework using node sequences, an ordered list extracted from unreliable sensor readings. Instead of estimating each position point separately in a movement trace, we convert the original tracking problem to the problem of finding the shortest path in a graph, which is equivalent to optimal matching of a series of node sequences. In addition to the basic design, multidimensional smoothing is developed to enhance tracking accuracy. Practical system deployment related issues are discussed in the paper, and the design is evaluated with both simulation and a system implementation using Pioneer III Robot and MICAz sensor nodes. In fact, tracking with node sequences provides a useful layer of abstraction, making the design framework generic and compatible with different physical sensing modalities.},
keywords={tracking;wireless sensor networks;unreliable node sequences;tracking;sensor networks;shortest path;optimal matching;Target tracking;Peer to peer computing;Wireless sensor networks;Working environment noise;Surveillance;Monitoring;Communications Society;Helium;Computer science;USA Councils},
doi={10.1109/INFCOM.2009.5062035},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062036,
author={E. Kehdi and B. Li},
booktitle={IEEE INFOCOM 2009},
title={Null Keys: Limiting Malicious Attacks Via Null Space Properties of Network Coding},
year={2009},
volume={},
number={},
pages={1224-1232},
abstract={The performance of randomized network coding can suffer significantly when malicious nodes corrupt the content of the exchanged blocks. Previous work have introduced error correcting codes by generalizing some well known bounds in coding theory. Such codes are based on introducing redundancy in space domain. Other approaches require the use of homomorphic hashing functions, which are computationally expensive. In this paper, we present a novel and computationally efficient security algorithm, referred to as Null Keys, to detect and contain malicious attacks based on the subspace properties of random linear network coding. The participating nodes verify the integrity of a block by checking if it belongs to the subspace spanned by the source blocks. This is possible when every node has a vector orthogonal to all the combinations of the source blocks. These vectors, referred to as null keys, belong to the null space of the source blocks and go through a random combination when distributed by the source. Unlike previous security approaches, our Null Keys algorithm allows nodes to rapidly detect corrupted blocks without changing the code or imposing redundancy on the exchanged data. We analytically evaluate the pollution produced by jamming attacks, and demonstrate the effectiveness of Null Keys by varying the strength of the malicious nodes. We also show, through extensive simulations, that the Null Keys approach is more effective than cooperative security using homomorphic hashing when it comes to limiting the pollution spread.},
keywords={block codes;cryptography;random codes;telecommunication security;null key security algorithm;malicious attack detection;null space property;randomized network coding;block integrity verification;homomorphic hashing;Null space;Network coding;Redundancy;Vectors;Data security;Pollution;Error correction codes;Computer networks;Data analysis;Jamming},
doi={10.1109/INFCOM.2009.5062036},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062037,
author={Q. Wang and H. Khurana and Y. Huang and K. Nahrstedt},
booktitle={IEEE INFOCOM 2009},
title={Time Valid One-Time Signature for Time-Critical Multicast Data Authentication},
year={2009},
volume={},
number={},
pages={1233-1241},
abstract={It is challenging to provide authentication to time-critical multicast data, where low end-to-end delay is of crucial importance. Consequently, it requires not only efficient authentication algorithms to minimize computational cost, but also avoidance of buffering packets so that the data can be immediately processed once being presented. Desirable properties for a multicast authentication scheme also include small communication overhead, tolerance to packet loss, and resistance against malicious attacks. In this paper, we propose a novel signature model - Time Valid One-Time Signature (TV-OTS) - to boost the efficiency of regular one-time signature schemes. Based on the TV-OTS model, we design an efficient multicast authentication scheme "TV-HORS" to meet the above needs. TV-HORS combines one-way hash chains with TV-OTS to avoid frequent public key distribution. It provides fast signing/verification and buffering-free data processing, which make it one of the fastest multicast authentication schemes to date in terms of end-to-end computational latency (on the order of microseconds). In addition, TV-HORS has perfect tolerance to packet loss and strong robustness against malicious attacks. The communication overhead of TV-HORS is much smaller than regular OTS schemes, and even smaller than RSA signature. The only drawback of TV-HORS is a relatively large public key of size 8 KB to 10 KB, depending on parameters.},
keywords={digital signatures;public key cryptography;time valid one-time signature;time-critical multicast data authentication;buffering packets;communication overhead;packet loss tolerance;malicious attack resistance;one-time signature schemes;public key distribution;buffering-free data processing;end-to-end computational latency;Time factors;Authentication;Public key;Computational efficiency;Phasor measurement units;Multicast algorithms;Power grids;Communications Society;Delay effects;Data processing},
doi={10.1109/INFCOM.2009.5062037},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062038,
author={A. Liu and P. Ning and C. Wang},
booktitle={IEEE INFOCOM 2009},
title={Lightweight Remote Image Management for Secure Code Dissemination in Wireless Sensor Networks},
year={2009},
volume={},
number={},
pages={1242-1250},
abstract={Wireless sensor networks are considered ideal candidates for a wide range of applications. It is desirable and sometimes necessary to reprogram sensor nodes through wireless links after they are deployed to remove bugs or add new functionalities. Several approaches (e.g., Seluge, Sluice) have been proposed recently for secure code dissemination in wireless sensor networks, all as security extensions to the state-of-the- art code dissemination system named Deluge. However, existing approaches all focused on securing the propagation of code images, but overlooked the security vulnerabilities in other image management aspects such as rebooting and erasing code images. In this paper, we identify the security vulnerabilities in epidemic image management in all existing solutions to secure code dissemination in wireless sensor networks. Such vulnerabilities allow an attacker to reboot a sensor network to undesirable images or erase critical images, exposing the network to security risks. We then develop a sequence of lightweight techniques to address these vulnerabilities. Our approach takes into consideration the limited resources on current sensor platforms, and removes the security vulnerabilities without introducing significant overhead. To evaluate the feasibility of our approach, we implement the proposed approach as a remote image management system named Seluge-ImageMan, which is intended to work with Seluge, a security extension to Deluge for injecting new code images. We perform a substantial set of experiments in the WiSeNeT sensor testbed, which consists of 72 MicaZ motes, to assess the performance overhead of Seluge-ImageMan. The experimental results indicate that our approach introduces very light overhead while completing the secure remote code image management solution for wireless sensor networks.},
keywords={image coding;wireless sensor networks;lightweight remote image management;secure code dissemination;wireless sensor networks;rebooting;Wireless sensor networks;Image sensors;USA Councils;Communication system security;Computer bugs;Performance evaluation;Testing;Communications Society;Computer network management;Computer science},
doi={10.1109/INFCOM.2009.5062038},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062039,
author={R. Zhang and Y. Zhang and K. Ren},
booktitle={IEEE INFOCOM 2009},
title={DPAC: Distributed Privacy-Preserving Access Control in Sensor Networks},
year={2009},
volume={},
number={},
pages={1251-1259},
abstract={The owner and users of a sensor network may be different, which necessitates privacy-preserving access control. On the one hand, the network owner need enforce strict access control so that the sensed data are only accessible to users willing to pay. On the other hand, users wish to protect their respective data access patterns whose disclosure may be used against their interests. This paper presents DP2AC, a Distributed Privacy- Preserving Access Control scheme for sensor networks, which is the first work of its kind. Users in DP2AC purchase tokens from the network owner whereby to query data from sensor nodes which will reply only after validating the tokens. The use of blind signatures in token generation ensures that tokens are publicly verifiable yet unlinkable to user identities, so privacy- preserving access control is achieved. A central component in DP2AC is to prevent malicious users from reusing tokens. We propose a suite of distributed techniques for token-reuse detection (TRD) and thoroughly compare their performance with regard to TRD capability, communication overhead, storage overhead, and attack resilience. The efficacy and efficiency of DP2AC are confirmed by detailed performance evaluations.},
keywords={authorisation;data privacy;distributed sensors;query processing;distributed privacy-preserving access control;sensor networks;query data;blind signatures;token generation;token-reuse detection;storage overhead;attack resilience;Access control;Base stations;Protection;Data privacy;Oceans;Sensor systems;Business;Companies;Communications Society;Peer to peer computing},
doi={10.1109/INFCOM.2009.5062039},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062040,
author={S. Borst and N. Hegde and A. Proutiere},
booktitle={IEEE INFOCOM 2009},
title={Mobility-Driven Scheduling in Wireless Networks},
year={2009},
volume={},
number={},
pages={1260-1268},
abstract={The design of scheduling policies for wireless data systems has been driven by a compromise between the objectives of high overall system throughput and the degree of fairness among users, while exploiting multi-user diversity, i.e., fast-fading variations. These policies have been thoroughly investigated in the absence of user mobility, i.e., without slow fading variations. In the present paper, we examine the impact of intra- and inter-cell user mobility on the trade-off between throughput and fairness, and on the suitable choice of alpha-fair scheduling policies. We consider a dynamic setting where users come and go over time as governed by random finite-size data transfers, and explicitly allow for users to roam around. It is demonstrated that the overall performance improves as the fairness parameter alpha is reduced, and in particular, that proportional fair scheduling may yield relatively poor performance, in sharp contrast to the standard scenario with only fast fading. Since a lower alpha tends to affect short-term fairness, we explore how to set the fairness parameter so as to strike the right balance between overall performance and short-term fairness. It is further established that mobility tends to improve the performance, even when the network operates under a local fair scheduling policy as opposed to a globally optimal strategy. We present extensive simulation results to confirm and illustrate the analytical findings.},
keywords={cellular radio;diversity reception;multiuser channels;random processes;scheduling;mobility-driven scheduling;wireless network;wireless data system;multiuser channel diversity;cell user mobility;random finite-size data transfer;Wireless networks;Throughput;Delay;Data systems;Resource management;Fading;Transmitters;Fluctuations;Communications Society;USA Councils},
doi={10.1109/INFCOM.2009.5062040},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062041,
author={S. -. Lee and S. Choudhury and A. Khoshnevis and S. Xu and S. Lu},
booktitle={IEEE INFOCOM 2009},
title={Downlink MIMO with Frequency-Domain Packet Scheduling for 3GPP LTE},
year={2009},
volume={},
number={},
pages={1269-1277},
abstract={This paper addresses the problem of frequency domain packet scheduling (FDPS) incorporating spatial division multiplexing (SDM) multiple input multiple output (MIMO) techniques on the 3GPP long term evolution (LTE) downlink. We impose the LTE MIMO constraint of selecting only one MIMO mode (spatial multiplexing or transmit diversity) per user per transmission time interval (TTI). First, we address the optimal MIMO mode selection (multiplexing or diversity) per user in each TTI in order to maximize the proportional fair (PF) criterion extended to frequency and spatial domains. We prove that the SU-MIMO (single-user MIMO) FDPS problem under the LTE requirement is NP-hard and therefore, we develop two approximation algorithms (one with full channel feedback and the other with partial channel feedback) with provable performance bounds. Based on 3GPP LTE system model simulations, the approximation algorithm with partial channel feedback is shown to have comparable performance to the one with full channel feedback, while significantly reducing the channel feedback overhead by nearly 50%.},
keywords={3G mobile communication;computational complexity;diversity reception;feedback;frequency division multiple access;MIMO communication;OFDM modulation;radio links;scheduling;space division multiplexing;wireless channels;LTE MIMO downlink;frequency-domain packet scheduling;3GPP long term evolution downlink;spatial division multiplexing;SDM;multiple input multiple output technique;transmit diversity;optimal single-user-MIMO mode selection;proportional fair criterion;FDPS problem;NP-hard problem;approximation algorithm;channel feedback;performance bound;OFDMA;Downlink;MIMO;Scheduling algorithm;Feedback;Base stations;Bandwidth;Laboratories;Frequency domain analysis;Frequency diversity;Approximation algorithms},
doi={10.1109/INFCOM.2009.5062041},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062042,
author={M. Andrews and L. Zhang},
booktitle={IEEE INFOCOM 2009},
title={Multiserver Scheduling with Contiguity Constraints},
year={2009},
volume={},
number={},
pages={1278-1286},
abstract={We consider a scheduling problem in which multiple servers are available to service multiple users in a time-slotted system. Each server has a time-dependent and user-dependent service rate for each user at each timeslot. A schedule specifies which server serves which user per timeslot, with a typical objective of maximizing the total rate that the users receive. The servers are numbered by a set of consecutive integers. A strict contiguity constraint enforces that each user is served by at most one contiguous interval of the servers. We show that a strict contiguity requirement makes the scheduling problem APX hard to solve, which means we cannot approximate an optimal schedule arbitrarily closely. On the positive side, we also offer two approximation algorithms, a simple one that guarantees a logarithmic approximation and a more complex one that guarantees a constant approximation. In addition, we also present a complete taxonomy of the scheduling variants that consider issues in the following dimensions, (i) Strict contiguity vs soft contiguity, where the latter allows multiple intervals of servers to serve the same user but imposes a penalty; (ii) Full buffer vs finite buffer, where the former assumes each user always has a large queue and for the latter the service a user receives is upper bounded by its queue size; (iii) slot-by-slot scheduling vs template scheduling, where the former creates a schedule for every timeslot and the latter creates a schedule for multiple timeslots at a time; (iv) Time-variant vs time-invariant service rates for template scheduling. For each variant, we present optimal algorithms if possible and approximation algorithms whenever NP-hardness prevails.},
keywords={approximation theory;computational complexity;network servers;optimisation;scheduling;multiserver scheduling;time-slotted system;time-dependent service rate;user-dependent service rate;APX hard problem;approximation algorithms;logarithmic approximation;constant approximation;strict contiguity;soft contiguity;full buffer;finite buffer;slot-by-slot scheduling;template scheduling;time-invariant service rates;NP-hardness;Optimal scheduling;Approximation algorithms;Traffic control;Communications Society;Taxonomy;Processor scheduling;Heuristic algorithms;Time measurement},
doi={10.1109/INFCOM.2009.5062042},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062043,
author={A. L. Stolyar and H. Viswanathan},
booktitle={IEEE INFOCOM 2009},
title={Self-Organizing Dynamic Fractional Frequency Reuse for Best-Effort Traffic through Distributed Inter-Cell Coordination},
year={2009},
volume={},
number={},
pages={1287-1295},
abstract={Self-optimization of the network, for the purposes of improving overall capacity and/or cell edge data rates, is an important objective for next generation cellular systems. We propose algorithms that automatically create efficient, soft fractional frequency reuse (FFR) patterns for enhancing performance of orthogonal frequency division multiple access (OFDMA) based cellular systems for forward link best effort traffic. The Multi- sector Gradient (MGR) algorithm adjusts the transmit powers of the different sub-bands by systematically pursuing maximization of the overall network utility. We show that the maximization can be done by sectors operating in a semi-autonomous way, with only some gradient information exchanged periodically by neighboring sectors. The Sector Autonomous (SA) algorithm adjusts its transmit powers in each sub-band independently in each sector using a non-trivial heuristic to achieve out- of-cell interference mitigation. This algorithm is completely autonomous and requires no exchange of information between sectors. Through extensive simulations, we demonstrate that both algorithms provide substantial performance improvements. In particular, they can improve the cell edge data throughputs significantly, by up to 66% in some cases for the MGR, while maintaining the overall sector throughput at the same level as that achieved by the traditional approach. The simulations also show that both algorithms lead the system to "self-organize" into efficient, soft FFR patterns with no a priori frequency planning.},
keywords={frequency allocation;OFDM modulation;self-organizing dynamic fractional frequency reuse;best-effort traffic;distributed inter-cell coordination;orthogonal frequency division multiple access;multi-sector gradient algorithm;cellular systems;sector autonomous algorithm;interference mitigation;Frequency conversion;Interference;Throughput;Telecommunication traffic;Utility programs;MIMO;Scheduling algorithm;Fading;Communications Society;Next generation networking},
doi={10.1109/INFCOM.2009.5062043},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062044,
author={A. Bremler-Barr and D. Hay and D. Hendler and R. M. Roth},
booktitle={IEEE INFOCOM 2009},
title={PEDS: A Parallel Error Detection Scheme for TCAM Devices},
year={2009},
volume={},
number={},
pages={1296-1304},
abstract={Ternary content-addressable memory (TCAM) devices are increasingly used for performing high-speed packet classification. A TCAM consists of an associative memory that compares a search key in parallel against all entries. TCAMs may suffer from error events that cause ternary cells to change their value to any symbol in the ternary alphabet "0","1","*". Due to their parallel access feature, standard error detection schemes are not directly applicable to TCAMs; an additional difficulty is posed by the special semantic of the "*" symbol. This paper introduces PEDS, a novel parallel error detection scheme that locates the erroneous entries in a TCAM device. PEDS is based on applying an error-detection code to each TCAM entry, and utilizing the parallel capabilities of the TCAM, by simultaneously checking the correctness of multiple TCAM entries. A key feature of PEDS is that the number of TCAM lookup operations required to locate all errors depends on the number of symbols per entry rather than the (orders-of-magnitude larger) number of TCAM entries. For large TCAM devices, a specific instance of PEDS requires only 200 lookups for 100-symbol entries, while a naive approach may need hundreds of thousands lookups. PEDS allows flexible and dynamic selection of trade-off points between robustness, space complexity, and number of lookups.},
keywords={computational complexity;content-addressable storage;error detection;PEDS;parallel error detection scheme;TCAM devices;ternary content-addressable memory devices;high-speed packet classification;associative memory;ternary cells;space complexity;Random access memory;Associative memory;Peer to peer computing;Communications Society;Computer vision;Error correction codes;Robustness;Routing;Monitoring;Data security},
doi={10.1109/INFCOM.2009.5062044},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062045,
author={A. Bremler-Barr and D. Hay and D. Hendler},
booktitle={IEEE INFOCOM 2009},
title={Layered Interval Codes for TCAM-Based Classification},
year={2009},
volume={},
number={},
pages={1305-1313},
abstract={Ternary content-addressable memories (TCAMs) are increasingly used for high-speed packet classification. TCAMs compare packet headers against all rules in a classification database in parallel and thus provide high throughput. TCAMs are not well-suited, however, for representing rules that contain range fields and prior art algorithms typically represent each such rule by multiple TCAM entries. The resulting range expansion can dramatically reduce TCAM utilization because it introduces a large number of redundant TCAM entries. This redundancy can be mitigated by making use of extra bits, available in each TCAM entry. We present a scheme for constructing efficient representations of range rules, based on the simple observation that sets of disjoint ranges may be encoded much more efficiently than sets of overlapping ranges. Since the ranges in real-world classification databases are, in general, non-disjoint, the algorithms we present split ranges between multiple layers each of which consists of mutually disjoint ranges. Each layer is then coded independently and assigned its own set of extra bits. Our layering algorithms are based on approximations for specific variants of interval-graph coloring. We evaluate these algorithms by performing extensive comparative analysis on real-life classification databases. Our analysis establishes that our algorithms reduce the number of redundant TCAM entries caused by range rules by more than 60% as compared with best range-encoding prior art.},
keywords={codes;content-addressable storage;graph colouring;layered interval codes;TCAM-based classification;ternary content-addressable memories;high-speed packet classification;interval-graph coloring;real-life classification databases;Databases;Computer science;Throughput;Art;Algorithm design and analysis;Communications Society;Performance evaluation;Performance analysis;Data analysis;Internet},
doi={10.1109/INFCOM.2009.5062045},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062046,
author={R. McGeer and P. Yalagandula},
booktitle={IEEE INFOCOM 2009},
title={Minimizing Rulesets for TCAM Implementation},
year={2009},
volume={},
number={},
pages={1314-1322},
abstract={Packet classification is a function increasingly used in a number of networking appliances and applications. Typically, this consists of a set of abstract classifications, and a set of rules which sort packets into the various classifications. For packet classification at line speeds, Ternary Content-Addressable Memories (TCAMs) have become a norm in most networking hardware. However, TCAMs are expensive and power-hungry. Hence, a packet classification ruleset need to be minimized before populating the TCAM. In this paper, we formulate the Ruleset Minimization Problem for TCAM as an abstract optimization problem based on two-level logic minimization, and propose an exact solution and a number of heuristics. We present experimental results with two different datasets-artificial filter sets generated using ClassBench tool suite and a real firewall Access Control List (ACL) from a large enterprise. We observe an average reduction of 41% in artificial filter sets and 72.5% reduction in the firewall ACL using the proposed heuristics.},
keywords={authorisation;content-addressable storage;pattern classification;TCAM implementation;packet classification;abstract classifications;ternary content-addressable memories;ruleset minimization problem;two-level logic minimization;artificial filter sets;firewall access control list;Minimization;Protocols;Home appliances;Switches;Filters;Packet switching;Logic testing;Sequential analysis;Communications Society;Hardware},
doi={10.1109/INFCOM.2009.5062046},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062047,
author={A. Vishwanath and V. Sivaraman and G. N. Rouskas},
booktitle={IEEE INFOCOM 2009},
title={Considerations for Sizing Buffers in Optical Packet Switched Networks},
year={2009},
volume={},
number={},
pages={1323-1331},
abstract={Optical packet switches of the foreseeable future are expected to have severely limited buffering capability, since storage of optical signals remains a difficult and expensive operation. Our observations in simulation of TCP and real-time traffic in networks with such small buffers have revealed regions of anomalous performance in which losses for real-time traffic become higher as buffers get larger. The detrimental impact of larger optical buffers is studied in this paper and three new contributions are made. First, we develop a Markov chain model that allows analytical computation of loss. Our model validates observations from simulation, and opens the doors to an analytical understanding of how various factors affect the anomaly. Second, we study the anomaly under realistic traffic mixes containing persistent and non-persistent TCP flows, and show that the traffic mix does not significantly alter the anomaly. Third, we show that larger diversity in packet size between TCP and real-time traffic increases the severity of the anomaly, and is an important consideration when sizing optical switch buffers, particularly since real-time and TCP ACK packets are significantly smaller than the TCP data packets. Our study informs switch manufacturers and network operators of factors to consider when selecting optical buffer sizes in order to achieve desired performance balance between TCP and real-time traffic.},
keywords={buffer circuits;Markov processes;optical switches;packet switching;telecommunication traffic;transport protocols;optical packet switched networks;optical signals;real-time traffic;optical buffers;Markov chain model;persistent TCP flows;nonpersistent TCP flows;optical switch buffers;TCP ACK packets;switch manufacturers;network operators;optical buffer sizes;Optical buffering;Optical packet switching;Telecommunication traffic;Traffic control;Buffer storage;Optical losses;Analytical models;Optical switches;Performance loss;Computational modeling},
doi={10.1109/INFCOM.2009.5062047},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062048,
author={M. Andrews and M. Dinitz},
booktitle={IEEE INFOCOM 2009},
title={Maximizing Capacity in Arbitrary Wireless Networks in the SINR Model: Complexity and Game Theory},
year={2009},
volume={},
number={},
pages={1332-1340},
abstract={In this paper we consider the problem of maximizing the number of supported connections in arbitrary wireless networks where a transmission is supported if and only if the signal-to-interference-plus-noise ratio at the receiver is greater than some threshold. The aim is to choose transmission powers for each connection so as to maximize the number of connections for which this threshold is met. We believe that analyzing this problem is important both in its own right and also because it arises as a subproblem in many other areas of wireless networking. We study both the complexity of the problem and also present some game theoretic results regarding capacity that is achieved by completely distributed algorithms. We also feel that this problem is intriguing since it involves both continuous aspects (i.e. choosing the transmission powers) as well as discrete aspects (i.e. which connections should be supported). Our results are: ldr We show that maximizing the number of supported connections is NP-hard, even when there is no background noise. This is in contrast to the problem of determining whether or not a given set of connections is feasible since that problem can be solved via linear programming. ldr We present a number of approximation algorithms for the problem. All of these approximation algorithms run in polynomial time and have an approximation ratio that is independent of the number of connections. ldr We examine a completely distributed algorithm and analyze it as a game in which a connection receives a positive payoff if it is successful and a negative payoff if it is unsuccessful while transmitting with nonzero power. We show that in this game there is not necessarily a pure Nash equilibrium but if such an equilibrium does exist the corresponding price of anarchy is independent of the number of connections. We also show that a mixed Nash equilibrium corresponds to a probabilistic transmission strategy and in this case such an equilibrium always exists and has a price of anarchy that is independent of the number of connections. This work was supported by NSF contract CCF-0728980 and was performed while the second author was visiting Bell Labs in Summer, 2008.},
keywords={approximation theory;channel capacity;computational complexity;distributed algorithms;game theory;radio networks;radio receivers;arbitrary wireless network capacity maximization;game theory;wireless receiver;distributed algorithm complexity;NP-hard problem;polynomial time approximation algorithm;channel condition;Wireless networks;Signal to noise ratio;Game theory;Distributed algorithms;Approximation algorithms;Nash equilibrium;Background noise;Linear programming;Polynomials;Algorithm design and analysis},
doi={10.1109/INFCOM.2009.5062048},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062049,
author={R. Gummadi and K. Jung and D. Shah and R. Sreenivas},
booktitle={IEEE INFOCOM 2009},
title={Computing the Capacity Region of a Wireless Network},
year={2009},
volume={},
number={},
pages={1341-1349},
abstract={We consider a wireless network of n nodes that communicate over a common wireless medium under some interference constraints. Our work is motivated by the need for an efficient and distributed algorithm to determine the n2 dimensional unicast capacity region of such a wireless network. Equivalently, given a vector of end-to-end rates between various source-destination pairs, we seek to determine if it can be supported by the network through a combination of routing and scheduling decisions. This question is known to be NP-hard and hard to even approximate within n<sup>1-o(1)</sup> factor for general graphs. In this paper, we first show that the whole n<sup>2</sup> dimensional unicast capacity region can be approximated to (1 plusmn epsiv) factor in polynomial time, and in a distributed manner, whenever the Max Weight Independent Set (MWIS) problem can be approximated in a similar fashion for the corresponding topology. We then consider wireless networks which are usually formed between nodes that are placed in a geographic area and come endowed with a certain geometry, and argue that such situations do lead to approximations to the MWIS problem (in fact, in a completely distributed manner, in a time that is essentially linear in n). Consequently, this gives us a polynomial algorithm to approximate the capacity of wireless networks to arbitrary accuracy. This result hence, is in sharp contrast with previous works that provide algorithms with at least a constant factor loss. An important ingredient in establishing our result is the transient analysis of the maximum weight scheduling algorithm, which can be of interest in its own right.},
keywords={approximation theory;computational complexity;distributed algorithms;radio networks;radiofrequency interference;scheduling;set theory;telecommunication network routing;telecommunication network topology;vectors;wireless network topology;interference constraint;distributed algorithm;dimensional unicast capacity region;end-to-end rate vector;network routing decision;network scheduling decision;NP-hard problem;polynomial time approximation;max weight independent set problem;Computer networks;Wireless networks;Unicast;Polynomials;Interference constraints;Distributed algorithms;Routing;Network topology;Geometry;Transient analysis},
doi={10.1109/INFCOM.2009.5062049},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062050,
author={C. W. Tan and M. Chiang and R. Srikant},
booktitle={IEEE INFOCOM 2009},
title={Fast Algorithms and Performance Bounds for Sum Rate Maximization in Wireless Networks},
year={2009},
volume={},
number={},
pages={1350-1358},
abstract={Sum rate maximization by power control is an important, challenging, and extensively studied problem in wireless networks. It is a nonconvex optimization problem and achieves a rate region that is in general nonconvex. We derive approximation ratios to the sum rate objective by studying the solutions to two related problems, sum rate maximization using an SIR approximation and max-min weighted SIR optimization. We also show that these two problems can be solved very efficiently, using much faster algorithms than the existing ones in the literature. Furthermore, using a new parameterization of the sum rate maximization problem, we obtain a characterization of the power controlled rate region and its convexity property in various asymptotic regimes. Engineering implications are discussed for IEEE 802.11 networks.},
keywords={concave programming;minimax techniques;power control;radio networks;radiofrequency interference;sum rate maximization;wireless network;power control;nonconvex optimization problem;signal-to-interference ratio approximation;max-min weighted SIR optimization;Wireless networks;Power control;Approximation algorithms;USA Councils;Time sharing computer systems;Closed-form solution;Interference channels;Communications Society;Power engineering and energy;Distributed algorithms},
doi={10.1109/INFCOM.2009.5062050},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062051,
author={A. Baron and R. Ginosar and I. Keslassy},
booktitle={IEEE INFOCOM 2009},
title={The Capacity Allocation Paradox},
year={2009},
volume={},
number={},
pages={1359-1367},
abstract={The Capacity Allocation Paradox (CAP) destabilizes a stable small-buffer network when a link capacity is increased. CAP is demonstrated in a basic 2 times 1 network topology. We show that it applies to fluid, wormhole and packet-switched networks, and prove that it applies to various scheduling algorithms such as fixed-priority, round-robin and exhaustive round-robin. Their capacity regions are modeled and surprising phenomena are described. For instance, once increasing a link capacity destabilizes a stable network, increasing it further to infinity might never restore stability. Further, we exhibit networks with arbitrarily tight link-capacity stability regions, in which any small deviation from an optimal link capacity might make the network unstable. Finally, we suggest ways to mitigate CAP, e.g. by using GPS scheduling.},
keywords={frequency allocation;network topology;packet switching;capacity allocation paradox;network topology;packet-switched networks;scheduling algorithms;exhaustive round-robin;round-robin;fixed-priority;Network topology;Stability;Queueing analysis;Algorithm design and analysis;Communications Society;Radio access networks;Scheduling algorithm;H infinity control;Global Positioning System;Information theory},
doi={10.1109/INFCOM.2009.5062051},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062052,
author={S. Kini and S. Ramasubramanian and A. Kvalbein and A. F. Hansen},
booktitle={IEEE INFOCOM 2009},
title={Fast Recovery from Dual Link Failures in IP Networks},
year={2009},
volume={},
number={},
pages={1368-1376},
abstract={This paper develops a novel mechanism for recovering from dual link failures in IP networks. The highlight of the developed routing approach is that a node re-routes a packet around the failed link without the knowledge of the second link failure. The proposed technique requires three protection addresses for every node, in addition to the normal address. Associated with every protection address of a node is a protection graph. Each link connected to the node is removed in at least one of protection graphs and every protection graph is guaranteed to be two-edge connected. The network recovers from the first failure by tunneling the packet to the next-hop node using one of the protection addresses of the next-hop node; and the packet is routed over the protection graph corresponding to that protection address. We prove that it is sufficient to provide up to three protection addresses per node to tolerate any arbitrary two link failures in a three-edge connected graph. We evaluate the effectiveness of the proposed technique over several network topologies.},
keywords={computer network reliability;graph theory;Internet;IP networks;telecommunication network routing;telecommunication network topology;dual link failure recovery;IP network;routing approach;protection graph;network topology;Internet;IP networks;Protection;Peer to peer computing;Routing;Costs;Tunneling;Network topology;Internet;Robustness;Computer networks},
doi={10.1109/INFCOM.2009.5062052},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062053,
author={P. Barford and N. Duffield and A. Ron and J. Sommers},
booktitle={IEEE INFOCOM 2009},
title={Network Performance Anomaly Detection and Localization},
year={2009},
volume={},
number={},
pages={1377-1385},
abstract={Detecting the occurrence and location of performance anomalies (e.g., high jitter or loss events) is critical to ensuring the effective operation of network infrastructures. In this paper we present a framework for detecting and localizing performance anomalies based on using an active probe-enabled measurement infrastructure deployed on the periphery of a network. Our framework has three components: an algorithm for detecting performance anomalies on a path, an algorithm for selecting which paths to probe at a given time in order to detect performance anomalies (where a path is defined as the set of links between two measurement nodes), and an algorithm for identifying the links that are causing an identified anomaly on a path (i.e., localizing). The problem of detecting an anomaly on a path is addressed by comparing probe-based measures of performance characteristics with performance guarantees for the network (e.g., SLAs). The path selection algorithm is designed to enable a tradeoff between ensuring that all links in a network are frequently monitored to detect performance anomalies, while minimizing probing overhead. The localization algorithm is designed to use existing path measurement data in such a way as to minimize the number of paths necessary for additional probing in order to identify the link(s) responsible for an observed performance anomaly. We assess the feasibility of our framework and algorithms by implementing them in ns-2 and conducting a set of simulation-based experiments using several different network topologies. Our results show that our method is able to accurately detect and localize performance anomalies in a timely fashion and with lower probe and computational overheads than previously proposed methodologies.},
keywords={security of data;network performance anomaly detection;occurrence detection;active probe-enabled measurement infrastructure;probe-based measures;path selection algorithm;localization algorithm;computational overheads;Probes;Monitoring;Iterative algorithms;Performance loss;Event detection;Jitter;Algorithm design and analysis;Telecommunication traffic;Communications Society;Time measurement},
doi={10.1109/INFCOM.2009.5062053},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062054,
author={H. X. Nguyen and R. Teixeira and P. Thiran and C. Diot},
booktitle={IEEE INFOCOM 2009},
title={Minimizing Probing Cost for Detecting Interface Failures: Algorithms and Scalability Analysis},
year={2009},
volume={},
number={},
pages={1386-1394},
abstract={The automatic detection of failures in IP paths is an essential step for operators to perform diagnosis or for overlays to adapt. We study a scenario where a set of monitors send probes toward a set of target end-hosts to detect failures in a given set of IP interfaces. Unfortunately, there is a large probing cost to monitor paths between all monitors and targets at a very high frequency. We make two major contributions to reduce this probing cost. First, we propose a formulation of the probe optimization problem which, in contrast to the established formulation, is not NP complete. Second, we propose two linear programming algorithms to minimize probing cost. Our algorithms combine low frequency per-path probing to detect per-interface failures at a higher frequency. We analyze our solutions both analytically and experimentally. Our theoretical results show that the probing cost increases linearly with the number of interfaces in a random power-law graph. We confirm this linear increase in Internet graphs measured from PlanetLab and RON. Hence, Internet graphs belong to the most costly class of graph to probe.},
keywords={fault diagnosis;graph theory;Internet;IP networks;linear programming;system recovery;probing cost;interface failure detection;scalability analysis;automatic failure detection;IP paths;fault diagnosis;IP interfaces;probe optimization;NP complete;linear programming algorithms;low frequency per-path probing;per-interface failures;random power-law graph;Internet graphs;PlanetLab;RON;Costs;Scalability;Failure analysis;Algorithm design and analysis;Probes;Condition monitoring;Frequency;Web and internet services;IP networks;Availability},
doi={10.1109/INFCOM.2009.5062054},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062055,
author={D. Veitch and B. Augustin and R. Teixeira and T. Friedman},
booktitle={IEEE INFOCOM 2009},
title={Failure Control in Multipath Route Tracing},
year={2009},
volume={},
number={},
pages={1395-1403},
abstract={Traceroute is widely used to report the path packets take between two Internet hosts, but the widespread deployment of load balancing routers breaks a basic assumption - that there is only a single such path. We specify an adaptive, stochastic probing algorithm, the multipath detection algorithm (MDA), to report all paths that probes can follow between a source and a destination. We establish the foundations of, and show how to calculate, rigorous statistical guarantees for the discovery of the entire multipath route. We explore algorithm cost/guarantee tradeoffs in real experiments and show the inadequacy of the classic practice of sending three probes per hop.},
keywords={Internet;resource allocation;telecommunication network routing;failure control;multipath route tracing;Traceroute;Internet hosts;load balancing routers;multipath detection algorithm;Probes;Load management;IP networks;Stochastic processes;Detection algorithms;Communications Society;Communication system control;Laboratories;Costs;Assembly},
doi={10.1109/INFCOM.2009.5062055},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062056,
author={B. B. Chen and M. C. Chan},
booktitle={IEEE INFOCOM 2009},
title={MobTorrent: A Framework for Mobile Internet Access from Vehicles},
year={2009},
volume={},
number={},
pages={1404-1412},
abstract={In this paper, we present MobTorrent, an on- demand, user-driven framework designed for vehicles which have intermittent high speed access to roadside WiFi access points (AP). Mobile nodes in MobTorrent use the WWAN network as a control channel. When a mobile client wants to initiate a download, instead of waiting for contact with the AP, it informs one (or multiple) selected AP(s) to prefetch the content. The scheduling algorithm in MobTorrent then replicates the prefetched data on the mobile helpers so that the total amount of data transferred and the average transfer rate to the mobile clients are maximized. Therefore, instead of limiting high speed data transfer to the short contact periods between APs and mobile clients, high speed transfers among vehicles are opportunistically exploited. Evaluation based on testbed measurement and trace-driven simulation shows that MobTorrent provides substantial improvement over existing architectures. For the case of a single AP, its performance approximates that of an off-line optimal scheduler. In case of multiple APs, our evaluation shows that MobTorrent's performance is robust in a variety of settings.},
keywords={Internet;mobile computing;mobile radio;road vehicles;scheduling;traffic engineering computing;wide area networks;wireless LAN;roadside WiFi access point;road vehicle;MobTorrent framework;mobile Internet access;user-driven framework design;WWAN network;wireless wide-area network;mobile client;trace-driven simulation;off-line optimal scheduling algorithm;Internet;Prefetching;Road vehicles;Ground penetrating radar;Multiaccess communication;Bandwidth;Computer science;Scheduling algorithm;Cities and towns;Communications Society},
doi={10.1109/INFCOM.2009.5062056},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062057,
author={R. Lu and X. Lin and H. Zhu and X. Shen},
booktitle={IEEE INFOCOM 2009},
title={SPARK: A New VANET-Based Smart Parking Scheme for Large Parking Lots},
year={2009},
volume={},
number={},
pages={1413-1421},
abstract={Searching for a vacant parking space in a congested area or a large parking lot and preventing auto theft are major concerns to our daily lives. In this paper, we propose a new smart parking scheme for large parking lots through vehicular communication. The proposed scheme can provide the drivers with real-time parking navigation service, intelligent anti-theft protection, and friendly parking information dissemination. Performance analysis via extensive simulations demonstrates its efficiency and practicality.},
keywords={traffic control;traffic engineering computing;Smart Parking;vehicular communications;parking information dissemination;VANET;intelligent anti-theft protection;SPARK;Sparks;Navigation;Space technology;Vehicles;Protection;Ad hoc networks;Communications technology;Communications Society;Information technology;Performance analysis},
doi={10.1109/INFCOM.2009.5062057},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062058,
author={S. Ioannidis and A. Chaintreau and L. Massoulie},
booktitle={IEEE INFOCOM 2009},
title={Optimal and scalable distribution of content updates over a mobile social network},
year={2009},
volume={},
number={},
pages={1422-1430},
abstract={We study the dissemination of dynamic content, such as news or traffic information, over a mobile social network. In this application, mobile users subscribe to a dynamic-content distribution service, offered by their service provider. To improve coverage and increase capacity, we assume that users share any content updates they receive with other users they meet. We make two contributions. First, we determine how the service provider can allocate its bandwidth optimally to make the content at users as "fresh" as possible. More precisely, we define a global fairness objective (namely, maximizing the aggregate utility over all users) and prove that the corresponding optimization problem can be solved by gradient descent. Second, we specify a condition under which the system is highly scalable: even if the total bandwidth dedicated by the service provider remains fixed, the expected content age at each user grows slowly (as log(n)) with the number of users n. To the best of our knowledge, our work is the first to address these two aspects (optimality and scalability) of the distribution of dynamic content over a mobile social network.},
keywords={content management;gradient methods;mobile computing;social networking (online);mobile social network;dynamic content dissemination;dynamic-content distribution service;gradient descent;service provider;Resource management;Steady-state;Social network services;Mobile communication;Optimization;Bandwidth},
doi={10.1109/INFCOM.2009.5062058},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062059,
author={E. Altman and P. Nain and J. -. Bermond},
booktitle={IEEE INFOCOM 2009},
title={Distributed Storage Management of Evolving Files in Delay Tolerant Ad Hoc Networks},
year={2009},
volume={},
number={},
pages={1431-1439},
abstract={This work focuses on a class of distributed storage systems whose content may evolve over time. Each component or node of the storage system is mobile and the set of all nodes forms a delay tolerant (ad hoc) network (DTN). The goal of the paper is to study efficient ways for distributing evolving files within DTNs and for managing dynamically their content. We specify to dynamic files where not only the latest version is useful but also previous ones; we restrict however to files where a file has no use if another more recent version is available. The DTN is composed of fixed number of nodes including a single source. At some points in time the source makes available a new version of a single file F. We consider both the cases when (a) nodes do not cooperate and (b) nodes cooperate. In case (a) only the source may transmit a copy of F to a node that it meets, while in case (b) any node may transmit a copy of F to a node that it meets. Scenario (a) is studied under the assumption that the source updates F at discrete times t = 0,1,.. .. Within each slot [t,t + 1) there is a fixed probability that a node meets the source. A file management policy is a set of rules specifying when the source transmits a copy of F to a node (say node i) that it meets; this decision only depends on the age of the version of F (if any) that node i is carrying, where the age is k if this version was created k-1 slots ago. We And the optimal static (resp. dynamic) policy which maximizes a general utility function under a constraint on the number of transmissions within a slot. In particular, we show the existence of a threshold dynamic policy. In scenario (b) F is updated at random points in time. Similar to scenerio (a) we assume that each node knows the age of the file it carries (the case where nodes only know the date of creation of a file is studied in (E. Altman et al., 2008)). Under Markovian assumptions regarding nodes mobility and update frequency of F, we study the stability of the system (aging of the nodes) and derive an (approximate) optimal static policy. We then revisit scenario (a) when the source does not know the number of nodes and the probability that the source meets a node in a slot, and we derive a stochastic approximation algorithm which we show to converge to the optimal static policy found in the complete information setting. Numerical results illustrate the respective performance of optimal static and dynamic policies as well as the benefit of node cooperation.},
keywords={ad hoc networks;approximation theory;Markov processes;mobility management (mobile radio);stochastic processes;storage management;distributed storage management;evolving files;delay tolerant ad hoc networks;file management policy;utility function;threshold dynamic policy;Markovian assumptions;node mobility;system stability;stochastic approximation algorithm;Disruption tolerant networking;Ad hoc networks;Peer to peer computing;Content management;Weather forecasting;Communications Society;Delay effects;Frequency;Stability;Aging},
doi={10.1109/INFCOM.2009.5062059},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062060,
author={F. Mathieu and G. Postelnicu and J. Reynier},
booktitle={IEEE INFOCOM 2009},
title={The Stable Configuration of Acyclic Preference-Based Systems},
year={2009},
volume={},
number={},
pages={1440-1448},
abstract={Acyclic preferences recently appeared as an elegant way to model many distributed systems. An acyclic instance admits a unique stable configuration, which can reveal the performance of the system. In this paper, we give the statistical properties of the stable configuration for three classes of acyclic preferences: node-based preferences, distance-based preferences, and random acyclic systems. Using random overlay graphs, we prove by means of a mean-field assumption and a fluid-limit technique that these systems have an asymptotically continuous independent rank distribution for a proper scaling, and the analytical solution is compared to simulations. These results provide a theoretical ground for validating the performance of bandwidth-based or proximity-based unstructured systems.},
keywords={graph theory;network theory (graphs);peer-to-peer computing;statistical distributions;stable configuration;acyclic preference-based system;distributed system;statistical property;node-based preference;distance-based preference;random acyclic system;random overlay graph;mean-field assumption;fluid-limit technique;asymptotically-continuous independent rank distribution;bandwidth-based unstructured system;proximity-based unstructured system;peer-to-peer network;Peer to peer computing;Battery charge measurement;Communications Society;Analytical models;Medical simulation;Educational institutions;Hospitals;Time measurement;Bandwidth;Predictive models},
doi={10.1109/INFCOM.2009.5062060},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062061,
author={R. Cuevas and M. Uruena and A. Banchs},
booktitle={IEEE INFOCOM 2009},
title={Routing Fairness in Chord: Analysis and Enhancement},
year={2009},
volume={},
number={},
pages={1449-1457},
abstract={In Peer-to-Peer (P2P) systems where stored objects are small, routing dominates the cost of publishing and retrieving an object. In such systems, the issue of fairly balancing the routing load among all nodes becomes critical. In this paper we address this issue for Chord-based P2P systems. We first present an analytical model to evaluate the routing fairness of Chord based on the well accepted Jain's Fairness Index (FI). Our model shows that Chord performs poorly, with a FI around 0.6, mainly due to the different sizes of the zones between nodes. Following this observation, we propose a simple enhancement to the Chord finger selection algorithm with the goal of mitigating this effect. The key advantage of our proposal as compared to previous approaches is that it does not add any overhead to the basic Chord algorithm. The proposed approach is evaluated analytically showing a very substantial improvement over Chord, with a FI around 0.9. We conduct an extensive large-scale simulation study to evaluate our proposal and validate the analysis. The simulation study includes, among other aspects, churn conditions, heterogeneous nodes and Zipf-like object popularity.},
keywords={peer-to-peer computing;resource allocation;routing fairness;peer-to-peer systems;object publishing;object retrieval;Chord finger selection algorithm;Routing;Peer to peer computing;Proposals;Fingers;Costs;Analytical models;Internet;Communications Society;Publishing;Performance evaluation},
doi={10.1109/INFCOM.2009.5062061},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062062,
author={D. Fukuchi and C. Sommer and Y. Sei and S. Honiden},
booktitle={IEEE INFOCOM 2009},
title={Distributed Arrays: A P2P Data Structure for Efficient Logical Arrays},
year={2009},
volume={},
number={},
pages={1458-1466},
abstract={Distributed hash tables (DHT) are used for data management in P2P environments. However, since most hash functions ignore relations between items, DHTs are not efficient for operations on related items. In this paper, we modify a DHT into a distributed array (DA) that enables efficient operations on logical arrays. The array elements of a DA are placed in a P2P overlay network according to a simple rule such that the load is balanced and the number of messages required to access elements sequentially is reduced. The number of messages required for array operations is much smaller than that for operations on DHTs. We demonstrate this theoretically and experimentally.},
keywords={data structures;peer-to-peer computing;distributed arrays;P2P data structure;efficient logical arrays;distributed hash tables;Logic arrays;Data structures;Peer to peer computing;Costs;Tail;Communications Society;Informatics;Environmental management;Intrusion detection;Indexing},
doi={10.1109/INFCOM.2009.5062062},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062063,
author={B. Zhang and T. S. E. Ng},
booktitle={IEEE INFOCOM 2009},
title={Exploiting Internet Delay Space Properties for Selecting Distinct Network Locations},
year={2009},
volume={},
number={},
pages={1467-1475},
abstract={Recent studies have discovered that the Internet delay space has many interesting properties such as triangle inequality violations (TIV), clustering structures and constrained growth. Understanding these properties has so far benefited the design of network models and network-performance-aware systems. In this paper, we consider an interesting, previously unexplored connection between Internet delay space properties and network locations. We show that this connection can be exploited to select nodes from distinct network locations for applications such as replica placement in overlay networks even when an adversary is trying to mis-guide the selection process.},
keywords={delays;Internet;Internet delay space;triangle inequality violations;clustering structures;constrained growth;network models design;network-performance-aware systems;network locations;replica placement;overlay networks;IP networks;Peer to peer computing;Delay;Routing;Internet;Communications Society;Computer science;Availability;Engineering profession;Government},
doi={10.1109/INFCOM.2009.5062063},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062064,
author={Z. Zhu and G. Cao and S. Zhu and S. Ranjan and A. Nucci},
booktitle={IEEE INFOCOM 2009},
title={A Social Network Based Patching Scheme for Worm Containment in Cellular Networks},
year={2009},
volume={},
number={},
pages={1476-1484},
abstract={Recently, cellular phone networks have begun allowing third-party applications to run over certain open-API phone operating systems such as Windows Mobile, Iphone and Google's Android platform. However, with this increased openness, the fear of rogue programs written to propagate from one phone to another becomes ever more real. This paper proposes a counter-mechanism to contain the propagation of a mobile worm at the earliest stage by patching an optimal set of selected phones. The counter-mechanism continually extracts a social relationship graph between mobile phones via an analysis of the network traffic. As people are more likely to open and download content that they receive from friends, this social relationship graph is representative of the most likely propagation path of a mobile worm. The counter mechanism partitions the social relationship graph via two different algorithms, balanced and clustered partitioning and selects an optimal set of phones to be patched first as those which have the capability to infect the most number of other phones. The performance of these partitioning algorithms is compared against a benchmark random partitioning scheme. Through extensive trace-driven experiments using real IP packet traces from one of the largest cellular networks in the US, we demonstrate the efficacy of our proposed counter-mechanism in containing a mobile worm.},
keywords={application program interfaces;cellular radio;invasive software;IP networks;mobile computing;mobile handsets;open systems;operating systems (computers);social networking (online);social network based patching scheme;worm containment;cellular phone networks;open-API phone operating systems;Windows Mobile;Iphone;Google Android platform;mobile worm;social relationship graph;network traffic;clustered partitioning;balanced partitioning;benchmark random partitioning;real IP packet;Grippers;Mobile communication;Mobile computing;Partitioning algorithms;Mobile handsets;Land mobile radio cellular systems;Clustering algorithms},
doi={10.1109/INFCOM.2009.5062064},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062065,
author={J. Omic and A. Orda and P. Van Mieghem},
booktitle={IEEE INFOCOM 2009},
title={Protecting Against Network Infections: A Game Theoretic Perspective},
year={2009},
volume={},
number={},
pages={1485-1493},
abstract={Security breaches and attacks are critical problems in today's networking. A key-point is that the security of each host depends not only on the protection strategies it chooses to adopt but also on those chosen by other hosts in the network. The spread of Internet worms and viruses is only one example. This class of problems has two aspects. First, it deals with epidemic processes, and as such calls for the employment of epidemic theory. Second, the distributed and autonomous nature of decision-making in major classes of networks (e.g., P2P, ad- hoc, and most notably the Internet) call for the employment of game theoretical approaches. Accordingly, we propose a unified framework that combines the N-intertwined, SIS epidemic model with a noncooperative game model. We determine the existence of a Nash equilibrium of the respective game and characterize its properties. We show that its quality, in terms of overall network security, largely depends on the underlying topology. We then provide a bound on the level of system inefficiency due to the noncooperative behavior, namely, the "price of anarchy" of the game. We observe that the price of anarchy may be prohibitively high, hence we propose a scheme for steering users towards socially efficient behavior.},
keywords={computer viruses;game theory;Internet;network infections;game theoretic perspective;security breaches;epidemic theory;noncooperative game model;network security;Protection;Game theory;IP networks;Network servers;Internet;Computer viruses;Employment;Decision making;Curing;Information security},
doi={10.1109/INFCOM.2009.5062065},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062066,
author={M. Lelarge and J. Bolot},
booktitle={IEEE INFOCOM 2009},
title={Economic Incentives to Increase Security in the Internet: The Case for Insurance},
year={2009},
volume={},
number={},
pages={1494-1502},
abstract={Entities in the Internet, ranging from individuals and enterprises to service providers, face a broad range of epidemic risks such as worms, viruses, and botnet-driven attacks. Those risks are interdependent risks, which means that the decision by an entity to invest in security and self-protect affects the risk faced by others (for example, the risk faced by an individual decreases when its providers increases its investments in security). As a result of this, entities tend to invest too little in self-protection, relative to the socially efficient level, by ignoring benefits conferred on by others. In this paper, we consider the problem of designing incentives to entities in the Internet so that they invest at a socially efficient level. In particular, we find that insurance is a powerful incentive mechanism which pushes agents to invest in self-protection. Thus, insurance increases the level of self-protection, and therefore the level of security, in the Internet. As a result, we believe that insurance should be considered as an important component of risk management in the Internet.},
keywords={incentive schemes;insurance;Internet;risk management;security of data;economic incentives;security;Internet;insurance;self-protection;risk management;Security;Insurance;Risk management;Web and internet services;Computer crime;Computer worms;IP networks;Computer viruses;Investments;Large-scale systems},
doi={10.1109/INFCOM.2009.5062066},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062067,
author={G. Zyba and G. M. Voelker and M. Liljenstam and A. Mehes and P. Johansson},
booktitle={IEEE INFOCOM 2009},
title={Defending Mobile Phones from Proximity Malware},
year={2009},
volume={},
number={},
pages={1503-1511},
abstract={As mobile phones increasingly become the target of propagating malware, their use of direct pair-wise communication mechanisms, such as Bluetooth and WiFi, pose considerable challenges to malware detection and mitigation. Unlike malware that propagates using the network, where the provider can employ centralized defenses, proximity malware can propagate in an entirely distributed fashion. In this paper we consider the dynamics of mobile phone malware that propagates by proximity contact, and we evaluate potential defenses against it. Defending against proximity malware is particularly challenging since it is difficult to piece together global dynamics from just pair-wise device interactions. Whereas traditional network defenses depend upon observing aggregated network activity to detect correlated or anomalous behavior, proximity malware detection must begin at the device. As a result, we explore three strategies for detecting and mitigating proximity malware that span the spectrum from simple local detection to a globally coordinated defense. Using insight from a combination of real-world traces, analytic epidemic models, and synthetic mobility models, we simulate proximity malware propagation and defense at the scale of a university campus. We find that local proximity-based dissemination of signatures can limit malware propagation. Globally coordinated strategies with broadcast dissemination are substantially more effective, but rely upon more demanding infrastructure within the provider.},
keywords={invasive software;mobile radio;mobile phones;proximity malware;direct pair-wise communication mechanisms;malware detection;analytic epidemic models;synthetic mobility models;Mobile handsets;Bluetooth;USA Councils;Mobile communication;Analytical models;Broadcasting;Communications Society;Computer science;Information technology;Propagation losses},
doi={10.1109/INFCOM.2009.5062067},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062068,
author={W. Cheng and X. Cheng and T. Znati and X. Lu and Z. Lu},
booktitle={IEEE INFOCOM 2009},
title={The Complexity of Channel Scheduling in Multi-Radio Multi-Channel Wireless Networks},
year={2009},
volume={},
number={},
pages={1512-1520},
abstract={The complexity of channel scheduling in Multi- Radio Multi-Channel (MR-MC) wireless networks is an open research topic. This problem asks for the set of edges that can support maximum amount of simultaneous traffic over orthogonal channels under a certain interference model. There exist two major interference models for channel scheduling, with one under the physical distance constraint, and one under the hop distance constraint. The complexity of channel scheduling under these two interference models serves as the foundation for many problems related to network throughput maximization. However, channel scheduling was proved to be NP-Hard only under the hop distance constraint for SR-SC wireless networks. In this paper, we fill the void by proving that channel scheduling is NP-Hard under both models in MR-MC wireless networks. In addition, we propose a polynomial-time approximation scheme (PTAS) framework that is applicable to channel scheduling under both interference models in MR-MC wireless networks. Furthermore, we conduct a comparison study on the two interference models and identify conditions under which these two models are equivalent for channel scheduling.},
keywords={computational complexity;optimisation;radio networks;wireless channels;channel scheduling;multi-radio multi-channel wireless networks;orthogonal channels;hop distance constraint;NP-hard;polynomial-time approximation scheme;Wireless networks;Interference constraints;Telecommunication traffic;Processor scheduling;Computer science;USA Councils;Traffic control;Throughput;Polynomials;Communications Society},
doi={10.1109/INFCOM.2009.5062068},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062069,
author={B. Han and V. S. A. Kumar and M. V. Marathe and S. Parthasarathy and A. Srinivasan},
booktitle={IEEE INFOCOM 2009},
title={Distributed Strategies for Channel Allocation and Scheduling in Software-Defined Radio Networks},
year={2009},
volume={},
number={},
pages={1521-1529},
abstract={Equipping wireless nodes with multiple radios can significantly increase the capacity of wireless networks, by making these radios simultaneously transmit over multiple non-overlapping channels. However, due to the limited number of radios and available orthogonal channels, designing efficient channel assignment and scheduling algorithms in such networks is a major challenge. In this paper, we present provably-good distributed algorithms for simultaneous channel allocation of individual links and packet-scheduling, in software-defined radio (SDR) wireless networks. Our distributed algorithms are very simple to implement, and do not require any coordination even among neighboring nodes. A novel access hash function or random oracle methodology is one of the key drivers of our results. With this access hash function, each radio can know the transmitters' decisions for links in its interference set for each time slot without introducing any extra communication overhead between them. Further, by utilizing the inductive-scheduling technique, each radio can also backoff appropriately to avoid collisions. Extensive simulations demonstrate that our bounds are valid in practice.},
keywords={channel allocation;channel capacity;cryptography;distributed algorithms;radio networks;radiofrequency interference;scheduling;software radio;telecommunication congestion control;telecommunication security;wireless channels;distributed algorithm;software-defined radio wireless network capacity;simultaneous channel allocation;packet scheduling algorithm;channel assignment algorithm;access hash function;random oracle methodology;inductive-scheduling technique;collision avoidance;interference set;Channel allocation;Radio network;Wireless networks;Throughput;Computer science;Interference;Educational institutions;Peer to peer computing;Scheduling algorithm;Distributed algorithms},
doi={10.1109/INFCOM.2009.5062069},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062070,
author={L. Fu and S. C. Liew and J. Huang},
booktitle={IEEE INFOCOM 2009},
title={Power Controlled Scheduling with Consecutive Transmission Constraints: Complexity Analysis and Algorithm Design},
year={2009},
volume={},
number={},
pages={1530-1538},
abstract={We study the joint power control and minimum-frame-length scheduling problem in wireless networks, under the physical interference model and subject to consecutive transmission constraints. We start by investigating the complexity of the problem and present the first NP-completeness proof in the literature. We propose a polynomial-time approximation algorithm, called guaranteed and greedy scheduling (GGS) algorithm, to tackle this problem. We prove a bounded approximation ratio of the proposed algorithm relative to the optimal scheduling algorithm. Moreover, the proposed algorithm significantly outperforms the state-of-the-art related algorithm. Interestingly, our algorithm together with its bounded approximation ratio is applicable even when the consecutive transmission constraint is relaxed. To the best of our knowledge, the proposed algorithm is the first known polynomial-time algorithm with a proven bounded approximation ratio for the joint power control and scheduling problem under the physical interference model. We further demonstrate the performance and advantages of our algorithm through extensive simulations.},
keywords={computational complexity;electromagnetic wave interference;greedy algorithms;radio networks;scheduling;theorem proving;time division multiple access;power controlled scheduling;consecutive transmission constraints;complexity analysis;algorithm design;minimum-frame-length scheduling problem;wireless networks;physical interference model;NP-completeness proof;guaranteed and greedy scheduling algorithm;polynomial-time approximation;bounded approximation ratio;consecutive transmission constraint;Algorithm design and analysis;Scheduling algorithm;Optimal scheduling;Approximation algorithms;Power control;Power system modeling;Interference constraints;Protocols;Wireless networks;Telecommunication traffic},
doi={10.1109/INFCOM.2009.5062070},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062071,
author={K. Sundaresan and S. Rangarajan},
booktitle={IEEE INFOCOM 2009},
title={Efficient Algorithms for Leveraging Spatial Reuse in OFDMA Relay Networks},
year={2009},
volume={},
number={},
pages={1539-1547},
abstract={We consider the problem of scheduling users with backlogged and finite buffers on the multiple OFDM carriers (channels) over the two hops of the relay-enabled wireless network. Motivated by the recent 802.16j standard, we consider two sub-carrier grouping models (PUSC, AMC). While the extent of diversity gain varies depending on the model, spatial reuse is common to both the models and is crucial in delivering the promised throughput benefits of relays. Hence leveraging spatial reuse forms the prime focus of this work with diversity gains being leveraged when available. We establish the hardness of the problem under these two models and propose efficient approximation algorithms for the scheduling problem considered. Our solutions are simple to implement at the base station, while also providing worst case guarantees. The proposed solutions are evaluated to highlight their benefits in a variety of network conditions.},
keywords={approximation theory;diversity reception;frequency division multiple access;OFDM modulation;optimisation;radio networks;wireless channels;OFDMA relay network;wireless network;channel diversity gain;leveraging spatial reuse;approximation algorithm;scheduling problem;NP-hard problem;Relays;Diversity methods;Wireless networks;OFDM;Scheduling algorithm;Throughput;Base stations;Spread spectrum communication;Communications Society;Mobile communication},
doi={10.1109/INFCOM.2009.5062071},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062072,
author={S. Antonakopoulos and L. Zhang},
booktitle={IEEE INFOCOM 2009},
title={Approximation Algorithms for Grooming in Optical Network Design},
year={2009},
volume={},
number={},
pages={1548-1556},
abstract={We study traffic grooming in optical network design. The goal is to aggregate low-bandwidth traffic streams to efficiently utilize high-bandwidth media such as wavelength channels. More precisely, given traffic demands to be routed in a network, the design problem is to define a collection of light paths such that each demand can follow a sequence of consecutive light paths. Each light path has unit-wavelength bandwidth, and multiple sub-wavelength demands may share a common light path. Traffic must enter and depart from a light path at its two endpoints only. Most previous work on grooming focused on the ring topology and typically dealt only with uniform bandwidth demands, whereas we consider more general settings. We study two objectives. One is to minimize total equipment cost necessary to support the light paths; the other is simply to minimize the number of light paths. Even for the extremely restricted special case of a line topology and traffic demands that request half-wavelength bandwidth, we show that both objectives are APX-hard to optimize, which means we cannot approximate the optimum arbitrarily closely. On the other extreme of generality, for arbitrary network topologies and traffic demands that request arbitrary amounts of bandwidth, we show a logarithmic approximation for cost minimization and a 2-approximation for minimizing light path counts. Furthermore, we discover that the special case of half-wavelength demands has rich combinatorial properties, closely related to graph problems such as cycle packing and pattern matching problems such as interchange distance in strings. We show how to approximate both objectives up to small constant factors in this case, while similarly improving the approximation and hardness of the interchange distance problem as well.},
keywords={approximation theory;optical fibre networks;telecommunication network topology;telecommunication traffic;optical network design;traffic grooming;low-bandwidth traffic streams;wavelength channels;light paths;traffic demands;unit-wavelength bandwidth;multiple subwavelength demands;line topology;network topologies;logarithmic approximation;total equipment cost minimization;graph problems;cycle packing;pattern matching problems;interchange distance problem;Approximation algorithms;Optical fiber networks;Optical design;Algorithm design and analysis;Telecommunication traffic;Bandwidth;Costs;Network topology;Aggregates;Streaming media},
doi={10.1109/INFCOM.2009.5062072},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062073,
author={K. Manousakis and K. Christodoulopoulos and E. Varvarigos},
booktitle={IEEE INFOCOM 2009},
title={Impairment-Aware Offline RWA for Transparent Optical Networks},
year={2009},
volume={},
number={},
pages={1557-1565},
abstract={We consider the offline version of the routing and wavelength assignment (RWA) problem in transparent all-optical networks. In such networks and in the absence of regenerators, the signal quality of transmission degrades due to physical layer impairments. We initially present an algorithm for solving the static RWA problem based on an LP relaxation formulation that tends to yield integer solutions. To account for signal degradation due to physical impairments, we model the effects of the path length, the path hop count, and the interference among ligthpaths by imposing additional (soft) constraints on RWA. The objective of the resulting optimization problem is not only to serve the connection requests using the available wavelengths, but also to minimize the total accumulated signal degradation on the selected lightpaths. Our simulation studies indicate that the proposed RWA algorithms select the lightpaths for the requested connections so as to avoid impairment generating sources, thus dramatically reducing the overall physical-layer blocking when compared to RWA algorithms that do not account for impairments.},
keywords={optical fibre networks;telecommunication network routing;wavelength assignment;impairment-aware offline RWA;routing and wavelength assignment problem;transparent all-optical networks;transmission signal quality;LP relaxation formulation;integer solutions;signal degradation;path length;path hop count;ligthpath interference;Optical fiber networks;Switches;Degradation;Physical layer;Crosstalk;Interference;Optical fiber communication;Filters;Computer networks;Optical computing},
doi={10.1109/INFCOM.2009.5062073},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062074,
author={S. Neumayer and G. Zussman and R. Cohen and E. Modiano},
booktitle={IEEE INFOCOM 2009},
title={Assessing the Vulnerability of the Fiber Infrastructure to Disasters},
year={2009},
volume={},
number={},
pages={1566-1574},
abstract={Communication networks are vulnerable to natural disasters, such as earthquakes or floods, as well as to physical attacks, such as an Electromagnetic Pulse (EMP) attack. Such real- world events happen in specific geographical locations and disrupt specific parts of the network. Therefore, the geographical layout of the network determines the impact of such events on the network's connectivity. In this paper, we focus on assessing the vulnerability of (geographical) networks to such disasters. In particular, we aim to identify the most vulnerable parts of the network. That is, the locations of disasters that would have the maximum disruptive effect on the network in terms of capacity and connectivity. We consider graph models in which nodes and links are geographically located on a plane, and model the disaster event as a line segment or a circular cut. We develop algorithms that find a worst- case line segment cut and a worst-case circular cut. Then, we obtain numerical results for a specific backbone network, thereby demonstrating the applicability of our algorithms to real-world networks. Our novel approach provides a promising new direction for network design to avert geographical disasters or attacks.},
keywords={disasters;optical fibre networks;vulnerability;fiber infrastructure;communication networks;natural disasters;geographical layout;network connectivity;worst-case line segment;EMP radiation effects;Optical fiber communication;Spine;Internet;Earthquakes;IP networks;Circuits;Electronic components;Telecommunication network topology;Protection},
doi={10.1109/INFCOM.2009.5062074},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062075,
author={J. He and M. Brandt-Pearce and S. Subramaniam},
booktitle={IEEE INFOCOM 2009},
title={Analysis of Blocking Probability for First-Fit RWA in Transmission Impaired Optical Networks},
year={2009},
volume={},
number={},
pages={1575-1583},
abstract={Wavelength routed optical networks can experience increased call blocking due to insufficient quality of transmission. The routing and wavelength assignment algorithm must verify the quality of the lightpath before accepting it. In this paper, analytical expressions for the total blocking probability are derived for first fit wavelength assignment for networks suffering from transmission impairments. Physical layer degradations considered include amplifier noise and crosstalk between WDM channels. The technique effectively predicts the performance for wavelength selection techniques that consider a single candidate channel or all channels for quality of transmission compliance. The analysis is also applicable to first-fit algorithms with different static channel orderings.},
keywords={crosstalk;probability;telecommunication channels;telecommunication network routing;wavelength division multiplexing;blocking probability;first-fit RWA;transmission impaired optical networks;wavelength routed optical networks;call blocking;routing assignment;wavelength assignment algorithm;amplifier noise;crosstalk;WDM channels;Optical fiber networks;Crosstalk;Wavelength assignment;Degradation;Algorithm design and analysis;Analytical models;Telecommunication traffic;Traffic control;Computer networks;Wavelength routing},
doi={10.1109/INFCOM.2009.5062075},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062076,
author={K. Elmeleegy and A. L. Cox},
booktitle={IEEE INFOCOM 2009},
title={EtherProxy: Scaling Ethernet By Suppressing Broadcast Traffic},
year={2009},
volume={},
number={},
pages={1584-1592},
abstract={Ethernet is the dominant technology for local area networks. This is mainly because of its autoconfiguration capability and its cost effectiveness. Unfortunately, a single Ethernet network can not scale to span a large enterprise network. A main reason for this is broadcast traffic resulting from many protocols running on top of Ethernet. This paper addresses Ethernet's scalability limits due to broadcast traffic. We studied and characterized broadcast traffic in Ethernet networks using traces collected from real networks. We found that broadcast is mainly used in Ethernet for service and resource discovery. For example, the address resolution protocol (ARP) uses broadcast to discover a MAC address that corresponds to an IP address. To avoid broadcast for service and resource discovery, we propose a new device, the EtherProxy. An EtherProxy uses caching to suppress broadcast traffic. EtherProxy is backward compatible and requires no changes to existing hardware, software, or protocols. Moreover, it requires no configuration. In our evaluation, we used real and synthetic workloads. Using both workloads, we experimentally demonstrate the effectiveness of the EtherProxy.},
keywords={access protocols;IP networks;local area networks;telecommunication traffic;EtherProxy;Ethernet;broadcast traffic;local area networks;autoconfiguration capability;large enterprise network;address resolution protocol;MAC address;IP address;service-resource discovery;Ethernet networks;Broadcasting;Telecommunication traffic;Protocols;Costs;Scalability;Local area networks;Switches;Voice mail;Virtual manufacturing},
doi={10.1109/INFCOM.2009.5062076},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062077,
author={H. Han and B. Sheng and C. C. Tan and Q. Li and S. Lu},
booktitle={IEEE INFOCOM 2009},
title={A Measurement Based Rogue AP Detection Scheme},
year={2009},
volume={},
number={},
pages={1593-1601},
abstract={This paper considers a category of rogue access points (APs) that pretend to be legitimate APs to lure users to connect to them. We propose a practical timing based technique that allows the user to avoid connecting to rogue APs. Our method employs the round trip time between the user and the DNS server to independently determine whether an AP is legitimate or not without assistance from the WLAN operator. We implemented our detection technique on commercially available wireless cards to evaluate their performance.},
keywords={Internet;wireless LAN;rogue AP detection;access points;practical timing-based technique;DNS server;WLAN operator;wireless cards;Wireless LAN;Signal to noise ratio;Joining processes;Timing;Communications Society;Software measurement;Laboratories;Paper technology;Educational institutions;USA Councils},
doi={10.1109/INFCOM.2009.5062077},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062078,
author={B. Han and L. Ji and S. Lee and B. Bhattacharjee and R. R. Miller},
booktitle={IEEE INFOCOM 2009},
title={All Bits Are Not Equal - A Study of IEEE 802.11 Communication Bit Errors},
year={2009},
volume={},
number={},
pages={1602-1610},
abstract={In IEEE 802.11 Wireless LAN (WLAN) systems, techniques such as acknowledgement, retransmission, and transmission rate adaptation, are frame-level mechanisms designed for combating transmission errors. Recently sub-frame level mechanisms such as frame combining have been proposed by the research community. In this paper, we present results obtained from our bit error study for identifying sub-frame error patterns because we believe that identifiable bit error patterns can potentially introduce new opportunities in channel coding, network coding, forward error correction (FEC), and frame combining mechanisms. We have constructed a number of IEEE 802.11 wireless LAN testbeds and conducted extensive experiments to study the characteristics of bit errors and their location distribution. Conventional wisdom dictates that bit error probability is the result of channel condition and ought to follow corresponding distribution. However our measurement results identify three repeatable bit error patterns that are not induced by channel conditions. We have verified that such error patterns are present in WLAN transmissions in different physical environments and across different wireless LAN hardware platforms. We also discuss our current hypotheses for the reasons behind these bit error probability patterns and how identifying these patterns may help improving WLAN transmission robustness.},
keywords={channel coding;error statistics;forward error correction;wireless LAN;IEEE 802.11 communication bit errors;wireless LAN;transmission errors;subframe error patterns;forward error correction;channel coding;network coding;frame combining mechanisms;Wireless LAN;Wireless communication;Transmitters;Error correction;Physical layer;Error probability;Electromagnetic scattering;Computer errors;USA Councils;Channel coding},
doi={10.1109/INFCOM.2009.5062078},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062079,
author={E. Altman and I. Menache and A. Suarez},
booktitle={IEEE INFOCOM 2009},
title={Team and Noncooperative Solutions to Access Control with Priorities},
year={2009},
volume={},
number={},
pages={1611-1619},
abstract={We consider decentralized medium-access control in which many pairwise interactions occur between randomly selected users that belong to a large population. In each local interaction, the users involved compete over an access opportunity. A given user has a fixed number of access attempts and a fixed budget for buying different priority levels. In each time-slot, the access is attributed to the user with the largest priority level. We analyze this problem under both cooperative as well as competitive frameworks. We show that unlike many standard team problems, optimal pure policies do not exist in the team framework, but both an optimal solution as well as equilibria exist within the class of mixed policies. We establish structural properties as well as explicit characterization of these: We show that the optimal policy requires only three priority levels, whereas the noncooperative game possesses a unique symmetric equilibrium point that uses at most two priority levels. Our analysis is applied to power control over wireless capture channels, where the budget constraint corresponds to the battery lifetime.},
keywords={authorisation;game theory;noncooperative solutions;decentralized medium-access control;noncooperative game possesses;power control;wireless capture channels;Access control;Power control;Batteries;Costs;Nash equilibrium;Quality of service;Communications Society;Laboratories;Mobile communication;Communication system control},
doi={10.1109/INFCOM.2009.5062079},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062080,
author={A. Nahir and A. Orda and A. Freund},
booktitle={IEEE INFOCOM 2009},
title={Topology Design and Control: A Game-Theoretic Perspective},
year={2009},
volume={},
number={},
pages={1620-1628},
abstract={We study the performance of non-cooperative networks in light of three major topology design and control considerations, namely the price of establishing a link, path delay, and path proneness to congestion or interference, the latter being modeled through the "relaying extent" of the nodes. We analyze these considerations and the tradeoffs between them from a game theoretic perspective, where each network element attempts to optimize its individual performance. We show that for all considered cases but one, the existence of a Nash equilibrium point is guaranteed. In addition, we demonstrate that the price of anarchy, i.e., the performance penalty incurred by non-cooperative behavior, may be prohibitively large; yet, we also show that such games usually admit at least one Nash equilibrium that is system-wide optimal, i.e., their price of stability is 1. This finding suggests that a major improvement can be achieved by providing a central ("social") agent with the ability to impose the initial configuration on the system.},
keywords={game theory;radio networks;telecommunication congestion control;telecommunication network routing;telecommunication network topology;topology design;topology control;game-theoretic perspective;noncooperative networks;path delay;path proneness;network element;Nash equilibrium point;social agent;Network topology;Nash equilibrium;Routing;Interference;Performance analysis;Lighting control;Relays;Game theory;Stability;Computer network reliability},
doi={10.1109/INFCOM.2009.5062080},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062081,
author={E. Altman and A. Kumar and C. Singh and R. Sundaresan},
booktitle={IEEE INFOCOM 2009},
title={Spatial SINR Games Combining Base Station Placement and Mobile Association},
year={2009},
volume={},
number={},
pages={1629-1637},
abstract={We study in this paper the question of determining locations of base stations (BSs) that may belong to the same or to competing service providers, taking into account the impact of these decisions on the behavior of intelligent mobile terminals who can connect to the base station that offers the best utility. We first study the SINR association-game: we determine the cells corresponding to each base stations, i.e. the locations at which mobile terminals prefer to connect to a given base station than to other. The signal to interference and noise ratio (SINR) is used as the quantity that determines the association. We make some surprising observations: (i) displacing a base station a little in one direction may result in a displacement of the boundary of the corresponding cell to the opposite direction; (ii) A cell corresponding to a BS may be the union of disconnected sub-cells. We then study the Stackelberg equilibrium in the combined BS location and mobile association problem: we determine where to locate the BSs so as to maximize the revenues obtained at the induced SINR mobile association game. We consider the cases of single frequency band and two frequency bands of operation. Finally, we also consider Stackelberg equilibria in two frequency systems with successive interference cancellation.},
keywords={game theory;interference suppression;mobile radio;base station;intelligent mobile terminal;SINR association-game;base station placement;mobile association;spatial SINR games;signal to interference and noise ratio;Stackelberg equilibrium;successive interference cancellation;Signal to noise ratio;Base stations;Frequency;Game theory;Mobile communication;Throughput;Costs;Interference cancellation;Communications Society;Communications technology},
doi={10.1109/INFCOM.2009.5062081},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062082,
author={X. Ai and V. Srinivasan and C. -. Tham},
booktitle={IEEE INFOCOM 2009},
title={Wi-Sh: A Simple, Robust Credit Based Wi-Fi Community Network},
year={2009},
volume={},
number={},
pages={1638-1646},
abstract={Wireless community networks, where users share wireless bandwidth is attracting tremendous interest from academia and industry. Companies such as FON have been successful in attracting large communities of users. However, solutions such as FON either require users to buy specialized FON routers or firmware modifications to existing routers. In this paper we propose a solution which requires no such sophisticated hardware. An alternative is to provide a solution which requires users to download a client software on to their PCs. While the solution appears simple it raises several issues of incentivizing users to share their bandwidth and also issues of preventing users from cheating behaviors which give them an unfair advantage. In this paper, we propose a system and solution which (i) requires only software downloads on PCs, (ii) is robust to tampering of the software, and intermittent monitoring of an access point by the owner, (iii) a credit based mechanism whereby users earn credits for sharing bandwidth and punishment and pricing mechanism whereby users are charged at a higher price whenever they are caught misbehaving. By making simple but plausible assumptions about user behavior, we show via analysis and extensive simulations that the system converges to a Pareto optimal Nash equilibrium. We further validate our system model, by running trace driven simulations on real world data. We believe that the solution provided by Wi-Sh is an attractive and more credible alternative to solutions such as FON.},
keywords={telecommunication network routing;wireless LAN;Wi-Fi community network;wireless community networks;wireless bandwidth;firmware modifications;router modification;client software;software downloads;Pareto optimal Nash equilibrium;Robustness;Bandwidth;Personal communication networks;Microprogramming;Hardware;Monitoring;Pricing;Pareto analysis;Analytical models;Nash equilibrium},
doi={10.1109/INFCOM.2009.5062082},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062083,
author={A. Cvetkovski and M. Crovella},
booktitle={IEEE INFOCOM 2009},
title={Hyperbolic Embedding and Routing for Dynamic Graphs},
year={2009},
volume={},
number={},
pages={1647-1655},
abstract={We propose an embedding and routing scheme for arbitrary network connectivity graphs, based on greedy routing and utilizing virtual node coordinates. In dynamic multihop packet-switching communication networks, routing elements can join or leave during network operation or exhibit intermittent failures. We present an algorithm for online greedy graph embedding in the hyperbolic plane that enables incremental embedding of network nodes as they join the network, without disturbing the global embedding. Even a single link or node removal may invalidate the greedy routing success guarantees in network embeddings based on an embedded spanning tree subgraph. As an alternative to frequent reembedding of temporally dynamic network graphs in order to retain the greedy embedding property, we propose a simple but robust generalization of greedy distance routing called Gravity-Pressure (GP) routing. Our routing method always succeeds in finding a route to the destination provided that a path exists, even if a significant fraction of links or nodes is removed subsequent to the embedding. GP routing does not require precomputation or maintenance of special spanning subgraphs and, as demonstrated by our numerical evaluation, is particularly suitable for operation in tandem with our proposed algorithm for online graph embedding.},
keywords={graph theory;greedy algorithms;hyperbolic equations;packet switching;telecommunication network routing;trees (mathematics);hyperbolic embedding;hyperbolic routing;dynamic graphs;arbitrary network connectivity graphs;virtual node coordinates;dynamic multihop packet-switching communication networks;routing elements;online greedy graph;embedded spanning tree subgraph;temporally dynamic network graphs;greedy distance routing;gravity-pressure routing;online graph embedding;Routing;Tree graphs;Peer to peer computing;Computer science;Communication networks;Extraterrestrial measurements;Communications Society;Spread spectrum communication;Robustness;Internet},
doi={10.1109/INFCOM.2009.5062083},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062084,
author={S. Toumpis and S. Gitzenis},
booktitle={IEEE INFOCOM 2009},
title={Load Balancing in Wireless Sensor Networks using Kirchhoff's Voltage Law},
year={2009},
volume={},
number={},
pages={1656-1664},
abstract={We study load balancing in wireless networks with a single class of traffic, focusing our attention on an important example, i.e., wireless sensor networks. The analysis is based on the wireless minimum cost problem, an optimization problem that implicitly captures the effects of interference through the novel use of an arc cost function that depends on the amount of traffic flowing through nearby links. We present an algorithm that optimizes the flow by modifying the traffic along cycles constructed during an initialization phase. Its operation may be viewed as trying to satisfy Kirchhoff's Voltage Law on an electric circuit containing interferistors. It is robust, distributed, with minimal communication overhead, and converges quickly. Furthermore, when the traffic is elastic, the algorithm can be easily modified to also perform congestion control.},
keywords={communication complexity;interference;minimisation;resource allocation;telecommunication congestion control;telecommunication traffic;wireless sensor networks;load balancing;wireless sensor network;Kirchhoff voltage law;minimum cost problem;optimization problem;congestion control;interference;arc cost function;Load management;Wireless sensor networks;Kirchhoff's Law;Communication system traffic control;Telecommunication traffic;Interference;Cost function;Computer networks;Circuits;Monitoring},
doi={10.1109/INFCOM.2009.5062084},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062085,
author={Z. Feng and Y. Yang},
booktitle={IEEE INFOCOM 2009},
title={Joint Transport, Routing and Spectrum Sharing Optimization for Wireless Networks with Frequency-Agile Radios},
year={2009},
volume={},
number={},
pages={1665-1673},
abstract={In this paper, we describe and analyze the design of a joint Transport, routing and spectrum sharing (TRSS) optimization algorithm for wireless networks with frequency- agile radios. We capitalize on the spectrum agility of modern radios which can configure both the central frequency and spectrum width of their channels. TRSS is fully distributed and is executed in two phases. In the first phase a dual driven optimizer is used to jointly adjust end-to-end transmission rate, flow-routing and spectrum width allocation. In the second phase, an innovative timing-window based spectrum access scheme is used for link layer scheduling. Analytical and simulation results show the effectiveness of our design.},
keywords={frequency agility;radio networks;radio spectrum management;scheduling;telecommunication network management;telecommunication network routing;spectrum sharing optimization;wireless networks;frequency-agile radios;spectrum agility;central frequency;spectrum width;dual driven optimizer;end-to-end transmission rate;flow-routing;spectrum width allocation;timing-window based spectrum access scheme;link layer scheduling;Routing;Wireless networks;Frequency;Design optimization;Algorithm design and analysis;Utility programs;Bandwidth;Bonding;Prototypes;Radio spectrum management},
doi={10.1109/INFCOM.2009.5062085},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062086,
author={L. Ying and S. Shakkottai and A. Reddy},
booktitle={IEEE INFOCOM 2009},
title={On Combining Shortest-Path and Back-Pressure Routing Over Multihop Wireless Networks},
year={2009},
volume={},
number={},
pages={1674-1682},
abstract={Back-pressure based algorithms based on the algorithm by Tassiulas and Ephremides have recently received much attention for jointly routing and scheduling over multi-hop wireless networks. However a significant weakness of this approach has been in routing, because the traditional back-pressure algorithm explores and exploits all feasible paths between each source and destination. While this extensive exploration is essential in order to maintain stability when the network is heavily loaded, under light or moderate loads, packets may be sent over unnecessarily long routes and the algorithm could be very inefficient in terms of end-to-end delay and routing convergence times. This paper proposes new routing/scheduling back-pressure algorithms that not only guarantees network stability (through-put optimality), but also adaptively selects a set of optimal routes based on shortest-path information in order to minimize average path-lengths between each source and destination pair. Our results indicate that under the traditional back-pressure algorithm, the end-to-end packet delay first decreases and then increases as a function of the network load (arrival rate). This surprising low-load behavior is explained due to the fact that the traditional back-pressure algorithm exploits all paths (including very long ones) even when the traffic load is light. On the otherhand, the proposed algorithm adaptively selects a set of routes according to the traffic load so that long paths are used only when necessary, thus resulting in much smaller end-to-end packet delays as compared to the traditional back-pressure algorithm.},
keywords={minimisation;radio networks;scheduling;telecommunication network routing;back-pressure routing;shortest-path routing;multihop wireless networks;back-pressure algorithm;end-to-end packet delay;routing convergence time;network stability;Routing;Spread spectrum communication;Wireless networks;Scheduling algorithm;Telecommunication traffic;Throughput;Delay;Stability;Communications Society;Convergence},
doi={10.1109/INFCOM.2009.5062086},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062087,
author={C. T. Pataguppe Suryanarayan Bhat and J. Zhang and M. -. Pun and V. H. Poor},
booktitle={IEEE INFOCOM 2009},
title={Distributed Opportunistic Scheduling With Two-Level Channel Probing},
year={2009},
volume={},
number={},
pages={1683-1691},
abstract={Distributed opportunistic scheduling (DOS) is studied for wireless ad-hoc networks in which many links contend for the channel using random access before data transmissions. Simply put, DOS involves a process of joint channel probing and distributed scheduling for ad-hoc (peer-to-peer) communications. Since, in practice, link conditions are estimated with noisy observations, the transmission rate has to be backed off from the estimated rate to avoid transmission outages. Then, a natural question to ask is whether it is worthwhile for the link with successful contention to perform further channel probing to mitigate estimation errors, at the cost of additional probing. Thus motivated, this work investigates DOS with two-level channel probing by optimizing the tradeoff between the throughput gain from more accurate rate estimation and the resulting additional delay. Capitalizing on optimal stopping theory with incomplete information, we show that the optimal scheduling policy is threshold-based and is characterized by either one or two thresholds, depending on network settings. Necessary and sufficient conditions for both cases are rigorously established. In particular, our analysis reveals that performing second-level channel probing is optimal when the first-level estimated channel condition falls in between the two thresholds. Finally, numerical results are provided to illustrate the effectiveness of the proposed DOS with two-level channel probing.},
keywords={ad hoc networks;scheduling;telecommunication channels;channel probing;distributed opportunistic scheduling;wireless ad-hoc networks;estimation errors;throughput gain;Optimal scheduling;Delay estimation;Ad hoc networks;Data communication;Peer to peer computing;Estimation error;Costs;Throughput;Added delay;Sufficient conditions},
doi={10.1109/INFCOM.2009.5062087},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062088,
author={B. Sadiq and S. J. Baek and G. de Veciana},
booktitle={IEEE INFOCOM 2009},
title={Delay-Optimal Opportunistic Scheduling and Approximations: The Log Rule},
year={2009},
volume={},
number={},
pages={1692-1700},
abstract={This paper considers the design of opportunistic packet schedulers for users sharing a time-varying wireless channel from the performance and the robustness points of view. Firstly, for a simplified model falling in the classical Markov decision process framework where arrival and channel statistics are known, we numerically compute and evaluate the characteristics of mean-delay-optimal scheduling policies. The computed policies exhibit radial sum-rate monotonicity (RSM), i.e., when users' queues grow linearly (i.e. scaled up by a constant), the scheduler allocates service in a manner that de-emphasizes the balancing of unequal queues in favor of maximizing current system throughput (being opportunistic). This is in sharp contrast to previously proposed policies, e.g., MaxWeight and Exp rule. The latter, however, are throughput-optimal, in that without knowledge of arrival/channel statistics they achieve stability if at all feasible. To meet performance and robustness objectives, secondly, we propose a new class of policies, called the Log rule, that are radial sum-rate monotone and provably throughput optimal. Our simulations for realistic wireless channels confirm the superiority of the Log rule which achieves up to 80% reduction in mean packet delays. However, recent asymptotic analysis showed that Exp rule is optimal in terms of minimizing the asymptotic probability of max-queue overflow. In turn, in a companion paper we have shown that an RSM policy minimizes the asymptotic probability of sum-queue overflow. Finally, we use extensive simulations to explore the various possible design objectives for opportunistic schedulers. When users see heterogenous channels, we find that minimizing the worst asymptotic exponent across users may excessively compromise the overall delay. Our simulations show that only if perfectly tuned to the load will the Exp rule achieve low homogenous tails across users. Otherwise the Log rule achieves a 20-75% reduction in the 99<sup>th</sup> percentile for most, if not all, the users. We conclude that for wireless environments, where precise resource allocation is virtually impossible, the Log rule may be more desirable for its robust and graceful degradation to unpredicted changes.},
keywords={delays;Markov processes;queueing theory;scheduling;time-varying channels;wireless channels;delay-optimal opportunistic scheduling;delay-optimal opportunistic approximation;log rule;opportunistic packet scheduler;users sharing;time-varying wireless channel;Markov decision process framework;arrival statistics;channel statistics;mean-delay-optimal scheduling policies;radial sum-rate monotonicity;unequal queue balancing;Delay;Robustness;Statistics;Processor scheduling;Throughput;Scheduling algorithm;Stability;Tail;Resource management;Degradation},
doi={10.1109/INFCOM.2009.5062088},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062089,
author={P. van de Ven and S. Borst and S. Shneer},
booktitle={IEEE INFOCOM 2009},
title={Instability of MaxWeight Scheduling Algorithms},
year={2009},
volume={},
number={},
pages={1701-1709},
abstract={MaxWeight scheduling algorithms provide an effective mechanism for achieving queue stability and guaranteeing maximum throughput in a wide variety of scenarios. The maximum-stability guarantees however rely on the fundamental premise that the system consists of a fixed set of sessions with stationary ergodic traffic processes. In the present paper we examine a scenario where the population of active sessions varies over time, as sessions eventually end while new sessions occasionally start. We identify a simple necessary and sufficient condition for stability, and show that MaxWeight policies may fail to provide maximum stability. The intuitive explanation is that these policies tend to give preferential treatment to flows with large backlogs, so that the rate variations of flows with smaller backlogs are not fully exploited. In the usual framework with a fixed collection of flows, the latter phenomenon cannot persist since the flows with smaller backlogs will build larger queues and gradually start receiving more service. With a dynamic population of flows, however, MaxWeight policies may constantly get diverted to arriving flows, while neglecting the rate variations of a persistently growing number of flows in progress with relatively small remaining backlogs. We also perform extensive simulation experiments to corroborate the analytical findings.},
keywords={queueing theory;scheduling;telecommunication traffic;MaxWeight-type scheduling algorithm;maximum throughput;maximum queue stability;stationary ergodic traffic;Scheduling algorithm;Stability;Throughput;Traffic control;Wireless networks;Interference;Communications Society;Mathematics;Computer science;USA Councils},
doi={10.1109/INFCOM.2009.5062089},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062090,
author={J. Zhang and S. C. Liew and L. Fu},
booktitle={IEEE INFOCOM 2009},
title={On Fast Optimal STDMA Scheduling over Fading Wireless Channels},
year={2009},
volume={},
number={},
pages={1710-1718},
abstract={Most prior studies on wireless spatial-reuse TDMA (STDMA) link scheduling for throughput optimization deal with the situation where instantaneous channel state information (CSI) is available. Under fast fading, however, the channel may change too quickly for the scheduler to track the instantaneous CSI. In this paper, instead of the instantaneous CSI, the scheduler performs its task according to the stochastic behavior of the channel state. A basic schedule consists of a set of simultaneously transmitting links. The essence of the scheduling problem is to determine a mixed schedule consisting of a weighted sum (TDMA mixture) of a number of basic schedules to optimize a certain utility objective. A key to reducing scheduling complexity is to identify the Pareto-efflcient basic schedules, referred to as the extreme-point schedules, so that the construction of the optimal mixed schedule can be based on the extreme-point schedules rather than all the basic schedules. The precise identification of the extreme-point schedules, however, is intractable computationally. We show in this paper that identifying a slightly larger superset of the extreme-point schedules can be highly efficient using a Perron-Frobenius condition: simulation experiments indicate that oftentimes there are only very few extraneous non-extreme-point schedules in the superset. Building on the effective identification method, we propose a fast scheduling algorithm. This algorithm beats the algorithm without using the identification method by a complexity-reduction factor of 166 in a 15-link network. In addition, numerical results suggest that our algorithm is robust to variations of system parameters.},
keywords={communication complexity;fading channels;Pareto optimisation;scheduling;stochastic processes;time division multiple access;optimal STDMA scheduling;fading wireless channel;wireless spatial-reuse TDMA link scheduling;channel state information;stochastic behavior;scheduling complexity reduction;Pareto-efflcient basic schedule;Fading;Processor scheduling;Throughput;Shadow mapping;Time division multiple access;Channel state information;Computational modeling;Communications Society;Stochastic processes;Scheduling algorithm},
doi={10.1109/INFCOM.2009.5062090},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062091,
author={G. Tan and M. Bertier and A. -. Kermarrec},
booktitle={IEEE INFOCOM 2009},
title={Visibility-Graph-Based Shortest-Path Geographic Routing in Sensor Networks},
year={2009},
volume={},
number={},
pages={1719-1727},
abstract={We study the problem of shortest-path geographic routing in a static sensor network. Existing algorithms often make routing decisions based on node information in local neighborhoods. However, it is shown by Kuhn et al. that such a design constraint results in a highly undesirable lower bound for routing performance: if a best route has length c, then in the worst case a route produced by any localized algorithm has length Omega(c<sup>2</sup>), which can be arbitrarily worse than the optimal. We present VIGOR, a visibility-graph-based routing protocol that produces routes of length Theta(c). Our design is based on the construction of a much reduced visibility graph, which guides nodes to find near-optimal paths. The per-node protocol overheads in terms of state information and message transmission depend only on the complexity of the field's large topological features, rather than on the network size. Simulation results show that our protocol dramatically outperforms localized protocols such as GPSR and GOAFR+ in both average and worst cases, with reasonable extra overheads.},
keywords={graph theory;routing protocols;wireless sensor networks;shortest-path geographic routing;static sensor network;localized algorithm;VIGOR;visibility-graph-based routing protocol;routing decision;message transmission;GPSR;GOAFR+;Routing protocols;Euclidean distance;Peer to peer computing;Algorithm design and analysis;Network topology;Communications Society;Wireless networks;Scalability;Resumes;Shape},
doi={10.1109/INFCOM.2009.5062091},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062092,
author={M. -. Tsai and F. -. Wang and H. -. Yang and Y. -. Cheng},
booktitle={IEEE INFOCOM 2009},
title={VirtualFace: An Algorithm to Guarantee Packet Delivery of Virtual-Coordinate-Based Routing Protocols in Wireless Sensor Networks},
year={2009},
volume={},
number={},
pages={1728-1736},
abstract={Because the global positioning system (GPS) consumes a large amount of power and does not work indoors, many virtual-coordinate-based routing protocols are proposed for wireless sensor networks in which geographic location information is unavailable. Each of them, however, cannot guarantee packet delivery or constructs a virtual coordinate system with a complex structure. In this paper, we propose a method capable of augmenting virtual-coordinate-based routing protocols to guarantee packet delivery. Firstly, we introduce the virtual face construction protocol and the virtual face naming protocol to construct and name virtual faces, respectively. Subsequently, the VirtualFace algorithm is presented to route a packet from a dead-end node to a progress node by traversing the boundaries of the virtual faces from face to face. Simulations show that virtual-coordinate-based routing protocols including GLIDER, Hop ID, GLDR, and VCap augmented with the VirtualFace algorithm guarantee packet delivery while ensuring moderate routing path length overhead costs.},
keywords={routing protocols;wireless sensor networks;VirtualFace algorithm;packet delivery;virtual-coordinate-based packet routing protocol;wireless sensor network;global positioning system;virtual face construction protocol;virtual face naming protocol;GLIDER protocol;Hop ID protocol;GLDR protocol;VCap protocol;Routing protocols;Wireless sensor networks;Global Positioning System;Peer to peer computing;Communications Society;Computer science;Electronic mail;Costs;Aggregates;Base stations},
doi={10.1109/INFCOM.2009.5062092},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062093,
author={R. Flury and S. V. Pemmaraju and R. Wattenhofer},
booktitle={IEEE INFOCOM 2009},
title={Greedy Routing with Bounded Stretch},
year={2009},
volume={},
number={},
pages={1737-1745},
abstract={Greedy routing is a novel routing paradigm where messages are always forwarded to the neighbor that is closest to the destination. Our main result is a polynomial-time algorithm that embeds combinatorial unit disk graphs (CUDGs - a CUDG is a UDG without any geometric information) into O(log<sup>2</sup>n)- dimensional space, permitting greedy routing with constant stretch. To the best of our knowledge, this is the first greedy embedding with stretch guarantees for this class of networks. Our main technical contribution involves extracting, in polynomial time, a constant number of isometric and balanced tree separators from a given CUDG. We do this by extending the celebrated Lipton-Tarjan separator theorem for planar graphs to CUDGs. Our techniques extend to other classes of graphs; for example, for general graphs, we obtain an O(log n)-stretch greedy embedding into O(log<sup>2</sup>n)-dimensional space. The greedy embeddings constructed by our algorithm can also be viewed as a constant-stretch compact routing scheme in which each node is assigned an O(log<sup>3</sup>n)-bit label. To the best of our knowledge, this result yields the best known stretch-space trade-off for compact routing on CUDGs. Extensive simulations on random wireless networks indicate that the average routing overhead is about 10%; only few routes have a stretch above 1.5.},
keywords={computational complexity;graph theory;greedy algorithms;Internet;telecommunication network routing;greedy routing;bounded stretch;polynomial-time algorithm;combinatorial unit disk graphs;greedy embedding;balanced tree separators;planar graphs;constant-stretch compact routing scheme;random wireless networks;Internet routing;Routing;Polynomials;Particle separators;IP networks;Greedy algorithms;Communications Society;USA Councils;Data mining;Tree graphs;Peer to peer computing},
doi={10.1109/INFCOM.2009.5062093},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062094,
author={G. Tan and M. Bertier and A. -. Kermarrec},
booktitle={IEEE INFOCOM 2009},
title={Convex Partition of Sensor Networks and Its Use in Virtual Coordinate Geographic Routing},
year={2009},
volume={},
number={},
pages={1746-1754},
abstract={Virtual coordinate geographic routing is an appealing geographic routing approach for its ability to work without physical location information. We examine two representative such routing protocols, namely NoGeo and BVR, and show through experiments and theoretical analysis their limitation in adapting to complex field topologies, in particular fields with concave holes. Based on the new insights, we propose a distributed convex partition protocol that divides the field to subareas with convex shapes, using only connectivity information. A new geographic routing protocol, called CONVEX, that builds upon the partitioning protocol is then described. Simulations demonstrate significant performance improvement of the new routing protocol over NoGeo and BVR, in terms of transmission stretch and maintenance overheads.},
keywords={geography;geophysics computing;routing protocols;wireless sensor networks;sensor networks;virtual coordinate geographic routing protocols;physical location information;routing protocols;NoGeo;complex field topologies;CONVEX;maintenance overheads;transmission stretch;Routing protocols;Shape;Global Positioning System;Peer to peer computing;Planarization;Partitioning algorithms;Communications Society;Topology;Ad hoc networks;Resumes},
doi={10.1109/INFCOM.2009.5062094},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062095,
author={A. Bender and R. Sherwood and D. Monner and N. Goergen and N. Spring and B. Bhattacharjee},
booktitle={IEEE INFOCOM 2009},
title={Fighting Spam with the NeighborhoodWatch DHT},
year={2009},
volume={},
number={},
pages={1755-1763},
abstract={In this paper, we present DHTBL, an anti-spam blacklist built upon a novel secure distributed hash table (DHT). We show how DHTBL can be used to replace existing DNS-based blacklists (DNSBLs) of IP addresses of mail relays that forward spam. Implementing a blacklist on a DHT improves resilience to DoS attacks and secures message delivery, when compared to DNSBLs. However, due to the sensitive nature of the blacklist, storing the data in a peer-to-peer DHT would invite attackers to infiltrate the system. Typical DHTs can withstand fail-stop failures, but malicious nodes may provide incorrect routing information, refuse to return published items, or simply ignore certain queries. The neighborhoodwatch DHT is resilient to malicious nodes and maintains the O(logiV) bounds on routing table size and expected lookup time. NeighborhoodWatch depends on two assumptions in order to make these guarantees: (1) the existence of an on-line trusted authority that periodically contacts and issues signed certificates to each node, and (2) for every sequence of k + 1 consecutive nodes in the ID space, at least one is alive and non-malicious. We show how NeighborhoodWatch maintains many of its security properties even when the second assumption is violated. Honest nodes in NeighborhoodWatch can detect malicious behavior and expel the responsible nodes from the DHT.},
keywords={cryptography;IP networks;peer-to-peer computing;peer-to-peer distributed hash table;IP addresses;table size routing;on-line trusted authority;Unsolicited electronic mail;Peer to peer computing;Computer crime;Routing;Databases;Postal services;Relays;Resilience;Security;Communications Society},
doi={10.1109/INFCOM.2009.5062095},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062096,
author={Z. Duan and P. Chen and F. Sanchez and Y. Dong and M. Stephenson and J. Barker},
booktitle={IEEE INFOCOM 2009},
title={Detecting Spam Zombies by Monitoring Outgoing Messages},
year={2009},
volume={},
number={},
pages={1764-1772},
abstract={Compromised machines are one of the key security threats on the Internet; they are often used to launch various security attacks such as DDoS, spamming, and identity theft. In this paper we address this issue by investigating effective solutions to automatically identify compromised machines in a network. Given that spamming provides a key economic incentive for attackers to recruit the large number of compromised machines, we focus on the subset of compromised machines that are involved in the spamming activities, commonly known as spam zombies. We develop an effective spam zombie detection system named SPOT by monitoring outgoing messages of a network. SPOT is designed based on a powerful statistical tool called Sequential Probability Ratio Test, which has bounded false positive and false negative error rates. Our evaluation studies based on a two- month email trace collected in a large U.S. campus network show that SPOT is an effective and efficient system in automatically detecting compromised machines in a network. For example, among the 440 internal IP addresses observed in the email trace, SPOT identifies 132 of them as being associated with compromised machines. Out of the 132 IP addresses identified by SPOT, 126 can be either independently confirmed (110) or highly likely (16) to be compromised. Moreover, only 7 internal IP addresses associated with compromised machines in the trace are missed by SPOT.},
keywords={electronic messaging;Internet;IP networks;probability;security of data;statistical testing;unsolicited e-mail;spam zombies detection;outgoing messages monitoring;security threats;Internet;security attacks;DDoS;identity theft;compromised machines;spamming activity;spam zombie detection system;SPOT;statistical tool;sequential probability ratio test;U.S. campus network;IP addresses;Unsolicited electronic mail;Internet;Probability;Sequential analysis;Aggregates;Statistical analysis;Condition monitoring;Power generation economics;Recruitment;Error analysis},
doi={10.1109/INFCOM.2009.5062096},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062097,
author={J. -. Ho and M. Wright and S. K. Das},
booktitle={IEEE INFOCOM 2009},
title={Fast Detection of Replica Node Attacks in Mobile Sensor Networks Using Sequential Analysis},
year={2009},
volume={},
number={},
pages={1773-1781},
abstract={Due to the unattended nature of wireless sensor networks, an adversary can capture and compromise sensor nodes, generate their replicas, and thus mount a variety of attacks with these replicas. Such attacks are dangerous because they allow the attacker to leverage the compromise of a few nodes to exert control over much of the network. Several replica node detection schemes have been proposed in the literature to defend against such attacks in static sensor networks. However, these schemes rely on fixed sensor locations and hence do not work in mobile sensor networks, where sensors are expected to move. In this work, we propose a fast and effective mobile replica node detection scheme using the Sequential Probability Ratio Test. To the best of our knowledge, this is the first work to tackle the problem of replica node attacks in mobile sensor networks. We show analytically and through simulation experiments that our scheme provides effective and robust replica detection capability with reasonable overheads.},
keywords={mobile radio;telecommunication security;wireless sensor networks;replica node attacks fast detection;mobile sensor networks;sequential analysis;wireless sensor networks;replica node detection schemes;static sensor networks;fixed sensor locations;sequential probability ratio test;Sequential analysis;Peer to peer computing;Wireless sensor networks;Monitoring;Hardware;Mobile robots;Protocols;Robustness;Robot sensing systems;Communications Society},
doi={10.1109/INFCOM.2009.5062097},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062098,
author={L. Yu and J. Li},
booktitle={IEEE INFOCOM 2009},
title={Grouping-Based Resilient Statistical En-Route Filtering for Sensor Networks},
year={2009},
volume={},
number={},
pages={1782-1790},
abstract={In sensor networks, the adversaries can inject false data reports from compromising nodes. Previous approaches for filtering false reports, notably statistical en-route filtering, adopt a simple strategy for grouping sensor nodes that requires redundant groups and thus decrease the filtering effectiveness. Worse still, they either suffer a threshold problem, which may lead to complete breakdown of the security protection when the threshold is exceeded, or are dependent on sink stationarity and specific routing protocols, which cannot work with mobile sinks and various routing protocols. In response to these, this paper proposes a scheme, referred to as grouping-based resilient statistical en-route filtering (GRSEF), in which nodes are grouped once deployed without requiring redundant groups and a location-aware approach based on terrain division along multiple axes is proposed for key derivation. The design of GRSEF, which is independent of sink stationarity and routing protocols, provides a well suitable en-routing filtering solution for sensor networks with mobile sinks. Analytical and simulation results verify that the scheme significantly improves the filtering effectiveness and efficiently achieves the resiliency against node compromise.},
keywords={filtering theory;routing protocols;statistical analysis;wireless sensor networks;grouping-based resilient statistical en-route filtering;sensor networks;false data reports;security protection;sink stationarity;location-aware approach;routing protocols;mobile sinks;Filtering;Peer to peer computing;Routing protocols;Data security;Authentication;Protection;Communications Society;Computer science;Electric breakdown;Analytical models},
doi={10.1109/INFCOM.2009.5062098},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062099,
author={A. Hagedorn and S. Agarwal and D. Starobinski and A. Trachtenberg},
booktitle={IEEE INFOCOM 2009},
title={Rateless Coding with Feedback},
year={2009},
volume={},
number={},
pages={1791-1799},
abstract={The erasure resilience of rateless codes, such as Luby-Transform (LT) codes, makes them particularly suitable to a wide variety of loss-prone wireless and sensor network applications, ranging from digital video broadcast to software updates. Yet, traditional rateless codes usually make no use of a feedback communication channel, a feature available in many wireless settings. As such, we generalize LT codes to situations where receiver(s) provide feedback to the broadcaster. Our approach, referred to as Shifted LT (SLT) code, modifies the robust soliton distribution of LT codes at the broadcaster, based on the number of input symbols already decoded at the receivers. While implementing this modification entails little change to the LT encoder and decoder, we show both analytically and through real experiments, that it achieves significant savings in communication complexity, memory usage, and overall energy consumption. Furthermore, we show that significant savings can be even achieved with a low number of feedback messages (on the order of the square root of the total number of input symbols) transmitted at a uniform rate. The practical benefits of Shifted LT codes are demonstrated through the implementation of a real over-the-air programming application for sensor networks, based on the Deluge protocol.},
keywords={communication complexity;feedback;protocols;transform coding;wireless sensor networks;rateless coding;erasure resilience;rateless codes;Luby-Transform codes;wireless sensor network;digital video broadcast;software updates;feedback communication channel;wireless settings;shifted LT code;robust soliton distribution;communication complexity;memory usage;energy consumption;feedback messages;over-the-air programming;Deluge protocol;Wireless sensor networks;Digital video broadcasting;Decoding;Resilience;Application software;Feedback communications;Robustness;Solitons;Complexity theory;Energy consumption},
doi={10.1109/INFCOM.2009.5062099},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062100,
author={D. E. Lucani and M. Stojanovic and M. Medard},
booktitle={IEEE INFOCOM 2009},
title={Random Linear Network Coding For Time Division Duplexing: When To Stop Talking And Start Listening},
year={2009},
volume={},
number={},
pages={1800-1808},
abstract={A new random linear network coding scheme for reliable communications for time division duplexing channels is proposed. The setup assumes a packet erasure channel and that nodes cannot transmit and receive information simultaneously. The sender transmits coded data packets back-to-back before stopping to wait for the receiver to acknowledge (ACK) the number of degrees of freedom, if any, that are required to decode correctly the information. We provide an analysis of this problem to show that there is an optimal number of coded data packets, in terms of mean completion time, to be sent before stopping to listen. This number depends on the latency, probabilities of packet erasure and ACK erasure, and the number of degrees of freedom that the receiver requires to decode the data. This scheme is optimal in terms of the mean time to complete the transmission of a fixed number of data packets. We show that its performance is very close to that of a full duplex system, while transmitting a different number of coded packets can cause large degradation in performance, especially if latency is high. Also, we study the throughput performance of our scheme and compare it to existing half-duplex go-back-N and selective repeat ARQ schemes. Numerical results, obtained for different latencies, show that our scheme has similar performance to the selective repeat in most cases and considerable performance gain when latency and packet error probability is high.},
keywords={automatic repeat request;error statistics;random codes;telecommunication network reliability;time division multiplexing;random linear network coding;time division duplexing;communication reliability;time division duplexing channels;packet erasure;full duplex system;half-duplex go-back-N schemes;selective repeat ARQ schemes;Network coding;Delay;Decoding;Throughput;Automatic repeat request;Feedback;Linear code;Performance gain;Wireless networks;Communications Society},
doi={10.1109/INFCOM.2009.5062100},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062101,
author={Y. Lin and B. Liang and B. Li},
booktitle={IEEE INFOCOM 2009},
title={Passive Loss Inference in Wireless Sensor Networks Based on Network Coding},
year={2009},
volume={},
number={},
pages={1809-1817},
abstract={The highly stochastic nature of wireless environments makes it desirable to monitor link loss rates in wireless sensor networks. In this paper, we study the loss inference problem in sensor networks with network coding. Unlike traditional transmission protocols, network coding offers reliable communication without using control messages for individual packets. We show, however, that network coding changes the fundamental connection between path and link loss probabilities such that new inference algorithms need to be developed. As end- to-end data are not sufficient to compute link loss rates precisely, we propose inference algorithms based on Bayesian principles to discover the set of highly lossy links in sensor networks. We show that our algorithms achieve high detection and low false-positive rates through extensive simulations.},
keywords={Bayes methods;encoding;wireless sensor networks;passive loss inference;wireless sensor networks;network coding;link loss rates;Bayesian principles;Wireless sensor networks;Network coding;Inference algorithms;Stochastic processes;Monitoring;Protocols;Telecommunication network reliability;Communication system control;Computer networks;Bayesian methods},
doi={10.1109/INFCOM.2009.5062101},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062102,
author={M. U. Ilyas and M. Kim and H. Radha},
booktitle={IEEE INFOCOM 2009},
title={Reducing Packet Losses in Networks of Commodity IEEE 802.15.4 Sensor Motes Using Cooperative Communication and Diversity Combination},
year={2009},
volume={},
number={},
pages={1818-1826},
abstract={This paper presents the 'Poor Man's SIMO System' (PMSS) which combines two ideas, cooperative communication and diversity combination, to reduce packet losses over links in Wireless Sensor Networks (WSN). The work is based on the IEEE 802.15.4 standard and is distinct from previous works that apply the same concepts because it foregoes the need for any changes to mote hardware. We describe a Poor Man's SIMO System protocol that governs the cooperation between receivers. Three diversity combination methods are evaluated including selection diversity, equal gain and maximal ratio combining. The latter relies on a model of the instantaneous Bit Error Rate (BER) driven by Channel State Information (CSI), i.e. Received Signal Strength Indication (RSSI) and Link Quality Indication (LQI). First, we demonstrate the PMSS on residual bit error traces in a fully reproducible manner. This is followed by an implementation of PMSS in C# on the .NET Micro Framework edition of the recently released Imote2 WSN mote platform. Both, trace based analysis and implementation demonstrate significant improvements over the single receiver baseline configuration. We deliberately verified PMSS by residual bit error traces and implementation to avoid the use of simulators that depend on abstract models of wireless channels.},
keywords={diversity reception;error statistics;personal area networks;wireless sensor networks;packet losses reduction;IEEE 802.15.4 sensor motes;cooperative communication;wireless sensor networks;IEEE 802.15.4 standard;diversity combination methods;selection diversity;maximal ratio combining;equal gain combining;bit error rate;channel state information;received signal strength indication;link quality indication;bit error traces;wireless channels;Diversity reception;Wireless sensor networks;Protocols;Receivers;Bit error rate;Hardware;Diversity methods;Automatic repeat request;Error correction;Communications Society},
doi={10.1109/INFCOM.2009.5062102},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062103,
author={C. -. Hsu and K. -. Lai and C. -. Chou and K. C. -. Lin},
booktitle={IEEE INFOCOM 2009},
title={ST-MAC: Spatial-Temporal MAC Scheduling for Underwater Sensor Networks},
year={2009},
volume={},
number={},
pages={1827-1835},
abstract={Underwater sensor networks (UWSNs) have attracted a lot of attention recently. Since data in UWSNs are transmitted by acoustic signals, the characteristics of a UWSN are different from those of a terrestrial sensor network. In other words, the high propagation delay of acoustic signals in UWSNs causes spatial-temporal uncertainty, and makes transmission scheduling in UWSNs a challenging problem. Hence, in this paper, we propose a spatial-temporal MAC scheduling protocol, called ST-MAC, which is designed to overcome spatial-temporal uncertainty based on TDMA-based MAC scheduling for energy saving and throughput improvement. We construct the <i>spatial-temporal</i> <i>conflict</i> <i>graph</i> (ST-CG) to describe the conflict delays among transmission links explicitly, and model ST-MAC as a new vertex coloring problem of ST-CG. We then propose a novel heuristic, called the <i>traffic-based</i> <i>one-step</i> <i>trial</i> <i>approach</i> (TOTA), to solve the coloring problem. In order to obtain the optimal solution of the scheduling problem, we also derive a mixed integer linear programming (MILP) model. Finally, we present a comprehensive performance study via simulations. The results show that ST-MAC can perform better than existing MAC schemes (such as S-MAC, ECDiG, and T-Lohi) in terms of the network throughput and energy cost.},
keywords={access protocols;graph theory;integer programming;linear programming;scheduling;telecommunication traffic;underwater acoustic communication;wireless sensor networks;ST-MAC;spatial-temporal MAC scheduling protocols;underwater sensor networks;acoustic signal transmission;propagation delay;spatial-temporal conflict graph;vertex coloring problem;traffic-based one-step trial approach;mixed integer linear programming;Sensor phenomena and characterization;Acoustic sensors;Underwater acoustics;Uncertainty;Media Access Protocol;Throughput;Propagation delay;Traffic control;Mixed integer linear programming;Costs},
doi={10.1109/INFCOM.2009.5062103},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062104,
author={P. Djukic and P. Mohapatra},
booktitle={IEEE INFOCOM 2009},
title={Soft-TDMAC: A Software TDMA-Based MAC over Commodity 802.11 Hardware},
year={2009},
volume={},
number={},
pages={1836-1844},
abstract={We design and implement Soft-TDMAC, a software Time Division Multiple Access (TDMA) based MAC protocol, running over commodity 802.11 hardware. Soft-TDMAC has a synchronization mechanism, which synchronizes all pairs of network clocks to within microseconds of each other. Building on pairwise synchronization, Soft-TDMAC achieves network wide synchronization. With, out-of-band, network wide synchronization Soft-TDMAC can schedule arbitrary TDMA transmission patterns. We summarize hundreds of hours of testing Soft-TDMAC on a multi-hop testbed. Our experimental results show that Soft-TDMAC synchronizes multi-hop networks to within a few microsecond sized TDMA slots. Soft-TDMAC can schedule transmissions to take end-to-end demands into account and in a way that decreases end-to-end delay. With no collisions, under good channel conditions, TCP achieves almost the full wireless channel bandwidth.},
keywords={synchronisation;time division multiple access;transport protocols;wireless channels;wireless LAN;Soft-TDMAC;software TDMA-based MAC;commodity 802.11 hardware;software time division multiple access;MAC protocol;synchronization mechanism;network clocks;pairwise synchronization;network wide synchronization;TDMA transmission patterns;multihop testbed;multihop networks;end-to-end delay;TCP;wireless channel bandwidth;Hardware;Synchronization;Time division multiple access;Testing;Media Access Protocol;Access protocols;Clocks;Buildings;Spread spectrum communication;Delay},
doi={10.1109/INFCOM.2009.5062104},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062105,
author={M. Sha and G. Xing and G. Zhou and S. Liu and X. Wang},
booktitle={IEEE INFOCOM 2009},
title={C-MAC: Model-Driven Concurrent Medium Access Control for Wireless Sensor Networks},
year={2009},
volume={},
number={},
pages={1845-1853},
abstract={This paper presents C-MAC, a new MAC protocol designed to achieve high-throughput bulk communication for data-intensive sensing applications. C-MAC exploits concurrent wireless channel access based on empirical power control and physical interference models. Nodes running C-MAC estimate the level of interference based on the physical signal-to-interference-plus-noise-ratio (SINR) model and adjust the transmission power accordingly for concurrent channel access. C-MAC employs a block-based communication mode that not only amortizes the overhead of channel assessment, but also improves the probability that multiple nodes within the interference range of each other can transmit concurrently. C-MAC has been implemented in TinyOS-1.x and extensively evaluated on Tmote nodes. Our experiments show that C-MAC significantly outperforms the state-of-art CSMA protocol in TinyOS with respect to system throughput, delay and energy consumption.},
keywords={access protocols;probability;radiofrequency interference;telecommunication control;wireless channels;wireless sensor networks;model-driven concurrent medium access control;wireless sensor networks;data-intensive sensing applications;physical interference models;physical signal-to-interference-plus-noise-ratio;concurrent channel access;TinyOS;energy consumption;Media Access Protocol;Wireless sensor networks;Interference;Access protocols;Power control;Signal to noise ratio;Multiaccess communication;Throughput;Delay systems;Energy consumption},
doi={10.1109/INFCOM.2009.5062105},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062106,
author={H. Lee and H. Kwon and A. Motskin and L. Guibas},
booktitle={IEEE INFOCOM 2009},
title={Interference-Aware MAC Protocol for Wireless Networks by a Game-Theoretic Approach},
year={2009},
volume={},
number={},
pages={1854-1862},
abstract={We propose an interference-aware MAC protocol using a simple transmission strategy motivated by a game- theoretic approach. We formulate a channel access game, which considers nodes concurrently transmitting in nearby clusters, incorporating a realistic wireless communication model - the SINR model. Under inter-cluster interference, we derive a decentralized transmission strategy, which achieves a Bayesian Nash Equilibrium (BNE). The proposed MAC protocol balances network throughput and battery consumption at each transmission. We compare our BNE-based decentralized strategy with a centralized globally optimal strategy in terms of efficiency and balance. We further show that the transmission threshold should be adaptively tuned depending on the number of active users in the network, crosstalk, ambient noise, transmission cost, and radio-dependent receiver sensitivity. We also present a simple dynamic procedure for nodes to efficiently find a Nash Equilibrium (NE) without requiring each node to know the total number of active nodes or the channel gain distribution, and prove that this procedure is guaranteed to converge.},
keywords={access protocols;Bayes methods;game theory;interference (signal);radio networks;MAC protocol;wireless networks;game theory;channel access game;wireless communication;SINR model;intercluster interference;Bayesian Nash equilibrium;Interference;Media Access Protocol;Wireless application protocol;Wireless networks;Access protocols;Nash equilibrium;Crosstalk;Game theory;Wireless communication;Signal to noise ratio},
doi={10.1109/INFCOM.2009.5062106},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062107,
author={C. Wang and X. -. Li and C. Jiang and S. Tang and Y. Liu and J. Zhao},
booktitle={IEEE INFOCOM 2009},
title={Scaling laws on multicast capacity of large scale wireless networks},
year={2009},
volume={},
number={},
pages={1863-1871},
abstract={In this paper, we focus on the networking-theoretic multicast capacity for both random extended networks (REN) and random dense networks (RDN) under Gaussian Channel model, when all nodes are individually power-constrained. During the transmission, the power decays along path with the attenuation exponent alpha &gt; 2. In REN and RDN, n nodes are randomly distributed in the square region with side-length radic(n) and 1, respectively. We randomly choose n<sub>s</sub> nodes as the sources of multicast sessions, and for each source v, we pick uniformly at random n<sub>d</sub> nodes as the destination nodes. Based on percolation theory, we propose multicast schemes and analyze the achievable throughput by considering all possible values of n<sub>s</sub> and n<sub>d</sub>. As a special case of our results, we show that for n<sub>s</sub> = Theta(n), the per-session multicast capacity of RDN is Theta((1)/(radic(n<sub>d</sub>n))) when n<sub>d</sub> = O((n)/((log n)<sup>3</sup>)) and is Theta((1)/(n)) when n<sub>d</sub> = Omega((1)/(log n)); the per-session multicast capacity of REN is Theta((1)/radic(n<sub>d</sub>n)) when n<sub>d</sub> = O((n)/((log n)<sup>alpha+1</sup>)) and is Theta((1)/(n<sub>d</sub>) ldr (log n)<sup>-(alpha)/(2)</sup>) when n<sub>d</sub> = Omega((n)/(log n)).},
keywords={Gaussian channels;multicast communication;random processes;wireless sensor networks;scaling laws;large scale wireless networks;networking-theoretic multicast capacity;random extended networks;random dense networks;Gaussian channel model;attenuation exponent;percolation theory;Large-scale systems;Wireless networks;Peer to peer computing;Computer science;Gaussian channels;Throughput;Transmitters;Interference;Routing;Signal to noise ratio},
doi={10.1109/INFCOM.2009.5062107},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062108,
author={O. Goussevskaia and R. Wattenhofer and M. M. Halldorsson and E. Welzl},
booktitle={IEEE INFOCOM 2009},
title={Capacity of Arbitrary Wireless Networks},
year={2009},
volume={},
number={},
pages={1872-1880},
abstract={In this work we study the problem of determining the throughput capacity of a wireless network. We propose a scheduling algorithm to achieve this capacity within an approximation factor. Our analysis is performed in the physical interference model, where nodes are arbitrarily distributed in Euclidean space. We consider the problem separately from the routing problem and the power control problem, i.e., all requests are single-hop, and all nodes transmit at a fixed power level. The existing solutions to this problem have either concentrated on special-case topologies, or presented optimality guarantees which become arbitrarily bad (linear in the number of nodes) depending on the network's topology. We propose the first scheduling algorithm with approximation guarantee independent of the topology of the network. The algorithm has a constant approximation guarantee for the problem of maximizing the number of links scheduled in one time-slot. Furthermore, we obtain a O(log n) approximation for the problem of minimizing the number of time slots needed to schedule a given set of requests. Simulation results indicate that our algorithm does not only have an exponentially better approximation ratio in theory, but also achieves superior performance in various practical network scenarios. Furthermore, we prove that the analysis of the algorithm is extendable to higher-dimensional Euclidean spaces, and to more realistic bounded-distortion spaces, induced by non-isotropic signal distortions. Finally, we show that it is NP-hard to approximate the scheduling problem to within n<sup>1-epsiv</sup> factor, for any constant epsiv &gt; 0, in the non-geometric SINR model, in which path-loss is independent of the Euclidean coordinates of the nodes.},
keywords={approximation theory;computational complexity;optimisation;radio networks;scheduling;telecommunication network topology;arbitrary wireless networks;wireless network throughput capacity;scheduling algorithm;approximation factor;physical interference model;Euclidean space;power control problem;fixed power level;network topology;approximation ratio;high-dimensional Euclidean space;bounded-distortion spaces;NP-hard problem;scheduling problem;Wireless networks;Scheduling algorithm;Network topology;Approximation algorithms;Throughput;Performance analysis;Interference;Routing;Power control;Signal analysis},
doi={10.1109/INFCOM.2009.5062108},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062109,
author={U. Niesen and P. Gupta and D. Shah},
booktitle={IEEE INFOCOM 2009},
title={The Multicast Capacity Region of Large Wireless Networks},
year={2009},
volume={},
number={},
pages={1881-1889},
abstract={We study the problem of determining the multicast capacity region of a wireless network of n nodes randomly located in an extended area and communicating with each other over Gaussian fading channels. We obtain an explicit information- theoretic characterization of the scaling of the multicast capacity region for n nodes in terms of 2n weighted cuts. These cuts only depend on the geometry of the locations of the source nodes and their destination nodes and the traffic demands between them, and thus can be readily evaluated. The results are constructive and provide a two-layer architecture for achieving nearly the entire multicast capacity region in the scaling sense: The top layer routes traffic from each of the source nodes to its set of destination nodes, and the bottom layer physically distributes/concentrates traffic among appropriate nodes through one of the two cooperative communication schemes - hierarchical relaying and multi-hopping - depending on the wireless-channel characteristics.},
keywords={fading channels;Gaussian channels;radio networks;telecommunication traffic;multicast capacity region scaling;large wireless networks;Gaussian fading channels;information-theoretic characterization;traffic;cooperative communication;hierarchical relaying;multihopping;wireless-channel characteristics;Wireless networks;Peer to peer computing;Telecommunication traffic;Unicast;Interference;Wireless mesh networks;Wireless sensor networks;Multicast protocols;Communications Society;Fading},
doi={10.1109/INFCOM.2009.5062109},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062110,
author={G. Alfano and M. Garetto and E. Leonardi},
booktitle={IEEE INFOCOM 2009},
title={Capacity Scaling of Wireless Networks with Inhomogeneous Node Density: Lower Bounds},
year={2009},
volume={},
number={},
pages={1890-1898},
abstract={We consider static ad hoc wireless networks comprising significant inhomogeneities in the node spatial distribution over the area, and analyze the scaling laws of their transport capacity as the number of nodes increases. In particular, we consider nodes placed according to a shot-noise Cox process, which allows to model the clustering behavior usually recognized in large-scale systems. For this class of networks, we propose novel scheduling and routing schemes which approach previously computed upper bounds to the per-flow throughput as the number of nodes tends to infinity.},
keywords={ad hoc networks;statistical analysis;telecommunication network routing;capacity scaling;inhomogeneous node density;ad hoc wireless networks;shot-noise Cox process;large-scale systems;routing schemes;scheduling schemes;Wireless networks;Throughput;Peer to peer computing;Routing;Upper bound;Large-scale systems;H infinity control;Interference;Network topology;Communications Society},
doi={10.1109/INFCOM.2009.5062110},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062111,
author={E. Altman and F. De Pellegrini},
booktitle={IEEE INFOCOM 2009},
title={Forward Correction and Fountain codes in Delay Tolerant Networks},
year={2009},
volume={},
number={},
pages={1899-1907},
abstract={Delay tolerant ad-hoc networks leverage the mobility of relay nodes to compensate for lack of permanent connectivity and thus enable communication between nodes that are out of range of each other. To decrease delivery delay, the information to be delivered is replicated in the network. Our objective in this paper is to study a class of replication mechanisms that include coding in order to improve the probability of successful delivery within a given time limit. We propose an analytical approach that allows to quantify tradeoffs between resources and performance measures (energy and delay). We study the effect of coding on the performance of the network while optimizing parameters that govern routing. Our results, based on fluid approximations, are compared to simulations which validate the model.},
keywords={ad hoc networks;delays;forward error correction;probability;telecommunication network routing;forward correction;fountain codes;delay tolerant networks;delay tolerant adhoc networks;relay nodes mobility;permanent connectivity;delivery delay;replication mechanisms;probability;routing;fluid approximations;Disruption tolerant networking;Relays;Peer to peer computing;Ad hoc networks;Delay;Routing protocols;Network coding;Communications Society;Performance analysis;Energy measurement},
doi={10.1109/INFCOM.2009.5062111},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062112,
author={W. Xiao and D. Starobinski},
booktitle={IEEE INFOCOM 2009},
title={Extreme Value FEC for Wireless Data Broadcasting},
year={2009},
volume={},
number={},
pages={1908-1916},
abstract={The advent of practical rateless codes enables implementation of highly efficient packet-level forward error correction (FEC) strategies for reliable data broadcasting in loss-prone wireless networks. Yet, the critical question of accurately quantifying the proper amount of redundancy has remained largely unsolved. In this paper, we exploit advances in extreme value theory to rigorously address this problem. Under the asymptotic regime of a large number of receivers, we derive a closed-form expression for the cumulative distribution function (CDF) of the completion time of file distribution. We show the existence of a phase transition associated with this CDF and accurately locate the transition point. We derive tight convergence bounds demonstrating the accuracy of the asymptotic estimate for the practical case of a finite number of receivers. We also provide an asymptotic closed-form expression on the expected completion time under heterogeneous packet loss. We demonstrate the benefits of our approach through simulation and through real experiments on a testbed of 20 Tmote Sky sensors. Specifically, we augment the existing Rateless Deluge software dissemination protocol with an extreme value FEC strategy. The experimental results reveal reduction by a factor of five in retransmission request messages and by a factor of two in total dissemination time, at the cost of a marginally higher number of data packet transmissions in the order of 5%.},
keywords={forward error correction;radio broadcasting;radio networks;radio receivers;telecommunication network reliability;wireless data broadcasting;rateless code;forward error correction;reliable data broadcasting;loss-prone wireless network;extreme value theory;cumulative distribution function;file distribution;receiver;asymptotic closed-form expression;packet loss;Tmote Sky sensor;Rateless Deluge software;request message retransmission;data packet transmission;Broadcasting;Forward error correction;Closed-form solution;Wireless networks;Redundancy;Distribution functions;Convergence;Testing;Protocols;Costs},
doi={10.1109/INFCOM.2009.5062112},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062113,
author={R. Cohen and G. Grebla and L. Katzir},
booktitle={IEEE INFOCOM 2009},
title={Cross-Layer Hybrid FEC/ARQ Reliable Multicast with Adaptive Modulation and Coding in Broadband Wireless Networks},
year={2009},
volume={},
number={},
pages={1917-1925},
abstract={In this paper we define and address a new problem that arises when a base station in a broadband wireless network wishes to multicast information to a large group of nodes and to guarantee some level of reliability using Application layer FEC codes. Every data block to be multicast is translated into a sequence of K + n packets, from which every receiver must receive at least K in order to correctly decode the block. The new problem is to determine which PHY layer MCS (Modulation and Coding Scheme) the base station should use for each packet. We present several variants of this problem, which differ in the number of ARQ (Automatic Repeat reQuest) rounds during which the delivery of a data block must be completed. Most of these variants are shown to be NP-hard. However, we present optimal solutions for practical instances, where the number of MCSs is small, and efficient approximations and heuristics for the general case of each variant.},
keywords={automatic repeat request;broadband networks;forward error correction;multicast communication;radio networks;adaptive modulation;broadband wireless networks;base station;multicast information;forward error correction codes;automatic repeat request;Automatic repeat request;Modulation coding;Wireless networks;Base stations;Forward error correction;Physical layer;Multicast algorithms;Computer network reliability;Decoding;WiMAX},
doi={10.1109/INFCOM.2009.5062113},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062114,
author={M. Levorato and U. Mitra and M. Zorzi},
booktitle={IEEE INFOCOM 2009},
title={On Optimal Control of Wireless Networks with Multiuser Detection, Hybrid ARQ and Distortion Constraints},
year={2009},
volume={},
number={},
pages={1926-1934},
abstract={We present a novel optimization framework based on stochastic control and Markov theory for wireless networks where users concurrently access the channel and implement retransmission-based error control. In order to let users transmit at the same time, we consider an interference mitigation, rather than a collision avoidance approach. Our focus is on the interaction between the stochastic processes modeling the various individual sources of the network. Due to retransmissions, transmission by a user does not only instantaneously interfere with other simultaneous communications, but also biases the future evolution of the stochastic processes describing the other users. We, thus, define a novel interference measure called process distortion, that takes this effect into account. We investigate the optimization of access and power control for a network with two groups of users where transmission by the second group is constrained by the process distortion generated to the first group. We present algorithms to solve the unconstrained and constrained infinite horizon average cost per stage problems modeling this scenario. We discuss in detail the application of this framework to cognitive networks.},
keywords={automatic repeat request;Markov processes;multiuser detection;optimal control;radiofrequency interference;telecommunication congestion control;wireless network optimal control;multiuser detection;hybrid ARQ;distortion constraints;optimization framework;stochastic control;Markov theory;retransmission-based error control;interference mitigation;collision avoidance approach;stochastic processes modeling;stochastic processes;unconstrained-constrained infinite horizon average cost;cognitive networks;Optimal control;Wireless networks;Multiuser detection;Automatic repeat request;Stochastic processes;Interference constraints;Error correction;Collision mitigation;Collision avoidance;Distortion measurement},
doi={10.1109/INFCOM.2009.5062114},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062115,
author={E. Perron and S. Diggavi and E. Telatar},
booktitle={IEEE INFOCOM 2009},
title={On Cooperative Wireless Network Secrecy},
year={2009},
volume={},
number={},
pages={1935-1943},
abstract={Given that wireless communication occurs in a shared and inherently broadcast medium, the transmissions are vulnerable to undesired eavesdropping. This occurs even when a point-to-point communication is sought, and hence a fundamental question is whether we can utilize the wireless channel properties to establish secrecy. In this paper we consider secret communication between two special nodes ("source" and "destination") in a wireless network with authenticated relays: the message communicated to the destination is to be kept information-theoretically (unconditionally) secret from any eavesdropper within a class. Since the transmissions are broadcast and interfere with each other, complex signal interactions occur. We develop cooperative schemes which utilize these interactions in wireless communication over networks with arbitrary topology, and give provable unconditional secrecy guarantees.},
keywords={radio networks;telecommunication network topology;telecommunication security;wireless channels;cooperative wireless network secrecy;wireless communication;eavesdropping;point-to-point communication;wireless channel properties;secret communication;authenticated relays;complex signal interactions;arbitrary topology;broadcast medium;shared medium;Wireless networks;Wireless communication;Broadcasting;Relays;Peer to peer computing;Cryptographic protocols;Communications Society;Network topology;Algorithm design and analysis;Information security},
doi={10.1109/INFCOM.2009.5062115},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062116,
author={T. Xu and Y. Cai},
booktitle={IEEE INFOCOM 2009},
title={Location Cloaking for Safety Protection of Ad Hoc Networks},
year={2009},
volume={},
number={},
pages={1944-1952},
abstract={Location information is crucial to design efficient and scalable ad hoc networks, yet the exposure of such information presents them significant safety threats. This paper investigates the problem of preventing an adversary from locating (and thus destroying) nodes based on their location information revealed explicitly in communications. Our idea is to reduce location resolution to achieve a desired level of safety protection. We define the safety level of a geographic region to be the ratio of its area and the number of nodes inside it. The higher safety level a region has, the less attractive for an adversary to search over it for the nodes. Thus, when a node has to disclose its location, it can compute a cloaking box that meets a desired level of safety requirement and report that as its current location information. Although the basic idea is simple, there are several challenges to implement it. First, each cloaking box must be as small as possible in order to minimize the impact of reduced location resolution on the efficiency of network operating and applications. Second, nodes must be able to compute their cloaking boxes without having to reveal their accurate position. Finally, given a sequence of cloaking boxes, they must not be correlated to refine an area whose safety level is less than the required. This paper addresses these challenges with cost-effective solutions in the context of both stationary and mobile ad hoc networks. Our extensive performance evaluation indicates that our proposed technique is efficient in node safety protection, and does not have a significant impact on the performance of networks and applications. Index Terms-Location safety, Location cloaking, ad hoc networks.},
keywords={ad hoc networks;protocols;location cloaking;safety protection;ad hoc networks;cost-effective solutions;performance evaluation;Safety;Protection;Ad hoc networks;Peer to peer computing;Routing protocols;Computer science;Cryptography;Communications Society;Mobile ad hoc networks;Global Positioning System},
doi={10.1109/INFCOM.2009.5062116},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062117,
author={L. Lu and J. Han and R. Xiao and Y. Liu},
booktitle={IEEE INFOCOM 2009},
title={ACTION: Breaking the Privacy Barrier for RFID Systems},
year={2009},
volume={},
number={},
pages={1953-1961},
abstract={In order to protect privacy, radio frequency identification (RFID) systems employ privacy-preserving authentication (PPA) to allow valid readers to explicitly authenticate their dominated tags without leaking private information. Typically, an RF tag sends an encrypted message to the reader, then the reader searches for the key that can decrypt the cipher to identify the tag. Due to the large-scale deployment of today's RFID systems, the key search scheme for any PPA requires a short response time. Previous designs construct balance-tree based key management structures to accelerate the search speed to 0(logN), where N is the number of tags. Being efficient, such approaches are vulnerable to compromising attacks. By capturing a small number of tags, compromising attackers are able to identify other tags that have not been corrupted. To address this issue, we propose an Anti- Compromising authenticaTION protocol, ACTION, which employs a novel sparse tree architecture, such that the key of every tag is independent from one another. The advantages of this design include: 1) resilience to the compromising attack, 2) reduction of key storage for tags from 0(logN) to 0(1), which is significant for resource critical tag devices, and 3) high search efficiency, which is 0(logN), as good as the best in the previous designs.},
keywords={cryptography;data privacy;message authentication;radiofrequency identification;trees (mathematics);ACTION;privacy barrier;RFID systems;privacy protection;radio frequency identification systems;privacy-preserving authentication;dominated tags;private information;RF tag;encrypted message;short response time;balance-tree based key management structures;Anti- Compromising authenticaTION protocol;sparse tree architecture;Privacy;Radiofrequency identification;Authentication;Protection;Radio frequency;Cryptography;Large-scale systems;Delay;Acceleration;Protocols},
doi={10.1109/INFCOM.2009.5062117},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062118,
author={J. T. Chiang and J. J. Haas and Y. C. Hu and P. R. Kumar and J. Choi},
booktitle={IEEE INFOCOM 2009},
title={Fundamental Limits on Secure Clock Synchronization and Man-In-The-Middle Detection in Fixed Wireless Networks},
year={2009},
volume={},
number={},
pages={1962-1970},
abstract={In this paper we present fundamental results on secure clock synchronization and man-in-the-middle detection using only timing information. Under the assumption of afflne clocks, we present a clock synchronization protocol that can operate on any channel on which data can be sent. We present a clock synchronization protocol from the literature and add verification steps on top of this protocol. These verification steps force man- in-the-middle attackers, who want to delay traffic between the endpoints and yet remain undetected, to impose only constant delays on packets. In a special case, we show that it is possible to identify and ignore attacker-delayed packets. We then show three different types of attackers: a half-duplex attacker that can always be caught using timing information alone, a double full-duplex attacker that can never be caught using only timing information, and a full-duplex attacker whose capability to perform man-in-the- middle attacks depends on its location relative to the endpoints and on the turnaround times of the endpoints. In particular, we prove that certain attackers are impossible to detect using only timing, and we construct defensive protocols that prevent all other man-in- the-middle delay attacks. A particularly noteworthy result is that a single attacker using the same radio technology as the endpoints can never successfully perform a man-in-the-middle attack to delay traffic. These results form a lightweight man-in-the-middle attack detection protocol, on top of which a wide variety of protocols can be built, including routing protocols and more sophisticated heavyweight protocols.},
keywords={radio networks;routing protocols;synchronisation;telecommunication security;secure clock synchronization;fixed wireless networks;clock synchronization protocol;packet delays;half-duplex attacker;double full-duplex attacker;timing information;endpoint turnaround times;defensive protocols;radio technology;man-in-the-middle attack detection protocol;routing protocols;Clocks;Synchronization;Wireless networks;Delay;Wireless sensor networks;Timing;Routing protocols;Wireless mesh networks;Job shop scheduling;Wiring},
doi={10.1109/INFCOM.2009.5062118},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062119,
author={Y. Bejerano and P. V. Koppol},
booktitle={IEEE INFOCOM 2009},
title={Improving Zap Response Time for IPTV},
year={2009},
volume={},
number={},
pages={1971-1979},
abstract={Channel zapping is the act of changing from one television channel to another. Zap response time is the time it takes for the new TV channel to start playing from the time a request to zap to that channel occurs. In digital TV systems such as IPTV where TV channel content is transported as an MPEG-2 or MPEG-4 data stream encapsulated in IP packets, zap response time is a significant concern. This is due to the requirement that for an MPEG stream, which is a series of groups of pictures (GOPs), playout can only start at the beginning of a GOP. The time between the occurrence of a zap event to the time to the beginning of the next GOP, which can be anywhere from less than a second to a few seconds, is a significant contributor to zap response time. It is widely accepted that such large zap response times can significantly hamper IPTV quality of experience (QoE) and an efficient solution to this problem is therefore a business imperative. Existing solutions to this problem are bandwidth inefficient either on the last mile, or within the video distribution network, or both. We present a novel solution that is significantly efficient in terms of bandwidth utilization and zap response time. We derive and present the theoretical bounds for our solution and quantitatively demonstrate the properties of our solution through results from extensive simulations.},
keywords={IPTV;video streaming;zap response time;IPTV;TV channel content;channel zapping;television channel;digital TV systems;MPEG-2;MPEG-4 data stream encapsulation;IP packets;groups of pictures;quality of experience;video distribution network;bandwidth utilization;IPTV;US Department of Transportation;Bandwidth;Streaming media;Digital TV;Delay effects;Communications Society;USA Councils;MPEG 4 Standard;Time factors},
doi={10.1109/INFCOM.2009.5062119},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062120,
author={T. W. Cho and M. Rabinovich and K. K. Ramakrishnan and D. Srivastava and Y. Zhang},
booktitle={IEEE INFOCOM 2009},
title={Enabling Content Dissemination Using Efficient and Scalable Multicast},
year={2009},
volume={},
number={},
pages={1980-1988},
abstract={Multicast is an approach that uses network and server resources efficiently to distribute information to groups. As networks evolve to become information-centric, users will increasingly demand publish-subscribe based access to fine-grained information, and multicast will need to evolve to (i) manage an increasing number of groups, with a distinct group for each piece of distributable content; (ii) support persistent group membership, as group activity can vary over time, with intense activity at some times, and infrequent (but still critical) activity at others. These requirements raise scalability challenges that are not met by today's multicast techniques. In this paper, we propose the MAD (multicast with adaptive dual-state) architecture to provide efficient multicast service at massive scale. MAD can scalably support a vast number of multicast groups, with varying activity over time, based on two key novel ideas: (i) decouple group membership from forwarding information, and (ii) apply an adaptive dual-state approach to optimize for the different objectives of active and inactive groups. We focus on the scalability characteristics of MAD and demonstrate through analysis, simulation and implementation that the architecture achieves high performance and efficiency.},
keywords={IP networks;multicast communication;content dissemination;publish-subscribe;scalability challenges;multicast with adaptive dual-state architecture;IP multicast;Subscriptions;Publishing;Network servers;Content management;Scalability;YouTube;Web and internet services;Feeds;Communications Society;Publish-subscribe},
doi={10.1109/INFCOM.2009.5062120},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062121,
author={V. Khare and B. Zhang},
booktitle={IEEE INFOCOM 2009},
title={Towards Economically Viable Infrastructure-Based Overlay Multicast Networks},
year={2009},
volume={},
number={},
pages={1989-1997},
abstract={Internet-scale dissemination of streaming contents (e.g., live sports games) can be achieved by infrastructure-based overlay multicast networks, where multicast service providers deliver the contents via dedicated servers strategically placed over the Internet. Given the huge amount of data traffic, one of the major operation costs is the ISP cost for network access. However, existing overlay multicast protocols only consider network performance metrics in building dissemination trees without taking into account the potentially high ISP cost they may incur. This paper presents a scheme, revenue-driven overlay multicast networks (ROMaN), to assign users to different servers in order to maximize the profit derived from providing multicast services. ROMaN exploits the fact that ISP charging functions are concave by assigning users to the cheapest available servers, and dynamically adjusts the assignment to accommodate the churns of group membership. The evaluation shows that ROMaN not only can reduce ISP cost substantially, but also has shorter end-to-end delay due to smaller overlay size, and the longer a user stays in the group the better the service it will receive.},
keywords={Internet;multicast protocols;network servers;telecommunication traffic;infrastructure-based overlay multicast network;Internet-scale dissemination;streaming content;multicast service provider;data traffic;ISP cost;network access;overlay multicast protocol;network performance metrics;dissemination trees;revenue-driven overlay multicast network;server;end-to-end delay;Costs;Network servers;Web server;Web and internet services;Delay;IP networks;Telecommunication traffic;Measurement;Streaming media;Business},
doi={10.1109/INFCOM.2009.5062121},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062122,
author={B. Xing and S. Mehrotra and N. Venkatasubramanian},
booktitle={IEEE INFOCOM 2009},
title={RADcast: Enabling Reliability Guarantees for Content Dissemination in Ad Hoc Networks},
year={2009},
volume={},
number={},
pages={1998-2006},
abstract={This paper deals with the problem of reliable and fast broadcast of mission-critical data with rich content over ad hoc networks. Existing approaches to dissemination reliability often assume network size knowledge, or that receivers know about the dissemination in advance. Without making similar assumptions, we propose a distinct approach which accommodates the varying reliability needs of applications. We develop the RADcast (reliable application data broadcast) protocol as an integration of two components: (a) Peddler, which ensures that receivers obtain the dissemination metadata, and (b) Pryer, which delivers the actual data to dissemination-aware receivers. We indicate how reliability guarantees/performance tradeoffs can be achieved by a careful instantiation of Peddler and Pryer. We implement RADcast on mobile devices inside a middleware and determine its feasibility. Furthermore, through extensive simulations, we show that RADcast achieves desired reliability in all cases, and performs consistently under varying network conditions and device mobilities. As compared to existing approaches, RADcast either incurs significantly lower latency/message overhead, or reduces latency by 50% with a tradeoff in message overhead.},
keywords={ad hoc networks;meta data;mobile radio;protocols;telecommunication network reliability;reliability guarantees;content dissemination;ad hoc networks;mission-critical data reliability;dissemination reliability;reliable application data broadcast protocols;dissemination metadata;dissemination-aware receivers;Peddler;Pryer;mobile devices;message overhead;Ad hoc networks;Broadcasting;Protocols;Computer network reliability;Telecommunication network reliability;Delay;Relays;Mission critical systems;Middleware;Fires},
doi={10.1109/INFCOM.2009.5062122},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062123,
author={A. Wierman and L. L. H. Andrew and A. Tang},
booktitle={IEEE INFOCOM 2009},
title={Power-Aware Speed Scaling in Processor Sharing Systems},
year={2009},
volume={},
number={},
pages={2007-2015},
abstract={Energy use of computer communication systems has quickly become a vital design consideration. One effective method for reducing energy consumption is dynamic speed scaling, which adapts the processing speed to the current load. This paper studies how to optimally scale speed to balance mean response time and mean energy consumption under processor sharing scheduling. Both bounds and asymptotics for the optimal speed scaling scheme are provided. These results show that a simple scheme that halts when the system is idle and uses a static rate while the system is busy provides nearly the same performance as the optimal dynamic speed scaling. However, the results also highlight that dynamic speed scaling provides at least one key benefit - significantly improved robustness to bursty traffic and mis-estimation of workload parameters.},
keywords={energy consumption;power aware computing;processor scheduling;telecommunication traffic;power-aware speed scaling;processor sharing systems;computer communication systems;dynamic speed scaling;processing speed;mean response time;mean energy consumption;processor sharing scheduling;optimal speed scaling scheme;bursty traffic;Delay;Energy consumption;Stochastic processes;Processor scheduling;USA Councils;Internet;Energy management;Power system management;Switches;Measurement},
doi={10.1109/INFCOM.2009.5062123},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062124,
author={L. Mastroleon},
booktitle={IEEE INFOCOM 2009},
title={Admissible Traces, Stability and Rate Management of Queueing / Switching Service Structures},
year={2009},
volume={},
number={},
pages={2016-2024},
abstract={This paper studies the slotted time Queueing / Switching Service Structure (QSSS) problem, where service configurations are selected dynamically in response to queue workload backlogs, so as to maintain stability (inflow-outflow balance) when possible. The only assumption on the traffic traces is that the incoming workload in each slot is bounded by some global (per trace), finite, burst ceiling. First, a key condition is identified, based on which each traffic trace can be classified as either admissible or non-admissible. Non-admissible traces drive the QSSS unstable, irrespective of the service configuration algorithm. Under admissible traces, the QSSS will remain stable when a Stabilizing Scheduling Algorithm (SSA) is utilized to control the service configuration. It is shown that Maximum-Weight-Matching (MWM) and Projective-Cone- Scheduling (PCS) belong to the SSA family. In addition, useful, theoretical tools are developed to facilitate trace classification. Second, the impact of rate management (i.e. power management) on stability is theoretically explored. In particular, under mild assumptions, it is proven that 'low workload backlog' decisions have little effect on the stability of a QSSS, provided that above a certain backlog threshold SSA-based service configuration controls are used.},
keywords={quality of service;queueing theory;telecommunication network management;telecommunication traffic;admissible traces;rate management;queueing service structures;slotted time Queueing;switching service structure;service configurations;queue workload backlogs;traffic traces;maximum-weight-matching;projective-cone-scheduling;Stability;Switches;Throughput;Packet switching;Personal communication networks;Scheduling algorithm;Traffic control;Energy management;Communications Society;Engineering management},
doi={10.1109/INFCOM.2009.5062124},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062125,
author={M. Kurant},
booktitle={IEEE INFOCOM 2009},
title={Exploiting the Path Propagation Time Differences in Multipath Transmission with FEC},
year={2009},
volume={},
number={},
pages={2025-2033},
abstract={We consider a transmission of a delay-sensitive data stream from a single source to a single destination. The reliability of this transmission may suffer from bursty packet losses - the predominant type of failures in today's Internet. An effective and well studied solution to this problem is to protect the data by a forward error correction (FEC) code and send the FEC packets over multiple paths. In this paper we show that the performance of such a multipath FEC scheme can often be further improved. Our key observation is that the propagation times on the available paths often significantly differ, usually by 10-100ms. We propose to exploit these differences by appropriate packet scheduling that we call 'Spread'. We evaluate our solution with a precise, analytical formulation and trace-driven simulations. Our studies show that Spread substantially outperforms the state-of-the-art solutions. It typically achieves two- to five-fold improvement (reduction) in the effective loss rate. Or conversely, keeping the same level of effective loss rate, Spread significantly decreases the observed delays and helps fighting the delay jitter.},
keywords={forward error correction;Internet;jitter;scheduling;telecommunication network reliability;path propagation time difference;multipath transmission;delay-sensitive data stream transmission;reliability;bursty packet loss;Internet;forward error correction;FEC code;FEC packet;Spread packet scheduling;delay jitter;Forward error correction;Internet;Relays;Delay effects;Redundancy;Extraterrestrial measurements;Communications Society;Computer science;Propagation delay;Propagation losses},
doi={10.1109/INFCOM.2009.5062125},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062126,
author={R. Bhatia and G. Narlikar and I. Rimac and A. Beck},
booktitle={IEEE INFOCOM 2009},
title={UNAP: User-Centric Network-Aware Push for Mobile Content Delivery},
year={2009},
volume={},
number={},
pages={2034-2042},
abstract={The consumer interest in mobile multimedia content is on the rise, driven by the higher bandwidth of 3G networks and by the availability of low-cost high-resolution mobile devices. However, providing a good user experience remains a challenge due to bandwidth bottlenecks at peak time, channel quality variations and high battery drain incurred by long data transmission times at lower bandwidths. Consequently high jitter, buffering delays and frequent network outages are ever so common for mobile multimedia services. The emerging mobile broadcast networks (e.g. BCMCS, MediaFLO, DVB-H) are well suited for efficient delivery of highly popular content but lack the on-demand, interactive and retransmission (for reliability) capabilities by virtue of being one-way. In addition, due to business reasons (e.g. MediaFLO) or technical reasons (e.g. BCMCS) service providers prefer to deliver only a limited number of popular channels over these networks. In this paper we propose a mobile content delivery architecture that takes wireless specifics into account to enhance the user experience with multimedia services. Our solution makes efficient use of the available bandwidth, does network and channel quality- aware content delivery on the unicast 3G network while at the same time efficiently and reliably schedules content delivery over the mobile broadcast network. In our solution the delivered content is pre-cached on the storage available on the mobile device. This provides a better user experience, reduces the peak load on the network, and reduces the battery drain on the mobile device. Motivated by the proposed architecture we study the problem of scheduling content over a hybrid unicast and broadcast mobile network and design efficient algorithms and heuristics for the problem.},
keywords={3G mobile communication;data communication;multimedia communication;wireless channels;user-centric network-aware push;mobile content delivery;3G networks;low-cost high-resolution mobile devices;bandwidth bottlenecks;data transmission;mobile broadcast networks;multimedia services;channel quality- aware content delivery;mobile broadcast network;hybrid unicast network;Bandwidth;Batteries;Multimedia communication;Unicast;Multimedia systems;Availability;Data communication;Jitter;Digital video broadcasting;Algorithm design and analysis},
doi={10.1109/INFCOM.2009.5062126},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062127,
author={S. -. Han and T. Nandagopal and Y. Bejerano and H. -. Choi},
booktitle={IEEE INFOCOM 2009},
title={Analysis of Spatial Unfairness in Wireless LANs},
year={2009},
volume={},
number={},
pages={2043-2051},
abstract={Physical layer capture is one of the basic causes of throughput unfairness in IEEE 802.11 Wireless LANs. While papers have analyzed the impact of capture on the overall throughput of a single 802.11 cell, we are unaware of any analysis of the relative unfairness among users as result of capture. Since this unfairness is related to the relative location of users, we call this as spatial unfairness. We provide, to the best of our knowledge, the first such analysis that characterizes the relative throughput of users as a function of the users' distance from the access point (AP). We experimentally validate our analysis using a single 802.11b cell testbed.},
keywords={computer network management;IEEE standards;wireless LAN;spatial unfairness analysis;wireless LAN;physical layer capture effect;IEEE 802.11;spatial unfairness;Wireless LAN;Throughput;Physical layer;Degradation;Protocols;Telecommunication traffic;Communications Society;Testing;USA Councils;Multiaccess communication},
doi={10.1109/INFCOM.2009.5062127},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062128,
author={J. Shi and E. Aryafar and T. Salonidis and E. W. Knightly},
booktitle={IEEE INFOCOM 2009},
title={Synchronized CSMA Contention: Model, Implementation and Evaluation},
year={2009},
volume={},
number={},
pages={2052-2060},
abstract={A class of CSMA protocols used in a broad range of wireless applications uses synchronized contention where nodes periodically contend at intervals of fixed duration. While several models exist for asynchronous CSMA contention used in protocols like IEEE 802.11 MAC, no model exists for synchronized CSMA contention that also incorporates realistic factors like clock drifts. In this paper, we introduce a model that quantifies the interplay of clock drifts with contention window size, control packet size, and carrier sense regulated by usage of guard time. Using an FPGA-based MAC protocol implementation and controlled experiments on a wireless testbed we evaluate the model predictions on the isolated and combined impact of these key performance factors to per-flow throughput and fairness properties in both single-hop and multi-hop networks. Our model and experimental evaluation reveal conditions on protocol parameters under which the throughput of certain flows can exponentially decrease; while at the same time, it enables solutions that can offset such problems in a predictable manner.},
keywords={carrier sense multiple access;radio networks;CSMA protocols;wireless applications;synchronized contention;FPGA;MAC protocol;single-hop networks;multihop networks;Multiaccess communication;Synchronization;Wireless application protocol;Media Access Protocol;Clocks;Predictive models;Throughput;Size control;Wireless sensor networks;Testing},
doi={10.1109/INFCOM.2009.5062128},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062129,
author={J. Naor and D. Raz and G. Scalosub},
booktitle={IEEE INFOCOM 2009},
title={Toward Optimal Utilization of Shared Random Access Channels},
year={2009},
volume={},
number={},
pages={2061-2069},
abstract={We consider a multipacket reception channel shared by several communication applications. This is the case, for example, in a single radio mesh network where neighboring cells use the same radio channel. In such scenarios, unlike the common multiple access model, several transmissions may succeed simultaneously, depending on the actual locations of the sending and receiving stations, and thus channel utilization may be greater than 1. Our goal is to derive a decentralized access control mechanism that maximizes the channel utilization, while taking into account fairness among the different users. We focus on a simple case where each user can adjust a single parameter that determines its transmission probability in any time slot, and develop such a protocol for the general problem, where users are distributed arbitrarily, based on strong motivation which is derived from analytical bounds for homogeneous interferences. We further show, using extensive simulations, that this protocol achieves a high utilization of radio resources compared to any other protocol (not necessarily based on a simple parameter), while maintaining fairness between all users.},
keywords={access protocols;interference (signal);packet radio networks;probability;radio reception;subscriber loops;wireless channels;shared random access channels;multipacket reception channel;communication applications;single radio mesh network;neighboring cells;radio channel;channel utilization;decentralized access control mechanism;transmission probability;protocol;homogeneous interferences;radio resources;Computer science;Interference;Access protocols;Media Access Protocol;Multiaccess communication;Communications Society;Application software;Mesh networks;Access control;Wireless networks},
doi={10.1109/INFCOM.2009.5062129},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062130,
author={Y. Cheng and X. Ling and W. Zhuang},
booktitle={IEEE INFOCOM 2009},
title={A Protocol-Independent Approach for Analyzing the Optimal Operation Point of CSMA/CA Protocols},
year={2009},
volume={},
number={},
pages={2070-2078},
abstract={This paper presents a protocol-independent approach to reveal a new insight into the performance of carrier sense multiple access with collision avoidance (CSMA/CA) protocols: the family of CSMA/CA protocols, independent of implementation details, share the same optimal operation point where the maximum protocol capacity is achieved. The protocol- independent analysis is inspired by the concept of virtual time slot. At the timescale of virtual-slot, all the CSMA/CA protocols show the same behavior pattern and, therefore, a generic virtual- slot based S-G (VS S-G) analysis is developed to compute the optimal operation point. The accuracy of the VS S-G analysis is benchmarked against the precise protocol-specific analysis, in particular, for the 802.11 distributed coordination function (DCF) and the 802.15.4 contention access period (CAP). Furthermore, this paper discusses how to integrate the network-layer queueing analysis with the VS S-G analysis at the medium access control (MAC) layer to form a generic cross-layer framework for call- level network capacity analysis.},
keywords={access protocols;radio networks;telecommunication congestion control;CSMA protocols;carrier sense multiple access protocols;collision avoidance protocols;protocol-independent analysis;virtual time slot;protocol-specific analysis;distributed coordination function;contention access period;network-layer queueing analysis;medium access control layer;call-level network capacity analysis;Multiaccess communication;Media Access Protocol;Access protocols;Wireless application protocol;Performance analysis;Collision avoidance;Wireless personal area networks;Wireless sensor networks;Wireless LAN;Wireless mesh networks},
doi={10.1109/INFCOM.2009.5062130},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062131,
author={C. Milling and S. Subramanian and S. Shakkottai and R. Berry},
booktitle={IEEE INFOCOM 2009},
title={Routing Over Multi-Hop Wireless Networks with Non-Ergodic Mobility},
year={2009},
volume={},
number={},
pages={2079-2087},
abstract={Routing to mobile nodes in a wireless network is conventionally performed by associating a static IP address (or a geographic location) to each node, and routing to that address using routing tables at intermediate nodes that are updated periodically to reflect mobility-induced network topology changes. This mode of routing works when the mobiles' speeds as well as the number of mobiles are small. However, in the presence of large number of fast-moving mobiles, such approaches are infeasible and can lead to excessive overheads, routing failures and hence, throughput loss. In this paper, we consider a wireless network over a domain with a collection of static nodes (that form a connected cover of the domain) and mobile nodes, where the mobile nodes can move in an arbitrary (non-ergodic) manner over sub-domains of the network. For such a system, we develop new routing algorithms (based on a spatial multi-resolution search) that we show are efficient both in terms of routing overheads and throughput. In particular, we show that the achievable rate region of the proposed algorithm is within a poly-logarithmic constant of the optimal rate region with non-ergodic mobility.},
keywords={mobile radio;radio networks;telecommunication network routing;telecommunication network topology;telecommunication network routing;telecommunication network topology;multihop wireless network;nonergodic mobility;static IP address;spatial multiresolution search;poly-logarithmic constant;Routing;Spread spectrum communication;Wireless networks;Peer to peer computing;Throughput;Algorithm design and analysis;Costs;Communications Society;Milling;Network topology},
doi={10.1109/INFCOM.2009.5062131},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062132,
author={A. D. Sarwate and A. G. Dimakis},
booktitle={IEEE INFOCOM 2009},
title={The Impact of Mobility on Gossip Algorithms},
year={2009},
volume={},
number={},
pages={2088-2096},
abstract={We analyze how node mobility can influence the convergence time of averaging gossip algorithms on networks. Our main result is that even a small number of fully mobile nodes can yield a significant decrease in convergence time. We develop a method for deriving lower bounds on the convergence time by merging nodes according to their mobility pattern. We use this method to show that if the agents have one-dimensional mobility in the same direction the convergence time is improved by at most a constant. We also obtain upper bounds on the convergence time using techniques from the theory of Markov chains and show that simple models of mobility can dramatically accelerate gossip as long as the mobility paths significantly overlap. We use simulations to show that our bounds are still valid for more general mobility models that seem analytically intractable, and further illustrate that different mobility patterns can have significantly different effects on the convergence of distributed algorithms.},
keywords={Markov processes;mobile radio;gossip algorithms;mobile nodes;convergence time;Markov chains;distributed algorithms;Convergence;Wireless sensor networks;Algorithm design and analysis;Peer to peer computing;Merging;Upper bound;Acceleration;Robustness;Communication standards;Floods},
doi={10.1109/INFCOM.2009.5062132},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062133,
author={B. McCarthy and C. Edwards and M. Dunmore},
booktitle={IEEE INFOCOM 2009},
title={Using NEMO to Support the Global Reachability of MANET Nodes},
year={2009},
volume={},
number={},
pages={2097-2105},
abstract={Mobile ad hoc network (MANET) routing protocols have been the focus of an accomplished research effort for many years within the networking community and now the results of this effort are beginning to show. With protocol development maturing (and now typically concentrating on a smaller number of standardised routing protocols), increasing numbers of deployment successes are materialising. However, despite these successes and the relative stability of the protocol implementations, seamlessly incorporating MANETs into the Internet still presents many challenges that have hindered their deployment in important mobile scenarios. In this paper we discuss the inherent properties that have affected the adoption of MANET solutions and present an innovative new protocol which has been designed to comprehensively address these challenges. Using performance results acquired from our experimental testbed, we demonstrate how our approach can be used to produce MANET solutions that are highly suited to use in synergy with the current Internet architecture. Our protocol is based on the concept of integrating MANET routing protocols with network mobility (NEMO) technologies to produce what is termed a MANEMO solution. This has meant that by utilising the properties of both of these technologies we have been able to realise a solution that provides mobile networks with the efficient localised communication and robustness of MANETs, as well as the global reachability and the ability to provide structured AAA that a NEMO approach can support.},
keywords={ad hoc networks;Internet;mobile computing;mobility management (mobile radio);reachability analysis;routing protocols;global reachability;MANET;mobile ad hoc network;routing protocols;Internet;network mobility technology;MANEMO;Mobile ad hoc networks;Routing protocols;Internet;Peer to peer computing;Testing;Communications Society;Computer networks;Topology;Access protocols;Proposals},
doi={10.1109/INFCOM.2009.5062133},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062134,
author={A. Mei and J. Stefa},
booktitle={IEEE INFOCOM 2009},
title={SWIM: A Simple Model to Generate Small Mobile Worlds},
year={2009},
volume={},
number={},
pages={2106-2113},
abstract={This paper presents small world in motion (SWIM), a new mobility model for ad-hoc networking. SWIM is relatively simple, is easily tuned by setting just a few parameters, and generates traces that look real-synthetic traces have the same statistical properties of real traces. SWIM shows experimentally and theoretically the presence of the power law and exponential decay dichotomy of inter-contact time, and, most importantly, our experiments show that it can predict very accurately the performance of forwarding protocols.},
keywords={ad hoc networks;mobile radio;protocols;statistical analysis;small world in motion;ad-hoc networks;statistical properties;power law-exponential decay dichotomy;forwarding protocols;Protocols;Humans;Peer to peer computing;Mobile computing;Power generation;Communications Society;Computer science;Computational modeling;Computer simulation;Probability distribution},
doi={10.1109/INFCOM.2009.5062134},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062135,
author={W. Saad and Z. Han and M. Debbah and A. Hjorungnes and T. Basar},
booktitle={IEEE INFOCOM 2009},
title={Coalitional Games for Distributed Collaborative Spectrum Sensing in Cognitive Radio Networks},
year={2009},
volume={},
number={},
pages={2114-2122},
abstract={Collaborative spectrum sensing among secondary users (SUs) in cognitive networks is shown to yield a significant performance improvement. However, there exists an inherent trade off between the gains in terms of probability of detection of the primary user (PU) and the costs in terms of false alarm probability. In this paper, we study the impact of this trade off on the topology and the dynamics of a network of SUs seeking to reduce the interference on the PU through collaborative sensing. Moreover, while existing literature mainly focused on centralized solutions for collaborative sensing, we propose distributed collaboration strategies through game theory. We model the problem as a non-transferable coalitional game, and propose a distributed algorithm for coalition formation through simple merge and split rules. Through the proposed algorithm, SUs can autonomously collaborate and self-organize into disjoint independent coalitions, while maximizing their detection probability taking into account the cooperation costs (in terms of false alarm). We study the stability of the resulting network structure, and show that a maximum number of SUs per formed coalition exists for the proposed utility model. Simulation results show that the proposed algorithm allows a reduction of up to 86.6% of the average missing probability per SU (probability of missing the detection of the PU) relative to the non-cooperative case, while maintaining a certain false alarm level. In addition, through simulations, we compare the performance of the proposed distributed solution with respect to an optimal centralized solution that minimizes the average missing probability per SU. Finally, the results also show how the proposed algorithm autonomously adapts the network topology to environmental changes such as mobility.},
keywords={cognitive radio;distributed algorithms;game theory;probability;telecommunication network topology;coalitional games;distributed collaborative spectrum sensing;cognitive radio networks;false alarm probability;network topology;collaborative sensing;distributed collaboration strategy;game theory;distributed algorithm;coalition formation;independent coalitions;detection probability;average missing probability;Collaboration;Cognitive radio;Detectors;Costs;Network topology;Interference;USA Councils;Distributed algorithms;Communications Society;Computer networks},
doi={10.1109/INFCOM.2009.5062135},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062136,
author={A. Aram and C. Singh and S. Sarkar and A. Kumar},
booktitle={IEEE INFOCOM 2009},
title={Cooperative Profit Sharing in Coalition Based Resource Allocation in Wireless Networks},
year={2009},
volume={},
number={},
pages={2123-2131},
abstract={We consider a network in which several service providers offer wireless access service to their respective subscribed customers through potentially multi-hop routes. If providers cooperate, i.e., pool their resources, such as spectrum and base stations, and agree to serve each others' customers, their aggregate payoffs, and individual shares, can potentially substantially increase through efficient utilization of resources and statistical multiplexing. The potential of such cooperation can however be realized only if each provider intelligently determines who it would cooperate with, when it would cooperate, and how it would share its resources during such cooperation. Also, when the providers share their aggregate revenues, developing a rational basis for such sharing is imperative for the stability of the coalitions. We model such cooperation using transferable payoff coalitional game theory. We first consider the scenario that locations of the base stations and the channels that each provider can use have already been decided apriori. We show that the optimum cooperation strategy, which involves the allocations of the channels and the base stations to mobile customers, can be obtained as solutions of convex optimizations. We next show that the grand coalition is stable in this case, i.e. if all providers cooperate, there is always an operating point that maximizes the providers' aggregate payoff, while offering each such a share that removes any incentive to split from the coalition. Next, we show that when the providers can choose the locations of their base stations and decide which channels to acquire, the above results hold in important special cases. Finally, we examine cooperation when providers do not share their payoffs, but still share their resources so as to enhance individual payoffs. We show that the grand coalition continues to be stable.},
keywords={channel allocation;game theory;radio networks;resource allocation;cooperative profit sharing;wireless networks resource allocation;wireless access service;multihop routes;coalition stability;game theory;channel allocations;mobile customers;convex optimizations;Resource management;Wireless networks;Base stations;Throughput;Licenses;Costs;Aggregates;Relays;Energy consumption;Spread spectrum communication},
doi={10.1109/INFCOM.2009.5062136},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062137,
author={T. C. Aysal and K. E. Barner},
booktitle={IEEE INFOCOM 2009},
title={On the Convergence of Perturbed Non-Stationary Consensus Algorithms},
year={2009},
volume={},
number={},
pages={2132-2140},
abstract={We consider consensus algorithms in their most general setting and provide conditions under which such algorithms are guaranteed to converge, almost surely, to a consensus. Let {A(t),B(t)} isin R<sup>N</sup> <sup>times</sup> <sup>N</sup> be (possibly) random, non-stationary matrices and {x(t),m(t)} isin R<sup>N</sup> <sup>times</sup> <sup>1</sup> be state and perturbation vectors, respectively. For any consensus algorithm of the form x(t + 1) = A(t)x(t) + B(t)m(t), we provide conditions under which consensus is achieved almost surely, i.e., Pr {lim<sub>trarrinfin</sub> x(t) = c1} = 1 for some c isin R. Moreover, we show that this general result subsumes recently reported results for specific consensus algorithms classes, including sum-preserving, non-sum-preserving, quantized and noisy gossip algorithms. Also provided are the e-converging time for any such converging iterative algorithm, i.e., the earliest time at which the vector x(t) is e close to consensus, and sufficient conditions for convergence in expectation to the initial node measurements average.},
keywords={iterative methods;matrix algebra;randomised algorithms;telecommunication network routing;vectors;perturbed nonstationary consensus algorithm convergence;random nonstationary matrix;perturbation vector;iterative algorithm;telecommunication network routing;sum-preserving gossip algorithm;nonsum-preserving gossip algorithm;randomized algorithm;Convergence;Signal processing algorithms;Iterative algorithms;Peer to peer computing;Computational modeling;Communications Society;Sufficient conditions;Time measurement;Ad hoc networks;Application software},
doi={10.1109/INFCOM.2009.5062137},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062138,
author={H. Xu and B. Li},
booktitle={IEEE INFOCOM 2009},
title={XOR-Assisted Cooperative Diversity in OFDMA Wireless Networks: Optimization Framework and Approximation Algorithms},
year={2009},
volume={},
number={},
pages={2141-2149},
abstract={Network coding has been leveraged with cooperative diversity to improve performance in single channel wireless networks. However, it is not clear how network coding based cooperative diversity can be exploited effectively in multi-channel networks where overhearing is not readily available. Moreover, the question of how to practically realize the promising gains available, including multi-user diversity, cooperative diversity and network coding in multi-channel networks, also remains unexplored. This work represents the first attempt to unravel these two questions. In this paper, we propose XOR-CD, a novel XOR-assisted cooperative diversity scheme in OFDMA wireless networks. It can greatly improve the relay efficiency by over 100% mostly, thus uplifting the throughput performance by over 30% compared to conventional cooperative diversity scheme. In addition, we formulate a unifying optimization framework that jointly considers relay assignment, relay strategy selection, channel assignment and power allocation to reap different forms of gains. We design efficient polynomial time algorithms to solve the NP-hard problem with provably the best approximation factor, and verify their effectiveness using realistic simulations.},
keywords={channel allocation;communication complexity;encoding;multiuser channels;OFDM modulation;radio networks;wireless channels;XOR-assisted cooperative diversity;OFDMA wireless network;optimization;approximation algorithm;network coding;multichannel networks;multiuser diversity;relay assignment;relay strategy selection;channel assignment;power allocation;polynomial time algorithm;NP-hard problem;Wireless networks;Approximation algorithms;Network coding;Cultural differences;Throughput;Frame relay;Decoding;Bidirectional control;Telecommunication traffic;Downlink},
doi={10.1109/INFCOM.2009.5062138},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062139,
author={A. Anandkumar and M. Wang and L. Tong and A. Swami},
booktitle={IEEE INFOCOM 2009},
title={Prize-Collecting Data Fusion for Cost-Performance Tradeoff in Distributed Inference},
year={2009},
volume={},
number={},
pages={2150-2158},
abstract={A novel formulation for optimal sensor selection and in-network fusion for distributed inference known as the prize- collecting data fusion (PCDF) is proposed in terms of optimal tradeoff between the costs of aggregating the selected set of sensor measurements and the resulting inference performance at the fusion center. For i.i.d. measurements, PCDF reduces to the prize-collecting Steiner tree (PCST) with the single-letter Kullback-Leibler divergence as the penalty at each node, as the number of nodes goes to infinity. PCDF is then analyzed under a correlation model specified by a Markov random field (MRF) with a given dependency graph. For a special class of dependency graphs, a constrained version of the PCDF reduces to the PCST on an augmented graph. In this case, an approximation algorithm is given with the approximation ratio depending only on the number of profitable cliques in the dependency graph. Based on these results, two heuristics are proposed for node selection under general correlation structure, and their performance is studied via simulations.},
keywords={approximation theory;sensor fusion;trees (mathematics);prize collecting data fusion;cost-performance tradeoff;distributed inference;optimal sensor selection;in-network fusion;prize-collecting data fusion;sensor measurements;prize-collecting Steiner tree;Kullback-Leibler divergence;Markov random field;dependency graph;augmented graph;approximation algorithm;Sensor fusion;Peer to peer computing;Routing;USA Councils;Cost function;H infinity control;Markov random fields;Approximation algorithms;Communications Society;Laboratories},
doi={10.1109/INFCOM.2009.5062139},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062140,
author={B. Yu and J. Li and Y. Li},
booktitle={IEEE INFOCOM 2009},
title={Distributed Data Aggregation Scheduling in Wireless Sensor Networks},
year={2009},
volume={},
number={},
pages={2159-2167},
abstract={Data aggregation is an essential operation in wireless sensor network applications. This paper focuses on the data aggregation scheduling problem. Based on maximal independent sets, a distributed algorithm to generate a collision-free schedule for data aggregation in wireless sensor networks is proposed. The time latency of the aggregation schedule generated by the proposed algorithm is minimized using a greedy strategy. The latency bound of the schedule is 24D + 6 Delta + 16, where D is the network diameter and Delta is the maximum node degree. The previous data aggregation algorithm with least latency has the latency bound (Delta- Delta 1)R, where R is the network radius. Thus in our algorithm Delta contributes to an additive factor instead of a multiplicative factor, which is a significant improvement. To the best of our knowledge, the proposed algorithm is the first distributed algorithm for data aggregation scheduling. This paper also proposes an adaptive strategy for updating the schedule when nodes fail or new nodes join in a network. The analysis and simulation results show that the proposed algorithm outperforms other aggregation scheduling algorithms.},
keywords={wireless sensor networks;distributed data aggregation scheduling;wireless sensor networks;distributed algorithm;multiplicative factor;Wireless sensor networks;Scheduling algorithm;Delay;Processor scheduling;Computer science;Peer to peer computing;Algorithm design and analysis;Energy consumption;Distributed algorithms;Analytical models},
doi={10.1109/INFCOM.2009.5062140},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062141,
author={C. Liu and G. Cao},
booktitle={IEEE INFOCOM 2009},
title={Minimizing the Cost of Mine Selection Via Sensor Networks},
year={2009},
volume={},
number={},
pages={2168-2176},
abstract={In this paper, we study sensor enabled landmine networks by formulating a minimum-cost mine selection problem. The problem arises in a target defence scenario, where the objective is to destroy the intruding targets using the minimum-cost pre-deployed mines. Due to the problem complexity, we first transform it using a novel bucket-tub model, and then propose several approximation algorithms. Among them, it is shown that the layering algorithm can achieve an approximation ratio of alpha ldr f, where alpha ges 1 is the tunable relaxation factor and f is the maximum number of mines that a target is associated with, and that the greedy algorithm has an approximation ratio of Sigma<sub>j</sub> R<sub>j</sub>, where R<sub>j</sub> is the coefficient in the related integer program. We also present a localized greedy algorithm which is shown to produce the same solution set as the global greedy algorithm. Theoretical analysis and extensive simulations demonstrate the effectiveness of the proposed algorithms.},
keywords={approximation theory;computational complexity;defence industry;greedy algorithms;integer programming;landmine detection;relaxation theory;wireless sensor networks;sensor networks;landmine networks;minimum-cost mine selection problem;target defence scenario;bucket-tub model;approximation algorithms;layering algorithm;tunable relaxation factor;integer program;localized greedy algorithm;problem complexity;Costs;Landmine detection;Greedy algorithms;Approximation algorithms;Explosions;Communications Society;Computer science;Electronic mail;Algorithm design and analysis;Analytical models},
doi={10.1109/INFCOM.2009.5062141},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062142,
author={L. Keller and M. Jafari Siavoshani and C. Fragouli and K. Argyraki and S. Diggavi},
booktitle={IEEE INFOCOM 2009},
title={Identity Aware Sensor Networks},
year={2009},
volume={},
number={},
pages={2177-2185},
abstract={In a significant class of sensor-network applications, the identities of the reporting sensors constitute the bulk of the communicated data, whereas the message itself can be as small as a single bit - for instance, in many cases, sensors are used to detect whether and where a certain interesting condition occured, or to track incremental environmental changes at fixed locations. In such scenarios, the traditional network-protocol paradigm of separately specifying the source identity and the message in distinct fields leads to inefficient communication. This work addresses the question of how should communication happen in such identity-aware sensor networks. We reexamine the traditional source-identity/message separation and propose a scheme for jointly encoding the two. We use this to develop a communication method for identity-aware sensor networks and show it to be energy efficient, simple to implement, and gracefully adaptable to scenarios frequently encountered in sensor networks - for instance, node failures, or large numbers of nodes where only few are active during each reporting round.},
keywords={wireless sensor networks;identity aware sensor networks;source-identity-message separation;energy efficient;wireless sensor networks;Pollution measurement;Temperature sensors;Encoding;Protocols;Energy efficiency;Peer to peer computing;Wireless sensor networks;Sensor phenomena and characterization;Temperature measurement;Energy consumption},
doi={10.1109/INFCOM.2009.5062142},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062143,
author={A. El-Atawy and E. Al-Shaer},
booktitle={IEEE INFOCOM 2009},
title={Building Covert Channels over the Packet Reordering Phenomenon},
year={2009},
volume={},
number={},
pages={2186-2194},
abstract={New modes of communication have shown themselves to be needed for more secure and private types of data. Steganography or data-hiding through covert channels can be highly motivated by today's security requirements and various needs of applications. Moreover, the amount of information in the Internet traffic is not bounded by what is contained in packets payload; there is considerable hidden capacity within packets and flows characteristics to build robust and stealthy covert channels. In this paper, we propose using the packet reordering phenomenon as the media to carry a hidden channel. As a naturally occurring behavior of packets traveling the Internet, it can as well be induced to send a signal to the receiving end. Specific permutations are selected to enhance the reliability of the channel, while their distribution was selected to imitate real traffic and increase stealthiness. The robustness of such channel is analyzed, and its bandwidth is calculated. A simple tool is implemented to communicate over the natural phenomenon of packet reordering. Reliability and capacity of the techniques are evaluated and promising results show the potential of the proposed approach.},
keywords={computer network reliability;data encapsulation;Internet;security of data;telecommunication channels;telecommunication security;telecommunication traffic;packet reordering phenomenon;data security;data privacy;data-hiding;Internet traffic;channel reliability;Payloads;Internet;Bandwidth;Steganography;Robustness;Computer hacking;Data security;Information security;Data communication;Transport protocols},
doi={10.1109/INFCOM.2009.5062143},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062144,
author={W. Jia and F. P. Tso and Z. Ling and X. Fu and D. Xuan and W. Yu},
booktitle={IEEE INFOCOM 2009},
title={Blind Detection of Spread Spectrum Flow Watermarks},
year={2009},
volume={},
number={},
pages={2195-2203},
abstract={Recently, the direct sequence spread-spectrum (DSSS)-based technique has been proposed to trace anonymous network flows. In this technique, homogeneous pseudo-noise (PN) codes are used to modulate multiple-bit signals that are embedded into the target flow as watermarks. This technique could be maliciously used to degrade an anonymous communication network. In this paper, we propose a simple single flow-based scheme to detect the existence of these watermarks. Our investigation shows that even if we have no knowledge of the applied PN code, we are still able to detect malicious DSSS watermarks via mean-square autocorrelation (MSAC) of a single modulated flow's traffic rate time series. MSAC shows periodic peaks due to self-similarity in the modulated traffic caused by homogeneous PN codes that are used in modulating multiple-bit signals. Our scheme has low complexity and does not require any PN-code synchronization. We evaluate this detection scheme's effectiveness via simulations and real-world experiments on Tor. Our results demonstrate a high detection rate with a low false positive rate. Our scheme is more flexible and accurate than an existing multi-flow-based approach in DSSS watermark detection.},
keywords={blind source separation;computer networks;correlation methods;mean square error methods;modulation;pseudonoise codes;spread spectrum communication;synchronisation;telecommunication traffic;time series;watermarking;blind detection;spread spectrum flow watermarks;direct sequence spread-spectrum;anonymous network flows;homogeneous pseudo-noise codes;multiple-bit signal modulation;target flow;anonymous communication network;malicious DSSS watermarks;mean-square autocorrelation;traffic rate time series;modulated traffic;PN-code synchronization;Spread spectrum communication;Watermarking;Modulation coding;Traffic control;Communication networks;Autocorrelation;Telecommunication traffic;Forensics;Communications Society;Degradation},
doi={10.1109/INFCOM.2009.5062144},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062145,
author={S. H. Sellke and C. -. Wang and S. Bagchi and N. Shroff},
booktitle={IEEE INFOCOM 2009},
title={TCP/IP Timing Channels: Theory to Implementation},
year={2009},
volume={},
number={},
pages={2204-2212},
abstract={There has been significant recent interest in covert communication using timing channels. In network timing channels, information is leaked by controlling the time between transmissions of consecutive packets. Our work focuses on network timing channels and provides two main contributions. The first is to quantify the threat posed by covert network timing channels. The other is to use timing channels to communicate at a low data rate without being detected. In this paper, we design and implement a covert TCP/IP timing channel. We are able to quantify the achievable data rate (or leak rate) of such a covert channel. Moreover, we show that by sacrificing data rate, the traffic patterns of the covert timing channel can be made computationally indistinguishable from that of normal traffic, which makes detecting such communication virtually impossible. We demonstrate the efficacy of our solution by showing significant performance gains in terms of both data rate and covertness over the state-of-the-art.},
keywords={IP networks;telecommunication channels;transport protocols;covert communication;covert TCP-IP timing channel;traffic patterns;TCPIP;Timing jitter;Books;Communication system security;Data security;Information security;Distributed computing;Communications Society;Intelligent networks;Communication system control},
doi={10.1109/INFCOM.2009.5062145},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062146,
author={Y. Fan and Y. Jiang and H. Zhu and X. Shen},
booktitle={IEEE INFOCOM 2009},
title={An Efficient Privacy-Preserving Scheme against Traffic Analysis Attacks in Network Coding},
year={2009},
volume={},
number={},
pages={2213-2221},
abstract={Privacy threat is one of the critical issues in network coding, where attacks such as traffic analysis can be easily launched by a malicious adversary once enough encoded packets are collected. Furthermore, the encoding/mixing nature of network coding precludes the feasibility of employing the existing privacy-preserving techniques, such as Onion routing, in network coding enabled networks. In this paper, we propose a novel privacy-preserving scheme against traffic analysis in network coding. With homomorphic encryption operation on global encoding vectors (GEVs), the proposed scheme offers two significant privacy-preserving features, packet flow untraceability and message content confidentiality, for efficiently thwarting the traffic analysis attacks. Moreover, the proposed scheme keeps the random coding feature, and each sink can recover the source packets by inverting the GEVs with a very high probability. Theoretical analysis and simulative evaluation demonstrate the validity and efficiency of the proposed scheme.},
keywords={computer networks;cryptography;data privacy;encoding;probability;random codes;telecommunication security;telecommunication traffic;privacy-preserving scheme;traffic analysis attack;network coding;privacy threat;Onion routing;homomorphic encryption;global encoding vector;packet flow untraceability;message content confidentiality;random coding;probability;Telecommunication traffic;Network coding;Privacy;Encoding;Communication system security;Cryptography;Traffic control;Military computing;Streaming media;Military communication},
doi={10.1109/INFCOM.2009.5062146},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062147,
author={D. Hu and S. Mao and J. H. Reed},
booktitle={IEEE INFOCOM 2009},
title={On Video Multicast in Cognitive Radio Networks},
year={2009},
volume={},
number={},
pages={2222-2230},
abstract={We investigate the challenging problem of enabling multicast video service in emerging cognitive radio (CR) networks. We propose a cross-layer optimization approach to multicast video in CR networks. Specifically, we model CR video multicast as an optimization problem, while considering important design factors including scalable video coding, video rate control, spectrum sensing, dynamic spectrum access, modulation, scheduling, retransmission, and primary user protection. The objective is to optimize the overall received video quality as well as achieving proportional fairness among multicast users, while keeping the interference to primary users below a prescribed threshold. Although the problem can be solved using advanced optimization techniques, we propose a sequential fixing algorithm and a greedy algorithm with low complexity and proven optimality gap. Our simulations using MPEG-4 fine grained scalability (FGS) demonstrate the efficacy and superior performance of the proposed approach as compared with an alternative equal allocation scheme.},
keywords={cognitive radio;multicast communication;optimisation;radio networks;video coding;cognitive radio networks;multicast video service;cross-layer optimization;scalable video coding;video rate control;spectrum sensing;dynamic spectrum access;modulation;scheduling;retransmission;user protection;MPEG-4;fine grained scalability;Cognitive radio;Chromium;Design optimization;Video coding;Modulation coding;Dynamic scheduling;Protection;Interference;Multicast algorithms;Greedy algorithms},
doi={10.1109/INFCOM.2009.5062147},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062148,
author={C. -. Hsu and M. Hefeeda},
booktitle={IEEE INFOCOM 2009},
title={Time Slicing in Mobile TV Broadcast Networks with Arbitrary Channel Bit Rates},
year={2009},
volume={},
number={},
pages={2231-2239},
abstract={Mobile TV networks have received significant attention from the industry and academia, as they have already been deployed in several countries and their expected market potential is huge. In such networks, a base station broadcasts TV channels in bursts with bit rates much higher than the encoding bit rates of the videos. This enables mobile receivers to receive a burst of traffic and then turn off their receiving circuit till the next burst to conserve energy. The base station needs to construct a transmission schedule for all bursts of different TV channels. Constructing optimal (in terms of energy saving) transmission schedules has been shown to be an NP-complete problem when the TV channels are encoded at arbitrary bit rates. In this paper, we propose a near-optimal approximation algorithm to solve this problem. We prove the correctness of the proposed algorithm and derive its approximation factor. We also conduct extensive evaluation of our algorithm using real implementation in a mobile TV testbed and simulations. Our experimental and simulation results show that the proposed algorithm: (i) is practical and produces correct burst schedules, (ii) achieves near-optimal energy saving for mobile devices, and (iii) runs efficiently in real time.},
keywords={approximation theory;mobile television;television broadcasting;mobile TV broadcast networks;arbitrary channel bit rates;base station;mobile receivers;energy saving;NP-complete problem;near-optimal approximation algorithm;near-optimal energy saving;Mobile TV;TV broadcasting;Bit rate;Job shop scheduling;Base stations;Approximation algorithms;Videos;Telecommunication traffic;Circuits;NP-complete problem},
doi={10.1109/INFCOM.2009.5062148},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062149,
author={M. Aezladen and R. Cohen and D. Raz},
booktitle={IEEE INFOCOM 2009},
title={Locally vs. Globally Optimized Flow-Based Content Distribution to Mobile Nodes},
year={2009},
volume={},
number={},
pages={2240-2248},
abstract={The paper deals with efficient distribution of timely information to flows of mobile devices. We consider the case where a set of information dissemination devices (IDDs) broadcast a limited amount of information to passing mobile nodes that are moving along well-defined paths. This is the case, for example, in intelligent transportation systems. We develop a novel model that captures the main aspects of the problem, and define a new optimization problem we call MBMAP (maximum benefit message assignment problem). We study the computational complexity of this problem in the global and local cases, and provide new approximation algorithms.},
keywords={computational complexity;information dissemination;mobile radio;flow-based content distribution;information dissemination devices;intelligent transportation systems;optimization problem;maximum benefit message assignment problem;computational complexity;mobile communication;Intelligent transportation systems;Broadcasting;Peer to peer computing;Mobile communication;Driver circuits;Mobile computing;Intelligent sensors;Roads;Communications Society;Computer science},
doi={10.1109/INFCOM.2009.5062149},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062150,
author={S. Gilbert and R. Guerraoui and D. R. Kowalski and C. Newport},
booktitle={IEEE INFOCOM 2009},
title={Interference-Resilient Information Exchange},
year={2009},
volume={},
number={},
pages={2249-2257},
abstract={This paper presents an efficient protocol for reliably exchanging information in a single-hop, multi-channel radio network subject to unpredictable interference. We model the interference by an adversary that can simultaneously disrupt up to t of the C available channels. We assume no shared secret keys or third-party infrastructure. The running time of our protocol depends on the gap between C and t: when the number of channels C = Q,(t<sup>2</sup>), the running time is linear; when only C = t +1 channels are available, the running time is exponential. We prove that exponential-time is unavoidable in the latter case. At the core of our protocol lies a combinatorial function, possibly of independent interest, described for the first time in this paper: the multi-selector. A multi-selector generates a sequence of channel assignments for each device such that every sufficiently large subset of devices is partitioned onto distinct channels by at least one of these assignments.},
keywords={channel allocation;radio networks;radiofrequency interference;interference-resilient information exchange;multichannel radio network;combinatorial function;channel assignment;Interference;Protocols;Jamming;Radio network;Fading;Communications Society;Paper technology;USA Councils;Memory;Fault tolerance},
doi={10.1109/INFCOM.2009.5062150},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062151,
author={S. Chen and K. R. Joshi and M. A. Hiltunen and W. H. Sanders and R. D. Schlichting},
booktitle={IEEE INFOCOM 2009},
title={Link Gradients: Predicting the Impact of Network Latency on Multitier Applications},
year={2009},
volume={},
number={},
pages={2258-2266},
abstract={Geographically dispersed deployments of large and complex multitier enterprise applications introduce many challenges, including those involved in predicting the impact of network latency on end-to-end transaction response times. Here, a measurement-based approach to quantifying this impact using a new metric called the link gradient is presented. A non-intrusive technique for measuring the link gradient in running systems using delay injection and spectral analysis is presented, along with experimental results on PlanetLab that demonstrate that the link gradient can be used to predict end-to-end responsiveness, even in new and untested application configurations.},
keywords={software metrics;Web services;link gradients;network latency;geographically dispersed deployments;complex multitier enterprise;end-to-end transaction response;measurement-based approach;nonintrusive technique;spectral analysis;PlanetLab;Extraterrestrial measurements;Application software;Web server;Transaction databases;USA Councils;Network servers;Communications Society;Laboratories;Delay systems;Spectral analysis},
doi={10.1109/INFCOM.2009.5062151},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062152,
author={H. -. Chen and J. R. Marden and A. Wierman},
booktitle={IEEE INFOCOM 2009},
title={On the Impact of Heterogeneity and Back-End Scheduling in Load Balancing Designs},
year={2009},
volume={},
number={},
pages={2267-2275},
abstract={Load balancing is a common approach for task assignment in distributed architectures. In this paper, we show that the degree of inefficiency in load balancing designs is highly dependent on the scheduling discipline used at each of the back-end servers. Traditionally, the back-end scheduler can be modeled as processor sharing (PS), in which case the degree of inefficiency grows linearly with the number of servers. However, if the back- end scheduler is changed to shortest remaining processing time (SRPT), the degree of inefficiency can be independent of the number of servers, instead depending only on the heterogeneity of the speeds of the servers. Further, switching the back-end scheduler to SRPT can provide significant improvements in the overall mean response time of the system as long as the heterogeneity of the server speeds is small.},
keywords={distributed processing;resource allocation;scheduling;software architecture;task analysis;heterogeneity impact;back-end scheduling;load balancing designs;task assignment;distributed architectures;back-end servers;processor sharing;shortest remaining processing time;Load management;Processor scheduling;Delay;Parallel processing;Information science;Grid computing;Web server;Network servers;Communications Society;Computer science},
doi={10.1109/INFCOM.2009.5062152},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062153,
author={D. Li and C. Guo and H. Wu and K. Tan and Y. Zhang and S. Lu},
booktitle={IEEE INFOCOM 2009},
title={FiConn: Using Backup Port for Server Interconnection in Data Centers},
year={2009},
volume={},
number={},
pages={2276-2285},
abstract={The goal of data center networking is to interconnect a large number of server machines with low equipment cost, high and balanced network capacity, and robustness to link/server faults. It is well understood that, the current practice where servers are connected by a tree hierarchy of network switches cannot meet these requirements (Fares et al., 2008 and Guo et al., 2008). In this paper, we explore a new server-interconnection structure. We observe that the commodity server machines used in today's data centers usually come with two built-in Ethernet ports, one for network connection and the other left for backup purpose. We believe that, if both ports are actively used in network connections, we can build a low-cost interconnection structure without the expensive higher-level large switches. Our new network design, called FiConn, utilizes both ports and only the low-end commodity switches to form a scalable and highly effective structure. Although the server node degree is only two in this structure, we have proven that FiConn is highly scalable to encompass hundreds of thousands of servers with low diameter and high bisection width. The routing mechanism in FiConn balances different levels of links. We have further developed a low-overhead traffic-aware routing mechanism to improve effective link utilization based on dynamic traffic state. Simulation results have demonstrated that the routing mechanisms indeed achieve high networking throughput.},
keywords={local area networks;network servers;telecommunication network routing;telecommunication switching;telecommunication traffic;FiConn;backup port;server interconnection;data center networking;network capacity;link faults;server faults;network switch;commodity server machine;Ethernet;network connection;network design;commodity switch;traffic-aware routing;link utilization;dynamic traffic state;networking throughput;Network servers;Switches;Routing;Traffic control;Costs;Robustness;Telecommunication traffic;Throughput;Companies;Bandwidth},
doi={10.1109/INFCOM.2009.5062153},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062154,
author={H. Jiang and A. Iyengar and E. Nahum and W. Segmuller and A. Tantawi and C. P. Wright},
booktitle={IEEE INFOCOM 2009},
title={Load Balancing for SIP Server Clusters},
year={2009},
volume={},
number={},
pages={2286-2294},
abstract={This paper introduces several novel load balancing algorithms for distributing session initiation protocol (SIP) requests to a cluster of SIP servers. Our load balancer improves both throughput and response time versus a single node, while exposing a single interface to external clients. We present the design, implementation and evaluation of our system using a cluster of Intel x86 machines running Linux. We compare our algorithms with several well-known approaches and present scalability results for up to 10 nodes. Our best algorithm, transaction least-work-left (TLWL), achieves its performance by integrating several features: knowledge of the SIP protocol; dynamic estimates of back-end server load; distinguishing transactions from calls; recognizing variability in call length; and exploiting differences in processing costs for different SIP transactions. By combining these features, our algorithm provides finer-grained load balancing than standard approaches, resulting in throughput improvements of up to 24 percent and response time improvements of up to two orders of magnitude. We present a detailed analysis of occupancy to show how our algorithms significantly reduce response time.},
keywords={mobile radio;network servers;resource allocation;signalling protocols;load balancing;SIP server clusters;session initiation protocol;Intel x86 machines;Linux;scalability;transaction least-work-left;back-end server load;general-purpose signaling protocol;mobile radio;Load management;Protocols;Web server;Clustering algorithms;Delay;Throughput;Peer to peer computing;Costs;Large-scale systems;Communications Society},
doi={10.1109/INFCOM.2009.5062154},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062155,
author={S. Huang and X. Liu and Z. Ding},
booktitle={IEEE INFOCOM 2009},
title={Optimal Sensing-Transmission Structure for Dynamic Spectrum Access},
year={2009},
volume={},
number={},
pages={2295-2303},
abstract={In cognitive wireless networks where secondary users (SUs) opportunistically access spectral white spaces of primary users (PUs), there exists an inherent tradeoff between sensing and transmission due to the competing goals of PU protection and SU access maximization. This paper studies means of sensing-transmission for SUs to better manage the competing goals by defining utility function to reward the SU for successful packet transmissions and to penalize it for colliding with PU. To maximize the SU utility, we present a threshold-based sensing-transmission structure that is optimal under a technical constraint. Both perfect sensing and imperfect sensing are considered, with or without SU acknowledgement of reception. This SU access scheme optimizes SU access efficiency while protecting PU performance. It sets a benchmark and provides insight for the design of sensing-transmission control in cognitive networks such as IEEE 802.22.},
keywords={radio networks;spread spectrum communication;optimal sensing-transmission structure;dynamic spectrum access;cognitive wireless networks;secondary users;primary users;SU access maximization;packet transmissions;IEEE 802.22;sensing-transmission control;Protection;Feedback;Optimal control;White spaces;Costs;Interference;Communications Society;USA Councils;Wireless networks;Cognitive radio},
doi={10.1109/INFCOM.2009.5062155},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062156,
author={J. Jia and J. Zhang and Q. Zhang},
booktitle={IEEE INFOCOM 2009},
title={Cooperative Relay for Cognitive Radio Networks},
year={2009},
volume={},
number={},
pages={2304-2312},
abstract={Cognitive radio has been proposed in recent years to promote the spectrum utilization by exploiting the existence of spectrum holes. The heterogeneity of both spectrum availability and traffic demand in secondary users has brought significant challenge for efficient spectrum allocation in cognitive radio networks. Observing that spectrum resource can be better matched to traffic demand of secondary users with the help of relay node that has rich spectrum resource, in this paper we exploit a new research direction for cognitive radio networks by utilizing cooperative relay to assist the transmission and improve spectrum efficiency. An infrastructure-based secondary network architecture has been proposed to leverage relay-assisted discontiguous OFDM (D-OFDM) for data transmission. In this architecture, relay node will be selected which can bridge the source and the destination using its common channels between those two nodes. With the introduction of cooperative relay, many unique problems should be considered, especially the issue for relay selection and spectrum allocation. We propose a centralized heuristic solution to address the new resource allocation problem. To demonstrate the feasibility and performance of cooperative relay for cognitive radio, a new MAC protocol has been proposed and implemented in a Universal Software Radio Peripheral (USRP)-based testbed. Experimental results show that the throughput of the whole system is greatly increased by exploiting the benefit of cooperative relay.},
keywords={access protocols;cognitive radio;OFDM modulation;radio spectrum management;resource allocation;software radio;cooperative relay;cognitive radio networks;spectrum utilization;spectrum holes;spectrum availability;traffic demand;spectrum allocation;spectrum resource;relay node;spectrum efficiency;infrastructure-based secondary network architecture;relay-assisted discontiguous;OFDM;data transmission;resource allocation problem;MAC protocol;universal software radio peripheral based testbed;Relays;Cognitive radio;Telecommunication traffic;Availability;OFDM;Data communication;Bridges;Resource management;Media Access Protocol;Software radio},
doi={10.1109/INFCOM.2009.5062156},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062157,
author={W. Kim and O. Khan and K. T. Truong and S. -. Choi and R. Grant and H. K. Wright and K. Mandke and R. C. Daniels and R. W. Heath Jr. and S. M. Nettles},
booktitle={IEEE INFOCOM 2009},
title={An Experimental Evaluation of Rate Adaptation for Multi-Antenna Systems},
year={2009},
volume={},
number={},
pages={2313-2321},
abstract={Increasingly wireless networks use multi-antenna nodes as in IEEE 802.11n and 802.16. The physical layer (PHY) in such systems may use the antennas to provide multiple streams of data (spatial multiplexing) or to increase the robustness of fewer streams. These physical layers also provide support for sending packets at different rates by changing the modulation and coding of transmissions. Rate adaptation is the problem of choosing the best transmission mode for the current channel and in these systems requires choosing both the level of spatial multiplexing and the modulation and coding. Hydra is an experimental wireless network node prototype in which both the MAC and PHY are highly programmable. Hydra's PHY is essentially the 802.11n PHY, and currently supports two antennas and the same modulations and codings as 802.11n. Because of limitations of our hardware platform, the actual rates are a factor of 10 smaller than 802.11n. The MAC is essentially the 802.11 MAC with extensions, including the ability to feedback channel state or rate information from the receiver. Hydra was designed to allow experimentation with real radios, PHYs, and network stacks over real-world channels and it is well suited to studying rate adaptation in multi-antenna systems. To allow controlled experimentation, we also have the ability to perform experiments over emulated channels using exactly the same MAC and PHY used for RF transmissions. We present rate control experiments based on transmission over both real and emulated channels. Our experiments include measurements for single antenna systems and two antenna systems using a single or multiple spatial streams. We study rate adaptation algorithms using both explicit and implicit feedback from the receiver. A novel aspect of our results is the first experimental study of adaptation between single and multiple spatial streams for 802.11n style systems.},
keywords={antennas;IEEE standards;multiuser channels;radio networks;space division multiplexing;telecommunication standards;experimental evaluation;multi-antenna system;multi-antenna nodes;IEEE 802.11n PHY;IEEE 802.16;physical layer;spatial multiplexing;packets;transmission modulation;transmission coding;transmission mode;hydra;wireless network node;hardware platform;feedback channel state;rate information;emulated channels;single antenna systems;two antenna systems;multiple spatial streams;rate adaptation algorithm;explicit feedback;implicit feedback;Physical layer;Modulation coding;Robustness;Media Access Protocol;Feedback;Radio frequency;Hardware;Radio control;Wireless networks;Communications Society},
doi={10.1109/INFCOM.2009.5062157},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062158,
author={H. Bany Salameh and M. Krunz and O. Younis},
booktitle={IEEE INFOCOM 2009},
title={Dynamic Spectrum Access Protocol Without Power Mask Constraints},
year={2009},
volume={},
number={},
pages={2322-2330},
abstract={In this work, we investigate a statistical approach for dynamic spectrum access and radio resource management (RRM) in opportunistic cognitive radio (CR) networks. We propose a distributed MAC protocol for such networks that enables unlicensed users to dynamically utilize the available spectrum while limiting the imposed interference on primary (PR) users. Our proposed protocol is novel in three aspects. First, it does not require CR users to coordinate with PR users. Second, it does not assume any predefined CR-to-PR power mask, and thus can exploit the available spectrum more efficiently. Third, it provides the PR users with a statistical guarantee on the fraction of time that their reception may be corrupted by CR users. To avoid corrupting PR user receptions, the protocol computes the maximum power that a CR transmission can use based on current network conditions. We show how to compute this maximum power by deriving models for the PR-to-CR and PR-to-PR interference. Simulation experiments illustrate that our MAC protocol can satisfy the statistical guarantee for PR users under various user deployment models and traffic loads.},
keywords={access protocols;cognitive radio;spectrum access protocol;power mask constraints;radio resource management;cognitive radio networks;MAC protocol;Access protocols;Chromium;Media Access Protocol;Interference constraints;Resource management;Cognitive radio;Computer networks;Computational modeling;Load modeling;Telecommunication traffic},
doi={10.1109/INFCOM.2009.5062158},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062159,
author={K. Kar and X. Luo and S. Sarkar},
booktitle={IEEE INFOCOM 2009},
title={Delay Guarantees for Throughput-Optimal Wireless Link Scheduling},
year={2009},
volume={},
number={},
pages={2331-2339},
abstract={We consider the question of obtaining tight delay guarantees for throughout-optimal link scheduling in arbitrary topology wireless ad-hoc networks. We consider two classes of scheduling policies: 1) a maximum queue-length weighted independent set scheduling policy, and 2) a randomized independent set scheduling policy where the independent set scheduling probabilities are selected optimally. Both policies stabilize all queues for any set of feasible packet arrival rates, and are therefore throughput-optimal. For these policies and i.i.d. packet arrivals, we show that the average packet delay is bounded by a constant that depends on the chromatic number of the interference graph, and the overall load on the network. We also prove that this upper bound is asymptotically tight in the sense that there exist classes of topologies where the expected delay attained by any scheduling policy is lower bounded by the same constant. Through simulations we examine the scaling of the average packet delay with respect to the overall load on the network, and the chromatic number of the link interference graph.},
keywords={ad hoc networks;graph theory;radio links;radiofrequency interference;scheduling;telecommunication network topology;delay guarantees;throughput-optimal wireless link scheduling;arbitrary topology wireless ad-hoc networks;independent set scheduling probabilities;packet arrivals;scheduling policy;chromatic number;link interference graph;Delay;Throughput;Network topology;Interference constraints;Stability;Probability;Processor scheduling;Telecommunication traffic;Traffic control;Statistics},
doi={10.1109/INFCOM.2009.5062159},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062160,
author={P. Wan and X. Xu and L. Wang and X. Jia and E. K. Park},
booktitle={IEEE INFOCOM 2009},
title={Minimum-Latency Beaconing Schedule in Multihop Wireless Networks},
year={2009},
volume={},
number={},
pages={2340-2346},
abstract={Minimum-latency beaconing schedule (MLBS) in synchronous multihop wireless networks seeks a schedule for beaconing with the shortest latency. This problem is NP-hard even when the interference radius is equal to the transmission radius. All prior works assume that the interference radius is equal to the transmission radius, and the best-known approximation ratio for MLBS under this special interference model is 7. In this paper, we present a new approximation algorithm called strip coloring for MLBS under the general protocol interference model. Its approximation ratio is at most 5 when the interference radius is equal to transmission radius, and is between 3 and 6 in general.},
keywords={approximation theory;communication complexity;interference (signal);protocols;radio networks;scheduling;minimum-latency beaconing scheduling;synchronous multihop wireless networks;shortest latency;NP-hard;interference radius;transmission radius;approximation ratio;strip coloring;general protocol interference model;Spread spectrum communication;Wireless networks;Interference;Network topology;Peer to peer computing;Delay;Processor scheduling;Protocols;Computer science;Communications Society},
doi={10.1109/INFCOM.2009.5062160},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062161,
author={L. Ying and S. Shakkottai},
booktitle={IEEE INFOCOM 2009},
title={Scheduling in Mobile Ad Hoc Networks with Topology and Channel-State Uncertainty},
year={2009},
volume={},
number={},
pages={2347-2355},
abstract={We study throughput-optimal scheduling/routing over mobile ad-hoc networks with time-varying (fading) channels. Traditional back-pressure algorithms (based on the work by Tassiulas and Ephremides) require instantaneous network state (topology, queues-lengths, and fading channel-state) in order to make scheduling/routing decisions. However, such instantaneous network-wide (global) information is hard to come by in practice, especially when mobility induces a time-varying topology. With information delays and a lack of global network state, different mobile nodes have differing "views" of the network, thus inducing uncertainty and inconsistency across mobile nodes in their topology knowledge and network state information. In such a setting, we first characterize the through-optimal rate region and develop a back-pressure-like scheduling algorithm, which we show is throughput-optimal. Then, by partitioning the geographic region spatially into disjoint tiles, and sharing delayed topology and network state information only among mobile nodes currently within each tile, we develop a localized low-complexity scheduling algorithm. The algorithm uses instantaneous local information (the queue length, channel state and current position at a mobile node) along with delayed network state information from nodes that were within its tile (i.e., from nodes that were within a nearby geographic region as opposed to network-wide information). The proposed algorithm is shown to be near-optimal, where the geographic distance over which delayed network-state information is shared determines the provable lower bound on the achievable throughput.},
keywords={ad hoc networks;channel allocation;fading channels;mobile radio;telecommunication network routing;telecommunication network topology;time-varying channels;mobile ad hoc network;network topology;channel-state uncertainty;throughput-optimal scheduling;network routing;time-varying channel;fading channel;back-pressure-like scheduling algorithm;queue length;channel state information;Mobile ad hoc networks;Network topology;Uncertainty;Scheduling algorithm;Tiles;Routing;Fading;Ad hoc networks;Partitioning algorithms;Throughput},
doi={10.1109/INFCOM.2009.5062161},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062162,
author={G. R. Gupta and N. Shroff},
booktitle={IEEE INFOCOM 2009},
title={Delay Analysis for Multi-Hop Wireless Networks},
year={2009},
volume={},
number={},
pages={2356-2364},
abstract={We analyze the delay performance of a multi-hop wireless network with a fixed route between each source-destination pair. There are arbitrary interference constraints on the set of links that can be served simultaneously at any given time. These interference constraints impose a fundamental lower bound on the delay performance of any scheduling policy for the system. We present a methodology to derive such lower bounds. For the tandem queue network, where the delay optimal policy is known, the expected delay of the optimal policy numerically coincides with the lower bound. We conduct extensive numerical studies to suggest that the average delay of the back-pressure scheduling policy can be made close to the lower bound by using appropriate functions of queue length.},
keywords={interference (signal);queueing theory;radio networks;scheduling;delay analysis;multihop wireless networks;source-destination pair;arbitrary interference constraints;scheduling policy;queue length;Spread spectrum communication;Wireless networks;Delay systems;Performance analysis;Interference constraints;Upper bound;Communications Society;Computer networks;Artificial intelligence;Stability},
doi={10.1109/INFCOM.2009.5062162},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062163,
author={H. Li and L. Zhong and K. Zheng},
booktitle={IEEE INFOCOM 2009},
title={Drowsy Transmission: Physical Layer Energy Optimization for Transmitting Random Packet Traffic},
year={2009},
volume={},
number={},
pages={2365-2373},
abstract={Energy efficiency has become increasingly important to mobile systems on which wireless interfaces are among the largest power consumers. While existing physical layer power optimization mostly focuses on improving the transmission efficiency, our recent work has showed that wireless interfaces can spend most of its time and energy in very short idle periods between transmitting two packets [9]. In this work, we present a physical layer optimization method, drowsy transmission, which explicitly considers the power cost of such idle periods in physical layer power optimization through joint power control/rate selection and power management. We provide a control theoretical formulation of the optimization problem and present a dynamic programming based solution and its approximation that is close form and practical. We further offer an on-line learning technique to cope with unknown channel and traffic. Using a power model from a commercial wireless network interface card, we demonstrate that drowsy transmission can reduce the energy per bit by 70% and 40% in comparison to power control/rate selection-based optimization and optimization with disjoint power control/rate selection and power management, respectively. Moreover, the achieved energy per bit is very close to the theoretical lower bound. Our evaluation shows that the proposed on-line learning technique can assess the channel and approach the performance under pre-known channel in as short as 200 ms. We also show that our optimization introduces negligible packet delays.},
keywords={approximation theory;dynamic programming;learning systems;mobile radio;power control;random processes;telecommunication control;telecommunication network management;telecommunication traffic;wireless channels;drowsy transmission;physical layer energy optimization;random packet traffic transmission;mobile system;wireless interface;joint power control;rate selection;power management;dynamic programming;approximation theory;online learning technique;wireless channel;Physical layer;Power control;Optimization methods;Energy management;Energy efficiency;Cost function;Dynamic programming;Communication system traffic control;Traffic control;Wireless networks},
doi={10.1109/INFCOM.2009.5062163},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062164,
author={M. Lu and J. Wu},
booktitle={IEEE INFOCOM 2009},
title={Opportunistic Routing Algebra and its Applications},
year={2009},
volume={},
number={},
pages={2374-2382},
abstract={Opportunistic routing (OR) has received much attention as a new routing paradigm due to its efficient utilization of broadcasting and spacial diversity of the wireless medium. Although numerous OR algorithms and protocols have been proposed to apply to various environments and integrate with numerous techniques, as far as we know, none of the existing works have used mathematical tools such as routing algebra to analyze the compatibility of routing metrics and routing protocols so as to provide a guideline for routing protocol design. In this paper, we design a new OR algebra based on the routing algebra proposed for inter-domain routing, identify the essential properties of OR in the mathematical language of the OR algebra, and analyze the design space in terms of routing metrics for various routing requirements.},
keywords={algebra;radio networks;routing protocols;opportunistic routing algebra;wireless medium;routing protocols;routing metrics;interdomain routing;Algebra;Routing protocols;Relays;Guidelines;Spread spectrum communication;Wireless networks;Broadcasting;Algorithm design and analysis;Communications Society;Application software},
doi={10.1109/INFCOM.2009.5062164},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062165,
author={A. Motskin and T. Roughgarden and P. Skraba and L. Guibas},
booktitle={IEEE INFOCOM 2009},
title={Lightweight Coloring and Desynchronization for Networks},
year={2009},
volume={},
number={},
pages={2383-2391},
abstract={We study the distributed desynchronization problem for graphs with arbitrary topology. Motivated by the severe computational limitations of sensor networks, we present a randomized algorithm for network desynchronization that uses an extremely lightweight model of computation, while being robust to link volatility and node failure. These techniques also provide novel, ultra-lightweight randomized algorithms for quickly computing distributed vertex colorings using an asymptotically optimal number of colors.},
keywords={distributed sensors;graph theory;randomised algorithms;telecommunication network reliability;lightweight coloring;distributed desynchronization problem;sensor networks;link volatility;node failure;ultralightweight randomized algorithms;distributed vertex colorings;graphs;Computer networks;Peer to peer computing;Computational modeling;Distributed computing;Greedy algorithms;Convergence;Protocols;State feedback;Communications Society;Network topology},
doi={10.1109/INFCOM.2009.5062165},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062166,
author={Z. Yang and Y. Liu and X. -. Li},
booktitle={IEEE INFOCOM 2009},
title={Beyond trilateration: On the localizability of wireless ad-hoc networks},
year={2009},
volume={},
number={},
pages={2392-2400},
abstract={The proliferation of wireless and mobile devices has fostered the demand of context aware applications, in which location is often viewed as one of the most significant contexts. Classically, trilateration is widely employed for testing network localizability; even in many cases it wrongly recognizes a localizable graph as non-localizable. In this study, we analyze the limitation of trilateration based approaches and propose a novel approach which inherits the simplicity and efficiency of trilateration, while at the same time improves the performance by identifying more localizable nodes. We prove the correctness and optimality of this design by showing that it is able to locally recognize all 1-hop localizable nodes. To validate this approach, a prototype system with 19 wireless sensors is deployed. Intensive and large-scale simulations are further conducted to evaluate the scalability and efficiency of our design.},
keywords={ad hoc networks;mobile radio;wireless ad-hoc network;network localizability;localizable graph;trilateration;1-hop localizable nodes;wireless sensor;Ad hoc networks;Peer to peer computing;Testing;Wireless sensor networks;Context awareness;Large-scale systems;Distance measurement;Mobile computing;Global Positioning System;Communications Society},
doi={10.1109/INFCOM.2009.5062166},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062167,
author={Y. Wang and S. Lederer and J. Gao},
booktitle={IEEE INFOCOM 2009},
title={Connectivity-Based Sensor Network Localization with Incremental Delaunay Refinement Method},
year={2009},
volume={},
number={},
pages={2401-2409},
abstract={We study the anchor-free localization problem for a large-scale sensor network with a complex shape, knowing network connectivity information only. The main idea follows from our previous work in which a subset of the nodes are selected as landmarks and the sensor field is partitioned into Voronoi cells with all the nodes closest to the same landmark grouped into the same cell. We extract the combinatorial Delaunay complex as the dual complex of the landmark Voronoi diagram and embed the combinatorial Delaunay complex as a structural skeleton. In this paper we develop a new landmark selection algorithm with incremental Delaunay refinement method. This algorithm does not assume any knowledge of the network boundary and runs in a distributed manner to select landmarks incrementally until both the global rigidity property (the Delaunay complex is globally rigid and thus can be embedded uniquely) and the coverage property (every node is not far from the embedded Delaunay complex) are met. The new algorithm substantially improves the robustness and applicability of the original localization algorithm, especially in networks with very low average degree (even non- rigid networks) and complex shapes.},
keywords={computational geometry;mesh generation;wireless sensor networks;landmark selection algorithm;incremental Delaunay refinement method;connectivity-based sensor network localization;Voronoi diagram;Peer to peer computing;Shape;Communications Society;Computer science;Large-scale systems;Data mining;Skeleton;Robustness;Hardware;Distance measurement},
doi={10.1109/INFCOM.2009.5062167},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062168,
author={L. Zhang and S. Chen and Y. Jian and Y. Fang},
booktitle={IEEE INFOCOM 2009},
title={Distributed Progressive Algorithm for Maximizing Lifetime Vector in Wireless Sensor Networks},
year={2009},
volume={},
number={},
pages={2410-2418},
abstract={Maximizing the operational lifetime of a sensor network is a critical problem in practice. Many prior works define the network's lifetime as the time before the first sensor in the network runs out of energy. However, when one sensor dies, the rest of the network can still work, as long as useful data generated by other sensors can reach the sink. More appropriately, we should maximize the lifetime vector of the network, consisting of the lifetimes of all sensors, sorted in ascending order. For this problem, there exists only a centralized algorithm that solves a series of linear programming problems with high-order complexities. This paper proposes a fully distributed progressive algorithm which iteratively produces a series of lifetime vectors, each better than the previous one. Instead of giving the optimal result in one shot after lengthy computation, the proposed distributed algorithm has a result at any time, and the more time spent gives the better result. We show that when the algorithm stabilizes, its result produces the maximum lifetime vector. Furthermore, simulations demonstrate that the algorithm is able to converge rapidly towards the maximum lifetime vector with low overhead.},
keywords={wireless sensor networks;distributed progressive algorithm;lifetime vector;wireless sensor networks;operational lifetime;Wireless sensor networks;Vectors;Linear programming;Iterative algorithms;Computer networks;Distributed computing;Distributed algorithms;Communications Society;Information science;Power engineering and energy},
doi={10.1109/INFCOM.2009.5062168},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062169,
author={M. Li and Y. Liu and J. Wang and Z. Yang},
booktitle={IEEE INFOCOM 2009},
title={Sensor Network Navigation without Locations},
year={2009},
volume={},
number={},
pages={2419-2427},
abstract={We propose a pervasive usage of the sensor network infrastructure as a cyber-physical system for navigating internal users in locations of potential danger. Our proposed application differs from previous work in that they typically treat the sensor network as a media of data acquisition while in our navigation application, in-situ interactions between users and sensors become ubiquitous. In addition, human safety and time factors are critical to the success of our objective. Without any pre-knowledge of user and sensor locations, the design of an effective and efficient navigation protocol faces non-trivial challenges. We propose to embed a road map system in the sensor network without location information so as to provide users navigating routes with guaranteed safety. We accordingly design efficient road map updating mechanisms to rebuild the road map in the event of changes in dangerous areas. In this navigation system, each user only issues local queries to obtain their navigation route. The system is highly scalable for supporting multiple users simultaneously. We implement a prototype system with 36 TelosB motes to validate the effectiveness of this design. We further conduct comprehensive and large-scale simulations to examine the efficiency and scalability of the proposed approach under various environmental dynamics.},
keywords={data acquisition;radionavigation;wireless sensor networks;sensor network navigation;sensor network infrastructure;cyber-physical system;data acquisition;human safety;time factors;user knowledge;sensor locations;navigation protocol;road map updating mechanisms;navigation system;navigation route;wireless sensor network;Navigation;Sensor systems;Road safety;Data acquisition;Humans;Time factors;Protocols;Prototypes;Large-scale systems;Scalability},
doi={10.1109/INFCOM.2009.5062169},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062170,
author={F. Li and J. Wu and A. Srinivasan},
booktitle={IEEE INFOCOM 2009},
title={Thwarting Blackhole Attacks in Disruption-Tolerant Networks using Encounter Tickets},
year={2009},
volume={},
number={},
pages={2428-2436},
abstract={Nodes in disruption-tolerant networks (DTNs) usually exhibit repetitive motions. Several recently proposed DTN routing algorithms have utilized the DTNs' cyclic properties for predicting future forwarding. The prediction is based on metrics abstracted from nodes' contact history. However, the robustness of the encounter prediction becomes vital for DTN routing since malicious nodes can provide forged metrics or follow sophisticated mobility patterns to attract packets and gain a significant advantage in encounter prediction. In this paper, we examine the impact of the blackhole attack and its variations in DTN routing. We introduce the concept of encounter tickets to secure the evidence of each contact. In our scheme, nodes adopt a unique way of interpreting the contact history by making observations based on the collected encounter tickets. Then, following the Dempster-Shafer theory, nodes form trust and confidence opinions towards the competency of each encountered forwarding node. Extensive real-trace-driven simulation results are presented to support the effectiveness of our system.},
keywords={mobility management (mobile radio);telecommunication network routing;blackhole attacks;disruption-tolerant networks;encounter tickets;future forwarding;DTN routing algorithms;mobility patterns;encounter prediction;Dempster-Shafer theory;Disruption tolerant networking;Peer to peer computing;History;Computer science;Robustness;Communications Society;Statistics;Uncertainty;Routing protocols;Security},
doi={10.1109/INFCOM.2009.5062170},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062171,
author={A. El-Atawy and E. Al-Shaer and T. Tran and R. Boutaba},
booktitle={IEEE INFOCOM 2009},
title={Adaptive Early Packet Filtering for Defending Firewalls Against DoS Attacks},
year={2009},
volume={},
number={},
pages={2437-2445},
abstract={A major threat to data networks is based on the fact that some traffic can be expensive to classify and filter as it will undergo a longer than average list of filtering rules before being rejected by the default deny rule. An attacker with some information about the access-control list (ACL) deployed at a firewall or an intrusion detection and prevention system (IDS/IPS) can craft packets that will have maximum cost. In this paper, we present a technique that is light weight, traffic-adaptive and can be deployed on top of any filtering mechanism to pre-filter unwanted expensive traffic. The technique utilizes Internet traffic characteristics coupled with a special carefully tuned representation of the policy to generate early defense policies. We use Boolean expressions built as binary decision diagrams (BDD) to represent relaxed versions of the policy that are faster to evaluate. Moreover, it is guaranteed that the technique will not add an overhead that will not be compensated by the gain in filtering time in the underlying filtering method. Evaluation has shown considerable savings to the overall filtering process, thus saving the firewall processing power and increasing overall throughput. Also, the overhead changes according to the traffic behavior, and can be tuned to guarantee its worst case time cost.},
keywords={adaptive filters;authorisation;binary decision diagrams;Boolean algebra;Internet;telecommunication security;telecommunication traffic;adaptive early packet filtering;firewalls;DoS attack;data network;default deny rule;access-control list;intrusion detection;prevention system;Internet traffic;Boolean expression;binary decision diagram;traffic behavior;Adaptive filters;Computer crime;Intrusion detection;Costs;Telecommunication traffic;Information filtering;Information filters;Internet;Character generation;Data structures},
doi={10.1109/INFCOM.2009.5062171},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062172,
author={F. Soldo and A. Markopoulou and K. Argyraki},
booktitle={IEEE INFOCOM 2009},
title={Optimal Filtering of Source Address Prefixes: Models and Algorithms},
year={2009},
volume={},
number={},
pages={2446-2454},
abstract={How can we protect the network infrastructure from malicious traffic, such as scanning, malicious code propagation, and distributed denial-of-service (DDoS) attacks? One mechanism for blocking malicious traffic is filtering: access control lists (ACLs) can selectively block traffic based on fields of the IP header. Filters (ACLs) are already available in the routers today but are a scarce resource because they are stored in expensive ternary content addressable memory (TCAM). In this paper, we develop, for the first time, a framework for studying filter selection as a resource allocation problem. Within this framework, we study four practical cases of source address/prefix filtering, which correspond to different attack scenarios and operator's policies. We show that filter selection optimization leads to novel variations of the multidimensional knapsack problem and we design optimal, yet computationally efficient, algorithms to solve them. We also evaluate our approach using data from Dshield.org and demonstrate that it brings significant benefits in practice. Our set of algorithms is a building block that can be immediately used by operators and manufacturers to block malicious traffic in a cost-efficient way.},
keywords={content-addressable storage;IP networks;knapsack problems;resource allocation;security of data;telecommunication network routing;telecommunication security;telecommunication traffic;optimal filtering;source address prefixes;network infrastructure protection;malicious traffic;scanning attacks;malicious code propagation attacks;distributed denial-of-service attacks;access control lists;IP header;routers;ternary content addressable memory;resource allocation problem;filter selection optimization;multidimensional knapsack problem;Filtering algorithms;Communication system traffic control;Traffic control;Filters;Protection;Computer crime;Access control;Associative memory;Resource management;Design optimization},
doi={10.1109/INFCOM.2009.5062172},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062173,
author={B. Y. Zhao and C. Chi and W. Gao and S. Zhu and G. Cao},
booktitle={IEEE INFOCOM 2009},
title={A Chain Reaction DoS Attack on 3G Networks: Analysis and Defenses},
year={2009},
volume={},
number={},
pages={2455-2463},
abstract={The IP multimedia subsystem (IMS) is being deployed in the third generation (3G) networks since it supports many kinds of multimedia services. However, the security of IMS networks has not been fully examined. This paper presents a novel DoS attack against IMS. By congesting the presence service, a core service of IMS, a malicious attack can cause chained automatic reaction of the system, thus blocking all the services of IMS. Because of the low-volume nature of this attack, an attacker only needs to control several clients to paralyze an IMS network supporting one million users. To address this DoS attack, we propose an online early defense mechanism, which aims to first detect the attack, then identify the malicious clients, and finally block them. We formulate this problem as a change-point detection problem, and solve it based on the non-parametric GRSh test. Through trace-driven experiments, we demonstrate that our defense mechanism can throttle this DoS attack within a short defense time window while generating few false alarms.},
keywords={3G mobile communication;IP networks;multimedia communication;security of data;chain reaction DoS attack;3G networks;IP multimedia subsystem;multimedia services;IMS networks;malicious attack;chained automatic reaction;online early defense mechanism;malicious clients;change-point detection;non-parametric GRSh test;defense time window;Computer crime;Resonance light scattering;Telecommunication traffic;Subscriptions;Protocols;Communications Society;Information analysis;Computer science;Maintenance engineering;Signal generators},
doi={10.1109/INFCOM.2009.5062173},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062174,
author={N. C. Fernandes and M. D. D. Moreira and O. C. M. B. Duarte},
booktitle={IEEE INFOCOM 2009},
title={An Efficient Filter-based Addressing Protocol for Autoconfiguration of Mobile Ad Hoc Networks},
year={2009},
volume={},
number={},
pages={2464-2472},
abstract={Address autoconfiguration is an important issue for ad hoc networks in order to provide autonomous networking and self-management. The IP address assignment for ad hoc nodes requires a distributed procedure that resolves all the address collisions in a dynamic network with fading channels, frequent partitions, and joining/leaving nodes. We propose a filter-based addressing protocol for autoconfiguration of mobile ad hoc networks that is lightweight and robust to packet losses. We present a probabilistic analysis of address collisions and discuss filters functionalities in the address autoconfiguration. We evaluate the performance of our protocol for static and mobile scenarios, considering joining nodes and partition mergings. Simulation results show that our protocol resolves all the address collisions and reduces up to 22 times the control traffic when compared to the other addressing protocols.},
keywords={filtering theory;IP networks;mobile computing;protocols;filter-based addressing protocol;mobile ad hoc networks;address autoconfiguration;IP address assignment;Protocols;Mobile ad hoc networks;Ad hoc networks;Filters;Merging;Peer to peer computing;Financial advantage program;Bandwidth;Analytical models;Network servers},
doi={10.1109/INFCOM.2009.5062174},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062175,
author={D. Koutsonikolas and Y. C. Hu and C. -. Wang},
booktitle={IEEE INFOCOM 2009},
title={Pacifier: High-Throughput, Reliable Multicast without ``Crying Babies'' in Wireless Mesh Networks},
year={2009},
volume={},
number={},
pages={2473-2481},
abstract={In contrast to unicast routing, high-throughput reliable multicast routing in wireless mesh networks (WMNs) has received little attention. There are two primary challenges to supporting high-throughput, reliable multicast in WMNs. The first is no different from unicast: wireless links are inherently lossy due to varying channel conditions and interference. The second, known as the "crying baby" problem, is unique to multicast: the multicast source may have varying throughput to different multicast receivers, and hence trying to satisfy the reliability requirement for poorly connected receivers can potentially result in performance degradation for the rest of the receivers. In this paper, we propose Pacifier, a new high-throughput reliable multicast protocol for WMNs. Pacifier seamlessly integrates four building blocks, namely, tree-based opportunistic routing, intra-flow network coding, source rate limiting, and round-robin batching, to support high-throughput, reliable multicast routing in WMNs, while at the same time effectively addresses the "crying baby" problem. Our evaluations show that Pacifier increases the average throughput over a practical, state-of-the-art reliable network coding-based protocol MORE by 171%, while improving the throughput of well-connected receivers by up to a factor of 20.},
keywords={multicast protocols;radio networks;routing protocols;Pacifier;wireless mesh networks;unicast routing;high-throughput reliable multicast routing;wireless links;crying baby problem;multicast source;multicast receiver;high throughput reliable multicast protocol;tree-based opportunistic routing;intra-flow network coding;source rate limiting;round robin batching;reliable network coding-based protocol;Pediatrics;Wireless mesh networks;Throughput;Forward error correction;Unicast;Multicast protocols;Routing protocols;Access protocols;Automatic repeat request;Computer network reliability},
doi={10.1109/INFCOM.2009.5062175},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062176,
author={K. R. Chowdhury and M. Di Felice and I. F. Akyildiz},
booktitle={IEEE INFOCOM 2009},
title={TP-CRAHN: a Transport Protocol for Cognitive Radio Ad-Hoc Networks},
year={2009},
volume={},
number={},
pages={2482-2490},
abstract={Existing research in transport protocols for wireless ad-hoc networks has focused on reliable end-to-end packet delivery under uncertain channel conditions, route failures due to node mobility and link congestion. In a cognitive radio (CR) environment, there are several key challenges that must be addressed apart from the above concerns. The intermittent spectrum sensing undertaken by the CR users, the activity of the licensed users of the spectrum, large-scale bandwidth variation based on spectrum availability, and the channel switching process need to be considered in the transport protocol design. In this paper, a window-based transport protocol for CR ad-hoc networks, TP-CRAHN, is proposed that distinguishes each of these events by a combination of explicit feedback from the intermediate nodes and the destination. This is achieved by adapting the classical TCP rate control algorithm running at the source to closely interact with the physical layer channel information, the link layer functions of spectrum sensing and buffer management, and a predictive mobility framework that is developed at the network layer. To the best of our knowledge, this is the first work on the transport layer to specifically address the concerns of the CR ad-hoc networks and our approach is thoroughly validated by simulation experiments.},
keywords={ad hoc networks;channel estimation;cognitive radio;mobile radio;transport protocols;TP-CRAHN;transport protocol;cognitive radio ad-hoc network;wireless ad-hoc network;TCP rate control algorithm;channel information;link layer function;spectrum sensing;buffer management;predictive mobility framework;Transport protocols;Cognitive radio;Ad hoc networks;Chromium;Wireless sensor networks;USA Councils;Peer to peer computing;Communications Society;Computer science;Computer network reliability},
doi={10.1109/INFCOM.2009.5062176},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062177,
author={J. Lu and K. Whitehouse},
booktitle={IEEE INFOCOM 2009},
title={Flash Flooding: Exploiting the Capture Effect for Rapid Flooding in Wireless Sensor Networks},
year={2009},
volume={},
number={},
pages={2491-2499},
abstract={We present the Flash flooding protocol for rapid network flooding in wireless sensor networks. Traditional flooding protocols can be very slow because of neighborhood contention: nodes cannot propagate the flood until neighboring nodes have finished their transmissions. The Flash flooding protocol avoids this problem by allowing concurrent transmissions among neighboring nodes. It relies on the capture effect to ensure that each node receives the flood from at least one of its neighbors, and introduces new techniques to either recover from or prevent too many concurrent transmissions. We evaluate the Flash flooding protocol on both a 48-node wireless sensor network testbed and in a trace-based simulator. Our results indicate that the Flash flooding protocol can reduce latency by as much as 80%, achieving flooding latencies near the theoretical lower bound without sacrificing coverage, reliability or power consumption.},
keywords={protocols;wireless sensor networks;wireless sensor networks;flash flooding protocol;rapid network flooding;neighborhood contention:;latency reduction;flooding latencies;Wireless sensor networks;Floods;Delay;Protocols;Peer to peer computing;Relays;Energy consumption;Propagation losses;Testing;Event detection},
doi={10.1109/INFCOM.2009.5062177},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062178,
author={Y. Kanizo and D. Hay and I. Keslassy},
booktitle={IEEE INFOCOM 2009},
title={Optimal Fast Hashing},
year={2009},
volume={},
number={},
pages={2500-2508},
abstract={This paper is about designing optimal high-throughput hashing schemes that minimize the total number of memory accesses needed to build and access an hash table. Recent schemes often promote the use of multiple-choice hashing. However, such a choice also implies a significant increase in the number of memory accesses to the hash table, which translates into higher power consumption and lower throughput. In this paper, we propose to only use choice when needed. Given some target hash table overflow rate, we provide a lower bound on the total number of needed memory accesses. Then, we design and analyze schemes that provably achieve this lower bound over a large range of target overflow values. Further, for the multilevel hash table scheme, we prove that the optimum occurs when its sub table sizes decrease in a geometric way, thus formally confirming a heuristic rule-of-thumb.},
keywords={data structures;storage management;optimal fast hashing;optimal high-throughput hashing scheme;memory access;multilevel hash table scheme;Energy consumption;Throughput;High-speed networks;Communications Society;Computer science;Counting circuits;Data structures;Computer aided manufacturing;CADCAM;Random access memory},
doi={10.1109/INFCOM.2009.5062178},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062179,
author={M. Behdadfar and H. Saidi and H. Alaei and B. Samari},
booktitle={IEEE INFOCOM 2009},
title={Scalar Prefix Search: A New Route Lookup Algorithm for Next Generation Internet},
year={2009},
volume={},
number={},
pages={2509-2517},
abstract={Currently, the increasing rate of routing lookups in Internet routers, the large number of prefixes and also the transition from IPV4 to IPV6, have caused Internet designers to propose new lookup algorithms and try to reduce the memory cost and the prefix search and update procedures times. Recently, some new algorithms are proposed trying to store the prefixes in a balanced tree to reduce the worst case prefix search and update times. These algorithms improve the search and update times compared to previous range based trees. In this paper it is shown that there is no need to treat the prefixes as ranges. It is only required to compare them like scalar values using a predefined rule. The method "scalar prefix search" which is presented here, is built on this concept and combining it with the proposed store and search methods, interprets each prefix as a number without any encoding, the need to convert it to the prefix end points or to use the trie based algorithms whose performance completely depends on IP address length. This method can be applied to many different tree structures. It is implemented using the binary search tree and some other balanced trees such as RB-tree, AVL-tree and B-tree for both IPV4 and IPV6 prefixes. Comparison results show better lookup and update performance or superior storage requirements for scalar prefix search in both average and worst cases, against current solutions like PIBT (Lu and Sahni, 2005) and LPFST (Wnn et al., 2005).},
keywords={Internet;IP networks;telecommunication network routing;tree searching;scalar prefix search;route lookup;next generation Internet;Internet router;IPV4;IPV6;memory cost;trie based algorithm;IP address length;tree structure;binary search tree;RB-tree;AVL-tree;Internet;Routing;Binary search trees;Communications Society;Algorithm design and analysis;Costs;Search methods;Encoding;Tree data structures;Data structures},
doi={10.1109/INFCOM.2009.5062179},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062180,
author={H. Song and F. Hao and M. Kodialam and T. V. Lakshman},
booktitle={IEEE INFOCOM 2009},
title={IPv6 Lookups using Distributed and Load Balanced Bloom Filters for 100Gbps Core Router Line Cards},
year={2009},
volume={},
number={},
pages={2518-2526},
abstract={Internet line speeds are expected to reach 100 Gbps in a few years. To match these line rates, a single router line card needs to forward more than 150 million packets per second. This requires a corresponding amount of longest prefix match operations. Furthermore, the increased use of IPv6 requires core routers to perform the longest prefix match on several hundred thousand prefixes varying in length up to 64 bits. It is a challenge to scale existing algorithms simultaneously in the three dimensions of increased throughput, table size and prefix length. Recently, Bloom filter-based IP lookup algorithms have been proposed. While these algorithms can take advantage of hardware parallelism and fast on-chip memory to achieve high performance, they have significant drawbacks (discussed in the paper) that impede their use in practice. In this paper, we present the distributed and load balanced bloom filters to address these drawbacks. We develop the practical IP lookup algorithm for use in 100 Gbps line cards. The regular and modular hardware architecture of our scheme directly maps to the state-of-art ASICs and FPGAs with reasonable resource consumption. Also, our scheme outperforms TCAMs on most metrics including cost, power dissipation, and board footprint.},
keywords={field programmable gate arrays;Internet;IP networks;parallel architectures;resource allocation;table lookup;telecommunication network routing;transport protocols;IPv6 lookup;Internet;distributed-and-load balanced bloom filter;core router line card;hardware parallel architecture;on-chip memory;FPGA;power dissipation;board footprint;resource consumption;bit rate 100 Gbit/s;Filters;Energy consumption;Throughput;Power dissipation;Random access memory;Hardware;Communications Society;USA Councils;Transceivers;Inspection},
doi={10.1109/INFCOM.2009.5062180},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062181,
author={E. Perron and D. Vasudevan and M. Vojnovic},
booktitle={IEEE INFOCOM 2009},
title={Using Three States for Binary Consensus on Complete Graphs},
year={2009},
volume={},
number={},
pages={2527-2535},
abstract={We consider the binary consensus problem where each node in the network initially observes one of two states and the goal for each node is to eventually decide which one of the two states was initially held by the majority of the nodes. Each node contacts other nodes and updates its current state based on the state communicated by the last contacted node. We assume that both signaling (the information exchanged at node contacts) and memory (computation state at each node) are limited and restrict our attention to systems where each node can contact any other node (i.e., complete graphs). It is well known that for systems with binary signaling and memory, the probability of reaching incorrect consensus is equal to the fraction of nodes that initially held the minority state. We show that extending both the signaling and memory by just one state dramatically improves the reliability and speed of reaching the correct consensus. Specifically, we show that the probability of error decays exponentially with the number of nodes N and the convergence time is logarithmic in N for large N. We also examine the case when the state is ternary and signaling is binary. The convergence of this system to consensus is again shown to be logarithmic in N for large N, and is therefore faster than purely binary systems. The type of distributed consensus problems that we study arises in the context of decentralized peer- to-peer networks, e.g. sensor networks and opinion formation in social networks - our results suggest that robust and efficient protocols can be built with rather limited signaling and memory.},
keywords={graph theory;peer-to-peer computing;complete graphs;binary consensus problem;binary signaling;error decays probability;binary systems;distributed consensus problems;decentralized peer-to-peer networks;Peer to peer computing;Convergence;Sampling methods;Social network services;Displays;Robustness;Communications Society;Web pages;Access protocols;Context modeling},
doi={10.1109/INFCOM.2009.5062181},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062182,
author={M. Kodialam and T. V. Lakshman and S. Sengupta},
booktitle={IEEE INFOCOM 2009},
title={Capacity of Multi-Hop Wireless Networks with Incomplete Traffic Specification},
year={2009},
volume={},
number={},
pages={2536-2540},
abstract={The capacity of wireless channels has been studied extensively by the information theory community over the years. There have been several efforts to extend this theory to multi-hop wireless networks. One approach to estimating the capacity of multihop wireless networks is to determine asymptotically how the capacity scales as the number of nodes in the network increases. In these models, the traffic is typically assumed to be uniform. Another approach assumes that node locations and channel conditions are known and the question is to determine whether a given traffic matrix can be routed on the wireless network. This usually involves solving jointly, routing, scheduling and power control problems to achieve the given traffic matrix. In practice, it is quite difficult to estimate the traffic matrix and further, the traffic matrix typically changes over time. In this paper, we are given the location of the nodes and the inter-node channel parameters. Instead of being provided a traffic matrix, we are provided with only the total amount of traffic that can originate and terminate at each node in the network. The objective is to determine if there exists a joint routing and scheduling policy that can handle any traffic matrix that satisfies these ingress/egress constraints. We derive necessary and sufficiency conditions for the problem for both the directional and omni-directional antenna cases. We solve the joint routing and scheduling problem for all traffic matrices that satisfy the ingress-egress constraints.},
keywords={channel capacity;directive antennas;information theory;matrix algebra;radio networks;scheduling;telecommunication network routing;telecommunication traffic;multihop wireless network;incomplete traffic specification;wireless channel capacity;information theory;capacity estimation;traffic matrix;joint routing problem;scheduling problem;power control problem;omni-directional antenna;ingress-egress constraints;Spread spectrum communication;Wireless networks;Telecommunication traffic;Communication system traffic control;Routing;Traffic control;Channel capacity;Information theory;Power control;Directive antennas},
doi={10.1109/INFCOM.2009.5062182},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062183,
author={S. -. Wu and C. -. Chen and M. -. Chen},
booktitle={IEEE INFOCOM 2009},
title={AAA: Asynchronous, Adaptive, and Asymmetric Power Management for Mobile Ad Hoc Networks},
year={2009},
volume={},
number={},
pages={2541-2545},
abstract={The Quorum-based Power Saving (QPS) protocols have been proposed to increase the energy efficiency of wireless communication. However, it remains challenging to apply existing QPS protocols to the Mobile Ad Hoc Networks (MANETs) as the timers of nodes are usually asynchronous, the incurred delay are expected to be adaptive, and the network topology is asymmetric. In this paper, we propose an Asynchronous, Adaptive, and Asymmetric (AAA) power management protocol that fulfills the unique requirements of MANETs. We present the asymmetric grid quorum system, a generalization of traditional grid-based quorum systems, to ensure the network connectivity. Theoretical analysis is conducted to demonstrate the benefits of AAA over previous arts.},
keywords={ad hoc networks;mobile computing;telecommunication power supplies;AAA;asynchronous, adaptive, and asymmetric power management;mobile ad hoc networks;quorum-based power saving protocols;wireless communication;QPS protocols;MANET;asymmetric grid quorum system;Energy management;Mobile ad hoc networks;Protocols;Sleep;Energy efficiency;Network topology;Data communication;Spine;Communications Society;Technology management},
doi={10.1109/INFCOM.2009.5062183},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062184,
author={Z. Li and F. R. Yu and M. Huang},
booktitle={IEEE INFOCOM 2009},
title={A Cooperative Spectrum Sensing Consensus Scheme in Cognitive Radios},
year={2009},
volume={},
number={},
pages={2546-2550},
abstract={Cooperative spectrum sensing is attracting more attention in cognitive radio networks. This paper proposes a fully distributed consensus-based cooperative spectrum sensing scheme to cope with both fixed and random bidirectional connections among secondary users. In the proposed scheme, secondary users can maintain coordination based on only local interactions without a centralized common receiver. Moreover, we prove that even inter-connected with unreliable bidirectional communication links, secondary users can still make an average consensus. Simulation results show that the proposed scheme can have significantly lower missing detection probability and false alarm probability.},
keywords={cognitive radio;probability;cognitive radio network;distributed consensus-based cooperative spectrum sensing scheme;bidirectional communication link;missing detection probability;false alarm probability;Cognitive radio;Receivers;Fading;Communications Society;Computer networks;Systems engineering and theory;Mathematics;Statistical distributions;Bidirectional control;Chromium},
doi={10.1109/INFCOM.2009.5062184},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062185,
author={L. Sang and A. Arora},
booktitle={IEEE INFOCOM 2009},
title={Capabilities of Low-Power Wireless Jammers},
year={2009},
volume={},
number={},
pages={2551-2555},
abstract={In this paper, motivated by the goal of modeling the fine-grain capabilities of jammers for the context of security in low-power wireless networks, we experimentally characterize jamming in networks of CC2420 radio motes and CC1000 radio motes. Our findings include that it is easy to locate J (relative to S and R) and choose its power level so that J can corrupt S's messages with high probability as well as corrupt individual S's bits with nontrivial probability. Internal jammers are however limited in at least two ways: One, it is hard for them to prevent R from detecting that it has received an uncorrupted message from S. And two, the outcome of their corruptions are not only not deterministic, even the probabilities of corrupted outcomes are time-varying. We therefore conclude that it is hard to predict the value resulting from colliding S's messages (bits) with J's messages (bits) and, conversely, to deduce the value sent by S's or J's from the corrupted value received by R.},
keywords={jamming;probability;wireless sensor networks;low-power wireless jammers;low-power wireless networks;network jamming;CC2420 radio motes;CC1000 radio motes;nontrivial probability;internal jammers;uncorrupted message;Jamming;Wireless networks;Communication system security;Interference;Protocols;Communications Society;Computer science;Context modeling;Computer security;Power system security},
doi={10.1109/INFCOM.2009.5062185},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062186,
author={X. Ta and G. Mao and B. D. O. Anderson},
booktitle={IEEE INFOCOM 2009},
title={On the Properties of Giant Component in Wireless Multi-Hop Networks},
year={2009},
volume={},
number={},
pages={2556-2560},
abstract={In this paper, we study the giant component, the largest component containing a non-vanishing fraction of nodes, in wireless multi-hop networks in R<sup>d</sup> (d = 1, 2). We assume that n nodes are randomly, independently and uniformly distributed in [0, 1]<sup>d</sup>, and each node has a uniform transmission range of r = r(n) and any two nodes can communicate directly with each other iff their Euclidean distance is at most r. For d = 1, we derive a closed-form analytical formula for calculating the probability of having a giant component of order above pn with any fixed 0.5 &lt; p les 1. The asymptotic behavior of one dimensional network having a giant component is investigated based on the derived result, which is distinctly different from its two dimensional counterpart. For d = 2, we derive an asymptotic analytical upper bound on the minimum transmission range at which the probability of having a giant component of order above qn for any fixed 0 &lt; q &lt; 1 tends to one as n rarr infin. Based on the result, we show that significant energy savings can be achieved if we only require a large percentage of nodes (e.g. 95%) to be connected rather than requiring all nodes to be connected. The results of this paper are of practical significance in the design and analysis of wireless ad hoc networks and sensor networks.},
keywords={ad hoc networks;wireless sensor networks;giant component;wireless multi-hop networks;non-vanishing fraction;uniform transmission range;Euclidean distance;closed-form analytical formula;asymptotic behavior;1D network;energy savings;wireless ad hoc networks;sensor networks;Spread spectrum communication;Australia;Peer to peer computing;Communications Society;Euclidean distance;Independent component analysis;Probability;Upper bound;Mobile ad hoc networks;Wireless sensor networks},
doi={10.1109/INFCOM.2009.5062186},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062187,
author={R. Urgaonkar and M. J. Neely},
booktitle={IEEE INFOCOM 2009},
title={Delay-Limited Cooperative Communication with Reliability Constraints in Wireless Networks},
year={2009},
volume={},
number={},
pages={2561-2565},
abstract={We investigate optimal resource allocation for delay-limited cooperative communication in time varying wireless networks. Motivated by real-time applications that have stringent delay constraints, we develop dynamic cooperation strategies that make optimal use of network resources to achieve a target outage probability (reliability) for each user subject to average power constraints. Using the technique of Lyapunov optimization, we first present a general framework to solve this problem and then derive quasi-closed form solutions for several cooperative protocols proposed in the literature.},
keywords={Lyapunov methods;optimisation;probability;protocols;radio networks;resource allocation;telecommunication network reliability;delay-limited cooperative communication;reliability constraint;optimal resource allocation;time varying wireless network;delay constraint;dynamic cooperation;network resource;outage probability;power constraint;Lyapunov optimization;quasiclosed form solution;cooperative protocol;Telecommunication network reliability;Wireless networks;Delay effects;Frame relay;Decoding;MIMO;Diversity methods;Protocols;Fading;Communications Society},
doi={10.1109/INFCOM.2009.5062187},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062188,
author={R. Sugihara and R. K. Gupta},
booktitle={IEEE INFOCOM 2009},
title={Optimizing Energy-Latency Trade-Off in Sensor Networks with Controlled Mobility},
year={2009},
volume={},
number={},
pages={2566-2570},
abstract={We consider the problem of planning path and speed of a "data mule" in a sensor network. This problem is encountered in various situations, such as modeling the motion of a data-collecting UAV (unmanned aerial vehicle) in a field of sensors for structural health monitoring. Our specific context here is use of a data mule as an alternative or supplement to multihop forwarding in a sensor network. While a data mule can reduce the energy consumption at each sensor node, it increases the latency from the time the data is generated at a node to the time the base station receives it. In this paper, we introduce the "data mule scheduling" or DMS framework that enables data mule motion planning to minimize the data delivery latency. The DMS framework is general; it can express many previously proposed problem formulations and problem settings related to data mules. We design algorithms for DMS and extend to the more general case of combined data mule and multihop forwarding to enable a flexible trade-off between energy consumption and data delivery latency. Using DMS, we can calculate the optimal way for node-to-node forwarding and data mule motion plan. Our implementation and simulation results using ns2 show nearly monotonic decrease of data delivery latency when each node can use more energy, thus vastly increasing the flexibility in the energy-latency trade-off for sensor network communications.},
keywords={motion control;path planning;scheduling;telecommunication control;wireless sensor networks;energy-latency trade-off optimisation;mobility control;path planning;speed planning;multihop forwarding;energy consumption reduction;base station;data mule scheduling;data mule motion planning;node-to-node forwarding;ns2;sensor network communications;Delay;Peer to peer computing;Energy consumption;Unmanned aerial vehicles;Base stations;Communication system control;Monitoring;Spread spectrum communication;Algorithm design and analysis;Communications Society},
doi={10.1109/INFCOM.2009.5062188},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062189,
author={L. Pan and H. Wu},
booktitle={IEEE INFOCOM 2009},
title={Smart Trend-Traversal: A Low Delay and Energy Tag Arbitration Protocol for Large RFID Systems},
year={2009},
volume={},
number={},
pages={2571-2575},
abstract={We propose a Smart Trend-Traversal (STT) protocol for RFID tag arbitration, which effectively reduces the collision overhead occurred in the arbitration process. STT, a query tree-based scheme, dynamically issues queries according to the online learned tag density and distribution; and therefore, it significantly reduces delay and energy consumption comparing with the existing tree-based and aloha-based protocols. Our analytic studies further show that the optimality of STT does not rely on any presumed network conditions, which is in sharp contrast to other available schemes and renders it a highly desirable and practical solution.},
keywords={protocols;radiofrequency identification;trees (mathematics);smart trend-traversal;RFID tag arbitration;query tree-based scheme;Protocols;Radiofrequency identification;Energy consumption;Large-scale systems;Degradation;Communications Society;Delay effects;RFID tags;Inventory management;Real time systems},
doi={10.1109/INFCOM.2009.5062189},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062190,
author={A. Alamdar Yazdi and S. Sorour and S. Valaee and R. Y. Kim},
booktitle={IEEE INFOCOM 2009},
title={Optimum Network Coding for Delay Sensitive Applications in WiMAX Unicast},
year={2009},
volume={},
number={},
pages={2576-2580},
abstract={MAC layer random network coding (MRNC) was proposed as an alternative to HARQ for reliable data transmission in WiMAX unicast. It has been shown that MRNC achieves a higher transmission efficiency than HARQ as it avoids the problem of ACK/NAK packet overhead and the additional redundancy resulting from their loss. However, it did not address the problem of restricting the number of transmissions to an upper bound which is important for delay sensitive applications. In this paper, we investigate a more structured MAC layer coding scheme that achieves the optimum performance in the delay sensitive traffic context while achieving the same overhead level as MRNC. We first formulate the delay sensitive traffic satisfaction, in such an environment, as a minimax optimization problem over all possible coding schemes. We then show that the MAC layer systematic network coding (MSNC), which transmits the packets once uncoded and employs random network coding for retransmissions, achieves the optimum performance for delay sensitive applications while achieving the same overhead level as MRNC.},
keywords={data communication;random codes;telecommunication network reliability;WiMax;optimum network coding;delay sensitive applications;WiMAX unicast;MAC layer random network coding;HARQ;reliable data transmission;MAC layer systematic network coding;Network coding;WiMAX;Unicast;Delay;Telecommunication traffic;Data communication;Redundancy;Propagation losses;Upper bound;Minimax techniques},
doi={10.1109/INFCOM.2009.5062190},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062191,
author={R. Serral-Gracia and Y. Labit and J. Domingo-Pascual and P. Owezarski},
booktitle={IEEE INFOCOM 2009},
title={Towards an Efficient Service Level Agreement Assessment},
year={2009},
volume={},
number={},
pages={2581-2585},
abstract={On-line end-to-end Service Level Agreement (SLA) monitoring is of key importance nowadays. For this purpose, past recent researches focused on measuring (when possible) or estimating (most of the times) network QoS or performance parameters. Up to now, attempts to provide accurate techniques for estimating such parameters have failed. In addition, live reporting of the estimated network status requires a huge amount of resources, and lead to unscalable systems. The originality of the contribution presented in this paper, relies on the statement that the accurate estimation of network QoS parameters is absolutely not required in most cases: specifically it is sufficient to be aware of service disruptions, i.e. when the QoS provided by the network collapses. For this purpose, we propose an algorithm for disruption detection of network services. The proposed solution is based on the use of the well-known Kullback-Leibler Divergence algorithm. More specifically, we work on simple to measure time series, i.e. received inter-packet arrival times. In addition of efficiently detecting network QoS disruptions, the algorithm, also drastically reduces the required resources, and the overhead produced by the traffic collection for scalable SLA monitoring systems. The validity of the proposal is verified both in terms of accuracy and consumed resources in a real testbed, using different traffic profiles.},
keywords={quality of service;telecommunication network management;time series;efficient service level agreement assessment;online end-to-end service level agreement monitoring;network QoS parameters;unscalable systems;network collapses;network services;Kullback-Leibler divergence algorithm;time series;inter-packet arrival times;traffic collection;Quality of service;Telecommunication traffic;Computer networks;Delay estimation;State estimation;Time measurement;Proposals;Testing;Communications Society;Broadband communication},
doi={10.1109/INFCOM.2009.5062191},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062192,
author={N. Sastry and K. Sollins and J. Crowcroft},
booktitle={IEEE INFOCOM 2009},
title={Delivery Properties of Human Social Networks},
year={2009},
volume={},
number={},
pages={2586-2590},
abstract={The recently proposed packet switched network paradigm takes advantage of human social contacts to opportunistically create data paths over time. Our goal is to examine the effect of the human contact process on data delivery. We find that the contact occurrence distribution is highly uneven: contacts between a few node-pairs occur too frequently, leading to inadequate mixing in the network, while the majority of contacts are rare, and essential for connectivity. This distribution of contacts leads to a significant variation in performance over short time windows. We discover that the formation of a large clique core during the window is correlated with the fraction of data delivered, as well as the speed of delivery. We then show that the clustering co-efficient of the contact graph over a time window is a good predictor of performance during the window. Taken together, our findings suggest new directions for designing forwarding algorithms in ad-hoc or delay-tolerant networking schemes using humans as data mules.},
keywords={ad hoc networks;packet switching;social networking (online);delivery property;human social network;packet switched network;human social contact;data path;data delivery;contact occurrence distribution;network connectivity;contact graph;ad-hoc network;delay-tolerant networking;Peer to peer computing;Evolution (biology);Humans;Data mining;Ad hoc networks;Delay},
doi={10.1109/INFCOM.2009.5062192},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062193,
author={J. Wu and B. Li},
booktitle={IEEE INFOCOM 2009},
title={Keep Cache Replacement Simple in Peer-Assisted VoD Systems},
year={2009},
volume={},
number={},
pages={2591-2595},
abstract={Peer-assisted Video-on-Demand (VoD) systems have not only received substantial recent research attention, but also been implemented and deployed with success in large-scale real- world streaming systems, such as PPLive. Peer-assisted Video- on-Demand systems are designed to take full advantage of peer upload bandwidth contributions with a cache on each peer. Since the size of such a cache on each peer is limited, it is imperative that an appropriate cache replacement algorithm is designed. There exists a tremendous level of flexibility in the design space of such cache replacement algorithms, including the simplest alternatives such as Least Recently Used (LRU). Which algorithm is the best to minimize server bandwidth costs, so that when peers need a media segment, it is most likely available from caches of other peers? Such a question, however, is arguably non-trivial to answer, as both the demand and supply of media segments are stochastic in nature. In this paper, we seek to construct an analytical framework based on optimal control theory and dynamic programming, to help us form an in-depth understanding of optimal strategies to design cache replacement algorithms. With such analytical insights, we have shown with extensive simulations that, the performance margin enjoyed by optimal strategies over the simplest algorithms is not substantial, when it comes to reducing server bandwidth costs. In most cases, the simplest choices are good enough as cache replacement algorithms in peer-assisted VoD systems.},
keywords={cache storage;peer-to-peer computing;real-time systems;video on demand;video servers;peer-assisted VoD systems;peer-assisted video-on-demand systems;real- world streaming systems;PPLive;peer upload bandwidth contributions;cache replacement algorithm;least recently used;server bandwidth;media segment;optimal control theory;dynamic programming;Algorithm design and analysis;Bandwidth;Large-scale systems;Streaming media;Costs;Stochastic processes;Optimal control;Dynamic programming;Performance analysis;Analytical models},
doi={10.1109/INFCOM.2009.5062193},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062194,
author={Y. Tang and E. Al-Shaer},
booktitle={IEEE INFOCOM 2009},
title={Overlay Fault Diagnosis Based on Evidential Reasoning},
year={2009},
volume={},
number={},
pages={2596-2600},
abstract={The attractive characteristics of overlay networks bring to overlay fault diagnosis new challenges, which include inaccessible underlying network information, incomplete and inaccurate network status observations, dynamic symptom-fault causality relationship, and multi-layer complexity. To address these challenges, we propose a novel evidential reasoning based overlay fault diagnosis technique called ERD. Firstly, by analyzing end-user observed network symptoms, ERD narrows down suspicious components, and investigates their status (i.e., good or bad) with likelihood measurement and uncertainty evaluation using a novel evidence-driven belief function. Next, ERD adapts to the changes in highly dynamic overlay networks by dynamically constructing plausible fault diagnosis graph based on belief evaluation. Finally, ERD conducts plausible fault reasoning to locate the root causes of observed network symptoms.},
keywords={case-based reasoning;fault diagnosis;Internet;telecommunication computing;overlay fault diagnosis;evidential reasoning;inaccessible underlying network information;inaccurate network status observations;incomplete network status observations;dynamic symptom-fault causality relationship;multilayer complexity;end-user observed network symptoms;evidence-driven belief function;plausible fault diagnosis graph;plausible fault reasoning;Internet;Fault diagnosis;Bayesian methods;Tomography;IP networks;Monitoring;USA Councils;Measurement uncertainty;Large-scale systems;Communications Society;Information technology},
doi={10.1109/INFCOM.2009.5062194},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062195,
author={D. T. -. Kao and A. Sabharwal},
booktitle={IEEE INFOCOM 2009},
title={Impact of Network Topology Knowledge on Fairness: A Geometric Approach},
year={2009},
volume={},
number={},
pages={2601-2605},
abstract={In this paper, we examine how the precision of network topology knowledge impacts the achievable degree of max-min fairness. We focus on time-division multiple access (TDMA) networks, and employ a model based on physical-layer events that sufficiently describes the topology effects with respect to TDMA. Using Jain's fairness index, our key contribution is a characterization of the fairness loss resulting from allocation of resources (in our case time-divisions) based on imprecise knowledge of topology. We find loss is more pronounced when a single link has low signal-to-noise ratio (SNR); i.e. links which have poor throughput also make the allocation more unfair. Conversely, our analysis suggests that if the relative error in estimating link qualities is identical for all links in the network, no one link dominates the fairness loss.},
keywords={geometry;minimax techniques;resource allocation;subscriber loops;telecommunication network topology;time division multiple access;network topology knowledge;geometric approach;max-min fairness;time-division multiple access networks;physical-layer events;Jain's fairness index;resource allocation;link quality estimation;Network topology;Throughput;Resource management;Time division multiple access;Wireless networks;Signal to noise ratio;Protocols;Communications Society;Iterative methods;Costs},
doi={10.1109/INFCOM.2009.5062195},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062196,
author={B. Wu and P. -. Ho and K. L. Yeung and J. Tapolcai and H. T. Mouftah},
booktitle={IEEE INFOCOM 2009},
title={CFP: Cooperative Fast Protection},
year={2009},
volume={},
number={},
pages={2606-2610},
abstract={We introduce cooperative fast protection (CFP) as a novel protection scheme in WDM networks. CFP achieves capacity-efficient fast protection with the features of node-autonomy and failure-independency. It differs from p-cycle by reusing the released working capacity of the disrupted lightpaths (i.e. stubs) in a cooperative manner. This is achieved by allowing all the failure-aware nodes to switch the traffic, such that the disrupted lightpaths can be protected even if the end nodes of the failed link are not on the protecting cycles. CFP also differs from FIPP p-cycle by not requiring the source node of the disrupted lightpath on the protecting cycle. By jointly optimizing both working and spare capacity placement, we formulate an ILP for CFP design. Numerical results show that CFP significantly outperforms p-cycle by achieving faster protection with much higher capacity efficiency.},
keywords={integer programming;linear programming;wavelength division multiplexing;cooperative fast protection;WDM network;ILP;integer linear programming;wavelength division multiplexing;Protection;Peer to peer computing;Switches;Optical network units;WDM networks;Communications Society;Telecommunication traffic;Design optimization;Wavelength division multiplexing;Informatics},
doi={10.1109/INFCOM.2009.5062196},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062197,
author={S. -. Lee and I. Pefkianakis and A. Meyerson and S. Xu and S. Lu},
booktitle={IEEE INFOCOM 2009},
title={Proportional Fair Frequency-Domain Packet Scheduling for 3GPP LTE Uplink},
year={2009},
volume={},
number={},
pages={2611-2615},
abstract={With the power consumption issue of mobile handset taken into account, single-carrier FDMA (SC-FDMA) has been selected for 3GPP long-term evolution (LTE) uplink multiple access scheme. Like in OFDMA downlink, it enables multiple users to be served simultaneously in uplink as well. However, its single carrier property requires that all the subcarriers allocated to a single user must be contiguous in frequency within each time slot. This contiguous allocation constraint limits the scheduling flexibility, and frequency-domain packet scheduling algorithms in such system need to incorporate this constraint while trying to maximize their own scheduling objectives. In this paper we explore this fundamental problem of LTE SC-FDMA uplink scheduling by adopting the conventional time-domain proportional fair algorithm to maximize its objective (i.e. proportional fair criteria) in the frequency-domain setting. We show the NP-hardness of the frequency-domain scheduling problem under this contiguous allocation constraint and present a set of practical algorithms fine tuned to this problem. We demonstrate that competitive performance can be achieved in terms of system throughput as well as fairness perspective, which is evaluated using 3GPP LTE system model simulations.},
keywords={3G mobile communication;communication complexity;frequency division multiple access;frequency-domain analysis;packet radio networks;radio links;time-domain analysis;proportional fair frequency-domain packet scheduling;power consumption issue;mobile handset;single-carrier FDMA;3GPP long-term evolution uplink;multiple access scheme;contiguous allocation constraint;frequency-domain packet scheduling algorithms;conventional time-domain proportional fair algorithm;frequency-domain setting;NP-hardness;Scheduling algorithm;Base stations;Bandwidth;Peak to average power ratio;Time domain analysis;Frequency domain analysis;Energy consumption;Mobile handsets;Frequency division multiaccess;Downlink},
doi={10.1109/INFCOM.2009.5062197},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062198,
author={J. Teng and C. Xu and W. Jia and D. Xuan},
booktitle={IEEE INFOCOM 2009},
title={D-Scan: Enabling Fast and Smooth Handoffs in AP-Dense 802.11 Wireless Networks},
year={2009},
volume={},
number={},
pages={2616-2620},
abstract={802.11 wireless networks have gained ever greater popularity nowadays. Apart from static wireless connections, people begin to expect more user-friendly features from this kind of networks, such as support for seamless roaming. In this paper, we study the handoff process in large AP-dense 802.11 networks, which is one of the most common forms of WiFi under usage. A series of field experiments are carried out and some critical handoff parameters are evaluated. With some newly discovered features, i.e. differentiated probe response time and rich AP information hidden in wireless traffic, we have managed to significantly improve the essential process of AP scan, a bottleneck towards fast and smooth handoffs. The solution is collectively called D-Scan (Scan in AP-Dense 802.11 networks). Real experiments are conducted to show the superiority of our solution.},
keywords={telecommunication traffic;wireless LAN;D-Scan;AP-Dense 802.11 wireless networks;static wireless connections;seamless roaming;handoff process;WiFi;handoff parameters;probe response time;wireless traffic;Wireless networks;Probes;Delay;Telecommunication traffic;Roaming;Large-scale systems;Communications Society;USA Councils;Cities and towns;Government},
doi={10.1109/INFCOM.2009.5062198},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062199,
author={S. Lakshmanan and K. Sundaresan and R. Kokku and A. Khojestepour and S. Rangarajan},
booktitle={IEEE INFOCOM 2009},
title={Towards Adaptive Beamforming in Indoor Wireless Networks: An Experimental Approach},
year={2009},
volume={},
number={},
pages={2621-2625},
abstract={Several research works have argued that adaptive beamforming has the potential to realize the high spectral efficiency requirements of next-generation wireless standards, and is especially well-suited for multipath-rich environments such as indoors. Most works have been limited to theory; few works in literature address the practical benefits and realizability of adaptive beamforming. In this paper, we design and implement the first indoor WLAN beamforming system with multi-element array antennas and software radio platforms, that forms a testbed for exploration of practical benefits of beamforming, and evaluation of algorithms for efficient beamforming in diverse environments. In the process of building the system, we identify and address several challenges with practical beamforming that are often ignored in theoretical works. Most importantly, channel estimation for forming the best beam to a user is hindered by oscillator drifts on the transmitter and receiver side that introduce hard-to-isolate phase and frequency offsets from the estimated channel coefficients. We describe these issues and incorporate novel solutions in our system to address them without requiring hardware modifications. We use the system to demonstrate the realizable benefits of adaptive beamforming in a typical indoor office environment.},
keywords={antenna arrays;indoor radio;software radio;wireless LAN;adaptive beamforming;indoor wireless networks;next-generation wireless standards;indoor WLAN beamforming system;multielement array antennas;software radio;channel estimation;Array signal processing;Wireless networks;Frequency estimation;Algorithm design and analysis;Wireless LAN;Antenna arrays;Software radio;Software testing;System testing;Software algorithms},
doi={10.1109/INFCOM.2009.5062199},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062200,
author={T. Bonald and A. Ibrahim and J. Roberts},
booktitle={IEEE INFOCOM 2009},
title={Enhanced Spatial Reuse in Multi-Cell WLANs},
year={2009},
volume={},
number={},
pages={2626-2630},
abstract={When IEEE 802.11 access points (APs) share the same channel in a multi-cell WLAN, their downlink transmissions can interfere. Typically, an AP whose scheduled transmission to some user is blocked by another cell will apply the CSMA/CA back-off algorithm and continue to make reattempts to the same user. Through analytical models and simulations, we demonstrate that significant capacity gains can be attained by choosing an alternative destination for the reattempt. Results demonstrate that a simple random choice of alternative destination brings almost the same gain as a more sophisticated algorithm that seeks to maximize spatial reuse.},
keywords={carrier sense multiple access;wireless LAN;multicell WLAN;IEEE 802.11 access point;CSMA/CA back-off algorithm;Traffic control;Telecommunication traffic;Scheduling algorithm;Analytical models;Wireless LAN;Downlink;Multiaccess communication;Stochastic processes;Communication system traffic control;Communications Society},
doi={10.1109/INFCOM.2009.5062200},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062201,
author={E. J. Rosensweig and J. Kurose},
booktitle={IEEE INFOCOM 2009},
title={Breadcrumbs: Efficient, Best-Effort Content Location in Cache Networks},
year={2009},
volume={},
number={},
pages={2631-2635},
abstract={For several years, web caching has been used to meet the ever-increasing Web access loads. A fundamental capability of all such systems is that of inter-cache coordination, which can be divided into two main types: explicit and implicit coordination. While the former allows for greater control over resource allocation, the latter does not suffer from the additional communication overhead needed for coordination. In this paper, we consider a network in which each router has a local cache that caches files passing through it. By additionally storing minimal information regarding caching history, we develop a simple content caching, location, and routing systems that adopts an implicit, transparent, and best-effort approach towards caching. Though only best effort, the policy outperforms classic policies that allow explicit coordination between caches.},
keywords={cache storage;Internet;resource allocation;telecommunication network routing;breadcrumbs;content location;cache networks;Web caching;Web access loads;inter-cache coordination;resource allocation;caching history;routing systems;Routing;Computer science;Resource management;Communications Society;Communication system control;History;Costs;Protocols;Topology;Content based retrieval},
doi={10.1109/INFCOM.2009.5062201},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062202,
author={E. Altman and I. Menache and A. Ozdaglar},
booktitle={IEEE INFOCOM 2009},
title={Noncooperative Load Balancing in the Continuum Limit of a Dense Network},
year={2009},
volume={},
number={},
pages={2636-2640},
abstract={In transportation network research, the main approach for predicting traffic distribution due to noncooperative vehicle choices has been through fluid type models. The basic model considers a continuum of infinitesimal "non-atomic" vehicles, each seeking the shortest path to its destination. The resulting equilibrium turns out to be much simpler to characterize in comparison to the finite-vehicle case, yet provides a good approximation to the latter. A less familiar fluid-type model uses a continuum limit for the network topology. The limit network is a continuum plane which inherits its cost structure from the original network, and the corresponding equilibrium is identified as the continuum traffic equilibrium. This paper considers a similar equilibrium notion in a framework of a load balancing problem involving two processors, each requiring non-negligible workload (or "flow") to be handled by network resources. Besides a congestion cost at each resource (which is identical to both processors), each resource induces a processor-dependent connection cost, which is a function of its geographic location. The processors autonomously route their flow onto the different resources, with the objective of minimizing (non-cooperatively) their total cost. Assuming that the number of resources is relatively large, we apply the continuum approximation within a line (or bus) topology and study the Nash equilibria of the processor interaction. This approximation enables us to explicitly characterize the equilibrium in several cases and to obtain insights on its structure, including tight bounds on the efficiency loss due to noncooperation.},
keywords={game theory;resource allocation;telecommunication network topology;noncooperative load balancing;dense network continuum limit;transportation network;traffic distribution prediction;noncooperative vehicle choices;finite-vehicle;network topology;limit network;continuum traffic equilibrium;processor-dependent connection cost;continuum approximation;line topology;bus topology;Nash equilibria;continuum noncooperative game;Load management;Routing;Vehicles;Telecommunication network topology;Telecommunication traffic;Traffic control;Cost function;Communications Society;Laboratories;Transportation},
doi={10.1109/INFCOM.2009.5062202},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062203,
author={R. Cohen and A. Landau},
booktitle={IEEE INFOCOM 2009},
title={"Not All At Once!" - A Generic Scheme for Estimating the Number of Affected Nodes While Avoiding Feedback Implosion},
year={2009},
volume={},
number={},
pages={2641-2645},
abstract={We present a generic scheme for estimating the size of a group of nodes affected by the same event in a large-scale network, such as a grid, a sensor network or a wireless broadband access network, while receiving only a small number of feedback messages from this group. Using the proposed scheme, a centralized gateway analyzes the transmission times of these feedback messages, defines a likelihood function for them, and then uses the Newton-Raphson method to find the number of affected nodes for which this function is maximized. We present complete mathematical analysis for the precision of the proposed algorithm and provide tight upper and lower bounds for the estimation error. These bounds allow us to improve the precision of our estimation, and to bring the error very close to 0.},
keywords={estimation theory;feedback;Newton-Raphson method;wireless sensor networks;generic scheme;feedback implosion;large scale network;sensor network;wireless broadband access network;feedback message;centralized gateway;likelihood function;Newton-Raphson method;mathematical analysis;estimation error;Feedback;Peer to peer computing;Wireless sensor networks;Estimation error;Broadcasting;Newton method;Mathematical analysis;Communications Society;Computer science;Clustering algorithms},
doi={10.1109/INFCOM.2009.5062203},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062204,
author={Y. Bejerano and Q. Dong},
booktitle={IEEE INFOCOM 2009},
title={Distributed Construction of Fault Resilient High Capacity Wireless Networks with Bounded Node Degree},
year={2009},
volume={},
number={},
pages={2646-2650},
abstract={Multi-hop wireless networks using directional antennas are increasingly deployed for various applications such as wireless backhaul. In such systems, each node is equipped with a few directional antennas. Each antenna is exclusively used for establishing a point-to-point link with another antenna on some neighboring node. Thus in the constructed network, each node cannot have more links than its number of installed antennas, which defines a degree bound on all nodes. Hence, a practically important problem is to construct fault resilient and high capacity networks with bounded node degree. The main contribution of this paper is two-fold. First, we propose a localized algorithm for building fault resilient wireless networks that contain the path with highest possible capacity for every pair of nodes, not only in normal cases but also in the presence of any node/link failure. Second, we give an algorithm for building fault resilient wireless networks with the lowest possible degree bound.},
keywords={directive antennas;fault tolerance;mobile antennas;mobile radio;radio links;fault resilient wireless network;high capacity wireless network;bounded node degree;multihop wireless network;directional antenna;wireless backhaul;point-to-point link;constructed network;localized algorithm;Wireless networks;Directional antennas;Peer to peer computing;Spread spectrum communication;Buildings;Wireless mesh networks;Network topology;Communications Society;Computer science;Costs},
doi={10.1109/INFCOM.2009.5062204},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062205,
author={J. F. Perez and B. Van Houdt},
booktitle={IEEE INFOCOM 2009},
title={Dimensioning an OBS Switch with Partial Wavelength Conversion and Fiber Delay Lines via a Mean Field Model},
year={2009},
volume={},
number={},
pages={2651-2655},
abstract={In this paper we introduce a mean field model to analyze an optical switch equipped with both wavelength converters (WCs) and fiber delay lines (FDLs) to resolve contention in OBS networks. Under some very general conditions, that is, a general burst size distribution and any Markovian burst arrival process at each wavelength, this model determines the minimum number of WCs required to achieve a zero loss rate as the number of wavelengths becomes large. The mean field result is exact as the number of wavelengths goes to infinity and turns out to be very accurate for systems with (a few) hundred wavelengths, commonly occurring when using wavelength division multiplexing (WDM). Moreover, we show that if the number of WCs is underdimensioned, (i) periodic system behavior may occur (with the period being the greatest common divisor of the burst lengths) and (ii) increasing the number of WCs may even worsen the loss rate under the often studied minimum horizon allocation policy (as opposed to the minimum gap policy). Finally, we further demonstrate that in terms of the loss rate, including (more) FDLs may have little or no effect on the number of WCs required to achieve a near-zero loss, especially for higher loads.},
keywords={convertors;optical burst switching;optical fibre networks;optical switches;wavelength division multiplexing;partial wavelength conversion;fiber delay lines;mean field model;wavelength converters;optical switch;Markovian burst arrival process;wavelength division multiplexing;minimum horizon allocation policy;optical burst switching;Delay lines;Optical wavelength conversion;Wavelength division multiplexing;Optical switches;Optical buffering;Optical fibers;Optical losses;Optical fiber networks;Mathematical model;Performance analysis},
doi={10.1109/INFCOM.2009.5062205},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062206,
author={X. Liu and C. Qiao and T. Wang},
booktitle={IEEE INFOCOM 2009},
title={Application-Specific, Agile and Private (ASAP) Platforms for Federated Computing Services over WDM Networks},
year={2009},
volume={},
number={},
pages={2656-2660},
abstract={Under the emerging paradigm of federated computing services (FCS), one can create multiple virtual infrastructures (VI), one for each distributed computing application or service offering. Each VI may consist of a number of geographically distributed computing clusters that are connected with a set of dedicated circuits. In general, a VI submitted by a user to the FCS provider is unmapped at the time of the submission in that the user does not specify which computing clusters to use. The primary challenge to the FCS provider in supporting these novel applications is to establish a VI optimally over the substrate network such as a WDM network connecting many computing clusters. In this paper, we study the optimization problems of jointly allocating computing and wavelength resources for establishing Vis. First, we devise a branch and bound algorithm based on the decomposition and Lagrangian relaxation techniques to obtain the exact optimal solution with the objective being the minimization of the resource leasing cost of a VI. Second, we propose efficient heuristics to deal with a large number of online requests for Vis and compare their performance with the optimal solution.},
keywords={minimisation;relaxation theory;resource allocation;tree searching;wavelength division multiplexing;workstation clusters;application-specific platform;agile platform;private platform;federated computing services;WDM networks;virtual infrastructures;distributed computing application;geographically distributed computing clusters;optimization problems;branch and bound algorithm;decomposition technique;Lagrangian relaxation technique;resource leasing cost minimization;heuristics;joint allocation;Computer networks;WDM networks;Distributed computing;Circuits;Joining processes;Resource management;Clustering algorithms;Lagrangian functions;Minimization methods;Cost function},
doi={10.1109/INFCOM.2009.5062206},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062207,
author={R. Maheshwari and J. Cao and S. R. Das},
booktitle={IEEE INFOCOM 2009},
title={Physical Interference Modeling for Transmission Scheduling on Commodity WiFi Hardware},
year={2009},
volume={},
number={},
pages={2661-2665},
abstract={The demand for capacity in WiFi networks is driving a new look at transmission scheduling-based link layers. One basic issue here is the use of accurate interference models to drive transmission scheduling algorithms. However, experimental work in this space has been limited. In this work, we use commodity WiFi hardware (specifically, 802.11a) for a comprehensive study of interference modeling for transmission scheduling on a mesh network setup. We focus on the well-known physical interference model for its realism. We propose use of the "graded" version of the model where feasibility of a link is probabilistic, as opposed to using the more traditional "thresholded" version, where feasibility is binary. We show experimentally that the graded model is significantly more accurate (80 percentile error 0.2 vs. 0.55 for thresholded model). We develop transmission scheduling experiments using greedy scheduling algorithms for the evacuation model for both interference models. We also develop similar experiments for optimal scheduling performance for the simplified one-shot scheduling. The scheduling experiments demonstrate clearly superior performance for the graded model, often by a factor of 2. We conclude by promoting use of this model for scheduling studies.},
keywords={greedy algorithms;radiofrequency interference;wireless LAN;physical interference modeling;transmission scheduling;commodity WiFi hardware;802.11a;greedy scheduling algorithm;Interference;Hardware;Optimal scheduling;Signal to noise ratio;Processor scheduling;Scheduling algorithm;Bit error rate;Physical layer;Computer science;Wireless networks},
doi={10.1109/INFCOM.2009.5062207},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062208,
author={J. Yu and Y. He and K. Wu and M. Tacca and A. Fumagalli and J. -. Vasseur},
booktitle={IEEE INFOCOM 2009},
title={A Queueing Model Framework of PCE-Based Inter-Area Path Computation},
year={2009},
volume={},
number={},
pages={2666-2670},
abstract={Path computation elements (PCE's) are used to compute end-to-end paths across multiple areas. Multiple PCE's may be dedicated to each area to provide sufficient path computation capacity and redundancy. An open problem is to which PCE to send the path computation request. This problem may be a non trivial problem if PCE's have uneven processing capacities. This paper presents a queueing model based on product form to estimate the latencies in path computation while accounting for the arrival rate of path computation requests. The model is used to find the PCE selection policy to minimize the overall expected latencies in path computation. Simulation studies demonstrate that the use of the simplistic product form approach yields reasonable approximations that are within up to 15% of the simulation results at practical offered loads.},
keywords={Internet;multiprotocol label switching;resource allocation;telecommunication traffic;interarea path computation;path computation elements;path computation capacity;path computation request;uneven processing capacities;queueing model;multiprotocol label switching;Computer networks;Tellurium;Delay;Multiprotocol label switching;Protocols;Computational modeling;Telecommunication traffic;Traffic control;Computer architecture;Communications Society},
doi={10.1109/INFCOM.2009.5062208},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062209,
author={C. -. Chen and C. Chekuri and D. Klabjan},
booktitle={IEEE INFOCOM 2009},
title={Topology Formation for Wireless Mesh Network Planning},
year={2009},
volume={},
number={},
pages={2671-2675},
abstract={In this paper, we propose greedy selection rounding (GSR), an efficient and near-optimal algorithm to design a wireless mesh network topology that maximizes the coverage of the users while ensuring that the network is resilient to node failures and and the deployment cost is under a given budget. In the case that GSR fails to find a solution satisfying the budget constraint, the incurred cost does not exceed the budget by a constant factor. Through extensive evaluation, we show that in all our test cases GSR always generates a topology above 95% of the optimal in terms of the number of covered users while never exceeding the budget by more than 15%.},
keywords={greedy algorithms;radio networks;telecommunication network planning;telecommunication network topology;topology formation;wireless mesh network planning;greedy selection rounding;near-optimal algorithm;network resilience;node failure;deployment cost;Network topology;Wireless mesh networks;Interference;Costs;Robustness;Communications Society;Urban planning;Computer science;Industrial engineering;Computer network management},
doi={10.1109/INFCOM.2009.5062209},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062210,
author={M. A. Ismail and W. Dabbous and A. Clerget},
booktitle={IEEE INFOCOM 2009},
title={A Multi-Burst Sliding Encoding for Mobile Satellite TV Broadcasting},
year={2009},
volume={},
number={},
pages={2676-2680},
abstract={Protection of data against long fading time is one of the greatest challenges posed by a satellite delivery system offering multimedia services to mobile devices like DVB-SH. To deal with this challenge several enhancements and modifications of the existing terrestrial mobile TV (DVB-H) are being considered. These solutions provide the required protection depth but they don't take into account the specificity of mobile handheld devices such as power consumption, memory constraints and chipsets implementation costs. In this paper, we propose an innovative algorithm (called Multi Burst Sliding Encoding or MBSE) that extends the DVB-H intra-burst (MPE-FEC) protection to an inter-burst protection so that complete burst losses could be recovered while taking into account the specificity of mobile handheld devices. Based on a clever organisation of the data, our algorithm allows to provide protection against long term fading while still using RS code implemented in DVB-H chipsets. We evaluate the performance of MBSE by both theoretical analysis as well as intensive simulations and experiments. The results also show good performance in terms of protection, battery and memory saving. The MBSE is now under standardisation and it is considered by the DVB Forum as the main solution for the DVB-SH class terminals.},
keywords={digital video broadcasting;direct broadcasting by satellite;mobile handsets;mobile television;multimedia communication;Reed-Solomon codes;telecommunication security;multiburst sliding encoding;mobile satellite TV broadcasting;data protection;fading time;satellite delivery system;multimedia services;mobile devices;DVB-SH;terrestrial mobile TV;mobile handheld devices;power consumption;memory constraints;chipsets implementation costs;innovative algorithm;DVB-H intra-burst protection;MPE-FEC protection;inter-burst protection;RS code;DVB-H chipsets;Encoding;Satellite broadcasting;TV broadcasting;Digital video broadcasting;Fading;Handheld computers;Multimedia systems;Mobile TV;Power system protection;Energy consumption},
doi={10.1109/INFCOM.2009.5062210},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062211,
author={R. Gandhi and Y. -. Kim and S. Lee and J. Ryu and P. -. Wan},
booktitle={IEEE INFOCOM 2009},
title={Approximation Algorithms for Data Broadcast in Wireless Networks},
year={2009},
volume={},
number={},
pages={2681-2685},
abstract={Broadcasting is a fundamental operation in wireless networks and plays an important role in the communication protocol design. In multihop wireless networks, however, interference at a node due to simultaneous transmissions from its neighbors makes it non-trivial to design a minimum-latency broadcast algorithm, which is known to be NP-complete. We present a simple 12-approximation algorithm for the one-to-all broadcast problem that improves all previously known guarantees for this problem. We then consider the all-to-all broadcast problem where each node sends its own message to all other nodes. For the all-to-all broadcast problem, we present two algorithms with approximation ratios of 20 and 34, improving the best result available in the literature. Finally, we report experimental evaluation of our algorithms. Our studies indicate that our algorithms perform much better in practice than the worst-case guarantees provided in the theoretical analysis and achieve up to 37% performance improvement over existing schemes.},
keywords={data communication;optimisation;radio networks;approximation algorithm;data broadcast;broadcasting;communication protocol design;multihop wireless networks;simultaneous transmissions;minimum latency broadcast algorithm;NP-complete;one-to-all broadcast problem;all-to-all broadcast problem;worst-case guarantee;Approximation algorithms;Broadcasting;Wireless networks;Delay;Computer science;Wireless application protocol;Algorithm design and analysis;Spread spectrum communication;Interference;Peer to peer computing},
doi={10.1109/INFCOM.2009.5062211},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062212,
author={S. Guo and M. Guo and V. Leung},
booktitle={IEEE INFOCOM 2009},
title={Exploring the Multicast Lifetime Capacity of WANETs with Directional Multibeam Antennas},
year={2009},
volume={},
number={},
pages={2686-2690},
abstract={We explore the multicast lifetime capacity of energy-limited wireless ad hoc networks using directional multibeam antennas by formulating and solving the corresponding optimization problem. In such networks, each node is equipped with a practical smart antenna array that can be configured to support multiple beams with adjustable orientation and beamwidth. The special case of this optimization problem in networks with single beams have been extensively studied and shown to be NP-hard. In this paper, we provide a globally optimal solution to this problem by developing a general MILP formulation that can apply to various configurable antenna models, many of which are not supported by the existing formulations. The multicast lifetime capacity is then quantitively studied by simulations. The experimental results show that using two-beam antennas can exploit most lifetime capacity of the networks for multicast communications.},
keywords={ad hoc networks;adaptive antenna arrays;computational complexity;directive antennas;multicast communication;multifrequency antennas;optimisation;multicast lifetime capacity;directional multibeam antennas;energy-limited wireless ad hoc networks;optimization problem;smart antenna array;beamwidth;NP-hard problem;multicast communications;Directive antennas;Directional antennas;Peer to peer computing;Mobile ad hoc networks;Antenna arrays;Batteries;Costs;Heuristic algorithms;Communications Society;Computer science},
doi={10.1109/INFCOM.2009.5062212},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062213,
author={N. Mitton and D. Simplot-Ryl and I. Stojmenovic},
booktitle={IEEE INFOCOM 2009},
title={Guaranteed Delivery for Geographical Anycasting in Wireless Multi-Sink Sensor and Sensor-Actor Networks},
year={2009},
volume={},
number={},
pages={2691-2695},
abstract={In the anycasting problem, a sensor wants to report event information to one of sinks or actors. We describe the first localized anycasting algorithms that guarantee delivery for connected multi-sink sensor-actor networks. Let S(x) be the closest actor/sink to sensor x, and |xS(x)| be distance between them. In greedy phase, a node s forwards the packet to its neighbor v that minimizes the ratio of cost cost(|sv|) of sending packet to v (here we specifically apply hop-count and power consumption metrics) over the reduction in distance (|sS(s)|-|vS(v)|) to the closest actor/sink. A variant is to forward to the first neighbor on the shortest weighted path toward v. If none of neighbors reduces that distance then recovery mode is invoked. It is done by face traversal toward the nearest connected actor/sink, where edges are replaced by paths optimizing given cost. A hop count based and two variants of localized power aware anycasting algorithms are described. We prove guaranteed delivery property analytically and experimentally.},
keywords={telecommunication network routing;wireless sensor networks;geographical anycasting;wireless multisink sensor network;sensor-actor network;packet forwarding;power consumption metrics;hop count;localized power aware anycasting;delivery property;Wireless sensor networks;Energy consumption;Routing protocols;Wireless networks;Costs;Unicast;Wireless application protocol;Communications Society;Europe;Peer to peer computing},
doi={10.1109/INFCOM.2009.5062213},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062214,
author={Y. Bejerano and P. V. Koppol},
booktitle={IEEE INFOCOM 2009},
title={Optimal Construction of Redundant Multicast Trees in Directed Graphs},
year={2009},
volume={},
number={},
pages={2696-2700},
abstract={In this paper, we consider the problem of protection of multicast connections against single link or node failures using redundant multicast trees (RMTs). RMTs connect the source node of a multicast connection to all its destinations in such a way that in the event of a single link or node failure in the network, every destination node is still connected to the source node in at least one of the two trees. Construction of RMTs has been studied extensively for undirected graphs, but not for directed graphs. Computing RMTs in directed graphs can be important in networks, for instance, where link capacity is available in one direction but not the other. Also, none of the schemes proposed in previous work provide a solution for finding optimal (minimal-cost) RMTs even for weighted undirected graphs. We show that a whole class of earlier schemes that are based on the concept of ear-decomposition cannot find optimal redundant trees in certain network topologies. To our knowledge, we are the first to consider the problem of finding optimal RMTs in directed graphs. We present a novel efficient scheme for construction of optimal RMTs in networks modeled as weighted undirected as well as directed graphs.},
keywords={directed graphs;multicast communication;trees (mathematics);redundant multicast trees;directed graph;link capacity;Tree graphs;Peer to peer computing;Protection;Ear;Costs;Computer networks;Communications Society;USA Councils;Network topology;Multimedia communication},
doi={10.1109/INFCOM.2009.5062214},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062215,
author={A. H. Rasti and M. Torkjazi and R. Rejaie and N. Duffield and W. Willinger and D. Stutzbach},
booktitle={IEEE INFOCOM 2009},
title={Respondent-Driven Sampling for Characterizing Unstructured Overlays},
year={2009},
volume={},
number={},
pages={2701-2705},
abstract={This paper presents Respondent-Driven Sampling (RDS) as a promising technique to derive unbiased estimates of node properties in unstructured overlay networks such as Gnutella. Using RDS and a previously proposed technique, namely Metropolized Random Walk (MRW) sampling, we examine the efficiency of estimating node properties in unstructured overlays and identify some of the key factors that determine the accuracy of sampling techniques. We evaluate the RDS and MRW techniques using simulation over a wide range of static and dynamic graphs as well as experiments over a widely deployed Gnutella network. Our study sheds light on how the connectivity structure among nodes and its dynamics affect the accuracy and efficiency of the two sampling techniques. Both techniques exhibit a rather similar performance over a wide range of scenarios. However, RDS significantly outperforms MRW when the overlay structure exhibits a combination of highly skewed node degrees and highly skewed (local) clustering coefficients.},
keywords={graph theory;peer-to-peer computing;random processes;sampling methods;respondent-driven sampling;unbiased estimate;node properties;unstructured overlay networks;metropolized random walk sampling;static graph;dynamic graph;Gnutella network;connectivity structure;clustering coefficient;Sampling methods;Peer to peer computing;Internet;Communications Society;Scalability;Crawlers;Large-scale systems;Degradation;Bandwidth},
doi={10.1109/INFCOM.2009.5062215},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062216,
author={M. Saxena and R. R. Kompella},
booktitle={IEEE INFOCOM 2009},
title={A Framework for Efficient Class-Based Scheduling},
year={2009},
volume={},
number={},
pages={2706-2710},
abstract={With an increasing requirement for network monitoring tools to classify traffic and track security threats, newer and efficient ways are needed for collecting traffic statistics and monitoring of network flows. However, traditional solutions based on random packet sampling treat all flows as equal and therefore, do not provide the flexibility required for these applications. In this paper, we propose a novel architecture called CLAMP that provides an efficient framework to implement size-based sampling. At the heart of CLAMP is a novel data structure called composite bloom filter (CBF) that consists of a set of bloom filters that work together to encapsulate various class definitions. In comparison to previous approaches that implement simple size-based sampling, our architecture requires substantially lower memory (upto 80x) and results in higher flow coverage (upto 8x more flows) under specific configurations.},
keywords={computer network management;data structures;pattern classification;sampling methods;statistical analysis;telecommunication network routing;telecommunication security;telecommunication traffic;class-based sampling;network flow monitoring tool;traffic classification;security threat tracking;traffic statistics;CLAMP architecture;size-based sampling;data structure;composite bloom filter;network routing;computer network management;Sampling methods;Telecommunication traffic;Filters;Clamps;Data structures;Computerized monitoring;Fluid flow measurement;Communications Society;Computer science;Intelligent networks},
doi={10.1109/INFCOM.2009.5062216},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062217,
author={A. Lall and M. Ogihara and J. Xu},
booktitle={IEEE INFOCOM 2009},
title={An Efficient Algorithm for Measuring Medium- to Large-Sized Flows in Network Traffic},
year={2009},
volume={},
number={},
pages={2711-2715},
abstract={It has been well recognized that identifying very large flows (i.e., elephants) in a network traffic stream is important for a variety of network applications ranging from traffic engineering to anomaly detection. However, we found that many of these applications have an increasing need to monitor not only the few largest flows (say top 20), but also all of the medium-sized flows (say top 20,000). Unfortunately, existing techniques for identifying elephant flows at high link speeds are not suitable and cannot be trivially extended for identifying the medium-sized flows. In this work, we propose a hybrid SRAM/DRAM algorithm for monitoring all elephant and medium-sized flows with strong accuracy guarantees. We employ a synopsis data structure (sketch) in SRAM to filter out small flows and preferentially sample medium and large flows to a flow table in DRAM. Our key contribution is to show how to maximize the use of SRAM and DRAM available to us by using a SRAM/DRAM hybrid data structure that can achieve more than an order of magnitude higher SRAM efficiency than previous methods. We design a quantization scheme that allows our algorithm to "read just enough" from the sketch at SRAM speed, without sacrificing much estimation accuracy. We provide analytical guarantees on the accuracy of the estimation and validate these by means of trace-driven evaluation using real- world packet traces..},
keywords={DRAM chips;SRAM chips;telecommunication network routing;telecommunication traffic;medium-to large-sized flow;network traffic;SRAM algorithm;DRAM algorithm;synopsis data structure;Telecommunication traffic;Random access memory;Monitoring;Sampling methods;Data structures;Quality of service;Mice;Bandwidth;Estimation error;Probability},
doi={10.1109/INFCOM.2009.5062217},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062218,
author={I. Constandache and S. Gaonkar and M. Sayler and R. R. Choudhury and L. Cox},
booktitle={IEEE INFOCOM 2009},
title={EnLoc: Energy-Efficient Localization for Mobile Phones},
year={2009},
volume={},
number={},
pages={2716-2720},
abstract={A growing number of mobile phone applications utilize physical location to express the context of information. Most of these location-based applications assume GPS capabilities. Unfortunately, GPS incurs an unacceptable energy cost that can reduce the phone's battery life to less than nine hours. Alternate localization technologies, based on WiFi or GSM, improve battery life at the expense of localization accuracy. This paper quantifies this important tradeoff that underlies a range of emerging services. Driven by measurements from Nokia N95 phones, we develop an energy-efficient localization framework called EnLoc. The framework characterizes the optimal localization accuracy for a given energy budget, and develops prediction- based heuristics for real-time use. Evaluation on traces from real users demonstrates the possibility of achieving good localization accuracy for a realistic energy budget.},
keywords={Global Positioning System;mobile computing;wireless LAN;EnLoc;energy-efficient localization;mobile phones;location-based applications;GPS;WiFi;GSM;Energy efficiency;Mobile handsets;Global Positioning System;GSM;Batteries;Energy measurement;Wireless sensor networks;Costs;Poles and towers;Communications Society},
doi={10.1109/INFCOM.2009.5062218},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062219,
author={J. Cao and A. Chen and T. Bu and A. Buvaneswari},
booktitle={IEEE INFOCOM 2009},
title={Monitoring Time-Varying Network Streams Using State-Space Models},
year={2009},
volume={},
number={},
pages={2721-2725},
abstract={In this paper, we develop an online monitoring methodology for the time-varying cyclical streams of network data, that combines a baseline state-space model and statistical control schemes to monitor departures from the baseline model. Parameters of the state space models are initialized based on the training data, and updated for each incoming observation. The statistical control schemes for monitoring are designed based on forecasting errors from the baseline model, under the framework of statistical change detection. We demonstrate the effectiveness of our methodology using measurement data from an EVDO wireless network.},
keywords={state-space methods;telecommunication network management;time-varying network streams;online monitoring methodology;time-varying cyclical streams;network data;baseline state space model;statistical control scheme;baseline model;statistical change detection;wireless network;Monitoring;Statistics;Base stations;Interpolation;Spline;Communications Society;State-space methods;Training data;Error correction;Predictive models},
doi={10.1109/INFCOM.2009.5062219},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062220,
author={D. Wu and C. Liang and Y. Liu and K. Ross},
booktitle={IEEE INFOCOM 2009},
title={View-Upload Decoupling: A Redesign of Multi-Channel P2P Video Systems},
year={2009},
volume={},
number={},
pages={2726-2730},
abstract={In current multi-channel live P2P video systems, there are several fundamental performance problems including exceedingly-large channel switching delays, long playback lags, and poor performance for less popular channels. These performance problems primarily stem from two intrinsic characteristics of multi-channel P2P video systems: channel churn and channel- resource imbalance. In this paper, we propose a radically different cross-channel P2P streaming framework, called view-upload decoupling (VUD). VUD strictly decouples peer downloading from uploading, bringing stability to multichannel systems and enabling cross-channel resource sharing. We propose a set of peer assignment and bandwidth allocation algorithms to properly provision bandwidth among channels, and introduce substream swarming to reduce the bandwidth overhead. We evaluate the performance of VUD via extensive simulations as well with a PlanetLab implementation. Our simulation and PlanetLab results show that VUD is resilient to channel churn, and achieves lower switching delay and better streaming quality. In particular, the streaming quality of small channels is greatly improved.},
keywords={bandwidth allocation;peer-to-peer computing;resource allocation;video communication;view-upload decoupling;multichannel live P2P video systems;channel switching delays;long playback lags;channel churn;channel-resource imbalance;cross-channel P2P streaming framework;cross-channel resource sharing;peer assignment;bandwidth allocation algorithms;PlanetLab;Streaming media;Bandwidth;Switches;Delay systems;Current measurement;Extraterrestrial measurements;Cable TV;Communications Society;Chaotic communication;Information science},
doi={10.1109/INFCOM.2009.5062220},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062221,
author={C. Wu and B. Li and S. Zhao},
booktitle={IEEE INFOCOM 2009},
title={Diagnosing Network-Wide P2P Live Streaming Inefficiencies},
year={2009},
volume={},
number={},
pages={2731-2735},
abstract={Large-scale live peer-to-peer (P2P) streaming applications have been successfully deployed in today's Internet. While they can accommodate millions of users simultaneously with hundreds of channels of programming, there still commonly exist channels and times where and when the streaming quality is unsatisfactory. In this paper, based on more than two terabytes and one year worth of live traces from UUSee, a large-scale commercial P2P live streaming system, we show an in-depth network-wide diagnosis of streaming inefficiencies, commonly present in mesh-based P2P streaming systems. We first identify an evolutionary pattern of low streaming quality in the system and the distribution of streaming inefficiencies across various streaming channels. We then carry out an extensive investigation to explore the causes to such streaming inefficiencies over different times and across different channels at specific times. The original discoveries we have brought forward include the two-sided effects of peer population on the streaming quality in a channel, the significant impact of inter-peer bandwidth bottlenecks at peak times, and the inefficient utilization of server capacities across concurrent channels. We conclude with a number of suggestions to improve real-world large-scale P2P streaming.},
keywords={Internet;media streaming;peer-to-peer computing;network-wide P2P live streaming diagnosis;large-scale live peer-to-peer streaming;Internet;mesh-based P2P streaming systems;channel streaming quality;interpeer bandwidth;Streaming media;Bandwidth;Large-scale systems;Web server;Peer to peer computing;Internet;Communications Society;Computer science;Application software;IP networks},
doi={10.1109/INFCOM.2009.5062221},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062222,
author={K. Shami and D. Magoni and H. Chang and W. Wang and S. Jamin},
booktitle={IEEE INFOCOM 2009},
title={Impacts of Peer Characteristics on P2PTV Networks Scalability},
year={2009},
volume={},
number={},
pages={2736-2740},
abstract={A P2PTV system allows users to watch live video streams redistributed by other users via a peer-to-peer (P2P) network. In an ideal world, each peer in a P2P network would be able to redistribute more bytes than it receives. A P2PTV system built from such peers can support a virtually unlimited number of peers; with only a single copy of content stream injected into the network, it can redistribute the content to all peers. Two factors in the development of the Internet prevented the realization of this scenario: the deployment of asymmetric access networks and the adoption of NAT boxes. For real-time live streaming, such peer asymmetry and incompatibility is a limiting factor on the P2P network scalability. We first develop a basic formal analysis of the effect of bandwidth asymmetry on P2P network scalability. Then we present several characteristics of peer asymmetry as measured on the Zattoo P2PTV network. Our simulation results, driven by the measured peer characteristics, confirm that we cannot rely on P2P network alone to distribute live streaming content on today's Internet.},
keywords={Internet;peer-to-peer computing;real-time systems;video streaming;peer characteristics;P2PTV networks scalability;P2PTV system;live video streams;peer-to-peer network;Internet;asymmetric access networks;NAT boxes;real-time live streaming;peer asymmetry;incompatibility;P2P network scalability;formal analysis;bandwidth asymmetry;Zattoo P2PTV network;Scalability;Streaming media;USA Councils;IP networks;Bandwidth;Broadcasting;Communications Society;Watches;Peer to peer computing;Network address translation},
doi={10.1109/INFCOM.2009.5062222},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062223,
author={C. Liang and Z. Fu and Y. Liu and C. W. Wu},
booktitle={IEEE INFOCOM 2009},
title={iPASS: Incentivized Peer-Assisted System for Asynchronous Streaming},
year={2009},
volume={},
number={},
pages={2741-2745},
abstract={As an efficient distribution mechanism, peer-to-peer technology has become a tremendously attractive solution to offload servers in large scale video streaming applications. However, in providing on-demand asynchronous streaming services, P2P streaming design faces two major challenges: how to schedule efficient video sharing between peers with asynchronous playback progresses? how to provide incentives for peers to contribute their resources to achieve a high level of system-wide quality-of-experience (QoE)? In this paper, we present iPASS, a novel mesh-based P2P VoD system, to address these challenges. Specifically, iPASS adopts a dynamic buffering-progress-based peering strategy to achieve high peer bandwidth utilization with low system maintenance cost. To provide incentives for peer uploading, iPASS employs a differentiated pre-fetching design that enables peers with higher contribution pre-fetch content at higher speed. Through packet-level simulations, it was demonstrated that iPASS can effectively offload server and the proposed distributed incentive algorithm motivates peers to contribute and collectively achieve a high level of of QoE.},
keywords={buffer storage;peer-to-peer computing;video streaming;iPASS;incentivized peer-assisted system;peer-to-peer technology;large scale video streaming;on-demand asynchronous streaming services;P2P streaming design;video sharing;quality-of-experience;QoE;mesh-based P2P VoD system;buffering-progress-based peering strategy;high peer bandwidth utilization;prefetching design;distributed incentive algorithm;Video sharing;Streaming media;Network servers;Bandwidth;Costs;Peer to peer computing;USA Councils;Large-scale systems;Hard disks;Topology},
doi={10.1109/INFCOM.2009.5062223},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062224,
author={S. Petrovic and P. Brown},
booktitle={IEEE INFOCOM 2009},
title={Large Scale Analysis of the eDonkey P2P File Sharing System},
year={2009},
volume={},
number={},
pages={2746-2750},
abstract={This paper presents a general numerical method to evaluate the download times in a P2P file sharing application with linear time and memory requirements with respect to the number of different files shared in the system. As this number typically exceeds several million and as the dependencies between file download times grow exponentially with the number of files, this method is useful to study realistic size systems. We apply the method on a system of 20000 files using parameters observed on a real network. The resulting performance is discussed. We show how the method may be extended to take into account initial waiting times before download and present the resulting performance.},
keywords={file organisation;peer-to-peer computing;large scale analysis;eDonkey;P2P file sharing system;Large-scale systems;Peer to peer computing;Internet;Mathematical model;Analytical models;Bandwidth;Communications Society;Telecommunication traffic;Performance evaluation;Traffic control},
doi={10.1109/INFCOM.2009.5062224},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062225,
author={C. Liu and J. Wu},
booktitle={IEEE INFOCOM 2009},
title={Efficient Geometric Routing in Three Dimensional Ad Hoc Networks},
year={2009},
volume={},
number={},
pages={2751-2755},
abstract={Efficient geometric routing algorithms have been studied extensively in two-dimensional ad hoc networks, or simply 2D networks. These algorithms are efficient and they have been proven to be the worst-case optimal, localized routing algorithms. However, few prior works have focused on efficient geometric routing in 3D networks due to the lack of an efficient method to limit the search once the greedy routing algorithm encounters a local-minimum, like face routing in 2D networks. In this paper, we tackle the problem of efficient geometric routing in 3D networks. We propose routing on hulls, a 3D analogue to face routing, and present the first 3D partial unit Delaunay triangulation (PUDT) algorithm to divide the entire network space into a number of closed subspaces. The proposed greedy- hull-greedy (GHG) routing is efficient because it bounds the local- minimum recovery process from the whole network to the surface structure (hull) of only one of the subspaces.},
keywords={ad hoc networks;geometry;greedy algorithms;mesh generation;telecommunication network routing;3D ad hoc networks;geometric routing algorithms;face routing;3D partial unit Delaunay triangulation algorithm;greedy-hull-greedy routing;local-minimum recovery process;Ad hoc networks;Routing protocols;Surface structures;Communications Society;Computer science;Geometry;Network topology;Costs;Performance evaluation},
doi={10.1109/INFCOM.2009.5062225},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062226,
author={M. Bateni and A. Gerber and M. Hajiaghayi and S. Sen},
booktitle={IEEE INFOCOM 2009},
title={Multi-VPN Optimization for Scalable Routing via Relaying},
year={2009},
volume={},
number={},
pages={2756-2760},
abstract={Enterprise networks are increasingly adopting layer 3 multiprotocol label switching (MPLS) virtual private network (VPN) technology to connect geographically disparate locations. The any-to-any direct connectivity model of this technology involves a very high memory footprint and is causing associated routing tables in the service provider's routers to grow very large. The concept of relaying was proposed earlier [6] to separately minimize the routing table memory footprint of individual VPNs, and involves selecting a small number of hub routers to maintain complete reachability information for that VPN, and enabling non-hub spoke routers with reduced routing tables to achieve any-to-any reachability by routing traffic via a hub. A large service provider network typically hosts many thousands of different VPNs. In this paper, we generalize relaying to the multi-VPN environment, and consider new constraints on resources shared across VPNs, such as router uplink bandwidth and memory. The hub selection problem involves complex tradeoffs along multiple dimensions including these shared resources, and the additional distance traversed by traffic. We formulate the hub selection as a constraint optimization problem and develop an algorithm with provable guarantees to solve this NP-complete problem. Evaluations using traces and configurations from a large provider and many real-world VPNs indicate that the resulting Relaying solution substantially reduces the total router memory requirement by 85% while smoothing out the utilization on each router and requiring only a small increase in the end-to-end path for the relayed traffic.},
keywords={multiprotocol label switching;optimisation;reachability analysis;telecommunication network routing;virtual private networks;multiVPN optimization;scalable routing;relaying;enterprise networks;multiprotocol label switching;virtual private network technology;geographically disparate locations;any-to-any direct connectivity model;associated routing tables;service provider routers;routing table memory footprint;hub routers;reachability information;non-hub spoke routers;any-to-any reachability;routing traffic;service provider network;router uplink bandwidth;constraint optimization problem;NP-complete problem;router memory requirement;relayed traffic;Routing;Relays;Virtual private networks;Telecommunication traffic;Bandwidth;Delay;Maintenance;Constraint optimization;Memory management;Cost function},
doi={10.1109/INFCOM.2009.5062226},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062227,
author={R. Kawahara and E. K. Lua and M. Uchida and S. Kamei and H. Yoshino},
booktitle={IEEE INFOCOM 2009},
title={On the Quality of Triangle Inequality Violation Aware Routing Overlay Architecture},
year={2009},
volume={},
number={},
pages={2761-2765},
abstract={It is known that Internet routing policies for both intra- and inter-domain routing can naturally give rise to triangle inequality violations (TIVs) with respect to quality of service (QoS) network metrics such as latencies between nodes. This motivates the exploitation of such TIVs phenomenon in network metrics to design TlV-aware routing overlay architecture which is capable of choosing quality overlay routing paths to improve end-to-end QoS without changing the underlying network architecture. Our idea is to find quality overlay routes between node pairs based on TIV optimization in terms of the latency and packet loss ratio, and that can offer near optimal routing quality in cost-effective and scalable manner. Our intuition to do this is to choose these overlay routes from a small set of transit nodes. We propose to assign nodes with transit selection frequency scores that are computed based on previous node usage for transit, and consolidate a small set of highly ranked transit nodes. For every node pair, we choose the best transit node in this small set for overlay routing, based on TIV optimization in latency and packet loss ratio. We analyze the quality of our TlV-aware routing overlay algorithm analytically and using real Internet measurements on latency and packet loss ratio. Our results show good quality performance in improving end-to-end QoS routing.},
keywords={Internet;optimisation;quality of service;telecommunication network routing;triangle inequality violation aware routing overlay architecture;Internet routing policies;intra-domain routing;inter-domain routing;quality of service network metrics;quality overlay routing paths;TIV optimization;end-to-end QoS routing;Routing;Delay;Quality of service;Internet;Frequency;IP networks;Peer to peer computing;Electronic mail;Analytical models;Loss measurement},
doi={10.1109/INFCOM.2009.5062227},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062228,
author={R. Sarkar and X. Zhu and J. Gao},
booktitle={IEEE INFOCOM 2009},
title={Spatial Distribution in Routing Table Design for Sensor Networks},
year={2009},
volume={},
number={},
pages={2766-2770},
abstract={We propose a generic routing table design principle for scalable routing on networks with bounded geometric growth. Given an inaccurate distance oracle that estimates the graph distance of any two nodes with constant factor upper and lower bounds, we augment it by storing the routing paths of pairs of nodes, selected in a spatial distribution, and show that the routing table enables 1 + epsiv stretch routing. In the wireless ad hoc and sensor network scenario, the geographic locations of the nodes serve as such an inaccurate distance oracle. Each node p selects O (log n loglog n) other nodes from a distribution proportional to 1/r<sup>2</sup> where r is the distance to p and the routing paths to these nodes are stored on the nodes along these paths in the network. The routing algorithm selects links conforming to a set of sufficient conditions and guarantees with high probability 1 + epsiv stretch routing with routing table size O(radicn log n loglog n) on average for each node. This scheme is favorable for its simplicity, generality and blindness to any global state. It is a good example that global routing properties emerge from purely distributed and uncoordinated routing table design.},
keywords={ad hoc networks;computational complexity;graph theory;telecommunication network routing;wireless sensor networks;routing table design;graph distance estimation;wireless ad hoc network;routing algorithm;wireless sensor network;Routing;Peer to peer computing;Euclidean distance;Wireless sensor networks;Sufficient conditions;Communications Society;Computer science;Blindness;Wireless communication;Radio communication},
doi={10.1109/INFCOM.2009.5062228},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062229,
author={G. Enyedi and P. Szilagyi and G. Retvari and A. Csaszar},
booktitle={IEEE INFOCOM 2009},
title={IP Fast ReRoute: Lightweight Not-Via without Additional Addresses},
year={2009},
volume={},
number={},
pages={2771-2775},
abstract={In order for IP to become a full-fledged carrier- grade transport technology, a native IP failure-recovery scheme is necessary that can correct failures in the order of milliseconds. IP fast reroute (IPFRR) intends to fill this gap, providing fast, local and proactive handling of failures right in the IP layer. Building on experiences and extensive measurement results collected with a prototype implementation of the prevailing IPFRR technique, Not-via, in this paper we identify high address management burden and computational complexity as the major causes of why commercial IPFRR deployment still lags behind, and we present a lightweight not-via scheme, which, according to our measurements, improves these issues.},
keywords={computational complexity;IP networks;telecommunication network routing;IP fast reroute;carrier-grade transport technology;IP failure-recovery scheme;address management;computational complexity;Testing;Prototypes;Computational complexity;Routing;Proposals;Communications Society;Informatics;Telecommunication traffic;Distributed algorithms;Peer to peer computing},
doi={10.1109/INFCOM.2009.5062229},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062230,
author={Y. Liu and L. Ni},
booktitle={IEEE INFOCOM 2009},
title={A Generalized Probabilistic Topology Control for Wireless Sensor Networks},
year={2009},
volume={},
number={},
pages={2776-2780},
abstract={Topology control is an effective method to improve energy-efficiency and increase the capacity in Wireless Sensor Networks (WSNs). To fully characterize WSNs with lossy links, we propose a novel probabilistic network model. Under this model, we meter the network quality using network reachability defined as the minimal of the upper limit of the end-to-end delivery ratio between any pair of nodes in the network.We attempt to find a minimal transmitting power for each node while the network reachability is above a given application- specified threshold, called probabilistic topology control (PTC). We prove that PTC is NP-hard and propose a fully distributed algorithm called BRASP. We prove that BRASP has the guaranteed performance. Two rules that must be followed by any algorithm have been identified. We conduct both simulations and prototype implementations based an 18-TelosB-node test- bed. The experimental results show that the network energy- efficiency can be improved by up to 250%. The average node degree is reduced by 50% which will lead to a great benefit for the network capacity.},
keywords={distributed algorithms;optimisation;probability;telecommunication control;telecommunication network topology;wireless sensor networks;generalized probabilistic topology control;wireless sensor network;probabilistic network model;network reachability;NP-hard problem;distributed algorithm;Network topology;Wireless sensor networks;Costs;Routing;Broadcasting;Energy efficiency;Peer to peer computing;Computer networks;Communication system control;Distributed algorithms},
doi={10.1109/INFCOM.2009.5062230},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062231,
author={C. Liu and G. Cao},
booktitle={IEEE INFOCOM 2009},
title={A Multi-Poller based Energy-Efficient Monitoring Scheme for Wireless Sensor Networks},
year={2009},
volume={},
number={},
pages={2781-2785},
abstract={For sensor networks deployed in unattended, harsh environments, the knowledge of sensor statuses such as liveness, node density and residue energy, is critical for maintaining the normal operation of the network. In this paper, we propose a poller-pollee based architecture to monitor the sensor status, focusing on two important issues: false alarm and energy efficiency. To reduce the false alarm rate, each pollee is monitored by multiple pollers. However, this approach will increase the power and bandwidth consumption. To address this issue, we propose a novel solution where the monitored sensor sends the status reports to different pollers in a round robin manner. In this way, bandwidth and power can be saved, and the false alarm rate can be reduced. We further propose a simple randomized algorithm to select the optimal number of pollers to stochastically minimize the expected total energy consumption due to monitoring. Theoretical analyses and simulations are used to demonstrate the effectiveness of the proposed techniques.},
keywords={stochastic processes;wireless sensor networks;multipoller based energy-efficient monitoring scheme;wireless sensor networks;poller-pollee based architecture;sensor status;bandwidth consumption;power consumption;false alarm rate;randomized algorithm;Energy efficiency;Wireless sensor networks;Round robin;Bandwidth;Energy consumption;Condition monitoring;Communications Society;Computerized monitoring;Computer science;Maintenance engineering},
doi={10.1109/INFCOM.2009.5062231},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062232,
author={R. Crepaldi and M. Montanari and I. Gupta and R. H. Kravets},
booktitle={IEEE INFOCOM 2009},
title={Using Failure Models for Controlling Data Availability in Wireless Sensor Networks},
year={2009},
volume={},
number={},
pages={2786-2790},
abstract={This paper presents Pirrus, a replica management system that addresses the problem of providing data availability on a wireless sensor network. Pirrus uses probabilistic failure models (e.g., derived from environmental conditions and estimation of available energy) to adaptively create and maintain a number of replicas of the data. Replica management is formulated as an energy optimization problem, then solved with a greedy heuristic that only uses information gathered from neighbors. Intuitively, Pirrus trades off the energy saved by limiting the number of replicas when the network health is good to extend the lifetime when more replicas are needed. Our simulation results show how the solution provided by Pirrus achieves good performance with a sustainable computational cost. Compared to the performance of a fixed number of replicas, Pirrus extends the network lifetime by more than 20%.},
keywords={optimisation;probability;wireless sensor networks;wireless sensor network;Pirrus;replica management system;probabilistic failure model;energy optimization problem;greedy heuristic;Wireless sensor networks;Availability;Batteries;Hardware;Telecommunication network reliability;Protocols;Communications Society;Communication system control;Energy management;Computational modeling},
doi={10.1109/INFCOM.2009.5062232},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062233,
author={H. Ma and X. Zhang and A. Ming},
booktitle={IEEE INFOCOM 2009},
title={A Coverage-Enhancing Method for 3D Directional Sensor Networks},
year={2009},
volume={},
number={},
pages={2791-2795},
abstract={In conventional directional sensor networks, coverage control for each sensor is based on a 2D directional sensing model. However, 2D directional sensing model cannot accurately characterize the actual application scene of image/video sensor networks. To remedy this deficiency, we propose a 3D directional sensor coverage-control model with tunable orientations. In order to improve the efficiency of target-detecting, we develop a virtual potential-field based coverage-enhancing scheme to improve the coverage performance. Furthermore, we apply the simulated annealing algorithm for objective optimization. The extensive simulations show the effectiveness of our proposed 3D sensing model and coverage enhancing method.},
keywords={simulated annealing;telecommunication congestion control;wireless sensor networks;3D directional sensor networks;2D directional sensing model;image-video sensor networks;virtual potential-field based coverage-enhancing scheme;simulated annealing algorithm;objective optimization;Sensor phenomena and characterization;Intelligent sensors;Image sensors;Layout;Simulated annealing;Sensor systems;Communications Society;Intelligent networks;Telecommunications;Multimedia systems},
doi={10.1109/INFCOM.2009.5062233},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062234,
author={Y. Zhu and S. J. Gortler and D. Thurston},
booktitle={IEEE INFOCOM 2009},
title={Sensor Network Localization Using Sensor Perturbation},
year={2009},
volume={},
number={},
pages={2796-2800},
abstract={Sensor network localization is an instance of the NP-HARD graph realization problem. Thus, methods used in practice are not guaranteed to find the correct localization, even if it is uniquely determined by the input distances. In this paper, we show the following: if the sensors are allowed to wiggle, giving us perturbed distance data, we can apply a novel algorithm to realize arbitrary generically globally rigid (GGR) graphs (or maximal vertex subsets in non-GGR graphs whose relative positions are fixed). And this algorithm works in any dimension. In the language of structural rigidity theory, our approach corresponds to calculating the approximate kernel of a generic stress matrix for the given graph and distance data. To make our algorithm suitable for real-world application, we present techniques for improving the robustness of the algorithm under noisy measurements, and a strategy for reducing the required number of measurements.},
keywords={graph theory;graphs;optimisation;wireless sensor networks;sensor network localization;sensor perturbation;NP-HARD graph realization problem;perturbed distance data;arbitrary generically globally rigid graphs;structural rigidity theory;approximate kernel;generic stress matrix;algorithm robustness;Stress;Noise reduction;Noise measurement;Communications Society;Kernel;Robustness;Coordinate measuring machines;Particle measurements},
doi={10.1109/INFCOM.2009.5062234},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062235,
author={C. -. Wang and X. Lin},
booktitle={IEEE INFOCOM 2009},
title={Fast Resource Allocation for Network-Coded Traffic - A Coded-Feedback Approach},
year={2009},
volume={},
number={},
pages={2801-2805},
abstract={In this paper, we develop a fast resource allocation algorithm that takes advantage of intra-session network coding. The algorithm maximizes the total utility of multiple unicast (or multicast) sessions subject to capacity constraints, where packets are coded within each session. Our solution is a primal solution that does not use duality or congestion prices. Thus, it does not require building up queues to achieve the optimal resource allocation. Hence, the queueing delay of the packets can be tightly controlled. The existing primal solution in the literature requires a separate graph-theoretic algorithm to find the min-cut of each session, whose complexity grows quadratically with the total number of nodes. In contrast, we provide a new coded-feedback approach whose complexity grows only linearly with the total number of nodes. More explicitly, by letting the ACK/feedback packets on the return paths also carry coding coefficients as does the forward coded traffic, key network information can be obtained more efficiently, which leads to a fast resource allocation scheme fully integrated with the network coding operation.},
keywords={feedback;graph theory;linear codes;multicast communication;queueing theory;resource allocation;telecommunication network routing;telecommunication traffic;optimal resource allocation algorithm;coded-feedback approach;intra-session network coding;multiple unicast session;queueing delay;graph-theoretic algorithm;linear network code;ACK/feedback packet;network traffic;network routing;Resource management;Telecommunication traffic;Network coding;Multicast algorithms;Unicast;Routing;Traffic control;Throughput;Delay;Peer to peer computing},
doi={10.1109/INFCOM.2009.5062235},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062236,
author={C. -. Hong and A. -. Pang},
booktitle={IEEE INFOCOM 2009},
title={Link Scheduling with QoS Guarantee for Wireless Relay Networks},
year={2009},
volume={},
number={},
pages={2806-2810},
abstract={The emerging wireless relay networks (WRNs) are expected to provide significant improvement on throughput and extension of coverage area for next-generation wireless systems. We study an optimization problem for multi-hop link scheduling with bandwidth and delay guarantees over WRNs. Our optimization problem is investigated under a general interference model with a generic objective. The objective can be based on various kinds of performance indexes (e.g., throughput, fairness and capacity) determined by service providers. We present efficient algorithms for maximizing the objective function. The experimental results indicate that our presented algorithms yield near-optimal performance.},
keywords={optimisation;quality of service;radio networks;radiofrequency interference;scheduling;multihop link scheduling;QoS guarantee;wireless relay network;next-generation wireless system;optimization problem;interference model;Relays;Spread spectrum communication;Bandwidth;Interference;Delay;Throughput;Scheduling algorithm;Resource management;Routing;Quality of service},
doi={10.1109/INFCOM.2009.5062236},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062237,
author={P. Coucheney and C. Touati and B. Gaujal},
booktitle={IEEE INFOCOM 2009},
title={Fair and Efficient User-Network Association Algorithm for Multi-Technology Wireless Networks},
year={2009},
volume={},
number={},
pages={2811-2815},
abstract={Recent mobile equipment (as well as the norm IEEE 802.21) offers the possibility for users to switch from one technology to another (vertical handover). This allows flexibility in resource assignments and, consequently, increases the potential throughput allocated to each user. In this paper, we design a fully distributed algorithm based on trial and error mechanisms that exploits the benefits of vertical handover by finding fair and efficient assignment schemes. On the one hand, mobiles gradually update the fraction of data packets they send to each network based on the rewards they receive from the stations. On the other hand, network stations send rewards to each mobile that represent the impact each mobile has on the cell throughput. This reward function is closely related to the concept of marginal cost in the pricing literature. Both the station and the mobile algorithms are simple enough to be implemented in current standard equipment. Based on tools from evolutionary games, potential games and replicator dynamics, we analytically show the convergence of the algorithm to fair and efficient solutions. Moreover, we show that after convergence, each user is connected to a single network cell which avoids costly repeated vertical handovers. To achieve fast convergence, several simple heuristics based on this algorithm are proposed and tested. Indeed, for implementation purposes, the number of iterations should remain in the order of a few tens.},
keywords={game theory;wireless LAN;user-network association algorithm;multitechnology wireless network;distributed algorithm;evolutionary game;replicator dynamics;Wireless networks;Throughput;Distributed algorithms;Game theory;Switches;Algorithm design and analysis;3G mobile communication;WiMAX;Convergence;Communications Society},
doi={10.1109/INFCOM.2009.5062237},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062238,
author={P. Belzarena and A. Ferragut and F. Paganini},
booktitle={IEEE INFOCOM 2009},
title={Network Bandwidth Allocation via Distributed Auctions with Time Reservations},
year={2009},
volume={},
number={},
pages={2816-2820},
abstract={This paper studies the problem of allocating network capacity through periodic auctions. We impose the following conditions: fully distributed solutions over an arbitrary network topology, and the requirement that resources allocated in a given auction are reserved for the entire duration of the connection, not subject to future contention. Under these conditions, we study the problem of selling capacity to optimize revenue for the operator. We first study optimal revenue for a single distributed auction in a general network. Next, the periodic auctions case is considered for a single link, modelling the optimal revenue problem as a Markov decision process (MDP); we develop a sequence of receding horizon approximations to its solution. Combining the two approaches we formulate a receding horizon optimization of revenue over a general network topology, that yields a distributed implementation. The proposal is demonstrated through simulations.},
keywords={bandwidth allocation;computer networks;convex programming;decision theory;Markov processes;resource allocation;telecommunication network topology;network bandwidth allocation;distributed auction;time reservation;network topology;resource allocation;revenue optimization;Markov decision process;receding horizon approximation sequence;convex optimization;service overlay network;Channel allocation;Bandwidth;Network topology;Resource management;Proposals;Computational modeling;Communications Society;Motion pictures;Stochastic processes;Distributed computing},
doi={10.1109/INFCOM.2009.5062238},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062239,
author={U. Ben-Porat and A. Bremler-Barr and H. Levy},
booktitle={IEEE INFOCOM 2009},
title={On the Exploitation of CDF Based Wireless Scheduling},
year={2009},
volume={},
number={},
pages={2821-2825},
abstract={Channel-aware scheduling strategies - such as the CDF Scheduler (CS) algorithm for the CDMA/HDR systems - provide an effective mechanism for utilizing the channel data rate for improving throughput performance in wireless data networks by exploiting channel fluctuations. A highly desired property of such a scheduling strategy is that its algorithm will be stable, in the sense that no user has incentive "cheating" the algorithm in order to increase his/her channel share (on the account of others). We present a scheme by which coordination allows a group of users to gain permanent increase in both their time slot share and in their throughput, on the expense of others, by misreporting their rates. We show that for large populations consisting of regular and coordinated users in equal numbers, the ratio of allocated time slots between a coordinated user and a regular one converges to e - 1 ap 1.7. Our scheme targets the very fundamental principle of CS (as opposed to just attacking implementation aspects), which bases its scheduling decisions on the cumulative distribution function (CDF) of the channel rates reported by users. Our scheme works both for the continuous channel spectrum and the discrete channel spectrum versions of the problem.},
keywords={channel allocation;code division multiple access;multiuser channels;resource allocation;scheduling;statistical distributions;wireless channels;wireless scheduling;channel-aware scheduling;CDMA-HDR system;channel data rate;throughput performance;wireless data network;channel fluctuation;channel sharing;user coordination;time slot share;time slot allocation;cumulative distribution function;continuous channel spectrum;discrete channel spectrum;Throughput;Resource management;Processor scheduling;Computer science;Scheduling algorithm;Fluctuations;Distribution functions;Wireless networks;Channel capacity;Communications Society},
doi={10.1109/INFCOM.2009.5062239},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062240,
author={C. Westphal and G. Pei},
booktitle={IEEE INFOCOM 2009},
title={Scalable Routing Via Greedy Embedding},
year={2009},
volume={},
number={},
pages={2826-2830},
abstract={We investigate the construction of greedy embeddings in polylogarithmic dimensional Euclidian spaces in order to achieve scalable routing through geographic routing. We propose a practical algorithm which uses random projection to achieve greedy forwarding on a space of dimension O(log(n)) where nodes have coordinates of size O(log(n)), thus achieving greedy forwarding using a route table at each node of polylogarithmic size with respect to the number of nodes. We further improve this algorithm by using a quasi-greedy algorithm which ensures greedy forwarding works along a path-wise construction, allowing us to further reduce the dimension of the embedding. The proposed algorithm, denoted GLoVE-U, is fully distributed and practical to implement. We evaluate the performance using extensive simulations and show that our greedy forwarding algorithm delivers low path stretch and scales properly.},
keywords={distributed processing;graph theory;greedy algorithms;telecommunication computing;telecommunication network routing;telecommunication network topology;scalable routing;greedy embedding;polylogarithmic dimensional Euclidian spaces;geographic routing;random projection;greedy forwarding;route table;quasi-greedy algorithm;GLoVE-U;Routing;Peer to peer computing;Scalability;Network topology;Extraterrestrial measurements;Communications Society;USA Councils;Bidirectional control;Protocols;Performance analysis},
doi={10.1109/INFCOM.2009.5062240},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062241,
author={Z. Zheng and P. Sinha and S. Kumar},
booktitle={IEEE INFOCOM 2009},
title={Alpha Coverage: Bounding the Interconnection Gap for Vehicular Internet Access},
year={2009},
volume={},
number={},
pages={2831-2835},
abstract={Vehicular Internet access via open WLAN access points (APs) has been demonstrated to be a feasible solution to provide opportunistic data service to moving vehicles. Using an in situ deployment, however, such a solution does not provide worst-case performance guarantees due to unpredictable intermittent connectivity. On the other hand, a solution that tries to cover every point in an entire road network with APs (full coverage) is not very practical due to the prohibitive deployment and operational cost. In this paper, we introduce a new notion of intermittent coverage for mobile users, called a-coverage, which provides worst-case guarantees on the interconnection gap while using significantly fewer APs than needed for full coverage. We propose efficient algorithms to verify whether a given deployment provides alpha-coverage and approximation algorithms for determining a deployment of APs that will provide alpha-coverage. We compare alpha-coverage with opportunistic access of open WLAN APs (modeled as a random deployment) via simulations over a real-world road network and show that using the same number of APs as random deployment, alpha-coverage bounds the interconnection gap to a much smaller distance than that in a random deployment.},
keywords={approximation theory;Internet;wireless LAN;alpha coverage;interconnection gap;vehicular Internet access;WLAN access point;approximation algorithm;Roads;Wireless LAN;Approximation algorithms;Costs;Web and internet services;Remote monitoring;Satellites;Communications Society;Vehicles;Random media},
doi={10.1109/INFCOM.2009.5062241},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062242,
author={T. Osogami},
booktitle={IEEE INFOCOM 2009},
title={A Fluid Limit for Cache Algorithms with General Request Processes},
year={2009},
volume={},
number={},
pages={2836-2840},
abstract={We introduce a formal limit, which we refer to as a fluid limit, of scaled stochastic models for a cache managed with the Least-Recently-Used algorithm when requests are issued according to general stochastic point processes, which may be non-stationary. We define our fluid limit as a superposition of dependent replications of the original system with smaller item sizes as the number of replications approaches infinity. We derive the average probability that a requested item is not in a cache (average miss probability) in the fluid limit. The usefulness of the fluid limit is demonstrated in two ways. First, our numerical experiments show that, when items are requested according to inhomogeneous Poisson processes, the average miss probability in the fluid limit closely approximates that in the original system as long as there are sufficient number of items. Second, we show that the asymptotic characteristics of the average miss probability as the cache size approaches infinity are often preserved in the fluid limit. This preservation is attractive since the asymptotic analysis in the fluid limit appears to be simpler than that in the original system. In addition, we show that the average miss probability in the fluid limit is asymptotically insensitive to particular dependencies in the requests when the request rates have a light tail, a property not known for the original system.},
keywords={cache storage;stochastic processes;cache algorithms;general request processes;formal limit;scaled stochastic models;least-recently-used algorithm;stochastic point processes;inhomogeneous Poisson processes;asymptotic characteristics;average miss probability;asymptotic analysis;request rates;Stochastic processes;Algorithm design and analysis;Performance analysis;H infinity control;Tail;Performance gain;Communications Society;Laboratories;Scalability;Web sites},
doi={10.1109/INFCOM.2009.5062242},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062243,
author={C. Guo and Y. Liu and W. Shen and H. J. Wang and Q. Yu and Y. Zhang},
booktitle={IEEE INFOCOM 2009},
title={Mining the Web and the Internet for Accurate IP Address Geolocations},
year={2009},
volume={},
number={},
pages={2841-2845},
abstract={In this paper, we present Structon, a novel approach that uses Web mining together with inference and IP traceroute to geolocate IP addresses with significantly better accuracy than existing automated approaches. Structon is composed of three ideas which we realize in three corresponding steps. First, we extract geolocation information of Web server IP addresses from Web pages. Second, we devise heuristic algorithms to improve both the accuracy and the coverage of the IP geolocation database using these Web server IP addresses and their geolocations as input. Third, for those segments that are not covered in the first two steps, we use IP traceroute to identify the access routers of those segments. When the location of the access router is known, we can deduce the location of the associated segment since it is co-located together with the access router. By mining 500-million Web pages collected in China in 2006 (11 percent of the total Web pages in China at that time), we are able to identify the geolocations for 103 million IP addresses. This represents nearly 88 percent IP addresses allocated to China in March 2008. Structon is 87.4 percent accurate at city granularity and up to 93.5 percent accurate at province level. We also used 10 day Windows Live client log to evaluate our client IP addresses coverage: Structon identified geolocations of 98.9 percent of client IP addresses.},
keywords={data mining;Internet;transport protocols;Web page mining;Structon;IP traceroute;geolocation information;Web server IP address;heuristic algorithm;IP geolocation database;access router;associated segment;Windows Live client log;client IP address coverage;Internet protocol;Internet;Web pages;Data mining;Web server;Databases;Cities and towns;Inference algorithms;Web mining;Telephony;Pattern matching},
doi={10.1109/INFCOM.2009.5062243},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062244,
author={T. Shu and S. Liu and M. Krunz},
booktitle={IEEE INFOCOM 2009},
title={Secure Data Collection in Wireless Sensor Networks Using Randomized Dispersive Routes},
year={2009},
volume={},
number={},
pages={2846-2850},
abstract={Compromised-node and denial-of-service are two key attacks in wireless sensor networks (WSNs). In this paper, we study routing mechanisms that circumvent (bypass) black holes formed by these attacks. We argue that existing multi-path routing approaches are vulnerable to such attacks, mainly due to their deterministic nature. So once an adversary acquires the routing algorithm, it can compute the same routes known to the source, and hence endanger all information sent over these routes. In this paper, we develop mechanisms that generate randomized multi-path routes. Under our design, the routes taken by the "shares" of different packets change over time. So even if the routing algorithm becomes known to the adversary, the adversary still cannot pinpoint the routes traversed by each packet. Besides randomness, the routes generated by our mechanisms are also highly dispersive and energy-efficient, making them quite capable of bypassing black holes at low energy cost. Extensive simulations are conducted to verify the validity of our mechanisms.},
keywords={randomised algorithms;telecommunication network routing;telecommunication security;wireless sensor networks;secure data collection;wireless sensor network;randomized dispersive route;compromised-node;denial-of-service;randomized multipath routing algorithm;black hole;Wireless sensor networks;Dispersion;Routing;Computer crime;Cryptography;Jamming;Topology;Information security;Communications Society;Energy efficiency},
doi={10.1109/INFCOM.2009.5062244},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062245,
author={D. Ficara and S. Giordano and G. Procissi and F. Vitucci and G. Antichi and A. Di Pietro},
booktitle={IEEE INFOCOM 2009},
title={Faster DFAs through Simple and Efficient Inverse Homomorphisms},
year={2009},
volume={},
number={},
pages={2851-2855},
abstract={Performing deep packet inspection at high speed is a fundamental task for network security and application-specific services. In state-of-the-art systems, sets of signatures to be searched are described by regular expressions, and finite automata (FAs) are employed for the search. In particular, deterministic FAs (DFAs) need a large amount of memory to represent current sets, therefore the target of many works has been the reduction of memory footprint of DFAs. This paper, instead, focuses on speed multiplication by enlarging the amount of bytes observed in the text (i.e., searching for k-bytes per state-traversal). For this purpose, an interesting yet simple inverse homomorphism is employed to reduce the amount of transitions in the modified DFA. The algorithm results to be remarkably faster than standard DFAs, and provides also a good compression scheme that is orthogonal to other schemes.},
keywords={data compression;digital signatures;finite automata;inverse homomorphisms;deep packet inspection;network security;application-specific services;deterministic finite automata;speed multiplication;compression scheme;signature searching;Birth disorders;Doped fiber amplifiers;Bandwidth;Automata;Inspection;Communications Society;Delay;Explosions;Encoding;Data security},
doi={10.1109/INFCOM.2009.5062245},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062246,
author={M. Srivatsa and A. Iyengar and L. Liu},
booktitle={IEEE INFOCOM 2009},
title={Privacy in VoIP Networks: A k-Anonymity Approach},
year={2009},
volume={},
number={},
pages={2856-2860},
abstract={Peer-to-peer VoIP (voice over IP) networks, exemplified by Skype, are becoming increasingly popular due to their significant cost advantage and richer call forwarding features than traditional public switched telephone networks. One of the most important features of a VoIP network is privacy (for VoIP clients). Unfortunately, most peer-to-peer VoIP networks neither provide personalization nor guarantee a quantifiable privacy level. In this paper we propose novel flow analysis attacks that demonstrate the vulnerabilities of peer-to-peer VoIP networks to privacy attacks. We present detailed experimental evaluation that demonstrates these attacks quantifying performance and scalability degradation.},
keywords={data privacy;Internet telephony;peer-to-peer computing;telecommunication security;network privacy;VoIP network;k-anonymity;peer-to-peer network;voice over IP network;flow analysis attack;privacy attack;Privacy;Peer to peer computing;Routing;Delay;Quality of service;Telecommunication traffic;Transport protocols;Internet telephony;Communications Society;Educational institutions},
doi={10.1109/INFCOM.2009.5062246},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062247,
author={K. P. N. Puttaswamy and A. Sala and O. Egecioglu and B. Y. Zhao},
booktitle={IEEE INFOCOM 2009},
title={Rome: Performance and Anonymity using Route Meshes},
year={2009},
volume={},
number={},
pages={2861-2865},
abstract={Deployed anonymous networks such as Tor focus on delivering messages through end-to-end paths with high anonymity. Selection of routers in the anonymous path construction is either performed randomly, or relies on self-described resource availability at routers, making systems vulnerable to low-resource attacks. In this paper, we investigate an alternative router and path selection mechanism for constructing efficient end-to-end paths with low loss of path anonymity. We propose a novel construct called a "route mesh," and a dynamic programming algorithm that determines optimal-latency paths from many random samples using only a small number of end-to-end measurements. We prove analytically that our path search algorithm finds the optimal path, and requires exponentially lower number of measurements compared to a standard measurement approach. In addition, our analysis shows that route meshes incur only a small loss in anonymity for its users.},
keywords={dynamic programming;telecommunication network routing;Rome;path selection mechanism;route mesh;dynamic programming algorithm;optimal-latency path;Dynamic programming;Heuristic algorithms;Delay;Relays;Algorithm design and analysis;Telecommunication traffic;Routing;Joining processes;Performance evaluation;Communications Society},
doi={10.1109/INFCOM.2009.5062247},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062248,
author={D. Brauckhoff and K. Salamatian and M. May},
booktitle={IEEE INFOCOM 2009},
title={Applying PCA for Traffic Anomaly Detection: Problems and Solutions},
year={2009},
volume={},
number={},
pages={2866-2870},
abstract={Spatial Principal Component Analysis (PCA) has been proposed for network-wide anomaly detection. A recent work has shown that PCA is very sensitive to calibration settings. Unfortunately, the authors did not provide further explanations for this observation. In this paper, we fill this gap and provide the reasoning behind the found discrepancies. We revisit PCA for anomaly detection and evaluate its performance on our data. We develop a slightly modified version of PCA that uses only data from a single router. Instead of correlating data across different spatial measurement points, we correlate the data across different metrics. With the help of the analyzed data, we explain the pitfalls of PCA and underline our argumentation with measurement results. We show that the main problem is that PCA fails to capture temporal correlation. We propose a solution to deal with this problem by replacing PCA with the Karhunen-Loeve transform. We find that when we consider temporal correlation, anomaly detection results are significantly improved.},
keywords={Karhunen-Loeve transforms;principal component analysis;telecommunication traffic;PCA;traffic anomaly detection;spatial principal component analysis;network-wide anomaly detection;calibration settings;spatial measurement points;temporal correlation;Karhunen-Loeve transform;Principal component analysis;Telecommunication traffic;Signal processing;Random variables;Communications Society;Calibration;Data analysis;Karhunen-Loeve transforms;Stochastic processes;Predictive models},
doi={10.1109/INFCOM.2009.5062248},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062249,
author={R. Mudumbai and S. K. Singh and U. Madhow},
booktitle={IEEE INFOCOM 2009},
title={Medium Access Control for 60 GHz Outdoor Mesh Networks with Highly Directional Links},
year={2009},
volume={},
number={},
pages={2871-2875},
abstract={We investigate an architecture for multi-Gigabit outdoor mesh networks operating in the unlicensed 60 GHz "millimeter (mm) wave" band. In this band, the use of narrow beams is essential for attaining the required link ranges in order to overcome the higher path loss at mm wave carrier frequencies. However, highly directional links make standard MAC methods for interference management, such as carrier sense multiple access, which rely on neighboring nodes hearing each other, become inapplicable. In this paper, we study the extent to which we can reduce, or even dispense with, interference management, by exploiting the reduction in interference due to the narrow beamwidths and the oxygen absorption characteristic of the 60 GHz band. We provide a probabilistic analysis of the interference incurred due to uncoordinated transmissions, and show that, for the parameters considered, the links in the network can be thought of as pseudo-wired. That is, interference can essentially be ignored in MAC design, and the challenge is to schedule half-duplex transmissions in the face of the "deafness" resulting from highly directional links. We provide preliminary simulation results to validate our approach.},
keywords={access protocols;interference suppression;millimetre waves;radio networks;radiofrequency interference;statistical analysis;telecommunication network management;telecommunication network topology;medium access control;60 GHz outdoor mesh network;directional link;millimeter wave band;interference management;carrier sense multiple access;probabilistic analysis;wireless network;Media Access Protocol;Mesh networks;Principal component analysis;Telecommunication traffic;Signal processing;Random variables;Communications Society;Calibration;Data analysis;Karhunen-Loeve transforms},
doi={10.1109/INFCOM.2009.5062249},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062250,
author={L. Li and R. Alimi and R. Ramjee and H. Viswanathan and Y. R. Yang},
booktitle={IEEE INFOCOM 2009},
title={muNet: Harnessing Multiuser Capacity in Wireless Mesh Networks},
year={2009},
volume={},
number={},
pages={2876-2880},
abstract={We present muNet, a wireless mesh network design and implementation to harness the multiuser capacity of wireless channels. Traditionally, media access control is designed to schedule one transmission between one sender and one receiver without interference at any given time. However, this design is suboptimal in terms of achieving the multiuser capacity of multi-access wireless channels. In muNet, we implement effective physical layer techniques called superposition coding and successive interference cancellation to enable simultaneous unicast transmissions from a single transmitter to multiple receivers as well as from multiple transmitters to a single receiver. We design the first practical MAC protocol that leverages such a physical layer and exposes the multiuser capacity to upper layers. We also present a simple, effective routing protocol that increases simultaneous transmission opportunities for the MAC layer. A proof-of-concept muNet is implemented on the GNU radio platform. Measurements on the implementation shows that the throughput gains of muNet are significant (up to 93%).},
keywords={access protocols;channel capacity;encoding;interference suppression;radio networks;radio receivers;radio transmitters;routing protocols;muNet;multiuser capacity;wireless mesh network design;media access control;transmission scheduling;multiaccess wireless channels;physical layer techniques;superposition coding;successive interference cancellation;simultaneous unicast transmissions;transmitter;receivers;MAC protocol;routing protocol;GNU radio platform;Wireless mesh networks;Media Access Protocol;Physical layer;Radio transmitters;Channel capacity;Interference cancellation;Unicast;Receivers;Routing protocols;Gain measurement},
doi={10.1109/INFCOM.2009.5062250},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062251,
author={J. Zhang and X. Jia},
booktitle={IEEE INFOCOM 2009},
title={Capacity Analysis of Wireless Mesh Networks with Omni or Directional Antennas},
year={2009},
volume={},
number={},
pages={2881-2885},
abstract={In this paper we analyze the capacity of wireless mesh networks that use omni or directional antennas. The capacity in our analysis is the end-to-end per-node throughput. Our analysis is based on the assumption that there is only one gateway in the network and all end-users' traffics go through the gateway. Non-gateway nodes are uniformly distributed in a two- dimensional region centered at the gateway. The main results of our analysis can be summarized as: 1) The capacity is O(1/N) for both omni and directional antennas, where N is number of nodes in the network. 2) the capacity is O(1g m/thetas) for m = 2, and O(1g m/thetas<sup>2</sup> 1g (1/thetas)) for m &gt; 2, where m is the number of antennas on each node, and thetas is the beamwidth of antennas.},
keywords={directive antennas;network servers;radio networks;telecommunication network topology;telecommunication traffic;wireless mesh network;directional antenna;telecommunication traffic;two- dimensional region;gateway;Wireless mesh networks;Directional antennas;Throughput;Peer to peer computing;Directive antennas;Ad hoc networks;Base stations;Computer science;Telecommunication traffic;Receiving antennas},
doi={10.1109/INFCOM.2009.5062251},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062252,
author={C. P. Luk and W. C. Lau and O. C. Yue},
booktitle={IEEE INFOCOM 2009},
title={Opportunistic Routing with Directional Antennas in Wireless Mesh Networks},
year={2009},
volume={},
number={},
pages={2886-2890},
abstract={Opportunistic routing significantly improves the average progress per transmission over unicast routing by leveraging the opportunistic receptions of multiple potential forwarders in wireless mesh networks. Prior studies mainly focus on networks with omni-directional antenna only. Our previous work suggests that not every node contributes equally in a transmission. By concentrating the beam energy at a particular direction, directional antennas may further improve the performance of opportunistic routing in multi-hop wireless networks. In this paper, we derive an analytical model which allows the incorporation of various node distribution models, radio channel models and antenna models to evaluate the average progress per transmission. It is found that a directional antenna with high directivity does not always improve the performance of opportunistic routing and an optimal beamwidth exists for each particular network. When compared to the case of omni-directional antenna, a directional antenna with optimal beamwidth and direction settings can achieve 30 to 50% performance gain, in terms of average progress per transmission, under typical network configurations. Moreover, such performance gain can be as high as 100% for radio propagation environments where the packet reception probabilities fall off slowly with distance.},
keywords={directive antennas;packet radio networks;probability;radiowave propagation;telecommunication network routing;wireless channels;opportunistic routing;directional antenna;wireless mesh network;beam energy;multihop wireless network;node distribution;radio channel;optimal beamwidth;network configuration;radio propagation;packet reception probability;Routing;Directional antennas;Wireless mesh networks;Transmitting antennas;Performance gain;Unicast;Spread spectrum communication;Analytical models;Directive antennas;Antennas and propagation},
doi={10.1109/INFCOM.2009.5062252},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062253,
author={D. Gupta and D. Wu and P. Mohapatra and C. -. Chuah},
booktitle={IEEE INFOCOM 2009},
title={Experimental Comparison of Bandwidth Estimation Tools for Wireless Mesh Networks},
year={2009},
volume={},
number={},
pages={2891-2895},
abstract={Measurement of available bandwidth in a network has always been a topic of great interest. This knowledge can be applied to a wide variety of applications and can be instrumental in providing quality of service to end users. Several probe-based tools have been proposed to measure available bandwidth in wired networks. However, the performance of these tools in the realm of wireless networks has not been evaluated extensively. In recent years, there has also been some work on estimating bandwidth in wireless networks via passively monitoring the channel and determining the 'busy' and 'idle' periods. However, such techniques have primarily been evaluated via simulations only. In this work, we perform an extensive experimental comparison study of both passive and active bandwidth estimation tools for 802.11-based wireless mesh networks. We investigate the impact of interference, packet loss, and 802.11 rate-adaptation, on the performance of these tools. Our results indicate that for wireless networks, a passive technique provides much greater accuracy than the probe-based tools.},
keywords={bandwidth allocation;protocols;quality of service;wireless channels;wireless LAN;bandwidth estimation;available bandwidth measurement;quality of service;probe-based tool;passive channel monitoring;802.11-based wireless mesh network;interference;packet loss;802.11 rate-adaptation;Bandwidth;Wireless mesh networks;Testing;Interference;Bit rate;Wireless networks;Quality of service;Media Access Protocol;Communications Society;Computer science},
doi={10.1109/INFCOM.2009.5062253},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062254,
author={P. Balister and S. Kumar},
booktitle={IEEE INFOCOM 2009},
title={Random vs. Deterministic Deployment of Sensors in the Presence of Failures and Placement Errors},
year={2009},
volume={},
number={},
pages={2896-2900},
abstract={Although random deployment is widely used in theoretical analysis of coverage and connectivity, and evaluation of various algorithms (e.g., sleep-wakeup), it has often been considered too expensive as compared to optimal deterministic deployment patterns when deploying sensors in real-life. Roughly speaking, a factor of log n additional sensors are needed in random deployment as compared to optimal deterministic deployment if n sensors are needed in a random deployment. This may be an illusion however, since all real-life large-scale deployments strategies result in some randomness, two prime sources being placement errors and sensor failures, either at the time of deployment or afterwards. In this paper, we consider the effects of placement errors and random failures on the density needed to achieve full coverage when sensors are deployed randomly versus deterministically. We compare three popular strategies for deployment. In the first strategy, sensors are deployed in an optimal lattice but enough sensors are colocated at each lattice point to compensate for failure and placement errors. In the second, only one sensor is deployed at each lattice point but lattice spacing is sufficiently shrunk to achieve a desired quality of coverage in the presence of failure and placement errors. In the third, a random deployment is used with appropriate density. We derive explicit expressions for the density needed for each of the three strategies to achieve a given quality of coverage, which are of independent interest. In comparing the three deployments, we find that if errors in placement are half the sensing range and failure probability is 50%, random deployment needs only around 10% higher density to provide a similar quality of coverage as the other two. We provide a comprehensive comparison to help a practitioner decide the lowest cost deployment strategy in real-life.},
keywords={wireless sensor networks;wireless sensor network;placement errors;failure probability;optimal deterministic deployment patterns;random deployment patterns;sensor failures;Lattices;Costs;Wireless sensor networks;Communications Society;Failure analysis;Pattern analysis;Algorithm design and analysis;Large-scale systems;Monitoring;Optimized production technology},
doi={10.1109/INFCOM.2009.5062254},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062255,
author={H. Saito and S. Shimogawa and S. Shioda and J. Harada},
booktitle={IEEE INFOCOM 2009},
title={Shape Estimation Using Networked Binary Sensors},
year={2009},
volume={},
number={},
pages={2901-2905},
abstract={An estimation method for the shape and size of a target object by using networked binary sensors whose locations are unknown is proposed. Each of those sensors, which individually are incapable of monitoring the target object's shape and size, sends only binary data describing whether or not it detects at every moment the target object. By using these data and the explicit formulas derived in this paper, we can estimate the size and the perimeter length of the target object even without sensor location information. An additional parameter, which is a function of the shape of the target object, can also be estimated when the target object is non-convex.},
keywords={estimation theory;wireless sensor networks;shape estimation;networked binary sensor;wireless sensor network;Shape;Object detection;Wireless sensor networks;Monitoring;Event detection;Communications Society;Laboratories;Sensor phenomena and characterization;Size measurement;Batteries},
doi={10.1109/INFCOM.2009.5062255},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062256,
author={H. Kang and J. L. Wong},
booktitle={IEEE INFOCOM 2009},
title={A Localized Multi-Hop Desynchronization Algorithm for Wireless Sensor Networks},
year={2009},
volume={},
number={},
pages={2906-2910},
abstract={This paper presents a new desynchronization algorithm aimed at providing collision-free transmission scheduling for single-hop and acyclic multi-hop wireless sensor networks. The desynchronization approach is resilient to the hidden terminal problem and topology changes. Each node distributively converges upon a single collision-free transmission slot, utilizing only minimal neighbor information. In addition, we propose two strategies which facilitate increased convergence time. We evaluate the proposed algorithm via simulations over a range of network densities on both single-hop and acyclic multi- hop networks. Convergence and throughput comparison are performed against two previously proposed desynchronization algorithms. Finally, using an experimental tested of TelosB motes we verify the performance, practicality, and correctness of the desynchronization algorithm on varying network topologies.},
keywords={telecommunication network topology;wireless sensor networks;localized multihop desynchronization algorithm;collision-free transmission scheduling;single-hop wireless sensor networks;acyclic multihop wireless sensor networks;desynchronization approach;hidden terminal problem;collision-free transmission slot;TelosB motes;network topologies;Spread spectrum communication;Wireless sensor networks;Scheduling algorithm;Network topology;Peer to peer computing;Convergence;Throughput;Round robin;Clocks;Communications Society},
doi={10.1109/INFCOM.2009.5062256},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062257,
author={X. Zhu and R. Sarkar and J. Gao},
booktitle={IEEE INFOCOM 2009},
title={Topological Data Processing for Distributed Sensor Networks with Morse-Smale Decomposition},
year={2009},
volume={},
number={},
pages={2911-2915},
abstract={We are interested in topological analysis and processing of the large-scale distributed data generated by sensor networks. Naturally, a large-scale sensor network is deployed in a geometric region with possibly holes and complex shape, and is used to sample some smooth physical signal field. We are interested in both the topology of the discrete sensor field in terms of the sensing holes (voids without sufficient sensors deployed), as well as the topology of the signal field in terms of its critical points (local maxima, minima and saddles). Towards this end, we develop distributed algorithms to construct the Morse-Smale decomposition, and study the performance benefits obtained by this approach. The sensor field is decomposed into simply-connected pieces, inside each of which the sensor signal is homogeneous, i.e., the data flows uniformly from a local maximum to a local minimum. The Morse-Smale decomposition can be efficiently constructed in the network locally, after which applications such as iso-contour queries, data-guided navigation and routing, data aggregation, and topologically faithful signal reconstructions benefit tremendously from it.},
keywords={distributed algorithms;distributed sensors;telecommunication network topology;wireless sensor networks;topological data processing;large-scale distributed sensor network;Morse-Smale decomposition;geometric region;discrete sensor field;distributed algorithm;local maximum;local minimum;Data processing;Routing;Network topology;Navigation;Large-scale systems;Distributed algorithms;Sensor phenomena and characterization;Communications Society;Computer science;Shape},
doi={10.1109/INFCOM.2009.5062257},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062258,
author={Hongbo Jiang and Wenping Liu and Dan Wang and Chen Tian and Xiang Bai and Xue Liu and Ying Wu and Wenyu Liu},
booktitle={IEEE INFOCOM 2009},
title={CASE: Connectivity-Based Skeleton Extraction in Wireless Sensor Networks},
year={2009},
volume={},
number={},
pages={2916-2920},
abstract={Many sensor network applications are tightly coupled with the geometric environment where the sensor nodes are deployed. The topological skeleton extraction has shown great impact on the performance of such services as location, routing, and path planning in sensor networks. Nonetheless, current studies focus on using skeleton extraction for various applications in sensor networks. How to achieve a better skeleton extraction has not been thoroughly investigated. There are studies on skeleton extraction from the computer vision community; their centralized algorithms for continuous space, however, is not immediately applicable for the discrete and distributed sensor networks. In this paper we present CASE: a novel connectivity-based skeleton extraction algorithm to compute skeleton graph that is robust to noise, and accurate in preservation of the original topology. In addition, no centralized operation is required. The skeleton graph is extracted by partitioning the boundary of the sensor network to identify the skeleton points, then generating the skeleton arcs, connecting these arcs, and finally refining the coarse skeleton graph. Our evaluation shows that CASE is able to extract a well-connected skeleton graph in the presence of significant noise and shape variations, and outperforms state-of-the-art algorithms.},
keywords={graph theory;telecommunication network topology;wireless sensor networks;wireless sensor network;connectivity-based skeleton extraction;geometric environment;topological skeleton extraction;centralized algorithm;continuous space;CASE;coarse skeleton graph;skeleton arc generation;Computer aided software engineering;Skeleton;Wireless sensor networks;Routing;Path planning;Application software;Computer vision;Partitioning algorithms;Noise robustness;Network topology},
doi={10.1109/INFCOM.2009.5062258},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062259,
author={T. Ji and E. Athanasopoulou and R. Srikant},
booktitle={IEEE INFOCOM 2009},
title={Optimal Scheduling Policies in Small Generalized Switches},
year={2009},
volume={},
number={},
pages={2921-2925},
abstract={We consider small generalized switches with less than or equal to four links, and study scheduling policies designed to minimize the total number of packets in the system. By focusing on very small switches, we are able to derive optimal or heavy-traffic optimal policies whose performance can then be compared to previously conjectured optimal policies. In particular, it has been conjectured that the max-weight policy with weight q<sup>alpha</sup> is optimal in heavy-traffic when alpha rarr 0. Our results show that this conjecture is not true.},
keywords={packet switching;scheduling;telecommunication links;telecommunication traffic;optimal scheduling policy;small generalized switches;packets;traffic optimal policy;max-weight policy;network links;Optimal scheduling;Switches;Telecommunication traffic;Traffic control;Interference;Packet switching;Communication switching;Scheduling algorithm;Wireless networks;Throughput},
doi={10.1109/INFCOM.2009.5062259},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062260,
author={Q. Li and R. Negi},
booktitle={IEEE INFOCOM 2009},
title={Scheduling in Multi-Hop Wireless Networks with Priorities},
year={2009},
volume={},
number={},
pages={2926-2930},
abstract={In this paper we consider prioritized maximal scheduling in multi-hop wireless networks, where the scheduler chooses a maximal independent set greedily according to a sequence specified by certain priorities. We show that if the probability distributions of the priorities are properly chosen, we can achieve the optimal (maximum) stability region using an i.i.d random priority assignment process, for any set of arrival processes that satisfy Law of Large Numbers. The pre- computation of the priorities is, in general, NP-hard, but there exists polynomial time approximation scheme (PTAS) to achieve any fraction of the optimal stability region. We next focus on the simple case of static priority and specify a greedy priority assignment algorithm, which can achieve the same fraction of the optimal stability region as the state of art result for longest queue first (LQF) schedulers. We also show that this algorithm can be easily adapted to satisfy delay constraints in the large deviations regime, and therefore, supports quality of service (QoS) for each link.},
keywords={computational complexity;greedy algorithms;probability;quality of service;queueing theory;radio networks;scheduling;multihop wireless networks;prioritized maximal scheduling;maximal independent set;probability distributions;optimal stability region;random priority assignment process;NP-hard;polynomial time approximation scheme;static priority;greedy priority assignment algorithm;longest queue first schedulers;quality of service;Spread spectrum communication;Wireless networks;Optimal scheduling;Processor scheduling;Stability;Scheduling algorithm;Delay;Quality of service;Random processes;Communications Society},
doi={10.1109/INFCOM.2009.5062260},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062261,
author={D. Panigrahi and B. Raman},
booktitle={IEEE INFOCOM 2009},
title={TDMA Scheduling in Long-Distance WiFi Networks},
year={2009},
volume={},
number={},
pages={2931-2935},
abstract={In the last few years, long-distance WiFi networks have been used to provide Internet connectivity in rural areas. The strong requirement to support real-time applications in these settings leads us to consider TDMA link scheduling. In this paper, we consider the FRACTEL architecture for long-distance mesh networks. We propose a novel angular interference model, which is not only practical, but also makes the problem of TDMA scheduling tractable. We then consider delay-bounded scheduling and present an algorithm which uses at most 1/3rd more time-slots than the optimal number of slots required without the delay bound. Our evaluation on various network topologies shows that the algorithm is practical, and more efficient in practice than its worst-case bound.},
keywords={interference (signal);Internet;radio links;scheduling;telecommunication network topology;time division multiple access;wireless LAN;long-distance WiFi networks;Internet connectivity;TDMA link scheduling;FRACTEL architecture;long-distance mesh networks;angular interference model;delay-bounded scheduling;network topologies;Time division multiple access;Delay;Interference;Mesh networks;Poles and towers;Scheduling algorithm;Directional antennas;IP networks;Network topology;Communications Society},
doi={10.1109/INFCOM.2009.5062261},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062262,
author={L. Bui and R. Srikant and A. Stolyar},
booktitle={IEEE INFOCOM 2009},
title={Novel Architectures and Algorithms for Delay Reduction in Back-Pressure Scheduling and Routing},
year={2009},
volume={},
number={},
pages={2936-2940},
abstract={The back-pressure algorithm is a well-known throughput-optimal algorithm. However, its delay performance may be quite poor even when the traffic load is not close to network capacity due to the following two reasons. First, each node has to maintain a separate queue for each commodity in the network, and only one queue is served at a time. Second, the backpressure routing algorithm may route some packets along very long routes. In this paper, we present solutions to address both of the above issues, and hence, improve the delay performance of the back-pressure algorithm. One of the suggested solutions also decreases the complexity of the queueing data structures to be maintained at each node.},
keywords={queueing theory;radio networks;scheduling;telecommunication network routing;telecommunication traffic;wireless network;back-pressure scheduling;back-pressure routing;network delay reduction;traffic load;queueing system;Scheduling algorithm;Delay;Routing;Resource management;USA Councils;Wireless networks;Peer to peer computing;Computational complexity;Communications Society;Lab-on-a-chip},
doi={10.1109/INFCOM.2009.5062262},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062263,
author={J. Corbo and S. Jain and M. Mitzenmacher and D. C. Parkes},
booktitle={IEEE INFOCOM 2009},
title={An Economically-Principled Generative Model of AS Graph Connectivity},
year={2009},
volume={},
number={},
pages={2941-2945},
abstract={End-to-end packet delivery in the Internet is achieved through a system of interconnections between the network domains of independent entities called autonomous systems (ASes). Inter-domain connections are the result of a complex, dynamic process of negotiated business relationships between pairs of ASes. We present an economically-principled generative model for autonomous system graph connectivity. While there is already a large literature devoted to understanding Internet connectivity at the AS level, many of these models are either static or based on generalized stochastics.},
keywords={graph theory;Internet;telecommunication traffic;economically-principled generative model;end-to-end packet delivery;Internet;autonomous system graph connectivity;network traffic;Telecommunication traffic;Traffic control;Internet;Gravity;Costs;Diffusion processes;Communications Society;IP networks;Stochastic processes;Throughput},
doi={10.1109/INFCOM.2009.5062263},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062264,
author={Y. Gu and L. Breslau and N. Duffield and S. Sen},
booktitle={IEEE INFOCOM 2009},
title={On Passive One-Way Loss Measurements Using Sampled Flow Statistics},
year={2009},
volume={},
number={},
pages={2946-2950},
abstract={The ability to scalably measure one-way packet loss across different network paths is vital to IP network management. However, the effectiveness of active-measurement techniques depends on being able to deploy measurement hosts at appropriate locations, and to inject necessary amounts of probe traffic without impacting the performance of interest. On the other hand, existing passive-measurement methods like [1] require router support and suffer from deployment limitations for the foreseeable future. In this paper, we propose a new estimation technique that does not require any new router features or measurement infrastructure, and only uses the sampled flow level statistics that are routinely collected in operational networks. The technique is designed to handle challenges of sampled flow-level aggregation such as information aggregation and non-alignment of flow records with measurement intervals. We develop three different schemes and derive analytical bounds on the variance of loss estimation from such a flow-based approach. Our analysis shows that link data rates are now becoming sufficiently large to counteract the effects on sampling on estimation accuracy.},
keywords={computer network management;IP networks;statistical analysis;telecommunication network routing;telecommunication traffic;one-way packet loss measurement;IP network management;active-measurement technique;network traffic;sampled flow level statistics;network router;Fluid flow measurement;Loss measurement;Statistics;Telecommunication traffic;Performance loss;Distortion measurement;Sampling methods;Monitoring;IP networks;Probes},
doi={10.1109/INFCOM.2009.5062264},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062265,
author={L. Y. Chen and N. Gautam},
booktitle={IEEE INFOCOM 2009},
title={Server Frequency Control Using Markov Decision Processes},
year={2009},
volume={},
number={},
pages={2951-2955},
abstract={For a wide range of devices and servers, Dynamic Frequency Scaling (DFS) can reduce energy consumption to various degrees by appropriately trading-off system performance. Efficient DFS policies are able to adjust server frequencies by extrapolating the transition of the highly varying workload without incurring much of implementation overhead. This paper models DFS policies of a single server using Markov Decision Processes (MDP). To accommodate the highly varying nature of workload in the proposed MDP, we adopt fluid approximation based on continuous time Markov chain and discrete time Markov chain modeling for the fluid workload generator respectively. Accordingly, we design two frequency controllers (FC), namely C-FC and D-FC, corresponding to the continuous and discrete modeling of the workload generator. We evaluate the proposed policies on synthetic and Web traces. The proposed C-FC and D-FC schemes ensure performance satisfaction with moderate energy saving as well as ease of implementation, in comparison with existing DFS policies.},
keywords={continuous time systems;discrete time systems;frequency control;Markov processes;server frequency control;Markov decision processes;dynamic frequency scaling;trading-off system performance;fluid approximation;continuous time Markov chain;discrete time Markov chain modeling;fluid workload generator;frequency controllers;continuous modeling;discrete modeling;Web traces;energy saving;Frequency control;Energy consumption;System performance;Energy management;Frequency modulation;Monitoring;Delay;Power system modeling;Communications Society;Laboratories},
doi={10.1109/INFCOM.2009.5062265},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062266,
author={P. Ramanujam and Z. Li and L. Higham},
booktitle={IEEE INFOCOM 2009},
title={Shadow Prices vs. Vickrey Prices in Multipath Routing},
year={2009},
volume={},
number={},
pages={2956-2960},
abstract={Shadow price and Vickrey price are two classic metrics that can be applied to measure the relative importance of links in a communication network. Each metric has been extensively investigated and enjoys important applications. We study the underlying connections between these two metrics with seemingly different definitions, under a general mathematical model of multipath multi-session multicast routing. We show that Vickrey prices provide upper-bounds for shadow prices in general, and the fine granularity version of Vickrey price, unit Vickrey price, equals exactly the maximum shadow price. We further design an efficient algorithm that computes all-link max/min shadow prices and unit Vickrey prices simultaneously, for unicast routing, reducing the complexity of a straightforward algorithm by an order of O(|E|).},
keywords={computational complexity;game theory;mathematical programming;minimax techniques;multicast communication;telecommunication network routing;all-link max/min shadow price;Vickrey price;multipath multisession multicast routing;communication network;mathematical programming model;algorithm complexity reduction;game theory;optimization problem;Routing;Multicast algorithms;Game theory;Cost function;Communication networks;Unicast;Telecommunication traffic;Communications Society;Computer science;Application software},
doi={10.1109/INFCOM.2009.5062266},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062267,
author={K. Kredo II and P. Djukic and P. Mohapatra},
booktitle={IEEE INFOCOM 2009},
title={STUMP: Exploiting Position Diversity in the Staggered TDMA Underwater MAC Protocol},
year={2009},
volume={},
number={},
pages={2961-2965},
abstract={In this paper, we propose the Staggered TDMA Underwater MAC Protocol (STUMP), a scheduled, collision free TDMA-based MAC protocol that leverages node position diversity and the low propagation speed of the underwater channel. STUMP uses propagation delay information to overlap node communication and increase channel utilization. Our work yields several important conclusions. First, leveraging node position diversity through scheduling yields large improvements in channel utilization. Second, STUMP does not require tight node synchronization to achieve high channel utilization, allowing nodes to use simple or more energy efficient synchronization protocols. Finally, we briefly present and evaluate algorithms that derive STUMP schedules.},
keywords={access protocols;channel allocation;diversity reception;radiowave propagation;scheduling;synchronisation;time division multiple access;underwater acoustic communication;wireless channels;node position diversity;staggered TDMA underwater MAC protocol;STUMP;underwater channel propagation speed;propagation delay;channel utilization;synchronization protocol;Time division multiple access;Media Access Protocol;Propagation delay;Peer to peer computing;Delay estimation;Underwater communication;Underwater acoustics;Scheduling algorithm;Acoustic devices;Communications Society},
doi={10.1109/INFCOM.2009.5062267},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062268,
author={S. Shetty and M. Song and C. Xin and E. K. Park},
booktitle={IEEE INFOCOM 2009},
title={A Learning-based Multiuser Opportunistic Spectrum Access Approach in Unslotted Primary Networks},
year={2009},
volume={},
number={},
pages={2966-2970},
abstract={Opportunistic spectrum access presents a new approach to wireless spectrum utilization and management. In this paper, we propose a non-cooperative based OSA approach: learning-based approach to allow multiple secondary users to achieve maximal throughput in an unslotted opportunistic spectrum access (OSA) network. In this approach, collisions among secondary users are taken into consideration while making channel sensing decisions. Spectrum maps for secondary users are estimated based on occurrence of collisions. Our approach allows secondary users to achieve maximal throughput by seeking independent spectrum opportunities without exchanging any control information among secondary users. Numerical results show that the learning-based approach obtains near-optimal performance in most of the scenarios.},
keywords={radio networks;radio spectrum management;telecommunication network management;learning-based multiuser opportunistic spectrum access approach;unslotted primary networks;wireless spectrum utilization;wireless spectrum management;Throughput;Cities and towns;Cognitive radio;Switches;Communications Society;Wireless sensor networks;Availability;Radio spectrum management;Interference constraints;Performance analysis},
doi={10.1109/INFCOM.2009.5062268},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062269,
author={P. Minero and M. Franceschetti},
booktitle={IEEE INFOCOM 2009},
title={Throughput of Slotted ALOHA with Encoding Rate Optimization and Multipacket Reception},
year={2009},
volume={},
number={},
pages={2971-2975},
abstract={This paper considers a slotted ALOHA random access system where users send packets to a common receiver with multipacket reception capability. A collection of m users access the shared medium independently of each other with probability p and, upon access, they choose an encoding rate. A collision occurs when the sum of the rates of all the users exceeds the capacity of the channel. We analytically characterize as a function of m and p the encoding rate which maximizes the expected global though put of the system. It is shown that for any value of p the throughput converges to one when m tends to infinity, hence there is no loss due to packet collisions. This is in striking contrast with the well known behavior of slotted ALOHA systems in which users cannot adjust the encoding rate. In that case the throughput decreases to zero as the number of users increases. Finally, assuming that users are selfish, we characterize the encoding rate which maximizes the expected individual throughput of each user, and show that the corresponding Nash equilibrium is not globally optimum.},
keywords={access protocols;encoding;radio reception;encoding rate optimization;multipacket reception;multipacket reception capability;global thoughput;slotted ALOHA systems;Nash equilibrium;Throughput;Encoding;Decoding;H infinity control;Delay;Feedback;Nash equilibrium;Transmitters;Game theory;Communications Society},
doi={10.1109/INFCOM.2009.5062269},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062270,
author={T. Shu and M. Krunz},
booktitle={IEEE INFOCOM 2009},
title={Coordinated Channel Access in Cognitive Radio Networks: A Multi-Level Spectrum Opportunity Perspective},
year={2009},
volume={},
number={},
pages={2976-2980},
abstract={In a cognitive radio network (CRN), spectrum opportunities should be efficiently utilized through careful coordination between cognitive radio (CR) users. In this paper, we formulate the coordinated channel access as a joint power/rate control and channel assignment optimization problem, with the objective of maximizing the sum-rate achieved by all CRs over all channels. The problem is formulated under a generalized multi-level spectrum opportunity framework, which reflects the microscopic spatial opportunity available to CRs. A centralized polynomial-time approximate algorithm to the problem is developed. We prove the algorithm's correctness and show its accuracy through numerical examples.},
keywords={cognitive radio;radio networks;telecommunication channels;coordinated channel access;cognitive radio networks;multi-level spectrum opportunity perspective;channel assignment optimization;polynomial-time approximate algorithm;Cognitive radio;Chromium;Signal to noise ratio;Microscopy;Communications Society;Polynomials;Information theory;Interference constraints;Frequency;Batteries},
doi={10.1109/INFCOM.2009.5062270},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062271,
author={D. Koutsonikolas and Y. C. Hu and C. -. Wang},
booktitle={IEEE INFOCOM 2009},
title={An Empirical Study of Performance Benefits of Network Coding in Multihop Wireless Networks},
year={2009},
volume={},
number={},
pages={2981-2985},
abstract={Recently, network coding has gained much popularity and several practical routing schemes have been proposed for wireless mesh networks that exploit interflow network coding for improved throughput. However, the evaluation of these protocols either assumed simple topologies and traffic patterns such as opposite flows along a single chain, or small, dense networks which have ample overhearing of each other's transmissions in addition to many overlapping flows. In this paper, we seek to answer the fundamental question: how much performance benefit from network coding can be expected for general traffic patterns in a moderate-sized wireless mesh network? We approach this question via an empirical study of both coordinated and opportunistic coding based protocols subject to general traffic patterns. Our study shows the performance benefits under both types of coding for general traffic patterns are extremely limited. We then analyze and uncover fundamental reasons for the limited performance benefits.},
keywords={encoding;packet radio networks;protocols;telecommunication network routing;telecommunication network topology;multihop wireless networks;routing schemes;wireless mesh networks;interflow network coding;network topology;traffic patterns;dense networks;opportunistic coding based protocols;Network coding;Spread spectrum communication;Wireless networks;Telecommunication traffic;Wireless mesh networks;Protocols;Routing;Throughput;Network topology;Performance analysis},
doi={10.1109/INFCOM.2009.5062271},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062272,
author={D. Saha and A. Dutta and D. Grunwald and D. Sicker},
booktitle={IEEE INFOCOM 2009},
title={PHY Aided MAC - A New Paradigm},
year={2009},
volume={},
number={},
pages={2986-2990},
abstract={Network protocols have traditionally been designed using a layered method in part because it is easier to implement some portions of network protocols in software and other portions must be implemented in hardware for performance reasons. These different implementation techniques enforce layer boundaries. In this paper, we show that with the advent of software defined radios, it becomes possible to blur those layer boundaries and produce higher performance network protocols as a result. In this paper we exploit a programmable physical layer and simultaneous transmission to have clients signal whether they have packets to send. By detecting the high energy at the simultaneous transmission, the AP gets the following information: a) which stations have packets to send and b) whether the traffic load is high, medium or low. Again, using the programmable physical layer, the AP schedules clients efficiently while wasting little of the spectrum on signaling overhead. The proposed protocol is a) fast, since no packet transmission is required for polling responses and all clients respond concurrently; b) reliable, as the poll response is contention free and c) scalable. We demonstrate the feasibility of implementing such a system using a FPGA based prototype software defined radio platform. We then show how the MAC protocol can scale using the QualNet network simulator and compare the performance to a contention based protocol.},
keywords={access protocols;field programmable gate arrays;software radio;PHY aided MAC;network protocols;software defined radios;programmable physical layer;traffic load;poll response;FPGA based prototype;MAC protocol;QualNet network simulator;contention based protocol;Physical layer;OFDM;Media Access Protocol;Software radio;Broadcasting;Multiaccess communication;Communications Society;USA Councils;Software performance;Hardware},
doi={10.1109/INFCOM.2009.5062272},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062273,
author={M. Ding and X. Cheng},
booktitle={IEEE INFOCOM 2009},
title={Robust Event Boundary Detection in Sensor Networks - A Mixture Model Based Approach},
year={2009},
volume={},
number={},
pages={2991-2995},
abstract={Detecting event frontline or boundary sensors in a complex sensor network environment is one of the critical problems for sensor network applications. In this paper, we propose a novel algorithm for event frontline sensor detection based on statistical mixture methods with model selection (Akaike, 1973). A boundary sensor is considered as being associated with a multimodal local neighborhood of (univariate or multivariate) sensing readings, and each non-boundary sensor is treated as being with a unimodal sensor reading neighborhood. Furthermore, the set of sensor readings within each sensor's spatial neighborhood is formulated using Gaussian mixture model (McLachlan and Peel, 2000). Two classes of boundary and non-boundary sensors can be effectively classified using the model selection techniques for finite mixture models. Our extensive experimental results demonstrate that our algorithm effectively detects the event boundary with a high accuracy under moderate noise levels.},
keywords={Gaussian processes;wireless sensor networks;robust event boundary detection;sensor networks;mixture model based approach;event frontline detection;boundary sensors;complex sensor network environment;sensor network applications;event frontline sensor detection;statistical mixture methods;model selection;multimodal local neighborhood;nonboundary sensor;unimodal sensor;spatial neighborhood;Gaussian mixture model;finite mixture models;Event detection;Multimodal sensors;Noise robustness;Computer science;USA Councils;Noise level;Information processing;Algorithm design and analysis;Data mining;Temperature sensors},
doi={10.1109/INFCOM.2009.5062273},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062274,
author={H. Yang and H. -. Shen and B. Sikdar and S. Kalyanaraman},
booktitle={IEEE INFOCOM 2009},
title={A Threshold Based MAC Protocol for Cooperative MIMO Transmissions},
year={2009},
volume={},
number={},
pages={2996-3000},
abstract={This paper develops a distributed, threshold based MAC protocol for cooperative multi input multi output (MIMO) transmissions in distributed wireless systems. The protocol uses a thresholding scheme that is updated dynamically based on the queue length at the sending node to achieve low power transmissions while ensuring stability of the transmission queues at the nodes. Simulation results are provided to evaluate the performance of the proposed protocol and compare it against regular point to point as well as fixed group size cooperative MIMO MAC protocols.},
keywords={access protocols;MIMO communication;threshold based MAC protocol;cooperative MIMO transmission;multiinput multioutput transmission;distributed wireless system;Media Access Protocol;MIMO;Wireless sensor networks;Peer to peer computing;Energy consumption;Stability;Bit error rate;Wireless application protocol;Communication system control;Throughput},
doi={10.1109/INFCOM.2009.5062274},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062275,
author={S. S. Ram and V. V. Veeravalli and A. Nedic},
booktitle={IEEE INFOCOM 2009},
title={Distributed Non-Autonomous Power Control through Distributed Convex Optimization},
year={2009},
volume={},
number={},
pages={3001-3005},
abstract={We consider the uplink power control problem where mobile users in different cells are communicating with their base stations. We formulate the power control problem as the minimization of a sum of convex functions. Each component function depends on the channel coefficients from all the mobile users to a specific base station and is assumed to be known only to that base station (only CSIR). We then view the power control problem as a distributed optimization problem that is to be solved by the base stations and propose convergent, distributed and iterative power control algorithms. These algorithms require each base station to communicate with the base stations in its neighboring cells in each iteration and are hence non-autonomous. Since the base stations are connected through a wired backbone the communication overhead is not an issue. The convergence of the algorithms is shown theoretically and also verified through numerical simulations.},
keywords={convex programming;minimisation;mobile computing;power control;distributed non-autonomous power control;distributed convex optimization;uplink power control problem;mobile users;minimization;convex function;channel coefficient;iterative power control algorithm;wired backbone;communication overhead;numerical simulation;Power control;Base stations;Signal to noise ratio;Iterative algorithms;Interference;Distributed control;Spine;Numerical simulation;Transmitters;Communications Society},
doi={10.1109/INFCOM.2009.5062275},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062276,
author={A. R. Curtis and A. Lopez-Ortiz},
booktitle={IEEE INFOCOM 2009},
title={Capacity Provisioning a Valiant Load-Balanced Network},
year={2009},
volume={},
number={},
pages={3006-3010},
abstract={Valiant load balancing (VLB), also called two-stage load balancing, is gaining popularity as a routing scheme that can serve arbitrary traffic matrices. To date, VLB network design is well understood on a logical full-mesh topology, where VLB is optimal even when nodes can fail. In this paper, we address the design and capacity provisioning of arbitrary VLB network topologies. First, we introduce an algorithm to determine if VLB can serve all traffic matrices when a fixed number of arbitrary links fail, and we show how to find a min-cost expansion of the network - via link upgrades and installs - so that it is resilient to these failures. Additionally, we propose a method to design a new VLB network under the fixed-charge network design cost model. Finally, we prove that VLB is no longer optimal on unrestricted topologies, and can require more capacity than shortest path routing to serve all traffic matrices on some topologies. These results rely on a novel theorem that characterizes the capacity VLB requires of links crossing each cut, i.e., a partition, of the network's nodes.},
keywords={resource allocation;telecommunication network routing;telecommunication network topology;telecommunication traffic;capacity provisioning;valiant load-balanced network;two-stage load balancing;routing scheme;arbitrary traffic matrices;VLB network design;logical full-mesh topology;VLB network topologies;min-cost expansion;fixed-charge network design cost model;Telecommunication traffic;Routing;Network topology;Traffic control;Load management;Peer to peer computing;Cost function;Virtual private networks;Communications Society;Computer science},
doi={10.1109/INFCOM.2009.5062276},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{5062277,
author={A. Ramamoorthy and V. Roychowdhury and S. K. Singh},
booktitle={IEEE INFOCOM 2009},
title={Selfish Distributed Compression over Networks},
year={2009},
volume={},
number={},
pages={3011-3015},
abstract={We consider the min-cost multicast problem (under network coding) with multiple correlated sources where each terminal wants to losslessly reconstruct all the sources. This can be considered as the network generalization of the classical distributed source coding (Slepian-Wolf) problem. We study the inefficiency brought forth by the selfish behavior of the terminals in this scenario by modeling it as a noncooperative game among the terminals. The solution concept that we adopt for this game is the popular local Nash equilibrium (Waldrop equilibrium) adapted for the scenario with multiple sources. The degradation in performance due to the lack of regulation is measured by the price of anarchy (POA), which is defined as the ratio between the cost of the worst possible Waldrop equilibrium and the socially optimum cost. Our main result is that in contrast with the case of independent sources, the presence of source correlations can significantly increase the price of anarchy. Towards establishing this result we make several contributions. We characterize the socially optimal flow and rate allocation in terms of four intuitive conditions. This result is a key technical contribution of this paper and is of independent interest as well. Next, we show that the Waldrop equilibrium is a socially optimal solution for a different set of (related) cost functions. Using this, we construct explicit examples that demonstrate that the POA &gt; 1 and determine near- tight upper bounds on the POA as well. The main techniques in our analysis are Lagrangian duality theory and the usage of the supermodularity of conditional entropy. Finally, all the techniques and results in this paper will naturally extend to a large class of network information flow problems where the Slepian-Wolf polytope is replaced by any contra-polymatroid (or more generally polymatroid-like set), leading to a nice class of succinct multi-player games and allow the investigation of other practical and meaningful scenarios beyond network coding as well.},
keywords={data compression;game theory;source coding;selfish distributed compression;mincost multicast problem;classical distributed source coding problem;Slepian-Wolf problem;noncooperative game;local Nash equilibrium;price of anarchy;rate allocation;Waldrop equilibrium;Lagrangian duality theory;network information flow problems;network coding;Network coding;Cost function;Source coding;Lagrangian functions;Entropy;Communications Society;Nash equilibrium;Degradation;Upper bound;Large-scale systems},
doi={10.1109/INFCOM.2009.5062277},
ISSN={0743-166X},
month={April},}

