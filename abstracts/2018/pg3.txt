@INPROCEEDINGS{8485864,
author={S. Bitton and Y. Emek and S. Kutten},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Efficient Jobs Dispatching in Emerging Clouds},
year={2018},
volume={},
number={},
pages={2033-2041},
abstract={This study was carried in the context of the development of technologies for a cloud that uses an optical network for internal communication. The problem addressed in this paper deals with dispatching jobs - units of work, to be performed by machines on the cloud. Sending (or migrating) a job to a machine involves establishing a lightpath (a la circuit switching); this incurs a significant setup cost, but while it exists, the lightpath's capacity is very high. Hence, moving one job is about as expensive as moving a set of jobs over the same lightpath. Our goal is to develop online network dispatching algorithms for a work conserving job scheduling. That is, consider a set of jobs the dispatcher is responsible for their executions on some set SM of machines. Any machine in the network may join or (when not executing a job) leave SM according to decisions made outside the scope of this paper. Whenever a machine joins the set or is in the set and has just finished executing a job, it issues a request for a new job to perform and the dispatcher must send this machine a job that has not been executed yet (if such exists). Every machine can perform any of the jobs, and each job is performed on a single machine. The main algorithmic challenge in this context boils down to the following questions: How many jobs should we send to a requesting machine (or to some intermediate storage to be distributed from there)? From the storage on which machine should these jobs be taken? The algorithms developed here are shown to be efficient in reducing the costs of establishing lightpaths. As opposed to related algorithms for delivering consumable resources (in other contexts), we prove that our online algorithms are fully competitive. We present randomized online algorithms for two different settings: in the first it is assumed that each message requires establishing a lightpath and thus, incurs a setup cost; in the second, we distinguish between messages that carry job sets and small control messages sent by the algorithm, where the latter type of messages is assumed to be sent over a designated (non-optical) control plane at a negligible cost. Our algorithms are quite simple, though the analysis turns out to be rather involved. They are designed (and rigorously analyzed) for a general architecture, but would be especially efficient in fat tree architectures - the common choice in many data centers.},
keywords={cloud computing;computer centres;optical computing;optical fibre networks;optimisation;resource allocation;telecommunication scheduling;efficient jobs dispatching;job sets;optical network;internal communication;circuit switching;randomized online algorithms;non-optical control plane;general architecture;data centers;Dispatching;Optical fiber networks;Data centers;Cloud computing;Conferences;Industrial engineering;Electronic mail},
doi={10.1109/INFOCOM.2018.8485864},
ISSN={},
month={April},}
@INPROCEEDINGS{8485924,
author={X. Tan and Z. Sun and D. Koutsonikolas and J. M. Jornet},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Enabling Indoor Mobile Millimeter-wave Networks Based on Smart Reflect-arrays},
year={2018},
volume={},
number={},
pages={270-278},
abstract={The millimeter-wave (mmWave) frequency band has been utilized in the IEEE 802.11ad standard to achieve multi-Gbps throughput. Despite the advantages, mmWave links are highly vulnerable to both user and environmental mobility. Since mmWave radios use highly directional antennas, the line-of-sight (LOS) signal can be easily blocked by various obstacles, such as walls, furniture, and humans. In the complicated indoor environment, it is highly possible that the blocked mmWave link cannot be restored no matter how the access point and the mobile user change their antenna directions. To address the problem and enable indoor mobile mmWave networks, in this paper, we introduce the reconfigurable 60 GHz reflect-arrays to establish robust mmWave connections for indoor networks even when the links are blocked by obstructions. First, the reconfigurable 60 GHz reflect-array is designed, implemented, and modeled. Then a three-party beam-searching protocol is designed for reflect-array-assisted 802.11ad networks. Finally, an optimal array deployment strategy is developed to minimize the link outage probability in indoor mobile mmWave networks. The proposed solution is validated and evaluated by both in-lab experiments and computer simulations.},
keywords={indoor radio;millimetre wave communication;mobile communication;protocols;reflectarray antennas;indoor mobile mmWave networks;smart reflect-arrays;millimeter-wave frequency band;IEEE 802.11ad standard;mmWave links;environmental mobility;mmWave radios;highly directional antennas;line-of-sight signal;mobile user;antenna directions;robust mmWave connections;indoor networks;reflect-array-assisted 802.11ad networks;optimal array deployment strategy;indoor mobile millimeter-wave networks;frequency 60.0 GHz;Indoor environments;Directive antennas;Protocols;Relays;Transceivers;Antenna arrays;Systems architecture},
doi={10.1109/INFOCOM.2018.8485924},
ISSN={},
month={April},}
@INPROCEEDINGS{8486389,
author={P. Wan and H. Yuan and J. Wang and J. Ren and Y. Zhang},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Joint Selection and Scheduling of Communication Requests in Multi-Channel Wireless Networks under SINR Model},
year={2018},
volume={},
number={},
pages={2312-2320},
abstract={Consider a set of communication requests in a multichannel wireless network, each of which is associated with a traffic demand of at most one unit of transmission time, and a weight representing the utility if its demand is fully met. A subset of them is said to be feasible if they can be scheduled within one unit of time. The problem Maximum-Weighted Feasible Set (MWFS) seeks a feasible subset with maximum total weight together with a transmission schedule of them whose length is at most one unit of time. This paper develops an efficient O(log2α) -approximation algorithms for the problem MWFS under the physical interference model (aka, SINR model) with a fixed monotone and sub-linear power assignment, where α is the maximum number of requests which can transmit successfully at the same time over the same channel.},
keywords={approximation theory;computational complexity;radio networks;radiofrequency interference;set theory;telecommunication scheduling;telecommunication traffic;wireless channels;joint communication request selection;joint communication request scheduling;MWFS problem;maximum-weighted feasible set problem;fixed monotone;sublinear power assignment;maximum total weight;transmission time;traffic demand;multichannel wireless networks;SINR model;physical interference model;transmission schedule;Interference;Approximation algorithms;Schedules;Signal to noise ratio;Conferences;Wireless communication;Silicon},
doi={10.1109/INFOCOM.2018.8486389},
ISSN={},
month={April},}
@INPROCEEDINGS{8486270,
author={Y. Bai and P. Hao and Y. Zhang},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={A Case for Web Service Bandwidth Reduction on Mobile Devices with Edge-Hosted Personal Services},
year={2018},
volume={},
number={},
pages={657-665},
abstract={We show that many popular mobile services suffer from excessive network bandwidth consumption. The root cause is that the existing mobile/cloud communication interfaces are designed and optimized for service providers rather than end user devices. Solving the problem is challenging, because of the conflicted interests of service providers and mobile devices. We propose Edge-hosted Personal Service (EPS), with which device-oriented solutions can be easily deployed without affecting service providers. EPS also enjoys other notable advantages, including enabling new mobile services, reducing loads on the cloud, and benefiting delay-sensitive applications. We demonstrate the usefulness of EPS by designing ETA (Edge-based web Traffic Adaptation), an effective solution for the excessive bandwidth consumption problem, and deploy ETA with a prototype EPS system. By exploring lightweight virtualization techniques, our EPS prototype system is highly scalable in terms of concurrent EPS instances, and secure in terms of resource isolation. The real-world evaluation shows that our ETA EPS can effectively reduce bandwidth for mobile devices with small overheads.},
keywords={bandwidth allocation;cloud computing;Internet;mobile computing;mobile radio;resource allocation;security of data;Web services;ETA EPS;mobile devices;excessive network bandwidth consumption;service providers;end user devices;device-oriented solutions;excessive bandwidth consumption problem;EPS prototype system;concurrent EPS instances;mobile services;edge-hosted personal services;Web service bandwidth reduction;Edge-based Web traffic adaptation;mobile-cloud communication interfaces;delay-sensitive applications;lightweight virtualization techniques;resource isolation;security;bandwidth reduction;Mobile handsets;Google;Bandwidth;Web services;Payloads;Servers;Cloud computing},
doi={10.1109/INFOCOM.2018.8486270},
ISSN={},
month={April},}
@INPROCEEDINGS{8486414,
author={Y. Zhang and J. Chen and L. Cai and J. Pan},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={EV Charging Network Design with Transportation and Power Grid Constraints},
year={2018},
volume={},
number={},
pages={2492-2500},
abstract={Connected electric vehicles (EVs) are a key component of future intelligent and green transportation systems, and the penetration of EVs depends on convenient and cost-effective charging services. In addition to being charged at home or on parking lots, a charging network is needed for EVs right off the road. This paper first focuses on the optimal charging network design for charging service providers, considering the time-varying and location-dependent demands from vehicles and constraints of power grids. To optimize the charging station locations and the number of chargers in each station, we first model the coverage area of each possible location to estimate the dynamic charging requirements of EVs. Then, we formulate the problem as profit maximization, which is a mixed-integer program. To make the problem tractable, we investigate the features of the problem and obtain a necessary condition to deploy a charging station and derive the upper and lower bounds of the number of chargers in each station. Given the analysis, we take two steps to transform and relax the problem to convex optimization. A fast-converging search algorithm is further proposed based on the profit of each possible location. Using real vehicle traces, simulation results show that the proposed algorithm can maximize the total profit when fewer charging stations and chargers are initially needed, which is more attractive for charging service providers.},
keywords={battery powered vehicles;integer programming;optimisation;power grids;transportation;green transportation systems;cost-effective charging services;optimal charging network design;service providers;location-dependent demands;power grids;charging station locations;possible location;dynamic charging requirements;problem tractable;vehicle traces;EV charging network design;power grid constraints;connected electric vehicles;future intelligent transportation systems;EV;profit maximization;mixed-integer program;necessary condition;convex optimization;fast-converging search algorithm;charging service providers;Charging stations;Power grids;Upper bound;Space stations;Nickel;Electric vehicle charging;Electric Vehicles;Charging Service;Charing Station;Power Grid},
doi={10.1109/INFOCOM.2018.8486414},
ISSN={},
month={April},}
@INPROCEEDINGS{8486390,
author={M. Xiao and C. Zhou and V. Swaminathan and Y. Liu and S. Chen},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={BAS-360°: Exploring Spatial and Temporal Adaptability in 360-degree Videos over HTTP/2},
year={2018},
volume={},
number={},
pages={953-961},
abstract={Today, 360-degree video streaming has become a popular Internet service with the rise of affordable virtual reality (VR) technologies. However, streaming 360-degree videos suffers from the prohibitive bandwidth demand. Existing bandwidth-efficient solutions mainly focus on exploiting the inherent spatial adaptability of 360-degree videos, delivering only video content (spatially-cut tiles) in the viewer's region of interest (ROI) with higher quality. Temporal adaptability, which has been widely leveraged in HTTP streaming, has not been well exploited to select proper quality for video segments according to the bandwidth variations. When these two dimensions of adaptability are jointly considered, bitrate selection for the tiles become more complicated and challenging. The importance of a tile with a spatial coordination played at a specific time should be quantified so that we can determine how to allocate bandwidth for improving the viewer's quality of experience. Furthermore, viewer's head orientation prediction is highly variable, which makes the determination of important tiles highly dynamic. In addition, network fluctuations are very common on the Internet. To overcome these challenges, we propose Bi-Adaptive Streaming for 360-degree videos (BAS-360°). In BAS-360°, both spatial and temporal adaptabilities are explored in the bitrate selection for different tiles. The objective is to minimize the bandwidth waste by allocating bandwidth to more important tiles (the tiles that are more likely to be watched). To tackle the high variability of visual region prediction and the unpredictable network fluctuations, we employ two features provided by HTT P /2: stream termination and stream priority, to efficiently organize tile delivery. Evaluation results show that BAS-360° outperforms naive tile-based 360-degree video streaming strategies when network fluctuations or errors in viewport predictions occur.},
keywords={hypermedia;image segmentation;Internet;transport protocols;video signal processing;video streaming;virtual reality;BAS-360;temporal adaptability;360-degree video streaming;bandwidth-efficient solutions;inherent spatial adaptability;HTTP streaming;video segments;tile-based 360-degree video;virtual reality technologies;Internet service;region of interest;Bi-Adaptive Streaming for 360-degree videos;Videos;Bandwidth;Bit rate;Visualization;Streaming media;Quality of experience;Conferences;HTTP streaming;360-degree video streaming;HTTP/2},
doi={10.1109/INFOCOM.2018.8486390},
ISSN={},
month={April},}
@INPROCEEDINGS{8485973,
author={Q. Liang and E. Modiano},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Network Utility Maximization in Adversarial Environments},
year={2018},
volume={},
number={},
pages={594-602},
abstract={Stochastic models have been dominant in network optimization theory for over two decades, due to their analytical tractability. However, these models fail to capture non-stationary or even adversarial network dynamics which are of increasing importance for modeling the behavior of networks under malicious attacks or characterizing short-term transient behavior. In this paper, we consider the network utility maximization problem in adversarial network settings. In particular, we focus on the tradeoffs between total queue length and utility regret which measures the difference in network utility between a causal policy and an “oracle” that knows the future within a finite time horizon. Two adversarial network models are developed to characterize the adversary's behavior. We provide lower bounds on the tradeoff between utility regret and queue length under these adversarial models, and analyze the performance of two control policies (i.e., the Drift-plus-Penalty algorithm and the Tracking Algorithm).},
keywords={optimisation;queueing theory;stochastic processes;telecommunication security;adversarial network settings;total queue length;utility regret;adversarial network models;adversarial models;adversarial environments;stochastic models;network optimization theory;analytical tractability;malicious attacks;network utility maximization problem;adversarial network dynamics;short-term transient behavior;Analytical models;Computational modeling;Stochastic processes;Optimization;Microsoft Windows;Heuristic algorithms;Wireless networks},
doi={10.1109/INFOCOM.2018.8485973},
ISSN={},
month={April},}
@INPROCEEDINGS{8486392,
author={V. Reddyvari and P. Parag and S. Shakkottai},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Mode-Suppression: A Simple and Provably Stable Chunk-Sharing Algorithm for P2P Networks},
year={2018},
volume={},
number={},
pages={2573-2581},
abstract={The ability of a P2P network to scale its throughput up in proportion to the arrival rate of peers has recently been shown to be crucially dependent on the chunk sharing policy employed. Some policies can result in low frequencies of a particular chunk, known as the missing chunk syndrome, which can dramatically reduce throughput and lead to instability of the system. For instance, commonly used policies that nominally “boost” the sharing of infrequent chunks such as the well-known rarest-first algorithm have been shown to be unstable. Recent efforts have largely focused on the careful design of boosting policies to mitigate this issue. We take a complementary viewpoint, and instead consider a policy that simply prevents the sharing of the most frequent chunk(s). Following terminology from statistics wherein the most frequent value in a data set is called the mode, we refer to this policy as mode suppression. We prove the stability of this algorithm using Lyapunov techniques. We also design a distributed version that suppresses the mode via an estimate obtained by sampling three randomly selected peers. We show numerically that both algorithms perform well at minimizing total download times, with distributed mode suppression outperforming all others that we tested against.},
keywords={Lyapunov methods;peer-to-peer computing;missing chunk syndrome;chunk-sharing algorithm;rarest-first algorithm;Lyapunov techniques;P2P Networks;distributed mode suppression;infrequent chunks;Servers;Peer-to-peer computing;Stability analysis;Throughput;Boosting;Frequency estimation;Clocks},
doi={10.1109/INFOCOM.2018.8486392},
ISSN={},
month={April},}
@INPROCEEDINGS{8485949,
author={B. Xia and S. Shakkottai and V. Subramanian},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Small-Scale Markets for Bilateral Resource Trading in the Sharing Economy},
year={2018},
volume={},
number={},
pages={2447-2455},
abstract={We consider a general small-scale market for agent-to-agent resource sharing, in which each agent could either be a server (seller) or a client (buyer) in each time period. In every time period, a server has a certain amount of resources that any client could consume, and randomly gets matched with a client. Our target is to maximize the resource utilization in such an agent-to-agent market, where the agents are strategic. During each transaction, the server gets money and the client gets resources. Hence, trade ratio maximization implies efficiency maximization of our system. We model the proposed market system through a Mean Field Game approach and prove the existence of the Mean Field Equilibrium, which can achieve an almost 100% trade ratio. Finally, we carry out a simulation study motivated by an agent-to-agent computing market, and a case study on a proposed photovoltaic market, and show the designed market benefits both individuals and the system as a whole.},
keywords={game theory;multi-agent systems;resource allocation;stock markets;agent-to-agent resource sharing;server;client;time period;resource utilization;trade ratio maximization;efficiency maximization;market system;agent-to-agent computing market;photovoltaic market;bilateral resource trading;sharing economy;mean field game approach;mean field equilibrium;market benefits;small-scale market;Servers;Currencies;Games;Computational modeling;Switches;Conferences},
doi={10.1109/INFOCOM.2018.8485949},
ISSN={},
month={April},}
@INPROCEEDINGS{8485811,
author={M. Ji and R. Chen},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Fundamental Limits of Wireless Distributed Computing Networks},
year={2018},
volume={},
number={},
pages={2600-2608},
abstract={We consider a wireless distributed computing network, where all computing nodes (workers) are connected via wireless medium obeying the seminal protocol channel model. In particular, we focus on the MapReduce-type platform, where each worker is assigned to compute some arbitrary output functions from F input files, which are distributively cached in all workers. The overall computation is decomposed into computing a set of “Map” and “Reduce” functions across all workers. The goal is to characterize the minimum computing latency as a function of the computation load. Unlike other related works, which consider either wireline settings or restrict the communication among workers to single-hop, here we focus on the wireless scenario and do not constrain any communication schemes. We propose a data set cache strategy based on a deterministic assignment of Maximum Distance Separable (MDS)-coded date sets over all input files, and a coded multicast transmission strategy where the workers send linearly coded computing results to each other in order to collectively satisfy their assigned tasks. We show that our approach can achieve a scalable communication latency, outperform the state of the art schemes in the order sense, and achieve the information theoretic outer bound within a multiplicative constant factor in practical parameter regimes.},
keywords={cache storage;channel coding;linear codes;multicast communication;protocols;radio networks;wireless channels;wireless scenario;data set cache strategy;Maximum Distance Separable-coded date sets;wireless distributed computing network;computing nodes;wireless medium;seminal protocol channel model;arbitrary output functions;computation load;Reduce functions;MapReduce-type platform;minimum computing latency;deterministic assignment;coded multicast transmission strategy;linearly coded computing results;information theoretic outer bound;multiplicative constant factor;Wireless communication;Wireless sensor networks;Communication system security;Protocols;Cloud computing;Conferences},
doi={10.1109/INFOCOM.2018.8485811},
ISSN={},
month={April},}
@INPROCEEDINGS{8486286,
author={Y. Cao and D. Veitch},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Network Timing, Weathering the 2016 Leap Second},
year={2018},
volume={},
number={},
pages={1826-1834},
abstract={We collect high resolution timing packet data from 459 public Stratum-1 NTP servers during the leap second event of Dec. 2016, including all those participating in the NTP Pool Project, using a testbed with GPS and atomic clock synchronized DAG cards. We report in detail on a wide variety of anomalous behaviors found both at the NTP protocol level, and in the detailed timestamp performance of the server clocks themselves, which can last days or even weeks after the event. Overall, only 37.3% of servers had Adequate performance overall.},
keywords={atomic clocks;Global Positioning System;network servers;protocols;synchronisation;DAG cards;anomalous behaviors;NTP protocol level;detailed timestamp performance;server clocks;network timing;2016 leap second;high resolution timing packet data;Dec. 2016;NTP Pool Project;atomic clock;public Stratum-1 NTP servers;Conferences},
doi={10.1109/INFOCOM.2018.8486286},
ISSN={},
month={April},}
@INPROCEEDINGS{8486331,
author={Z. Zhang and Y. Sun and A. Sabharwal and Z. Chen},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Impact of Channel State Misreporting on Multi-user Massive MIMO Scheduling Performance},
year={2018},
volume={},
number={},
pages={917-925},
abstract={The robustness of system throughput with scheduling is a critical issue. In this paper, we analyze the sensitivity of multi-user scheduling performance to channel misreporting in systems with massive antennas. The main result is that for the round-robin scheduler combined with max-min power control, the channel magnitude misreporting is harmful to the scheduling performance and has a different impact from the purely physical layer analysis. Specifically, for the homogeneous users that have equal average signal-to-noise ratios (SNRs), underreporting is harmful, while overreporting is beneficial to others. In under-reporting, the asymptotic rate loss on others is derived, which is tight when the number of antennas is huge. One interesting observation in our research is that the rate loss “periodically” increases and decreases as the number of misreporters grows. For the heterogeneous users that have various SNRs, both underreporting and overreporting can degrade the scheduler performance. We observe that strong misreporting changes the user grouping decision and hence greatly decreases some users' rates regardless of others gaining rate improvements, while with carefully designed weak misreporting, the scheduling decision keeps fixed and the rate loss on others is shown to grow nearly linearly with the number of misreporters.},
keywords={antenna arrays;MIMO communication;minimax techniques;power control;telecommunication scheduling;wireless channels;multiuser massive MIMO scheduling performance;system throughput;multiuser scheduling performance;massive antennas;round-robin scheduler;max-min power control;channel magnitude misreporting;purely physical layer analysis;homogeneous users;average signal-to-noise ratios;underreporting;overreporting;under-reporting;asymptotic rate loss;heterogeneous users;user grouping decision;scheduling decision;SNR;channel state misreporting impact;strong misreporting;weak misreporting;MIMO communication;Downlink;Signal to noise ratio;Resource management;Processor scheduling;Antennas;Precoding},
doi={10.1109/INFOCOM.2018.8486331},
ISSN={},
month={April},}
@INPROCEEDINGS{8486341,
author={H. Ge and R. A. Berry},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Dominant Strategy Allocation of Divisible Network Resources with Limited Information Exchange},
year={2018},
volume={},
number={},
pages={2753-2761},
abstract={A fundamental problem in many network systems is how to allocate limited resources among competing agents, who may have their own incentives. The well-known Vickrey-Clarke-Groves (VCG) mechanism provides an elegant solution to this incentive issue. In particular, VCG implements the socially optimal outcome in dominant strategies. However, it is also well-known that this mechanism can require an excessive amount of communication. Approaches have been studied that relax the communication requirements while also relaxing the incentive guarantees to use Nash equilibria instead of dominant strategies. Here, we take a different approach and study mechanisms with limited information that still have dominant strategy outcomes, but suffer an efficiency loss. We characterize this loss for the case of a single divisible resource. We first consider a mechanism in which information is limited by quantizing the resource into a finite number of units and allocating each of these to one agent via a VCG mechanism. This limits each agent to submitting a finite number of real values. We subsequently consider the case where each value is also quantized before being reported by each agent. Finally, we present numerical examples of the performance of these mechanisms.},
keywords={game theory;resource allocation;telecommunication network management;Nash equilibria;communication requirements;socially optimal outcome;incentive issue;Vickrey-Clarke-Groves mechanism;competing agents;limited information exchange;divisible network resources;dominant strategy allocation;VCG mechanism;single divisible resource;incentive guarantees;Resource management;Optimization;Conferences;Information exchange;Nash equilibrium;Electronic mail;Bandwidth},
doi={10.1109/INFOCOM.2018.8486341},
ISSN={},
month={April},}
@INPROCEEDINGS{8486394,
author={R. Hou and I. Jahja and L. Luu and P. Saxena and H. Yu},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Randomized View Reconciliation in Permissionless Distributed Systems},
year={2018},
volume={},
number={},
pages={2528-2536},
abstract={In a sybil attack, an adversary creates a large number of fake identities/nodes and have them join the system. Computational puzzles have long been investigated as a possible sybil defense: If a node fails to solve the puzzle in a timely fashion, it will no longer be accepted by other nodes. However, it is still possible for a malicious node to behave in such a way that it is accepted by some honest nodes but not other honest nodes. This results in different honest nodes having different views on which set of nodes should form the system. Such view divergence, unfortunately, breaks the overarching assumption required by many existing security protocols. Partly spurred by the growing popularity of Bitcoin, researchers have recently formalized the above view divergence problem and proposed interesting solutions (which we call view reconciliation protocols). For example, in CRYPTO 2015, Andrychowicz and Dziembowski proposed a view reconciliation protocol with Θ(N) time complexity, with N being the number of honest nodes in the system. All existing view reconciliation protocols so far have a similar Θ(N) time complexity. As this paper's main contribution, we propose a novel view reconciliation protocol with a time complexity of only Θ([ln N/ln ln N]). To achieve such an exponential improvement, we aggressively exploit randomization.},
keywords={computational complexity;peer-to-peer computing;protocols;security of data;randomized view reconciliation;permissionless distributed systems;sybil attack;malicious node;view divergence problem;view reconciliation protocol;fake identities;honest nodes;security protocols;time complexity;fake nodes;sybil defense;Bitcoin;randomization;Protocols;Bitcoin;Time complexity;Error probability;Computer science},
doi={10.1109/INFOCOM.2018.8486394},
ISSN={},
month={April},}
@INPROCEEDINGS{8486242,
author={J. Zhang and J. Sun and R. Zhang and Y. Zhang and X. Hu},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Privacy-Preserving Social Media Data Outsourcing},
year={2018},
volume={},
number={},
pages={1106-1114},
abstract={User-generated social media data are exploding and of high demand in public and private sectors. The disclosure of intact social media data exacerbates the threats to user privacy. In this paper, we first identify a text-based user-linkage attack on current data outsourcing practices, in which the real users in an anonymized dataset can be pinpointed based on the users' unprotected text data. Then we propose a framework for differentially privacy-preserving social media data outsourcing for the first time in literature. Within our framework, social media data service providers can outsource perturbed datasets to provide users differential privacy while offering high data utility to social media data consumers. Our differential privacy mechanism is based on a novel notion of E - text indistinguishability, which we propose to thwart the text-based user-linkage attack. Extensive experiments on real-world and synthetic datasets confirm that our framework can enable high-level differential privacy protection and also high data utility.},
keywords={data protection;outsourcing;social networking (online);text analysis;social media data consumers;text-based user-linkage attack;user-generated social media data;differentially privacy-preserving social media data outsourcing;social media data service providers;differential privacy protection;unprotected text data;E-text indistinguishability;Data privacy;Outsourcing;Privacy;Twitter;Companies},
doi={10.1109/INFOCOM.2018.8486242},
ISSN={},
month={April},}
@INPROCEEDINGS{8485960,
author={G. Jung and P. Rahimzadeh and Z. Liu and S. Ha and K. Joshi and M. Hiltunen},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Virtual Redundancy for Active-Standby Cloud Applications},
year={2018},
volume={},
number={},
pages={1916-1924},
abstract={VM redundancy is the foundation of resilient cloud applications. While active-active approaches combined with load balancing and autoscaling are usually resource efficient, the stateful nature of many cloud applications often necessitates 1+1 (or 1+n) active-standby approaches. Keeping the standbys, however, could result in inefficient utilization of cloud resources. We explore an intriguing cloud-based solution, where standby VMs from active-standby applications are selectively overbooked to reduce resources reserved for failures. The approach requires careful VM placement to avoid a situation where multiple standby VMs activate simultaneously on the same host and thus cannot get the full resource entitlement. Indeed today's clouds do not have this visibility to the applications. We rectify this situation through ShadowBox, a novel redundancy-aware VM scheduler that optimizes the placement and activation of standby VMs, while assuring applications' resource entitlements. Evaluation on a large-scale cloud shows that ShadowBox can significantly improve resource utilization (i.e., more than 2.5 times than traditional approaches) while minimizing the impact on applications' entitlements.},
keywords={cloud computing;resource allocation;scheduling;software reliability;virtual machines;active-active approaches;load balancing;autoscaling;active-standby approaches;cloud resources;large-scale cloud;resource utilization;virtual redundancy;active-standby cloud applications;VM redundancy;cloud-based solution;VM placement;redundancy-aware VM scheduler;Cloud computing;Servers;Ear;Redundancy;Resource management;Topology;Conferences},
doi={10.1109/INFOCOM.2018.8485960},
ISSN={},
month={April},}
@INPROCEEDINGS{8485958,
author={Y. Liu and Z. Li},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={aLeak: Privacy Leakage through Context - Free Wearable Side-Channel},
year={2018},
volume={},
number={},
pages={1232-1240},
abstract={We revisit a crucial privacy problem in this paper - can the sensitive information, like the passwords and personal data, frequently typed by user on mobile devices be inferred through the motion sensors of wearable device on user's wrist, e.g., smart watch or wrist band? Existing works have achieved the initial success under certain context-aware conditions, such as 1) the horizontal keypad plane, 2) the known keyboard size, 3) and/or the last keystroke on a fixed “enter” button. Taking one step further, the key contribution of this paper is to fully demonstrate, more importantly alarm people, the further risks of typing privacy leakage in much more generalized context-free scenarios, which are related to most of us for the daily usage of mobile devices. We validate this feasibility by addressing a series of unsolved challenges and developing a prototype system aLeak. Extensive experiments show the efficacy of aLeak, which achieves promising successful rates in the attack from more than 300 rounds of different users' typings on various mobile platforms without any context-related information.},
keywords={data privacy;mobile computing;sensitive information;passwords;personal data;mobile devices;motion sensors;wearable device;smart watch;context-aware conditions;horizontal keypad plane;mobile platforms;context-related information;privacy problem;privacy leakage;aLeak;context-free wearable side-channel;Keyboards;Trajectory;Side-channel attacks;Privacy;Wearable sensors;Password},
doi={10.1109/INFOCOM.2018.8485958},
ISSN={},
month={April},}
@INPROCEEDINGS{8485987,
author={Z. Zhou and B. Chen},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={RFID Counting over Time-Varying Channels},
year={2018},
volume={},
number={},
pages={1142-1150},
abstract={For many applications that use RFID technology, it is important to count the number of RFID tags accurately. However, the wireless channel between the RFID tags and readers can introduce communication errors, and the error rate may vary significantly over time. No existing protocol can perform RFID counting robustly (i.e., maintaining the estimation quality) over time-varying channels. In this paper, we design RRC, a Robust RFID Counting protocol that offers provable guarantees on estimation quality over time-varying channels. Specifically, regardless of how the communication errors occur, the final output generated by RRC is always a standard (e, δ) estimate of the correct count n. Furthermore, the expected amount of time needed by RRC is O(Y + 1/2 + (log log n)2) for a constant 6, where Y is the number of communication errors encountered by RRC. This makes the efficiency of RRC asymptotically near-optimal.},
keywords={channel estimation;protocols;radiofrequency identification;time-varying channels;wireless channels;RFID tag technology;communication error rate;robust RFID counting protocol;RRC;estimation quality;wireless channel;time-varying channels;Protocols;Robustness;Estimation;Error analysis;Time-varying channels;RFID tags},
doi={10.1109/INFOCOM.2018.8485987},
ISSN={},
month={April},}
@INPROCEEDINGS{8486223,
author={I. Livadariu and K. Benson and A. Elmokashfi and A. Dhamdhere and A. Dainotti},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Inferring Carrier-Grade NAT Deployment in the Wild},
year={2018},
volume={},
number={},
pages={2249-2257},
abstract={Given the increasing scarcity of IPv4 addresses, network operators are resorting to measures to expand their address pool or prolong the life of existing addresses. One such approach is Carrier-Grade NAT (CGN), where many end-users in a network share a single public IPv4 address. There is limited data about the prevalence of CGN, despite the implications on performance, security, and ultimately, the adoption of IPv6. In this work, we present passive measurement-based techniques for detecting CGN deployments across the entire Internet, without the requirement of access to machines behind a CGN. Specifically, we identify patterns in how client IP addresses are observed at M-Lab servers and at the UCSD network telescope to infer whether those clients are behind a CGN. We apply our methods on data collected from 2014 to 2016. We find that CGN deployment is increasing rapidly. Overall, we infer that 4.1K autonomous systems are deploying CGN, 6 times the number inferred by the most recent studies.},
keywords={computer network security;Internet;IP networks;telecommunication traffic;carrier-grade NAT deployment;Internet;M-Lab servers;autonomous systems;CGN deployment;UCSD network telescope;client IP addresses;passive measurement-based techniques;IPv6;single public IPv4 address;address pool;network operators;IP networks;Internet;Conferences;Telescopes;Autonomous systems;Organizations;Measurement},
doi={10.1109/INFOCOM.2018.8486223},
ISSN={},
month={April},}
@INPROCEEDINGS{8486297,
author={L. Bao and C. Q. Wu and H. Qi and W. Chen and X. Zhang and W. Han and W. Wei and E. Tail and H. Wang and J. Zhai and X. Chen},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={LAS: Logical-Block Affinity Scheduling in Big Data Analytics Systems},
year={2018},
volume={},
number={},
pages={522-530},
abstract={Parallel computing combined with distributed data storage and management has been widely adopted by most big data analytics systems. Scheduling computing tasks to improve data locality is crucial to the performance of such systems. While existing schedulers target near-data scheduling on top of physical data blocks, these systems face a new scheduling problem where computing tasks process table-based datasets directly and access large physical blocks indirectly through their indices stored in associated small logical blocks. This new problem invalidates the basic assumption made by many existing algorithms on near-data scheduling. In this paper, we propose a Logical-block Affinity Scheduling (LAS) algorithm to coordinate the near-data scheduling of computing tasks and the placement of logical blocks for a desired balance between data-locality and load-balancing to maximize system throughput. The proposed algorithm is implemented and evaluated using a well-known big data benchmark and a practical production system deployed in public clouds. Extensive experimental results illustrate the performance superiority of LAS over three existing scheduling algorithms.},
keywords={Big Data;data analysis;parallel processing;resource allocation;scheduling;storage management;big data analytics systems;parallel computing;distributed data storage;near-data scheduling;physical data blocks;scheduling problem;Logical-block Affinity Scheduling algorithm;data-locality;distributed data management;table-based datasets;load-balancing;parallel data processing;Task analysis;Scheduling;Servers;Big Data;Scheduling algorithms;Sparks},
doi={10.1109/INFOCOM.2018.8486297},
ISSN={},
month={April},}
@INPROCEEDINGS{8485875,
author={Y. Geng and Y. Yang and G. Cao},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Energy-Efficient Computation Offloading for Multicore-Based Mobile Devices},
year={2018},
volume={},
number={},
pages={46-54},
abstract={Modern mobile devices are equipped with multicore-based processors, which introduce new challenges on computation offloading. With the big.LITTLE architecture, instead of only deciding locally or remotely running a task in the traditional architecture, we have to consider how to exploit the new architecture to minimize energy while satisfying application completion time constraints. In this paper, we address the problem of energy-efficient computation offloading on multicore-based mobile devices running multiple applications. We first formalize the problem as a mixed-integer nonlinear programming problem that is NP-hard, and then propose a novel heuristic algorithm to jointly solve the offloading decision and task scheduling problems. The basic idea is to prioritize tasks from different applications to make sure that both application time constraints and task-dependency requirements are satisfied. To find a better schedule while reducing the schedule searching overhead, we propose a critical path based solution which recursively checks the tasks and moves tasks to the right CPU cores to save energy. Simulation and experimental results show that our offloading algorithm can significantly reduce the energy consumption of mobile devices while satisfying the application completion time constraints.},
keywords={computational complexity;integer programming;mobile computing;multiprocessing systems;nonlinear programming;power aware computing;processor scheduling;energy-efficient computation offloading;multicore-based mobile devices;modern mobile devices;multicore-based processors;traditional architecture;multiple applications;mixed-integer nonlinear programming problem;offloading decision;different applications;application time constraints;task-dependency requirements;critical path based solution;offloading algorithm;energy consumption;application completion time constraints;Task analysis;Multicore processing;Cloud computing;Mobile handsets;Energy consumption;Time factors},
doi={10.1109/INFOCOM.2018.8485875},
ISSN={},
month={April},}
@INPROCEEDINGS{8486393,
author={L. Bedogni and M. Fiore and C. Glacet},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Temporal Reachability in Vehicular Networks},
year={2018},
volume={},
number={},
pages={81-89},
abstract={Upcoming mobile network technologies developed in the context of 5G and DSRC are expected to finally legitimize direct data transfers among vehicles as a standard communication paradigm. We investigate fundamental properties of the topology of vehicular networks built on top of these emerging vehicle-to-vehicle communication technologies. Our study yields multiple elements of originality: ( i) it addresses temporal connectivity, which has been poorly investigated despite a high relevance for vehicular network operations; (ii) it introduces exact but computationally efficient models of the temporal connectivity of vehicular networks; (iii) it evaluates the proposed models in urban settings that exhibit an unprecedented combination of dependability, scale and generality of vehicular mobility. This approach lets us unveil an apparent scale-and city-invariant law of temporal reachability in vehicular networks. Finally, we open our original scenarios to the research community, so as to ensure reproducibility of our results and foster further investigations of vehicular network performance.},
keywords={5G mobile communication;vehicular ad hoc networks;vehicle-to-vehicle communication technologies;temporal connectivity;vehicular network operations;vehicular mobility;temporal reachability;vehicular network performance;mobile network technologies;DSRC;5G network;Trajectory;Global Positioning System;Roads;Calibration;Public transportation;5G mobile communication;Computational modeling},
doi={10.1109/INFOCOM.2018.8486393},
ISSN={},
month={April},}
@INPROCEEDINGS{8486337,
author={K. Chen and L. Huang},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Timely-Throughput Optimal Scheduling with Prediction},
year={2018},
volume={},
number={},
pages={603-611},
abstract={Motivated by the increasing importance of providing delay-guaranteed services in general computing and communication systems, and the recent wide adoption of learning and prediction in network control, in this work, we consider a general stochastic single-server multi-user system and investigate the fundamental benefit of predictive scheduling in improving timely-throughput, being the rate of packets that are delivered to destinations before their deadlines. By adopting an error rate-based prediction model, we first derive a Markov decision process (MDP) solution to optimize the timely-throughput objective subject to an average resource consumption constraint. Based on a packet-level decomposition of the MDP, we explicitly characterize the optimal scheduling policy and rigorously quantify the timely-throughput improvement due to predictive-service, which scales as Θ(p[C1[((a-amaxq))/(p-q)] ρτ+ C2(1-[1/p])](1-ρD)), where a, amax, ρ ∈ (0,1), C1> 0, C2≥ 0 are constants, p is the true-positive rate in prediction, Q is the false-negative rate, τ is the packet deadline and D is the prediction window size. We also conduct extensive simulations to validate our theoretical findings. Our results provide novel insights into how prediction and system parameters impact performance and provide useful guidelines for designing predictive low-latency control algorithms.},
keywords={delays;Markov processes;multiuser channels;queueing theory;radio networks;telecommunication scheduling;network control;predictive scheduling;error rate-based prediction model;Markov decision process solution;MDP;average resource consumption constraint;packet-level decomposition;optimal scheduling policy;timely-throughput improvement;predictive-service;true-positive rate;false-negative rate;packet deadline;prediction window size;timely-throughput optimal scheduling;delay-guaranteed services;general stochastic single-server multiuser system;timely-throughput objective;Servers;Predictive models;Delays;Optimal scheduling;Markov processes;Microsoft Windows;Scheduling},
doi={10.1109/INFOCOM.2018.8486337},
ISSN={},
month={April},}
@INPROCEEDINGS{8486010,
author={B. He and H. Xu and L. Jin and G. Guo and Y. Chen and G. Weng},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={An Investigation into Android In-App Ad Practice: Implications for App Developers},
year={2018},
volume={},
number={},
pages={2465-2473},
abstract={In-app advertising has served as the major revenue source for millions of app developers in the mobile Internet ecosystem. Ad networks play an important role in app monetization by providing third-party libraries for developers to choose and embed into their apps. However, developers lack guidelines on how to choose from hundreds of ad networks and various ad features to maximize their revues without hurting the user experience of their apps. Our work aims to uncover the best practice and provide app developers guidelines on ad network selection and ad placement. To this end, we investigate 697 unique APIs from 164 ad networks which are extracted from 277,616 Android apps, develop a methodology of ad type classification based on UI interaction and behavior, and perform a large scale measurement study of in-app ads with static analysis techniques at the API granularity. We found that developers have more choices about ad networks than several years before. Most developers are conservative about ad placement and about 71% apps contain at most one ad library. In addition, the likeliness of an app containing ads depends on the app category to which it belongs. The app categories featuring young audience usually contain the most ad libraries maybe because of the ad-tolerance characteristic of young people. Furthermore, we propose a terminology and classify mobile ads into five ad types: Embedded, Popup, Notification, Offerwall, and Floating. We found that embedded and popup ad types are popular with apps in nearly all categories. Our results also suggest that developers should embed at most 6 ad libraries into an app, which otherwise would anger the app users. Also, a developer should use at most one ad network when her app is still at the initial stage and could start using more (2 or 3) ad networks when the app becomes popular. Our research is the first to reveal the preference of both developers and users for ad networks and ad types.},
keywords={advertising data processing;application program interfaces;Internet;mobile computing;app category;ad library;app users;In-app advertising;app monetization;ad network selection;in-app ads;Floating mobile ads;Offerwall mobile ads;Notification mobile ads;Popup mobile ads;Embedded mobile ads;API granularity;UI interaction;mobile Internet ecosystem;Android In-App Ad Practice;app developers;Libraries;Androids;Humanoid robots;Advertising;Ecosystems;Conferences;Guidelines},
doi={10.1109/INFOCOM.2018.8486010},
ISSN={},
month={April},}
@INPROCEEDINGS{8485952,
author={K. Xiao and C. Zhu and J. Xie and Y. Zhou and X. Zhu and W. Zhang},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Dynamic Defense Strategy against Stealth Malware Propagation in Cyber-Physical Systems},
year={2018},
volume={},
number={},
pages={1790-1798},
abstract={Stealth malware, a representative tool of advanced persistent threat (APT) attacks, in particular poses an increased threat to cyber-physical systems (CPS). Due to the use of stealthy and evasive techniques (e.g., zero-day exploits, obfuscation techniques), stealth malwares usually render conventional heavyweight countermeasures (e.g., exploits patching, specialized ant-malware program) inapplicable. Light-weight countermeasures (e.g., containment techniques), on the other hand, can help retard the spread of stealth malwares, but the ensuing side effects might violate the primary safety requirement of CPS. Hence, defenders need to find a balance between the gain and loss of deploying light-weight countermeasures. To address this challenge, we model the persistent anti-malware process as a shortest-path tree interdiction (SPTI) Stackelberg game, and safety requirements of CPS are introduced as constraints in the defender's decision model. Specifically, we first propose a static game (SSPTI), and then extend it to a multi-stage dynamic game (DSPTI) to meet the need of real-time decision making. Both games are modelled as bi-level integer programs, and proved to be NP-hard. We then develop a Benders decomposition algorithm to achieve the Stackelberg Equilibrium of SSPTI. Finally, we design a model predictive control strategy to solve DSPTI approximately by sequentially solving an approximation of SSPTI. The extensive simulation results demonstrate that the proposed dynamic defense strategy can achieve a balance between fail-secure ability and fail-safe ability while retarding the stealth malware propagation in CPS.},
keywords={cyber-physical systems;decision making;game theory;integer programming;invasive software;predictive control;dynamic defense strategy;stealth malware propagation;cyber-physical systems;advanced persistent threat attacks;CPS;primary safety requirement;shortest-path tree interdiction Stackelberg game;model predictive control strategy;APT attacks;specialized anti-malware program;static game;SSPTI;DSPTI;multi-stage dynamic game;real-time decision making;bi-level integer programs;Benders decomposition algorithm;Malware;Safety;Games;Security;Loss measurement;Conferences;Cyber-physical systems},
doi={10.1109/INFOCOM.2018.8485952},
ISSN={},
month={April},}
@INPROCEEDINGS{8486344,
author={J. Chen and S. Yao and Q. Yuan and K. He and S. Ji and R. Du},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={CertChain: Public and Efficient Certificate Audit Based on Blockchain for TLS Connections},
year={2018},
volume={},
number={},
pages={2060-2068},
abstract={In recent years, real-world attacks against PKI take place frequently. For example, malicious domains' certificates issued by compromised CAs are widespread, and revoked certificates are still trusted by clients. In spite of a lot of research to improve the security of SSL/TLS connections, there are still some problems unsolved. On one hand, although log-based schemes provided certificate audit service to quickly detect CAs' misbehavior, the security and data consistency of log servers are ignored. On the other hand, revoked certificates checking is neglected due to the incomplete, insecure and inefficient certificate revocation mechanisms. Further, existing revoked certificates checking schemes are centralized which would bring safety bottlenecks. In this paper, we propose a blockchain-based public and efficient audit scheme for TLS connections, which is called Certchain. Specially, we propose a dependability-rank based consensus protocol in our blockchain system and a new data structure to support certificate forward traceability. Furthermore, we present a method that utilizes dual counting bloom filter (DCBF) with eliminating false positives to achieve economic space and efficient query for certificate revocation checking. The security analysis and experimental results demonstrate that CertChain is suitable in practice with moderate overhead.},
keywords={auditing;certification;cryptographic protocols;data privacy;data structures;public key cryptography;query processing;system monitoring;telecommunication security;certificate audit service;TLS connections;SSL connections;log-based schemes;certificate revocation mechanisms;dual counting bloom filter;DCBF;data structure;dependability-rank based consensus protocol;Certchain;blockchain-based public;revoked certificates checking;log servers;data consistency;CertChain;security analysis;certificate revocation checking;certificate forward traceability;blockchain system;Protocols;Servers;Data structures;Electronic mail;Monitoring},
doi={10.1109/INFOCOM.2018.8486344},
ISSN={},
month={April},}
@INPROCEEDINGS{8486209,
author={Y. Yang and J. Luo},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Boosting the Throughput of LED-Camera VLC via Composite Light Emission},
year={2018},
volume={},
number={},
pages={315-323},
abstract={LED-Camera Visible Light Communication (VLC) is gaining increasing attention, thanks to its readiness to be implemented with Commercial Off-The-Shelf devices and its potential to deliver pervasive data services indoors. Nevertheless, existing LED-Camera VLC systems employ mainly low-order modulations such as On-Off Keying (OOK) given the simplicity of their implementation, yet such rudimentary modulations cannot yield a high throughput. In this paper, we investigate various opportunities of using a high-order modulation to boost the throughput of LED-Camera VLC systems, and we decide that Amplitude-Shift Keying (ASK) is the most suitable scheme given the limited operating frequency of such systems. However, directly driving an LED to emit different levels of luminance may suffer heavy distortions caused by the nonlinear behavior of LED. As a result, we innovatively propose to generate ASK using the composition of light emission. In other words, we digitally control the On-Off states of several groups of LED chips, so that their light emissions compose in the air to produce various ASK symbols. We build a prototype of this novel ASK-based VLC system and demonstrate its superior performance over existing systems: it achieves a rate of 2 kbps at a 1 m distance with only a single LED luminaire.},
keywords={amplitude shift keying;cameras;free-space optical communication;light emitting diodes;optical distortion;optical modulation;novel ASK-based VLC system;single LED luminaire;pervasive data services;composite light emission;amplitude-shift keying;commercial off-the-shelf devices;on-off keying;OOK;nonlinear behavior;LED-camera VLC systems;LED-camera VLC throughput;on-off state control;LED chips;high-order modulation;rudimentary modulations;low-order modulations;LED-Camera Visible Light Communication;distance 1.0 m;bit rate 2 kbit/s;Light emitting diodes;Frequency shift keying;Amplitude shift keying;Cameras;Throughput;Visible Light Communication;Collaborative Transmissions;Amplitude-Shift Keying},
doi={10.1109/INFOCOM.2018.8486209},
ISSN={},
month={April},}
@INPROCEEDINGS{8486280,
author={N. Sapountzis and T. Spyropoulos and N. Nikaein and U. Salim},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Joint Optimization of User Association and Dynamic TDD for Ultra-Dense Networks},
year={2018},
volume={},
number={},
pages={2681-2689},
abstract={Ultra-dense small cell networks will require sophisticated user association algorithms that consider (i) channel characteristics, (ii) base station load, and (iii) uplink/downlink (UL/DL) traffic profiles. They will also be characterized by high spatio-temporal variability in UL/DL traffic demand, due to the fewer users per BS. In this direction, Dynamic TDD is a promising new technique to match BS resources to actual demand. While plenty of literature exists on the problem of user association, and some recent on dynamic TDD, most works consider these separately. In this paper, we argue that user association policies are strongly coupled with the allocation of resources between UL and DL. We propose an algorithm that decomposes the problem into separate subproblems that can each be solved efficiently and in a distributed manner, and prove convergence to the global optimum. Simulation results suggest that our approach can improve UL and DL performance at the same time, with an aggregate improvement of more than 2×, compared to user association under static TDD allocation.},
keywords={cellular radio;optimisation;resource allocation;telecommunication traffic;time division multiplexing;global optimum;resource allocation;channel characteristics;UL-DL traffic demand;uplink-downlink traffic profiles;user association algorithms;ultra-dense small cell networks;static TDD allocation;user association policies;dynamic TDD;BS resources;high spatio-temporal variability;base station load;joint optimization;Interference;Signal to noise ratio;Optimization;Resource management;Heuristic algorithms;Base stations;Downlink},
doi={10.1109/INFOCOM.2018.8486280},
ISSN={},
month={April},}
@INPROCEEDINGS{8486276,
author={N. Budhdev and M. C. Chan and T. Mitra},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={PR3: Power Efficient and Low Latency Baseband Processing for LTE Femtocells},
year={2018},
volume={},
number={},
pages={2357-2365},
abstract={In order to provide greater network capacity, the use of small base stations such as Femtocells has increased to allow higher spectrum reuse. In these Femtocells, base station designers have started to explore the use of general purpose multi-core architectures to provide greater flexibility. Multi-core architectures allow power-performance trade-off possibilities through techniques such as Dynamic Voltage Frequency Scaling (DVFS) and Power Gating. In this work, we propose a power management framework based on reinforcement learning called PR3, which uses both DVFS and Power Gating. Our approach is unique as it introduces a feedback from the network scheduler and baseband processor to the Power Governor, so that information about both the network and computation workloads are included in the decision making. Evaluation on a hardware platform (Odroid XU3) running PHY LTE uplink baseband processing benchmark, shows that PR3performs well in terms of both power and latency. It is able to save upto 50% power while maintaining low processing latency. PR3is also adaptive, making it effective over a wide range of traffic loads.},
keywords={femtocellular radio;learning (artificial intelligence);Long Term Evolution;microprocessor chips;multiprocessing systems;telecommunication power management;power management framework;reinforcement learning;Power Gating;network scheduler;baseband processor;Power Governor;Odroid XU3;LTE Femtocells;base stations;base station designers;Dynamic Voltage Frequency Scaling;LTE;network capacity;multicore architectures;Baseband;Femtocells;Power demand;Long Term Evolution;Computer architecture;Cellular networks},
doi={10.1109/INFOCOM.2018.8486276},
ISSN={},
month={April},}
@INPROCEEDINGS{8485931,
author={A. Mohan and A. Gopalan and A. Kumar},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Reduced-State, Optimal Medium Access Control for Wireless Data Collection Networks},
year={2018},
volume={},
number={},
pages={567-575},
abstract={Motivated by medium access control for resource-challenged wireless sensor networks whose main purpose is data collection, we consider the problem of queue scheduling with reduced queue state information. In particular, we consider a model with N sensor nodes, with pair-wise dependence, such that nodes i and i+1, 1 ≤ i ≤ N-1 cannot transmit together. For N=3, 4, and 5, we develop new throughput-optimal scheduling policies requiring only the empty-nonempty state of each queue, and also revisit previously proposed policies to rigorously establish their throughput-and delay-optimality. For N=3, there exists a sum-queue length optimal scheduling policy that requires only the empty-nonempty state of each queue. We show, however, that for N ≥ 4, there is no scheduling policy that uses only the empty-nonempty states of the queues and is sum-queue length optimal uniformly over all arrival rate vectors. We then extend our results to a more general class of interference constraints, namely, a star of cliques. Our throughput-optimality results rely on two new arguments: a Lyapunov drift lemma specially adapted to policies that are queue length-agnostic, and a priority queueing analysis for showing strong stability. Our study throws up some counterintuitive conclusions: 1) knowledge of queue length information is not necessary to achieve optimal throughput/delay performance for a large class of interference networks, 2) it is possible to perform throughput-optimal scheduling by merely knowing whether queues in the network are empty or not, and 3) it is also possible to be throughput-optimal by not always scheduling the maximum possible number of nonempty queues. We also show the results of numerical experiments on the performance of queue length agnostic scheduling vs. queue length aware scheduling, on several interference networks.},
keywords={access protocols;queueing theory;radio networks;telecommunication scheduling;queue length-agnostic scheduling;optimal throughput-delay performance;reduced-state optimal medium access control;resource-challenged wireless sensor networks;rate vectors;Lyapunov drift lemma;interference constraints;sum-queue length optimal scheduling policy;delay-optimality;empty-nonempty state;throughput-optimal scheduling policies;pair-wise dependence;N sensor nodes;reduced queue state information;wireless data collection networks;queue length aware scheduling;nonempty queues;interference networks;queue length information;priority queueing analysis;Interference;Throughput;Optimal scheduling;Media Access Protocol;Delays;Schedules;Wireless sensor networks;Wireless Sensor Networks;Medium Access Control (MAC) protocols;Optimal Polling;Delay Minimization;Hybrid MACs;Self-Organizing Networks;Internet of Things (IoT)},
doi={10.1109/INFOCOM.2018.8485931},
ISSN={},
month={April},}
@INPROCEEDINGS{8486249,
author={Y. Qiao},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Robust Loss Inference in the Presence of Noisy Measurements},
year={2018},
volume={},
number={},
pages={738-746},
abstract={This paper addresses the problem of inferring link loss rates based on network performance tomography in noisy network systems. Since the network tomography emerged, all existing tomography-based methods are limited to the basic condition that both network topologies and end-to-end routes must be absolutely accurate, which in most cases is impractical, especially for large-scale heterogeneous networks. To overcome the impracticability of tomography-based methods, we propose a robust tomography-based loss inference method capable of accurately inferring all link loss rates even when the network system may change dynamically. The new method first measures the end-to-end loss rates of selected paths to reduce the probing cost, and then calculates an upper bound for the loss rate of each link using the measurement results. Finally, it finds all the link loss rates that most closely conform to the measurement results within their upper bounds. Compared with two traditional loss inference methods (with and without path selection, respectively), the results strongly confirm the promising performance of our proposed approach.},
keywords={multicast communication;telecommunication links;telecommunication network routing;telecommunication network topology;telecommunication traffic;tomography;network topologies;end-to-end routes;large-scale heterogeneous networks;robust tomography-based loss inference method;link loss rates;end-to-end loss rates;robust loss inference;noisy measurements;network performance tomography;noisy network systems;Loss measurement;Tomography;Routing;Robustness;Network topology;Upper bound;Monitoring;Network tomography;Loss inference;Uncertain network;Least-squares problem},
doi={10.1109/INFOCOM.2018.8486249},
ISSN={},
month={April},}
@INPROCEEDINGS{8486257,
author={S. Khatuya and N. Ganguly and J. Basak and M. Bharde and B. Mitra},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={ADELE: Anomaly Detection from Event Log Empiricism},
year={2018},
volume={},
number={},
pages={2114-2122},
abstract={A large population of users gets affected by sudden slowdown or shutdown of an enterprise application. System administrators and analysts spend considerable amount of time dealing with functional and performance bugs. These problems are particularly hard to detect and diagnose in most computer systems, since there is a huge amount of system generated supportability data (counters, logs etc.) that need to be analyzed. Most often, there isn't a very clear or obvious root cause. Timely identification of significant change in application behavior is very important to prevent negative impact on the service. In this paper, we present ADELE, an empirical, data-driven methodology for early detection of anomalies in data storage systems. The key feature of our solution is diligent selection of features from system logs and development of effective machine learning techniques for anomaly prediction. ADELE learns from system's own history to establish the baseline of normal behavior and gives accurate indications of the time period when something is amiss for a system. Validation on more than 4800 actual support cases shows~83% true positive rate and~12% false positive rate in identifying periods when the machine is not performing normally. We also establish the existence of problem “signatures” which help map customer problems to already seen issues in the field. ADELE's capability to predict early paves way for online failure prediction for customer systems.},
keywords={learning (artificial intelligence);program debugging;security of data;storage management;system monitoring;customer systems;enterprise application;computer systems;data storage systems;system logs;anomaly prediction;ADELE;functional bugs;data-driven methodology;anomaly detection from event log empiricism;performance bugs;machine learning techniques;Computer bugs;Databases;Anomaly detection;Conferences;Machine learning;Predictive models;Data storage systems;System Log;Anomaly Detection},
doi={10.1109/INFOCOM.2018.8486257},
ISSN={},
month={April},}
@INPROCEEDINGS{8485917,
author={J. Hua and H. Sun and Z. Shen and Z. Qian and S. Zhong},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Accurate and Efficient Wireless Device Fingerprinting Using Channel State Information},
year={2018},
volume={},
number={},
pages={1700-1708},
abstract={Due to the loose authentication requirement between access points (APs) and clients, it is notoriously known that WLANs face long-standing threats such as rogue APs and network freeloading. Take the rogue AP problem as an example, unfortunately encryption alone does not provide authentication. APs need to be equipped with certificates that are trusted by clients ahead of time. This requires either the presence of PKI for APs or other forms of pre-established trust (e.g., distributing the certificates offline), none of which is widely used. Before any strong security solution is deployed, we still need a practical solution that can mitigate the problem. In this paper, we explore a non-cryptographic solution that is readily deployable today on end hosts (e.g., smartphones and laptops) without requiring any changes to the APs or the network infrastructure. The solution infers the Carrier Frequency Offsets (CFOs) of wireless devices from Channel State Information (CSI) as their hardware fingerprints without any special hardware requirement. CFO is attributed to the oscillator drift, which is a fundamental physical property that cannot be manipulated easily and remains fairly consistent over time but varies significantly across devices. The real experiments on 23 smartphones and 34 APs (with both identical and different brands) in different scenarios demonstrate that the detection rate could exceed 94%.},
keywords={authorisation;computer network security;cryptography;wireless LAN;hardware fingerprints;special hardware requirement;Channel State Information;security solution;wireless devices;Carrier Frequency Offsets;network infrastructure;end hosts;noncryptographic solution;practical solution;certificates offline;pre-established trust;encryption;rogue AP problem;network freeloading;WLANs face long-standing threats;clients;access points;loose authentication requirement;Hardware;Authentication;Wireless fidelity;Wireless communication;Phase measurement;Communication system security;device fingerprinting;attack detection;authentication;wireless networks},
doi={10.1109/INFOCOM.2018.8485917},
ISSN={},
month={April},}
@INPROCEEDINGS{8486229,
author={A. Mukhopadhyay and N. Hegde and M. Lelarge},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Optimal Content Replication and Request Matching in Large Caching Systems},
year={2018},
volume={},
number={},
pages={288-296},
abstract={We consider models of content delivery networks in which the servers are constrained by two main resources: memory and bandwidth. In such systems, the throughput crucially depends on how contents are replicated across servers and how the requests of specific contents are matched to servers storing those contents. In this paper, we first formulate the problem of computing the optimal replication policy which if combined with the optimal matching policy maximizes the throughput of the caching system in the stationary regime. It is shown that computing the optimal replication policy for a given system is an NP-hard problem. A greedy replication scheme is proposed and it is shown that the scheme provides a constant factor approximation guarantee. We then propose a simple randomized matching scheme which avoids the problem of interruption in service of the ongoing requests due to re-assignment or repacking of the existing requests in the optimal matching policy. The dynamics of the caching system is analyzed under the combination of proposed replication and matching schemes. We study a limiting regime, where the number of servers and the arrival rates of the contents are scaled proportionally, and show that the proposed policies achieve asymptotic optimality. Extensive simulation results are presented to evaluate the performance of different policies and study the behavior of the caching system under different service time distributions of the requests.},
keywords={approximation theory;cache storage;computational complexity;Internet;network servers;optimisation;caching system;optimal content replication;request matching;content delivery networks;optimal replication policy;optimal matching policy;NP-hard problem;greedy replication scheme;simple randomized matching scheme;asymptotic optimality;service time distributions;Servers;Resource management;Microsoft Windows;Bandwidth;Conferences;Electronic mail;Computational modeling},
doi={10.1109/INFOCOM.2018.8486229},
ISSN={},
month={April},}
@INPROCEEDINGS{8486372,
author={K. Gao and J. Zhang and Y. Richard Yang and J. Bi},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Prophet: Fast Accurate Model-Based Throughput Prediction for Reactive Flow in DC Networks},
year={2018},
volume={},
number={},
pages={720-728},
abstract={As modern network applications (e.g., large data analytics) become more distributed and can conduct application-layer traffic adaptation, they demand better network visibility to better orchestrate their data flows. As a result, the ability to predict the available bandwidth for a set of flows has become a fundamental requirement of today's networking systems. While there are previous studies addressing the case of non-reactive flows, the prediction for reactive flows, e.g., flows managed by TCP congestion control algorithms, still remains an open problem. In this paper, we identify three challenges in providing throughput prediction for reactive flows: throughput dynamics, heterogeneous reactive control mechanisms, and source-constrained flows. Based on a previous theoretical model, we introduce a novel learning-based prediction system with a key component named fast factor learning (FFL) model. We adopt novel techniques to overcome practical concerns such as scalability, convergence and unknown system parameters. A system, Prophet, is proposed leveraging the emerging technologies of Software Defined Networking (SDN) to realize the model. Evaluations demonstrate that our solution achieves significant accuracy in a wide range of settings.},
keywords={learning (artificial intelligence);software defined networking;telecommunication congestion control;telecommunication network topology;telecommunication traffic;transport protocols;software defined networking;FFL model;fast accurate model-based;Prophet;novel learning-based prediction system;source-constrained flows;heterogeneous reactive control mechanisms;throughput dynamics;TCP congestion control algorithms;nonreactive flows;networking systems;data flows;network visibility;application-layer traffic adaptation;data analytics;modern network applications;DC networks;reactive flow;Throughput;Bandwidth;Estimation;Optimization;Conferences;Computational modeling;Predictive models},
doi={10.1109/INFOCOM.2018.8486372},
ISSN={},
month={April},}
@INPROCEEDINGS{8486231,
author={F. Zhou and L. Liu and K. Zhang and G. Trajcevski and J. Wu and T. Zhong},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={DeepLink: A Deep Learning Approach for User Identity Linkage},
year={2018},
volume={},
number={},
pages={1313-1321},
abstract={The typical aim of User Identity Linkage (UIL) is to detect when users from across different social platforms are actually one and the same individual. Existing efforts to address this problem of practical relevance span from user-profile-based, through user-generated-content-based, user-behavior-based approaches to supervised or unsupervised learning frameworks, to subspace learning-based models. Most of them often require extraction of relevant features (e.g., profile, location, biography, networks, behavior, etc.) to model the user consistently across different social networks. However, these features are mainly derived based on prior knowledge and may vary for different platforms and applications. Inspired by the recent successes of deep learning in different tasks, especially in automatic feature extraction and representation, we propose a deep neural network based algorithm for UIL, called DeepLink. It is a novel end-to-end approach in a semi-supervised learning manner, without involving any hand-crafting features. Specifically, DeepLink samples the networks and learns to encode network nodes into vector representation to capture local and global network structures which, in turn, can be used to align anchor nodes through deep neural networks. A dual learning based paradigm is exploited to learn how to transfer knowledge and update the linkage using the policy gradient method. Experiments conducted on several public datasets show that DeepLink outperforms the state-of-the-art methods in terms of both linking precision and identity-match ranking.},
keywords={feature extraction;gradient methods;learning (artificial intelligence);neural nets;social networking (online);User Identity Linkage;UIL;supervised learning frameworks;unsupervised learning frameworks;automatic feature extraction;deep neural network;end-to-end approach;semisupervised learning manner;hand-crafting features;network nodes;local network structures;global network structures;dual learning based paradigm;social platforms;social networks;DeepLink;deep learning approach;policy gradient method;Couplings;Feature extraction;Social network services;Training;Machine learning;Task analysis;Neural networks;user identity linkage;social networks;deep learning;reinforcement learning},
doi={10.1109/INFOCOM.2018.8486231},
ISSN={},
month={April},}
@INPROCEEDINGS{8485830,
author={M. Shao and J. Li and F. Chen and X. Chen},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={An Efficient Framework for Detecting Evolving Anomalous Subgraphs in Dynamic Networks},
year={2018},
volume={},
number={},
pages={2258-2266},
abstract={Evolving anomalous subgraphs detection in dynamic networks is an important and challenging problem that has arisen in multiple applications and is NP-hard in general. The evolving characteristic makes most existing methods incapable to tackle this problem effectively and efficiently, as it involves huge search spaces and continuous changes of evolving connected subgraphs, especially when the data are free of distributions. This paper presents a generic efficient framework, namely dynamic evolving anomalous subgraphs scanning (dGraphScan), to address this problem. We generalize traditional nonparametric scan statistics, and propose a large class of scan statistic functions for measuring the significance of evolving subgraphs in dynamic networks. Furthermore, we make a number of computational studies to optimize this large class of nonparametric scan statistic functions. Specifically, we first decompose each scan statistic function as a sequence of subproblems with provable guarantees, and then propose efficient approximation algorithms for tackling each subproblem, while analyzing their theoretical properties and providing rigorous approximation guarantees. Extensive experiments on three real-world datasets demonstrate that our general framework performs superior over state-of-the-art methods.},
keywords={approximation theory;computational complexity;graph theory;network theory (graphs);optimisation;dynamic networks;anomalous subgraphs detection;NP-hard;connected subgraphs;dynamic evolving anomalous subgraphs;scan statistic function;nonparametric scan statistic functions;approximation algorithms;dGraphScan;Approximation algorithms;Conferences;Heuristic algorithms;Botnet;Roads;Nonparametric statistics;Computer science;evolving anomalous subgraphs detection;dynamic networks;nonparametric scan statistics;approximation algorithm},
doi={10.1109/INFOCOM.2018.8485830},
ISSN={},
month={April},}
@INPROCEEDINGS{8486396,
author={Y. Ren and T. Huang and K. C. Lin and Y. Tseng},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={On Scalable Service Function Chaining with$\mathcal{O}(1)$Flowtable Entries},
year={2018},
volume={},
number={},
pages={702-710},
abstract={The emergence of Network Function Virtualization (NFV) enables flexible and agile service function chaining in a Software Defined Network (SDN). While this virtualization technology efficiently offers customization capability, it however comes with a cost of consuming precious TCAM resources. Due to this, the number of service chains that an SDN can support is limited by the flowtable size of a switch. To break this limitation, this paper presents CRT-Chain, a service chain forwarding protocol that requires only constant flowtable entries, regardless of the number of service chain requests. The core of CRT-Chain is an encoding mechanism that leverages Chinese Remainder Theorem (CRT) to compress the forwarding information into small labels. A switch does not need to insert forwarding rules for every service chain request, but only needs to conduct very simple modular arithmetic to extract the forwarding rules directly from CRT-Chain's labels attached in the header. We further incorporate prime reuse and path segmentation in CRT-Chain to reduce the header size and, hence, save bandwidth consumption. Our evaluation results show that, when a chain consists of no more than 5 functions, CRT-Chain actually generates a header smaller than the legacy 32-bit header defined in IETF. By enabling prime reuse and segmentation, CRT-Chain further reduces the total signaling overhead to a level lower than the conventional scheme, showing that CRT-Chain not only enables scalable flowtable-free chaining but also improves network efficiency.},
keywords={IP networks;number theory;protocols;software defined networking;virtualisation;flowtable-free chaining;Network Function Virtualization;agile service function chaining;service chain request;scalable service function chaining;software defined network;SDN;CRT-Chain labels;Chinese Remainder Theorem;O(1) flowtable entries;service chain forwarding protoco;Switches;Protocols;Encoding;Cathode ray tubes;Routing;Network function virtualization;Bandwidth},
doi={10.1109/INFOCOM.2018.8486396},
ISSN={},
month={April},}
@INPROCEEDINGS{8486255,
author={H. Ramezani and T. Khan and O. B. Akan},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Information Theoretical Analysis of Synaptic Communication for Nanonetworks},
year={2018},
volume={},
number={},
pages={2330-2338},
abstract={Communication among neurons is the highly evolved and efficient nanoscale communication paradigm, hence the most promising technique for biocompatible nanonetworks. This necessitates the understanding of neuro-spike communication from information theoretical perspective to reach a reference model for nanonetworks. This would also contribute towards developing ICT-based diagnostics techniques for neuro-degenerative diseases. Thus, in this paper, we focus on the fundamental building block of neuro-spike communication, i.e., signal transmission over a synapse, to evaluate its information transfer rate. We aim to analyze a realistic synaptic communication model, which for the first time, encompasses the variation in vesicle release probability with time, synaptic geometry and the re-uptake of neurotransmitters by pre-synaptic terminal. To achieve this objective, we formulate the mutual information between input and output of the synapse. Then, since this communication paradigm has memory, we evaluate the average mutual information over multiple transmissions to find its overall capacity. We derive a closed-form expression for the capacity of the synaptic communication as well as calculate the capacity-achieving input probability distribution. Finally, we find the effects of variation in different synaptic parameters on the information capacity and prove that the diffusion process does not decrease the information a neural response carries about the stimulus in real scenario.},
keywords={channel capacity;diseases;neural nets;neurophysiology;probability;efficient nanoscale communication paradigm;highly evolved nanoscale communication paradigm;information theoretical analysis;information capacity;different synaptic parameters;capacity-achieving input probability distribution;average mutual information;pre-synaptic terminal;synaptic geometry;realistic synaptic communication model;information transfer rate;fundamental building block;neuro-degenerative diseases;ICT-based diagnostics techniques;reference model;information theoretical perspective;neuro-spike communication;biocompatible nanonetworks;Neurotransmitters;Neurons;Calcium;Synapses;Numerical models;Nanobioscience;Capacity planning;Nanonetworks;molecular communication;neuro-spike communication;information capacity;synaptic transmission},
doi={10.1109/INFOCOM.2018.8486255},
ISSN={},
month={April},}
@INPROCEEDINGS{8486283,
author={L. Lu and J. Yu and Y. Chen and H. Liu and Y. Zhu and Y. Liu and M. Li},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={LipPass: Lip Reading-based User Authentication on Smartphones Leveraging Acoustic Signals},
year={2018},
volume={},
number={},
pages={1466-1474},
abstract={To prevent users' privacy from leakage, more and more mobile devices employ biometric-based authentication approaches, such as fingerprint, face recognition, voiceprint authentications, etc., to enhance the privacy protection. However, these approaches are vulnerable to replay attacks. Although state-of-art solutions utilize liveness verification to combat the attacks, existing approaches are sensitive to ambient environments, such as ambient lights and surrounding audible noises. Towards this end, we explore liveness verification of user authentication leveraging users' lip movements, which are robust to noisy environments. In this paper, we propose a lip reading-based user authentication system, LipPass, which extracts unique behavioral characteristics of users' speaking lips leveraging build-in audio devices on smartphones for user authentication. We first investigate Doppler profiles of acoustic signals caused by users' speaking lips, and find that there are unique lip movement patterns for different individuals. To characterize the lip movements, we propose a deep learning-based method to extract efficient features from Doppler profiles, and employ Support Vector Machine and Support Vector Domain Description to construct binary classifiers and spoofer detectors for user identification and spoofer detection, respectively. Afterwards, we develop a binary tree-based authentication approach to accurately identify each individual leveraging these binary classifiers and spoofer detectors with respect to registered users. Through extensive experiments involving 48 volunteers in four real environments, LipPass can achieve 90.21% accuracy in user identification and 93.1% accuracy in spoofer detection.},
keywords={acoustic signal processing;audio signal processing;authorisation;data protection;feature extraction;learning (artificial intelligence);message authentication;mobile computing;pattern classification;smart phones;support vector machines;LipPass;privacy protection;lip reading-based user authentication system;Doppler profiles;deep learning-based method;binary classifiers;spoofer detectors;smartphones;biometric-based authentication;Support Vector Machine;replay attacks;lip movement patterns;binary tree-based authentication;build-in audio devices;acoustic signals;feature extraction;Lips;Authentication;Acoustics;Doppler effect;Smart phones;Feature extraction},
doi={10.1109/INFOCOM.2018.8486283},
ISSN={},
month={April},}
@INPROCEEDINGS{8486373,
author={M. Wei and Z. Lu and Y. Tang and X. Lu},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={How Can Cyber-Physical Interdependence Affect the Mitigation of Cascading Power Failure?},
year={2018},
volume={},
number={},
pages={2501-2509},
abstract={Utilizing advanced communication technologies to facilitate power system monitoring and control, the smart grid is envisioned to be more robust and resilient against cascading failures. Although the integration of communication network does benefit the smart grid in many aspects, such benefits should not overshadow the fact that the interdependence between the communication network and the power infrastructure makes the smart grid more fragile to cascading failures. Thus, it is essential to understand the impact of such cyber-physical integration with interdependence from both positive and negative perspectives. In this paper, we develop a systematic framework to analyze the benefits and drawbacks of the cyber-physical interdependence. We use theoretical analysis and system-level simulations to characterize the impact of such interdependence. We identify two phases during the progress of failure propagation where the integrated communication and interdependence helps and hinders the mitigation of the failure, respectively, which provides practical guidance to smart grid system design and optimization.},
keywords={cyber-physical systems;power engineering computing;power grids;power system measurement;power system reliability;risk management;smart power grids;communication technologies;power system monitoring;cascading failures;communication network;power infrastructure;cyber-physical integration;positive perspectives;negative perspectives;system-level simulations;failure propagation;integrated communication;smart grid system design;optimization;cyber-physical interdependence;power system control;cascading power failure mitigation;smart grid;Smart grids;Power system faults;Power system protection;Load modeling;Communication networks;Smart grid;failure propagation;cascading failure;load shedding control;modeling and simulations},
doi={10.1109/INFOCOM.2018.8486373},
ISSN={},
month={April},}
@INPROCEEDINGS{8485944,
author={Q. Zhou and M. Elbadry and F. Ye and Y. Yang},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Heracles: Scalable, Fine-Grained Access Control for Internet-of-Things in Enterprise Environments},
year={2018},
volume={},
number={},
pages={1772-1780},
abstract={Scalable, fine-grained access control for Internet-of-Things is needed in enterprise environments, where thousands of subjects need to access possibly one to two orders of magnitude more objects. Existing solutions offer all-or-nothing access, or require all access to go through a cloud backend, greatly impeding access granularity, robustness and scale. In this paper, we propose Heracles, an IoT access control system that achieves robust, fine-grained access control at enterprise scale. Heracles adopts a capability-based approach using secure, unforgeable tokens that describe the authorizations of subjects, to either individual or collections of objects in single or bulk operations. It has a 3-tier architecture to provide centralized policy and distributed execution desired in enterprise environments, and delegated operations for responsiveness of resource-constrained objects. Extensive security analysis and performance evaluation on a testbed prove that Heracles achieves robust, responsive, fine-Qrained access control in large scale enterprise environments.},
keywords={authorisation;business data processing;Internet of Things;Internet-of-Things;fine-grained access control;Heracles;fine-Qrained access control;IoT;enterprise environments;access control system;Access control;Permission;Robustness;Public key;Conferences;Computer architecture},
doi={10.1109/INFOCOM.2018.8485944},
ISSN={},
month={April},}
@INPROCEEDINGS{8485919,
author={M. Hossain and J. Xie},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Off-sensing and Route Manipulation Attack: A Cross-Layer Attack in Cognitive Radio based Wireless Mesh Networks},
year={2018},
volume={},
number={},
pages={1376-1384},
abstract={Cognitive Radio (CR) has garnered much attention in the last decade, while the security issues are not fully studied yet. Existing research on attacks and defenses in CR - based networks focuses mostly on individual network layers, whereas cross-layer attacks remain fortified against single-layer defenses. In this paper, we shed light on a new vulnerability in cross-layer routing protocols and demonstrate how a perpetrator can exploit this vulnerability to manipulate traffic flow around it. We propose this cross-layer attack in CR-based wireless mesh networks (CR-WMNs), which we call off-sensing and route manipulation (OS-RM) attack. In this cross-layer assault, off-sensing attack is launched at the lower layers as the point of attack but the final intention is to manipulate traffic flow around the perpetrator. We also introduce a learning strategy for a perpetrator, so that it can gather information from the collaboration with other network entities and capitalize this information into knowledge to accelerate its malice intentions. Simulation results show that this attack is far more detrimental than what we have experienced in the past and need to be addressed before commercialization of CR-based networks.},
keywords={cognitive radio;routing protocols;telecommunication security;wireless mesh networks;route manipulation attack;cross-layer attack;individual network layers;single-layer defenses;cross-layer routing protocols;CR-based wireless mesh networks;cross-layer assault;traffic flow;off-sensing and route manipulation attack;cognitive radio based wireless mesh networks;Sensors;Routing protocols;Knowledge engineering;Wireless sensor networks;Wireless communication;FCC;Routing},
doi={10.1109/INFOCOM.2018.8485919},
ISSN={},
month={April},}
@INPROCEEDINGS{8485823,
author={J. Cordova-Garcia and D. Xie and X. Wang},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Restricting Involuntary Extension of Failures in Smart Grids using Social Network Metrics},
year={2018},
volume={},
number={},
pages={2510-2518},
abstract={Modern communication technologies are expected to be available in the future Smart Grids to enable the control of equipments over the whole power grid. In this paper, we consider such networked control approach to address failures that may occur at any location of the grid, due to attacks or unit malfunction, and provide a wide-scale solution that prevent the failure impacts from spreading over a large area. Different from literature work that focuses on modifying power equations under the standard constraints of the power system, we estimate the impact of controlling different nodes on topological areas of the grid based on social metrics, which are derived from the graph capturing both the topological and electrical properties of the power grid. We propose a failure control algorithm for topological containment of failures in smart grid. Our algorithm also takes careful consideration of the impact the planned control has on the grid to avoid the possibly involuntary failure extension. We show that social metrics can efficiently trade off between the topological and electrical characteristics revealed by the power grid graph representation. We evaluate the performance against networked control strategies that only use power models to determine the actions to be performed at power nodes. Our results show that the proposed control scheme can effectively contain failures within their original location range.},
keywords={control engineering computing;failure analysis;graph theory;power system control;smart power grids;social networking (online);power equations;power system;topological properties;electrical properties;failure control algorithm;topological containment;power grid graph representation;networked control strategies;involuntary extension;social network metrics;modern communication technologies;Smart Grids;Monitoring;Smart grids;Measurement;Load modeling;Admittance;Communication networks},
doi={10.1109/INFOCOM.2018.8485823},
ISSN={},
month={April},}
@INPROCEEDINGS{8486208,
author={S. A. Pambudi and W. Wang and C. Wang},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Fast Rendezvous for Spectrum-Agile IoT Devices with Limited Channel Hopping Capability},
year={2018},
volume={},
number={},
pages={1385-1393},
abstract={The explosive number of IoT nodes and adoption of software-defined radio have enabled an efficient method of exploiting idle frequency spectrums called dynamic spectrum access (DSA). The foremost problem in DSA is for a pair of nodes to rendezvous and form a control channel prior to communication. Existing schemes require a channel hopping (CH) pattern with length O(N2), which is overly complex especially when the number of channels N is large. Moreover, the CH patterns are designed assuming DSA nodes have unlimited CH capability, which is hardly satisfied by nodes with long frequency switching time and limited sensing capacity. In this paper, we design a low-complexity rendezvous scheme that account for CH capability limits. The CH capability is captured using spectrum slice graphs that describe the possible channels for the next hop, given the currently-visited channel. By viewing the CH patterns as random walks over the spectrum graphs, we assign the walks with optimal transition probabilities that achieve the smallest rendezvous delay. The resulting symmetric random CH (S-RCH) scheme, which is suitable for IoT nodes without predetermined roles, achieves a lower rendezvous delay than existing Modular Modified Clock (MMC) scheme and offers more than 80 % successful rendezvous in mobile networks.},
keywords={cognitive radio;Internet of Things;radio spectrum management;software radio;wireless channels;rendezvous delay;symmetric random CH scheme;Modular Modified Clock scheme;spectrum graphs;possible channels;spectrum slice graphs;CH capability limits;low-complexity rendezvous scheme;limited sensing capacity;long frequency switching time;unlimited CH capability;DSA nodes;CH patterns;control channel;dynamic spectrum access;idle frequency spectrums;software-defined radio;IoT nodes;explosive number;limited channel hopping capability;spectrum-agile IoT devices;Bandwidth;Tuning;Sensors;Time-frequency analysis;Wireless communication;Conferences;Electronic mail},
doi={10.1109/INFOCOM.2018.8486208},
ISSN={},
month={April},}
@INPROCEEDINGS{8486265,
author={X. Gong and Q. Hua and L. Qian and D. Yu and H. Jin},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Communication-Efficient and Privacy-Preserving Data Aggregation without Trusted Authority},
year={2018},
volume={},
number={},
pages={1250-1258},
abstract={Privacy-preserving data aggregation has been extensively studied in the past decades. However, most of these works target at specific aggregation functions such as additive or multiplicative aggregation functions. Meanwhile, they assume there exists a trusted authority which facilitates the keys and other information distribution. In this paper, we aim to devise a communication efficient and privacy-preserving protocol that can exactly compute arbitrary data aggregation functions without trusted authority. In our model, there exist one untrusted aggregator and n participants. We assume that all communication channels are insecure and are subject to eavesdropping attacks. Our protocol is designed under the semi-honest model, and it can also tolerate k (k ≤ n-2) collusive adversaries. Our protocol achieves (n - k) -source anonymity. That is, for the source of each collected data aparting from the colluded participants, what the aggregator learns is only from one of the (n - k) non-colluded ones. Compared with recent work [1] that computes arbitrary aggregation functions by collecting all the participants' data using the trusted authority, our protocol increases merely by at most a factor of O(([logn/loglogn])2) in terms of computation time and communication cost. The key of our protocol is that we have designed algorithms that can efficiently assign unique sequence numbers to each participant without the trusted authority.},
keywords={data aggregation;data privacy;protocols;security of data;trusted authority;privacy-preserving data aggregation;privacy-preserving protocol;untrusted aggregator;communication channels;aggregation functions;arbitrary data aggregation functions;eavesdropping attacks;communication-efficient data aggregation;semihonest model;Protocols;Data aggregation;Computational modeling;Privacy;Additives;Encryption},
doi={10.1109/INFOCOM.2018.8486265},
ISSN={},
month={April},}
@INPROCEEDINGS{8485828,
author={L. Luo and H. Yu and Z. Ye and X. Du},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Online Deadline-Aware Bulk Transfer Over Inter-Datacenter WANs},
year={2018},
volume={},
number={},
pages={630-638},
abstract={Many large-scale compute-intensive and mission-critical online service applications are being deployed on geo-distributed datacenters, which require transfers of bulk business data over Wide Area Networks (WANs). The bulk transfers are often associated with different requirements on deadlines, either a complete transfer before a hard deadline or a best-effort delivery within a soft deadline. In this paper, we study the online bulk transfer problem over inter-datacenter WANs, while taking into consideration the requests with a mixture of hard and soft deadlines. We use Linear Programming (LP) to mathematically formulate the problem with the objective of maximizing a system utility represented by the service provider's revenue, taking into account the revenue earned from deadline-met transfers and the penalty paid for deadline-missed ones. We propose an online framework to efficiently manage mixed bulk transfers and design a competitive algorithm that applies the primal-dual method to make routing and resource allocation based on the LP. We perform theoretical analysis to prove that the proposed approach can achieve a competitive ratio of (e-1)/e with little link capacity augmentation. In addition, we conduct comprehensive simulations to evaluate the performance of our method. Simulation results show that our method irrespective of the revenue model, can accept at least 25% more transfer requests and improve the network utilization by at least 35%, compared to prior solutions.},
keywords={computer centres;linear programming;resource allocation;wide area networks;hard deadlines;soft deadlines;service provider;deadline-met transfers;online framework;mixed bulk transfers;transfer requests;online deadline-aware bulk transfer;inter-datacenter WANs;large-scale compute-intensive;mission-critical;geo-distributed datacenters;bulk business data;different requirements;complete transfer;online bulk transfer problem;Resource management;Electronics packaging;Routing;Bandwidth;Channel allocation;Heuristic algorithms;Linear programming},
doi={10.1109/INFOCOM.2018.8485828},
ISSN={},
month={April},}
@INPROCEEDINGS{8486003,
author={D. Naboulsi and A. Mermouri and R. Stanica and H. Rivano and M. Fiore},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={On User Mobility in Dynamica Cloud Radio Access Networks},
year={2018},
volume={},
number={},
pages={1583-1591},
abstract={The development of virtualization techniques enables an architectural shift in mobile networks, where resource allocation, or even signal processing, become software functions hosted in a data center. The centralization of computing resources and the dynamic mapping between baseband processing units (BBUs) and remote antennas (RRHs) provide an increased flexibility to mobile operators, with important reductions of operational costs. Most research efforts on Cloud Radio Access Networks (CRAN) consider indeed an operator perspective and network-side performance indicators. The impact of such new paradigms on user experience has been instead overlooked. In this paper, we shift the viewpoint, and show that the dynamic assignment of computing resources enabled by CRAN generates a new class of mobile terminal handover that can impair user quality of service. We then propose an algorithm that mitigates the problem, by optimizing the mapping between BBUs and RRHs on a time-varying graph representation of the system. Furthermore, we show that a practical online BBU-RRH mapping algorithm achieves results similar to an oracle-based scheme with perfect knowledge of future traffic demand. We test our algorithms with two large-scale real-world datasets, where the total number of handovers, compared with the current architectures, is reduced by more than 20%. Moreover, if a small tolerance to dropped calls is allowed, 30% less handovers can be obtained.},
keywords={mobile radio;quality of service;radio access networks;resource allocation;Cloud Radio Access Networks;resource allocation;mobile networks;architectural shift;virtualization techniques;handovers;practical online BBU-RRH mapping algorithm;time-varying graph representation;mobile terminal handover;dynamic assignment;network-side performance indicators;CRAN;mobile operators;remote antennas;dynamic mapping;data center;software functions;signal processing;Handover;Computer architecture;Radio access networks;Quality of experience;Conferences;Cloud computing},
doi={10.1109/INFOCOM.2018.8486003},
ISSN={},
month={April},}
@INPROCEEDINGS{8486275,
author={A. Tomassilli and F. Giroire and N. Huin and S. Pérennes},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Provably Efficient Algorithms for Placement of Service Function Chains with Ordering Constraints},
year={2018},
volume={},
number={},
pages={774-782},
abstract={A Service Function Chain (SFC) is an ordered sequence of network functions, such as load balancing, content filtering, and firewall. With the Network Function Virtualization (NFV) paradigm, network functions can be deployed as pieces of software on generic hardware, leading to a flexibility of network service composition. Along with its benefits, NFV brings several challenges to network operators, such as the placement of virtual network functions. In this paper, we study the problem of how to optimally place the network functions within the network in order to satisfy all the SFC requirements of the flows. Our optimization task is to minimize the total deployment cost. We show that the problem can be seen as an instance of the Set Cover Problem, even in the case of ordered sequences of network functions. It allows us to propose two logarithmic factor approximation algorithms which have the best possible asymptotic factor. Further, we devise an optimal algorithm for tree topologies. Finally, we evaluate the performances of our proposed algorithms through extensive simulations. We demonstrate that near-optimal solutions can be found with our approach.},
keywords={approximation theory;computer networks;optimisation;telecommunication computing;trees (mathematics);virtualisation;network service composition;network operators;virtual network functions;set cover problem;logarithmic factor approximation algorithms;tree topologies;NFV paradigm;ordering constraints;service function chains placement;network function virtualization;Approximation algorithms;Network function virtualization;Conferences;Servers;Software;Optimization},
doi={10.1109/INFOCOM.2018.8486275},
ISSN={},
month={April},}
@INPROCEEDINGS{8486204,
author={K. Wang and K. Psounis},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Scheduling and Resource Allocation in 802.11ax},
year={2018},
volume={},
number={},
pages={279-287},
abstract={802.11ax introduces OFDMA to WiFi. It thus enables multiplexing users/user groups in the frequency domain. WiFi networks usually operate in a multipath environment which generates a frequency selective channel. Hence, the capacity of a user/user group changes over different subcarriers. A good scheduling and resource allocation scheme can maximize the sum rate by allocating users and user groups on subcarriers based on their CSI and other system considerations. In this paper we investigate how to optimally assign users and user groups to subcarriers with the goal of maximizing the user sum rate in the context of 802.11ax. We introduce a novel divide and conquer based algorithm which we prove to be optimal under the assumption that a user can be assigned to more than one resource unit (RU) which consists of one ore more subcarriers. This serves as a tight upper bound on the actual problem where users/user groups can be assigned to a single RU only per the 802.11ax standard. We then introduce two practical algorithms for the actual problem, a greedy one and a recursive one which jointly splits the bandwidth into RUs and schedules users on them. Extensive simulations comparing the performance of the aforementioned algorithms establish that our practical schemes achieve very good performance in all studied scenarios.},
keywords={divide and conquer methods;resource allocation;telecommunication scheduling;wireless LAN;resource allocation scheme;resource unit;single RU;divide and conquer based algorithm;CSI;frequency selective channel;multipath environment;WiFi networks;multiplexing user-user groups;OFDMA;scheduling;IEEE 802.11ax standard;Bandwidth;Channel capacity;OFDM;Resource management;Schedules;Wireless fidelity;Downlink},
doi={10.1109/INFOCOM.2018.8486204},
ISSN={},
month={April},}
@INPROCEEDINGS{8486295,
author={X. LI and J. D. Smith and T. N. Dinh and M. T. Thai},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Adaptive Crawling with Multiple Bots: A Matroid Intersection Approach},
year={2018},
volume={},
number={},
pages={1349-1357},
abstract={In this work, we examine the problem of adaptively uncovering network topology in an incomplete network, to support more accurate decision making in various real-world applications, such as modeling for reconnaissance attacks and network probing. While this problem has been partially studied, we provide a novel take on it by modeling it with a set of crawlers termed “bots” which can uncover independent portions of the network in parallel. Accordingly, we develop three adaptive algorithms, which make decisions based on previous observations due to incomplete information, namely AGP, a sequential method; FastAGP, a parallel algorithm; and ALSP, an extension of FastAGP uses local search to improve guarantees. These algorithms are proven to have 1/3, 1/7, and 1/ (5 + ϵ) approximation ratios, respectively. The key analysis of these algorithms is the connection between adaptive algorithms and an intersection of multiple partition matroids. We conclude with an evaluation of these algorithms to quantify the impact of both adaptivity and parallelism. We find that in practice, adaptive approaches perform significantly better, while FastAGP performs nearly as well as AGP in most cases despite operating in a massively parallel fashion. Finally, we show that a balance between the quantity and quality of bots is ideal for maximizing observation of the network.},
keywords={approximation theory;combinatorial mathematics;decision making;graph theory;optimisation;parallel algorithms;search problems;social networking (online);software agents;adaptive crawling;multiple bots;reconnaissance attacks;network probing;adaptive algorithms;AGP;FastAGP;parallel algorithm;multiple partition matroids;network topology;matroid intersection;sequential method;ALSP;online social network;Probes;Reconnaissance;Adaptation models;Partitioning algorithms;Network topology;Approximation algorithms;Adaptive systems;Network Probing;Adaptive Approximation Algorithms;Online Social Networks;Matroid Intersection;Optimization},
doi={10.1109/INFOCOM.2018.8486295},
ISSN={},
month={April},}
@INPROCEEDINGS{8486342,
author={J. Cho and T. J. Moore},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Percolation-based Network Adaptability Under Correlated Failures},
year={2018},
volume={},
number={},
pages={2186-2194},
abstract={Percolation theory has been studied to investigate network resilience by identifying a critical value of node occupation probability where a giant component (i.e., a largest component in a network) represents high network resilience. However, the concept of network resilience studied in percolation theory has been limited to measuring network fault-tolerance in the presence of failures/attacks. In this work, we take a step to extend the concept of network resilience beyond fault-tolerance by introducing network adaptability. We consider a tactical network where a node is executing multiple tasks by belonging to multiple task groups. In this type of tactical networks, a node is limited with its resource while aiming to maximize its resource utilization and task execution without being overloaded. We investigate network resilience and resource utilization of the tactical network where the network is attacked by either infectious or non-infectious attacks. To mitigate the impact of the failed/attacked nodes in the network, we propose a suite of the proposed adaptation strategies to deal with correlated, cascading failures caused by overloaded nodes due to increased workload introduced by other failed nodes. We conduct a comparative performance analysis of the set of proposed adaptation strategies and a baseline scheme with no adaptation based on metrics including a size of a giant component, node resource utilization, a number of active tasks in execution, and adaptation cost. Our simulation results show that a large size of a giant component does not necessarily ensure high resource utilization of nodes and task performance in the given tactical network.},
keywords={fault tolerance;military communication;percolation;probability;resource allocation;telecommunication network reliability;telecommunication security;high network resilience;percolation theory;network fault-tolerance;task execution;node resource utilization;percolation-based network adaptability;Resilience;Task analysis;Resource management;Fault tolerance;Fault tolerant systems;Power system faults;Power system protection},
doi={10.1109/INFOCOM.2018.8486342},
ISSN={},
month={April},}
@INPROCEEDINGS{8486333,
author={G. Vardoyan and C. V. Hollot and D. Towsley},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Towards Stability Analysis of Data Transport Mechanisms: A Fluid Model and an Application},
year={2018},
volume={},
number={},
pages={666-674},
abstract={The Transmission Control Protocol (TCP) utilizes congestion avoidance and control mechanisms as a preventive measure against congestive collapse and as an adaptive measure in the presence of changing network conditions. The set of available congestion control algorithms is diverse, and while many have been studied from empirical and simulation perspectives, there is a notable lack of analytical work for some variants. To gain more insight into the dynamics of these algorithms, we: (1) propose a general modeling scheme consisting of a set of functional differential equations of retarded type (RFDEs) and of the congestion window as a function of time; (2) apply this scheme to TCP Reno and demonstrate its equivalence to a previous, well known model for TCP Reno; (3) show an application of the new framework to the widely-deployed congestion control algorithm TCP CUBIC, for which analytical models are few and limited; and (4) validate the model using simulations. Our modeling framework yields a fluid model for TCP CUBIC. From a theoretical analysis of this model, we discover that TCP CUBIC is locally uniformly asymptotically stable-a property of the algorithm previously unknown.},
keywords={differential equations;telecommunication congestion control;transport protocols;transmission control protocol;congestion control algorithms;TCP CUBIC;RFDE;retarded type;stability analysis;congestion control algorithm;empirical simulation perspectives;network conditions;adaptive measure;congestive collapse;control mechanisms;congestion avoidance;data transport mechanisms;fluid model;TCP Reno;congestion window;functional differential equations;Mathematical model;Analytical models;Stability analysis;Asymptotic stability;Computational modeling;Adaptation models;Heuristic algorithms},
doi={10.1109/INFOCOM.2018.8486333},
ISSN={},
month={April},}
@INPROCEEDINGS{8485927,
author={T. Yang and C. G. Brinton and C. Joe-Wong},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Predicting Learner Interactions in Social Learning Networks},
year={2018},
volume={},
number={},
pages={1322-1330},
abstract={We consider the problem of predicting link formation in Social Learning Networks (SLN), a type of social network that forms when people learn from one another through structured interactions. While link prediction has been studied for general types of social networks, the evolution of SLNs over their lifetimes coupled with their dependence on which topics are being discussed presents new challenges for this type of network. To address these challenges, we develop a time-series prediction methodology that uses a recurrent neural network architecture to pass network state between time periods, and that models over three types of SLN features updated in each period: neighborhood-based (e.g., resource allocation), path-based (e.g., shortest path), and post-based (e.g., topic similarity). Through evaluation on four real-world datasets from Massive Open Online Course (MOOC) discussion forums, we find that our method obtains substantial improvements over a Bayesian model and an unsupervised baseline, with AUCs typically above 0.75 and reaching 0.97 depending on the dataset. Our feature importance analysis shows that while neighborhood-based features contribute the most to the results, post-based and path-based features add additional information that significantly improve the predictions. We also find that several input features have opposite directions of correlation between link formation and post quality, suggesting that response time and quality are two competing objectives to be accounted for in SLN link recommendation systems.},
keywords={computer aided instruction;educational courses;neural net architecture;recommender systems;recurrent neural nets;social networking (online);time series;learner interactions;Social Learning Networks;link formation;social network;structured interactions;link prediction;time-series prediction methodology;recurrent neural network architecture;network state;time periods;SLN features;Massive Open Online Course discussion forums;neighborhood-based features;path-based features;SLN link recommendation systems;Predictive models;Message systems;Feature extraction;Social network services;Computational modeling;Discussion forums;Bayes methods},
doi={10.1109/INFOCOM.2018.8485927},
ISSN={},
month={April},}
@INPROCEEDINGS{8486243,
author={A. Garcia-Saavedra and X. Costa-Perez and D. J. Leith and G. Iosifidis},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={FluidRAN: Optimized vRAN/MEC Orchestration},
year={2018},
volume={},
number={},
pages={2366-2374},
abstract={Virtualized Radio Access Network (vRAN) architectures constitute a promising solution for the densification needs of 5G networks, as they decouple Base Stations (BUs) functions from Radio Units (RUs) allowing the processing power to be pooled at cost-efficient Central Units (CUs). vRAN facilitates the flexible function relocation (split selection), and therefore enables splits with less stringent network requirements compared to state-of-the-art fully Centralized (C-RAN) systems. In this paper, we study the important and challenging vRAN design problem. We propose a novel modeling approach and a rigorous analytical framework, FluidRAN, that minimizes RAN costs by jointly selecting the splits and the RUs-CUs routing paths. We also consider the increasingly relevant scenario where the RAN needs to support multi-access edge computing (MEC) services, that naturally favor distributed RAN (D-RAN) architectures. Our framework provides a joint vRAN/MEC solution that minimizes operational costs while satisfying the MEC needs. We follow a data-driven evaluation method, using topologies of 3 operational networks. Our results reveal that (i) pure C-RAN is rarely a feasible upgrade solution for existing infrastructure, (ii) FluidRAN achieves significant cost savings compared to D-RAN systems, and (iii) MEC can increase substantially the operator's cost as it pushes vRAN function placement back to RUs.},
keywords={energy conservation;next generation networks;optimisation;quality of service;radio access networks;telecommunication power management;telecommunication signalling;telecommunication traffic;FluidRAN;optimized vRAN/MEC orchestration;Virtualized Radio Access Network architectures;Base Stations functions;Radio Units;processing power;cost-efficient Central Units;flexible function relocation;split selection;stringent network requirements;C-RAN;rigorous analytical framework;RAN costs;RUs-CUs;increasingly relevant scenario;operational networks;vRAN design problem;vRAN function placement;D-RAN systems;significant cost savings;feasible upgrade solution;MEC needs;operational costs;joint vRAN/MEC solution;RAN architectures;multiaccess edge computing services;Delays;Copper;Routing;Topology;Computational modeling;Radio access networks;5G mobile communication},
doi={10.1109/INFOCOM.2018.8486243},
ISSN={},
month={April},}
@INPROCEEDINGS{8486335,
author={D. da Hora and K. van Doorselaer and K. van Oost and R. Teixeira},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Predicting the effect of home Wi-Fi quality on QoE},
year={2018},
volume={},
number={},
pages={944-952},
abstract={Poor Wi-Fi quality can disrupt home users' internet experience, or the Quality of Experience (QoE). Detecting when Wi-Fi degrades QoE is extremely valuable for residential Internet Service Providers (ISPs) as home users often hold the ISP responsible whenever QoE degrades. Yet, ISPs have little visibility within the home to assist users. Our goal is to develop a system that runs on commodity access points (APs) to assist ISPs in detecting when Wi-Fi degrades QoE. Our first contribution is to develop a method to detect instances of poor QoE based on the passive observation of Wi-Fi quality metrics available in commodity APs (e.g., PHY rate). We use support vector regression to build predictors of QoE given Wi-Fi quality for popular internet applications. We then use K-means clustering to combine per-application predictors to identify regions of Wi-Fi quality where QoE is poor across applications. We call samples in these regions as poor QoE samples. Our second contribution is to apply our predictors to Wi-Fi metrics collected over one month from 3479 APs of customers of a large residential ISP. Our results show that QoE is good most of the time, still we find 11.6% of poor QoE samples. Worse, approximately 21% of stations have more than 25% poor QoE samples. In some cases, we estimate that Wi-Fi quality causes poor QoE for many hours, though in most cases poor QoE events are short.},
keywords={Internet;pattern clustering;quality of experience;regression analysis;support vector machines;wireless LAN;home Wi-Fi quality;home users;residential Internet Service Providers;ISPs;Wi-Fi quality metrics;Wi-Fi metrics;QoE;Quality of Experience;access points;support vector regression;K-means clustering;Wireless fidelity;Quality of experience;Measurement;Streaming media;Quality of service;Degradation;YouTube},
doi={10.1109/INFOCOM.2018.8486335},
ISSN={},
month={April},}
@INPROCEEDINGS{8485956,
author={J. Zhang and A. Sinha and J. Llorca and A. Tulino and E. Modiano},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Optimal Control of Distributed Computing Networks with Mixed-Cast Traffic Flows},
year={2018},
volume={},
number={},
pages={1880-1888},
abstract={Distributed computing networks, tasked with both packet transmission and processing, require the joint optimization of communication and computation resources. We develop a dynamic control policy that determines both routes and processing locations for packets upon their arrival at a distributed computing network. The proposed policy, referred to as Universal Computing Network Control (UCNC), guarantees that packets i) are processed by a specified chain of service functions, ii) follow cycle-free routes between consecutive functions, and iii) are delivered to their corresponding set of destinations via proper packet duplications. UCNC is shown to be throughput-optimal for any mix of unicast and multicast traffic, and is the first throughput-optimal policy for non-unicast traffic in distributed computing networks with both communication and computation constraints. Moreover, simulation results suggest that UCNC yields substantially lower average packet delay compared with existing control policies for unicast traffic.},
keywords={computer networks;multicast communication;optimal control;telecommunication control;telecommunication network routing;telecommunication traffic;UCNC;cycle-free routes;unicast traffic;multicast traffic;throughput-optimal policy;Universal Computing Network Control;dynamic control policy;computation resources;packet transmission;distributed computing network;mixed-cast traffic flows;distributed Computing networks;optimal Control;Unicast;Routing;Computational modeling;Process control;Cloud computing;Heuristic algorithms},
doi={10.1109/INFOCOM.2018.8485956},
ISSN={},
month={April},}
@INPROCEEDINGS{8486336,
author={Z. Han and H. Tan and R. Wang and S. Tang and F. C. M. Lau},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Online Learning based Uplink Scheduling in HetNets with Limited Backhaul Capacity},
year={2018},
volume={},
number={},
pages={2348-2356},
abstract={Heterogeneous cellular networks (HetNets) can significantly improve the spectrum efficiency, where low-power low-complexity base stations (Pico-BSs) are deployed inside the coverage of macro base stations (Macro-BSs). Due to cross-tier interference, joint detection of the uplink signals is widely adopted so that a Pico-BS can either detect the uplink signals locally or forward them to the Macro-BS for processing. The latter can achieve increased throughput at the cost of additional backhaul transmission. However, in existing literature the delay of the backhaul links was often neglected. In this paper, we study the delay-optimal uplink scheduling problem in HetNets with limited backhaul capacity. Local signal detection or joint signal detection is scheduled in a unified delay-optimal framework. Specifically, we first prove that the problem is NP-hard and then formulate it as a Markov Decision Process problem. We propose an efficient and effective algorithm, called OLIUS, that can deal with the exponentially growing state and action spaces. Furthermore, OLIUS is online learning based which does not require any prior statistical knowledge on user behavior or channel characteristics. We prove the convergence of OLIUS and derive an upper bound on its approximation error. Extensive experiments in various scenarios show that our algorithm outperforms existing methods in reducing delay and power consumption.},
keywords={approximation theory;cellular radio;communication complexity;decision theory;Markov processes;optimisation;signal detection;telecommunication scheduling;delay-optimal uplink scheduling problem;HetNets;unified delay-optimal framework;OLIUS;heterogeneous cellular networks;spectrum efficiency;cross-tier interference;NP-hard problem;uplink signal detection;low-power low-complexity base stations;backhaul transmission link capacity;picoBS;macrobase stations;Markov decision process problem;macroBS;Uplink;Interference;Delays;Scheduling;Signal detection;Macrocell networks;Base stations},
doi={10.1109/INFOCOM.2018.8486336},
ISSN={},
month={April},}
@INPROCEEDINGS{8486215,
author={J. Daly and E. Torng},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={ByteCuts: Fast Packet Classification by Interior Bit Extraction},
year={2018},
volume={},
number={},
pages={2654-2662},
abstract={Many networking devices, such as firewalls and routing tables, rely upon packet classifiers to define their behavior for various kinds of network traffic. Since these devices have real-time constraints, it is important for packet classification to be as fast as possible. We present a new method, ByteCuts, which includes two major improvements over existing tree-based packet classifiers. First, it introduces a new cutting method that is more efficient than existing methods. Second, ByteCuts intelligently partitions the rule list into multiple trees in a way that supports the cutting method and reduces rule replication. We compare ByteCuts to several existing methods such as HyperCuts, HyperSplit, and SmartSplit. We find that ByteCuts outperforms SmartSplit, the previous fastest classifier, in every metric; specifically, ByteCuts is able to classify packets 58% faster, can be constructed in seconds rather than minutes, and uses orders of magnitude less memory.},
keywords={firewalls;pattern classification;telecommunication network routing;telecommunication traffic;trees (mathematics);ByteCuts;fast packet classification;interior bit extraction;networking devices;routing tables;network traffic;tree-based packet classifiers;cutting method;firewalls;rule list;rule replication reduction;Vegetation;Decision trees;Memory management;Conferences;Real-time systems;Measurement;Indexes},
doi={10.1109/INFOCOM.2018.8486215},
ISSN={},
month={April},}
@INPROCEEDINGS{8486363,
author={T. Tran and D. T. Huynh},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Symmetric Connectivity Algotirthms in Multiple Directional Antennas Wireless Sensor Networks},
year={2018},
volume={},
number={},
pages={333-341},
abstract={In this paper, we investigate the Antenna Orientation (AO) and Antenna Orientation and Power Assignment (AOPA) problems concerning symmetric connectivity in Directional Wireless Sensor Networks (DWSNs) where each sensor node is equipped with 2 ≤ k ≤ 5 directional antennas having beamwidth θ ≥ 0. The AO problem for DWSNs is closely related with the well-known Euclidean Degree-Bounded Minimum Bottleneck Spanning Tree (EBMBST) problem where different cases for the degree bound have been studied. While current works on DWSNs focus on solving each case of k(=2, 3, 4) separately, we propose a uniform approach for the AO problem that yields constant-factor approximation algorithms for the AO as well as the EBMBST problem where the degree bound is between 2 and 4. Our method achieves the same constant factors. For the AOPA problem, to the best of our knowledge, our paper provides the first results concerning this problem. We show that the problem is NP-hard when 2 ≤ k ≤ 4. We also establish the first constant-factor approximation algorithms for the problem. Finally, we perform some simulations to understand the practical performance of our algorithms.},
keywords={approximation theory;directive antennas;wireless sensor networks;sensor node;constant-factor approximation algorithms;AOPA problem;directional wireless sensor networks;directional antennas;symmetric connectivity algorithms;multiple directional antennas wireless sensor networks;antenna orientation and power assignment;Approximation algorithms;Directional antennas;Directive antennas;Wireless sensor networks;Transmitting antennas;Conferences},
doi={10.1109/INFOCOM.2018.8486363},
ISSN={},
month={April},}
@INPROCEEDINGS{8485855,
author={A. S. Bedi and K. Rajawat and M. Coupechoux},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={An Online Approach to D2D Trajectory Utility Maximization Problem},
year={2018},
volume={},
number={},
pages={1610-1618},
abstract={This paper considers the problem of designing the user trajectory in a device-to-device communications setting. We consider a pair of pedestrians connected through a D2D link. The pedestrians seek to reach their respective destinations, while using the D2D link for data exchange applications such as file transfer, video calling, and online gaming. In order to enable better D2D connectivity, the pedestrians are willing to deviate from their respective shortest paths, at the cost of reaching their destinations slightly late. A generic trajectory optimization problem is formulated and solved for the case when full information about the problem in known in advance. Motivated by the D2D user's need to keep their destinations private, we also formulate a regularized variant of the problem that can be used to develop a fully online algorithm. The proposed online algorithm is quite efficient, and is shown to achieve a sublinear offline regret while satisfying the required mobility constraints exactly. The theoretical results are backed by detailed numerical tests that establish the efficacy of the proposed algorithms under various settings.},
keywords={cellular radio;mobile radio;optimisation;pedestrians;radio links;online approach;D2D Trajectory Utility Maximization Problem;user trajectory;device-to-device communications;pedestrians;data exchange applications;file transfer;video calling;online gaming;respective shortest paths;generic trajectory optimization problem;fully online algorithm;regularized variant;sublinear offline regret;mobility constraints;numerical tests;time 2.0 d;Device-to-device communication;Trajectory optimization;Heuristic algorithms;Robots},
doi={10.1109/INFOCOM.2018.8485855},
ISSN={},
month={April},}
@INPROCEEDINGS{8485947,
author={W. Li and X. Li and H. Li and G. Xie},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={CutSplit: A Decision-Tree Combining Cutting and Splitting for Scalable Packet Classification},
year={2018},
volume={},
number={},
pages={2645-2653},
abstract={Efficient algorithmic solutions for multi-field packet classification have been a challenging problem for many years. This problem is becoming even worse in the era of Software Defined Network (SDN), where flow tables with increasing complexities are playing a central role in the forwarding plane of SDN. In this paper, we first conduct an unprecedented in-depth reasoning on issues that led to the unsuccess of the major quests for scalable algorithmic solutions. With the insights obtained, we propose a practical framework called CutSplit, which can exploit the benefits of cutting and splitting techniques adaptively. By addressing the central problem caused by uncontrollable rule replications suffered by the major efforts, CutSplit not only pushes the performance of algorithmic packet classification more closely to hardware-based solutions, but also reduces the memory consumption to a practical level. Moreover, our work achieves low pre-processing time for rule updates, a problem that has long been ignored by previous decision-trees, but is becoming more relevant in the context of SDN due to frequent updates of rules. Experimental results show that using ClassBench, CutSplit achieves a memory reduction over 10 times, as well as 3x improvement on performance in terms of the number of memory access on average.},
keywords={computational complexity;decision trees;inference mechanisms;Internet;pattern classification;software defined networking;CutSplit;scalable packet classification;efficient algorithmic solutions;multifield packet classification;Software Defined Network;SDN;flow tables;increasing complexities;central role;forwarding plane;in-depth reasoning;scalable algorithmic solutions;practical framework;splitting techniques;central problem;uncontrollable rule replications;algorithmic packet classification;hardware-based solutions;low pre-processing time;rule updates;decision-tree;ClassBench;memory reduction;Classification algorithms;Memory management;Decision trees;Optimization;IP networks;Conferences;Software defined networking;Packet Classification;OpenFlow;Decision Tree;Algorithm;Firewall},
doi={10.1109/INFOCOM.2018.8485947},
ISSN={},
month={April},}
@INPROCEEDINGS{8485963,
author={Q. Qin and K. Poularakis and G. Iosifidis and L. Tassiulas},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={SDN Controller Placement at the Edge: Optimizing Delay and Overheads},
year={2018},
volume={},
number={},
pages={684-692},
abstract={Fog architectures at the network edge are becoming a popular research trend to provide elastic resources and services to end-users, where the processing capacity resides at the network periphery as opposed to traditional data-centers. Despite their momentum, the control plane of these architectures remains complex and challenging to implement. To enhance control capability, in this work, we propose to use Software Defined Networking. SDN moves the control logic off data plane devices and onto external network entities, the controllers. We provide a proof-of-concept implementation of a multi-controller edge system and measure traffic delay and overheads. The results reveal the sensitivity of delay to the location of controllers and the magnitude of inter-controller and controller-node overheads. Guided by the above, we model the problem of determining the placement of controllers in the edge network. Using linearization and supermodular function techniques, we present approximation solutions which perform close to optimal and better than state-of-the-art methods.},
keywords={cloud computing;computer network management;distributed processing;optimisation;resource allocation;software defined networking;telecommunication congestion control;intercontroller overhead;delay optimization;traffic delay;data plane devices;control logic;Software Defined Networking;network periphery;elastic resources;network edge;fog architectures;SDN controller placement;edge network;controller-node overheads;external network entities;Delays;Smart phones;Peer-to-peer computing;Computer architecture;Emulation;Wireless communication;Approximation algorithms},
doi={10.1109/INFOCOM.2018.8485963},
ISSN={},
month={April},}
@INPROCEEDINGS{8485909,
author={T. V. Xuan Phuong and R. Ning and C. Xin and H. Wu},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Puncturable Attribute-Based Encryption for Secure Data Delivery in Internet of Things},
year={2018},
volume={},
number={},
pages={1511-1519},
abstract={While the Internet of Things (IoT) is embraced as important tools for efficiency and productivity, it is becoming an increasingly attractive target for cybercriminals. This work represents the first endeavor to develop practical Puncturable Attribute Based Encryption schemes that are light-weight and applicable in IoTs. In the proposed scheme, the attribute-based encryption is adopted for fine grained access control. The secret keys are puncturable to revoke the decryption capability for selected messages, recipients, or time periods, thus protecting selected important messages even if the current key is compromised. In contrast to conventional forward encryption, a distinguishing merit of the proposed approach is that the recipients can update their keys by themselves without key re-issuing from the key distributor. It does not require frequent communications between IoT devices and the key distribution center, neither does it need deleting components to expunge existing keys to produce a new key. Moreover, we devise a novel approach which efficiently integrates attribute-based key and punctured keys such that the key size is roughly the same as that of the original attribute-based encryption. We prove the correctness of the proposed scheme and its security under the Decisional Bilinear Diffie-Hellman (DBDH) assumption. We also implement the proposed scheme on Raspberry Pi and observe that the computation efficiency of the proposed approach is comparable to the original attribute-based encryption. Both encryption and decryption can be completed within tens of milliseconds.},
keywords={authorisation;Internet of Things;private key cryptography;telecommunication security;productivity;fine grained access control;secret keys;key distributor;IoT devices;key distribution center;attribute-based key;punctured keys;key size;decryption;Puncturable Attribute-Based Encryption;secure data delivery;attribute-based encryption;Puncturable Attribute Based Encryption schemes;forward encryption;Internet of Things;cybercriminals;Encryption;Receivers;Internet of Things;Conferences;Access control;Attribute-Based Encryption;Internet-of-Things;Lagrange Polynomial;Linear Secret Sharing},
doi={10.1109/INFOCOM.2018.8485909},
ISSN={},
month={April},}
@INPROCEEDINGS{8485896,
author={N. Liakopoulos and G. Paschos and T. Spyropoulos},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Robust User Association for Ultra Dense Networks},
year={2018},
volume={},
number={},
pages={2690-2698},
abstract={We study the user association problem in the context of dense networks, where standard adaptive algorithms become ineffective. The paper proposes a novel data-driven technique leveraging the theory of robust optimization. The main idea is to predict future traffic fluctuations, and use the predictions to design association maps before the actual arrival of traffic. Although the actual playout of the map is random due to prediction error, the maps are robustly designed to handle uncertainty, preventing constraint violations, and maximizing the expectation of a convex utility function, which allows to accurately balance base station loads. We propose a generic iterative algorithm, referred to as GRMA, which is shown to converge to the optimal robust map. The optimal maps have the intriguing property that they jointly optimize the predicted load and the variance of the prediction error. We validate our robust maps in Milano-area traces, with dense coverage and find that we can reduce violations from 25% (achieved by an adaptive algorithm) down to almost zero.},
keywords={5G mobile communication;iterative methods;optimisation;quality of service;radio networks;telecommunication traffic;association maps;prediction error;convex utility function;base station loads;generic iterative algorithm;adaptive algorithm;robust user association;ultra dense networks;user association problem;standard adaptive algorithms;Base stations;Robustness;Quality of service;Optimization;Wireless communication;Conferences;5G mobile communication},
doi={10.1109/INFOCOM.2018.8485896},
ISSN={},
month={April},}
@INPROCEEDINGS{8486401,
author={T. Chen and Y. Zhu and Z. Li and J. Chen and X. Li and X. Luo and X. Lin and X. Zhange},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Understanding Ethereum via Graph Analysis},
year={2018},
volume={},
number={},
pages={1484-1492},
abstract={Being the largest blockchain with the capability of running smart contracts, Ethereum has attracted wide attention and its market capitalization has reached 20 billion USD. Ethereum not only supports its cryptocurrency named Ether but also provides a decentralized platform to execute smart contracts in the Ethereum virtual machine. Although Ether's price is approaching 200 USD and nearly 600K smart contracts have been deployed to Ethereum, little is known about the characteristics of its users, smart contracts, and the relationships among them. To fill in the gap, in this paper, we conduct the first systematic study on Ethereum by leveraging graph analysis to characterize three major activities on Ethereum, namely money transfer, smart contract creation, and smart contract invocation. We design a new approach to collect all transaction data, construct three graphs from the data to characterize major activities, and discover new observations and insights from these graphs. Moreover, we propose new approaches based on cross-graph analysis to address two security issues in Ethereum. The evaluation through real cases demonstrates the effectiveness of our new approaches.},
keywords={contracts;cryptography;electronic money;financial data processing;graph theory;Internet;virtual machines;smart contracts;smart contract creation;smart contract invocation;cross-graph analysis;Ethereum virtual machine;Ethereum;Conferences},
doi={10.1109/INFOCOM.2018.8486401},
ISSN={},
month={April},}
@INPROCEEDINGS{8485834,
author={C. Harper and M. Pierobon and M. Magarini},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Estimating Information Exchange Performance of Engineered Cell-to-cell Molecular Communications: A Computational Approach},
year={2018},
volume={},
number={},
pages={729-737},
abstract={Biological cells naturally exchange information for adapting to the environment, or even influencing other cells. One of the latest frontiers of synthetic biology stands in engineering cells to harness these natural communication processes for tissue engineering and cancer treatment, amongst others. Although experimental success has been achieved in this direction, approaches to characterize these systems in terms of communication performance and their dependence on design parameters are currently limited. In contrast to more classical communication systems, information in biological cells is propagated through molecules and biochemical reactions, which in general result in nonlinear input-output behaviors with system-evolution-dependent stochastic effects that are not amenable to analytical closed-form characterization. In this paper, a computational approach is proposed to characterize the information exchange in these systems, based on stochastic simulation of biochemical reactions and the estimation of information-theoretic parameters from sample distributions. In particular, this approach focuses on engineered cell-to-cell communications with a single transmitter and receiver, and it is applied to characterize the performance of a realistic system. Numerical results confirm the feasibility of this approach to be at the basis of future forward engineering practices for these communication systems.},
keywords={biomedical communication;molecular communication (telecommunication);stochastic processes;tissue engineering;cancer treatment;biological cells;nonlinear input-output behaviors;system-evolution-dependent stochastic effects;computational approach;information-theoretic parameters;natural communication processes;information exchange performance estimation;biochemical reaction simulation;transmitter;receiver;synthetic biology;engineered cell-to-cell molecular communications;Receivers;Chemicals;Transmitters;Computational modeling;Communication systems;Synthetic biology;Information exchange;Molecular Communication;Synthetic Biology;Mutual Information;Stochastic Simulation},
doi={10.1109/INFOCOM.2018.8485834},
ISSN={},
month={April},}
@INPROCEEDINGS{8486426,
author={X. Zhang and J. Knockel and J. R. Crandall},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={ONIS: Inferring TCP/IP-based Trust Relationships Completely Off-Path},
year={2018},
volume={},
number={},
pages={2069-2077},
abstract={We present ONIS, a new scanning technique that can perform network measurements such as: inferring TCP/IP-based trust relationships off-path, stealthily port scanning a target without using the scanner's IP address, detecting off-path packet drops between two international hosts. These tasks typically rely on a core technique called the idle scan, which is a special kind of port scan that appears to come from a third machine called a zombie. The scanner learns the target's status from the zombie by using its TCP/IP side channels. Unfortunately, the idle scan assumes that the zombie has IP identifiers (IPIDs) which exhibit the now-discouraged behavior of being globally incrementing. The use of this kind of IPID counter is becoming increasingly rare in practice. Our technique, unlike the idle scan, is based on a much more advanced IPID generation scheme, that of the prevalent Linux kernel. Although Linux's IPID generation scheme is specifically intended to reduce information flow, we show that using Linux machines as zombies in an indirect scan is still possible. ONIS has 87% accuracy, which is comparable to nmap's implementation of the idle scan at 86%. ONIS's much broader choice of zombies will enable it to be a widely used technique which can fulfill various network measurement tasks.},
keywords={inference mechanisms;Linux;transport protocols;ONIS scanning technique;inferring TCP-IP-based trust relationships completely off-path;IP address identifiers;off-path packet drop detection;TCP-IP side channels;Linux IPID generation scheme;Linux kernel;stealthily port scanning technique;Linux machines;Linux;IP networks;Kernel;Probes;Extraterrestrial measurements;Current measurement;Conferences},
doi={10.1109/INFOCOM.2018.8486426},
ISSN={},
month={April},}
@INPROCEEDINGS{8485996,
author={G. Sallam and G. R. Gupta and B. Li and B. Ji},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Shortest Path and Maximum Flow Problems Under Service Function Chaining Constraints},
year={2018},
volume={},
number={},
pages={2132-2140},
abstract={With the advent of Network Function Virtualization (NFV), Physical Network Functions (PNFs) are gradually being replaced by Virtual Network Functions (VNFs) that are hosted on general purpose servers. Depending on the call flows for specific services, the packets need to pass through an ordered set of network functions (physical or virtual) called Service Function Chains (SFC) before reaching the destination. Conceivably for the next few years during this transition, these networks would have a mix of PNFs and VNFs, which brings an interesting mix of network problems that are studied in this paper: (1) How to find an SFC-constrained shortest path between any pair of nodes? (2) What is the achievable SFC-constrained maximum flow? (3) How to place the VNFs such that the cost (the number of nodes to be virtualized) is minimized, while the maximum flow of the original network can still be achieved even under the SFC constraint? In this work, we will try to address such emerging questions. First, for the SFC-constrained shortest path problem, we propose a transformation of the network graph to minimize the computational complexity of subsequent applications of any shortest path algorithm. Second, we formulate the SFC-constrained maximum flow problem as a fractional multicommodity flow problem, and develop a combinatorial algorithm for a special case of practical interest. Third, we prove that the VNFs placement problem is NP-hard and present an alternative Integer Linear Programming (ILP) formulation. Finally, we conduct simulations to elucidate our theoretical results.},
keywords={computational complexity;graph theory;integer programming;virtualisation;PNFs;ordered set;network problems;SFC-constrained shortest path problem;network graph;SFC-constrained maximum flow problem;fractional multicommodity flow problem;VNFs placement problem;physical network functions;service function chaining constraints;network function virtualization;virtual network functions;computational complexity;combinatorial algorithm;NP-hard;integer linear programming formulation;NFV;ILP;Wide area networks;Servers;Network function virtualization;Shortest path problem;Conferences;Approximation algorithms;Integer linear programming},
doi={10.1109/INFOCOM.2018.8485996},
ISSN={},
month={April},}
@INPROCEEDINGS{8486433,
author={Y. Tian and W. Wei and Q. Li and F. Xu and S. Zhong},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={MobiCrowd: Mobile Crowdsourcing on Location-based Social Networks},
year={2018},
volume={},
number={},
pages={2726-2734},
abstract={The great potential of mobile crowdsourcing has started to attract attention of both industries and the research community. However, current commercial mobile crowdsourcing marketplaces are unsatisfactory because of the limited worker base and functionality. In this paper, we first revisit the foundation of performing mobile crowdsourcing on location-based social networks (LBSNs) through specially designed survey studies and comparison experiments involving hundreds of users. Our results reveal that active check-ins are good indicators of picking a right user to perform tasks, and LBSN could be an ideal platform for mobile crowdsourcing given proper services provided. We then propose both the centralized and decentralized design of MobiCrowd, a mobile crowdsourcing service built on LBSNs. Our evaluation, through trace-driven simulation and real-world experiments, demonstrates that the proposed schemes can effectively find workers for mobile crowdsourcing tasks associated with different venues by analyzing their location check-in histories.},
keywords={data analysis;mobile computing;sensor fusion;social networking (online);location-based social networks;mobile crowdsourcing service;mobile crowdsourcing tasks;MobiCrowd;mobile crowdsourcing marketplaces;LBSNs;location check-in histories;Task analysis;Crowdsourcing;Facebook;Conferences;Sensors;History},
doi={10.1109/INFOCOM.2018.8486433},
ISSN={},
month={April},}
@INPROCEEDINGS{8486398,
author={T. Li and Y. Chen and R. Zhang and Y. Zhang and T. Hedgpeth},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Secure Crowdsourced Indoor Positioning Systems},
year={2018},
volume={},
number={},
pages={1034-1042},
abstract={Indoor positioning systems (IPSes) can enable many location-based services in large indoor environments where GPS is not available or reliable. Mobile crowdsourcing is widely advocated as an effective way to construct IPS maps. This paper presents the first systematic study of security issues in crowd-sourced WiFi-based IPSes to promote security considerations in designing and deploying crowdsourced IPSes. We identify three attacks on crowdsourced WiFi-based IPSes and propose the corresponding countermeasures. The efficacy of the attacks and also our countermeasures are experimentally validated on a prototype system. The attacks and countermeasures can be easily extended to other crowdsourced IPSes.},
keywords={crowdsourcing;indoor navigation;indoor radio;location based services;mobile computing;telecommunication security;wireless LAN;prototype system;secure crowdsourced indoor positioning systems;location-based services;indoor environments;mobile crowdsourcing;security considerations;crowdsourced WiFi-based IPSes;IPS maps;IP networks;Databases;Crowdsourcing;Prototypes;Wireless fidelity;Indoor environments;Security},
doi={10.1109/INFOCOM.2018.8486398},
ISSN={},
month={April},}
@INPROCEEDINGS{8486377,
author={X. Jiang and H. Lu and C. W. Chen},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Enabling Quality-Driven Scalable Video Transmission over Multi-User NOMA System},
year={2018},
volume={},
number={},
pages={1952-1960},
abstract={Recently, non-orthogonal multiple access (NOMA) has been proposed to achieve higher spectral efficiency over conventional orthogonal multiple access. Although it has the potential to meet increasing demands of video services, it is still challenging to provide high performance video streaming. In this research, we investigate, for the first time, a multi-user NOMA system design for video transmission. Various NOMA systems have been proposed for data transmission in terms of throughput or reliability. However, the perceived quality, or the quality-of-experience of users, is more critical for video transmission. Based on this observation, we design a quality-driven scalable video transmission framework with cross-layer support for multiuser NOMA. To enable low complexity multi-user NOMA operations, a novel user grouping strategy is proposed. The key features in the proposed framework include the integration of the quality model for encoded video with the physical layer model for NOMA transmission, and the formulation of multiuser NOMA-based video transmission as a quality-driven power allocation problem. As the problem is non-concave, a global optimal algorithm based on the hidden monotonic property and a suboptimal algorithm with polynomial time complexity are developed. Simulation results show that the proposed multi-user NOMA system outperforms existing schemes in various video delivery scenarios.},
keywords={communication complexity;multi-access systems;polynomials;quality of experience;telecommunication network reliability;video communication;quality-driven power allocation problem;video delivery scenarios;data transmission;quality-driven scalable video transmission framework;user grouping strategy;nonorthogonal multiple access system;spectral efficiency;video encoding;low complexity multiuser NOMA-based video transmission;orthogonal multiple access;video streaming;reliability;quality-of-experience;cross-layer support;nonconcave global optimal algorithm;hidden monotonic property;suboptimal algorithm;polynomial time complexity;NOMA;Streaming media;Resource management;Silicon carbide;Data communication;Complexity theory;Optimization;Multi-media transmission;NOMA;cross-layer framework;monotonic optimization},
doi={10.1109/INFOCOM.2018.8486377},
ISSN={},
month={April},}
@INPROCEEDINGS{8485905,
author={X. Ran and H. Chen and X. Zhu and Z. Liu and J. Chen},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={DeepDecision: A Mobile Deep Learning Framework for Edge Video Analytics},
year={2018},
volume={},
number={},
pages={1421-1429},
abstract={Deep learning shows great promise in providing more intelligence to augmented reality (AR) devices, but few AR apps use deep learning due to lack of infrastructure support. Deep learning algorithms are computationally intensive, and front-end devices cannot deliver sufficient compute power for real-time processing. In this work, we design a framework that ties together front-end devices with more powerful backend “helpers” (e.g., home servers) to allow deep learning to be executed locally or remotely in the cloud/edge. We consider the complex interaction between model accuracy, video quality, battery constraints, network data usage, and network conditions to determine an optimal offloading strategy. Our contributions are: (1) extensive measurements to understand the tradeoffs between video quality, network conditions, battery consumption, processing delay, and model accuracy; (2) a measurement-driven mathematical framework that efficiently solves the resulting combinatorial optimization problem; (3) an Android application that performs real-time object detection for AR applications, with experimental results that demonstrate the superiority of our approach.},
keywords={augmented reality;cloud computing;learning (artificial intelligence);mobile computing;video signal processing;edge video analytics;augmented reality devices;deep learning algorithms;front-end devices;video quality;measurement-driven mathematical framework;Mobile Deep Learning Framework;DeepDecision;backend helpers;home servers;cloud/edge;Machine learning;Computational modeling;Streaming media;Measurement;Real-time systems;Neural networks;Batteries},
doi={10.1109/INFOCOM.2018.8485905},
ISSN={},
month={April},}
@INPROCEEDINGS{8486351,
author={L. Zheng and C. Joe-Wong and M. Andrews and M. Chiang},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Optimizing Data Plans: Usage Dynamics in Mobile Data Networks},
year={2018},
volume={},
number={},
pages={2474-2482},
abstract={As the U.S. mobile data market matures, Internet service providers (ISPs) generally charge their users with some variation on a quota-based data plan with overage charges. Common variants include unlimited, prepaid, and usage-based data plans. However, despite a recent flurry of research on optimizing mobile data pricing, few works have considered how these data plans affect users' consumption behavior. In particular, while users with such plans have a strong incentive to plan their usage over the month, they also face uncertainty in their future data usage needs that would make such planning difficult. In this work, we develop a dynamic programming model of users' consumption decisions over the month that takes this uncertainty into account. We use this model to quantify which types of users would benefit from different types of data plans, using these conditions to extrapolate the optimal types of data plans that ISPs should offer. Our theoretical findings are complemented by numerical simulations on a dataset of user usage from a large U.S. ISP. The results help mobile users to choose data plans that maximize their utilities and ISPs to gain profit by understanding their user behavior while choosing what data plans to offer.},
keywords={dynamic programming;Internet;mobile communication;optimisation;mobile data pricing;quota-based data plan;mobile data networks;data plans;Data models;Pricing;Uncertainty;Conferences;Smart phones;Stochastic processes;Web and internet services},
doi={10.1109/INFOCOM.2018.8486351},
ISSN={},
month={April},}
@INPROCEEDINGS{8486386,
author={M. Harishankar and N. Srinivasan and C. Joe-Wong and P. Tague},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={To Accept or Not to Accept: The Question of Supplemental Discount Offers in Mobile Data Plans},
year={2018},
volume={},
number={},
pages={2609-2617},
abstract={As demand for Internet usage increases, Internet service providers (ISPs) have begun to explore pricing-based solutions to dampen data demand. However few explicitly consider the dual problem of monetizing idle network capacity at uncongested times. PopData is a recent initiative from Verizon that does so by offering supplemental discount offers (SDOs) at these times, in which users can pay a fixed fee in exchange for unlimited data in the next hour. This work is the first of its kind to assess the benefits and viability of SDOs by modeling user and ISP decisions as a game, considering both overall monthly decisions and hour-to-hour decisions throughout the month. We first use our monthly model to show that users are generally willing to accept some SDO offers, allowing the ISP to increase its revenue. We then show that users face a complex hourly decision problem as to which SDOs they should accept over their billing cycles, since they are unaware of their exact future needs or when future SDOs will be made. The ISP faces a similarly challenging problem in deciding when to offer SDOs so as to maximize its revenue, subject to users' decisions. We develop optimal decision criteria for users and ISPs to decide whether to make or accept SDO offers. Our analysis shows that both users and ISPs can benefit from these offers, and we verify this through numerical experiments on a one-week trace of 20 cellular data users. We find that ISPs can exploit user uncertainty in when future SDOs will be made to optimize its revenue.},
keywords={Internet;mobile communication;pricing;supplemental discount offers;mobile data plans;Internet service providers;pricing-based solutions;dual problem;idle network capacity;uncongested times;ISP decisions;hour-to-hour decisions;complex hourly decision problem;optimal decision criteria;Internet usage;cellular data users;Uncertainty;Pricing;Games;Wireless fidelity;Computational modeling;Data models;Cellular networks},
doi={10.1109/INFOCOM.2018.8486386},
ISSN={},
month={April},}
@INPROCEEDINGS{8486206,
author={H. Gupta and A. Eryilmaz and R. Srikant},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Low-Complexity, Low-Regret Link Rate Selection in Rapidly-Varying Wireless Channels},
year={2018},
volume={},
number={},
pages={540-548},
abstract={We consider the problem of transmitting at the optimal rate over a rapidly-varying wireless channel with unknown statistics when the feedback about channel quality is very limited. One motivation for this problem is that, in emerging wireless networks, the use of mm Wave bands means that the channel quality can fluctuate rapidly and thus, one cannot rely on full channel-state feedback to make transmission rate decisions. Inspired by related problems in the context of multi-armed bandits, we consider a well-known algorithm called Thompson sampling to address this problem. However, unlike the traditional multi-armed bandit problem, a direct application of Thompson sampling results in a computational and storage complexity that grows exponentially with time. Therefore, we propose an algorithm called Modified Thompson sampling (MTS), whose computational and storage complexity is simply linear in the number of channel states and which achieves at most logarithmic regret as a function of time when compared to an optimal algorithm which knows the probability distribution of the channel states.},
keywords={computational complexity;millimetre wave communication;optimisation;statistical distributions;time-varying channels;wireless channels;rapidly-varying wireless channel;channel quality;wireless networks;channel-state feedback;computational complexity;low-complexity link rate selection;probability distribution;mmwave bands;storage complexity;low-regret link rate selection;channel states;Modified Thompson sampling;multiarmed bandits;transmission rate decisions;Wireless communication;Throughput;Resource management;Conferences;Probability distribution;Propagation losses;Optimization;Link Rate Selection;Thompson Sampling;Regret Minimization;Computational Complexity},
doi={10.1109/INFOCOM.2018.8486206},
ISSN={},
month={April},}
@INPROCEEDINGS{8486262,
author={F. Zhou and M. Y. Naderi and K. Sankhe and K. Chowdhury},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Making the Right Connections: Multi-AP Association and Flow Control in 60GHz Band},
year={2018},
volume={},
number={},
pages={1214-1222},
abstract={Emerging network architectures in 60GHz millimeter wave bands will likely use dense deployment of access points (APs) given the high attenuation and frequent line-of-sight related outages. We show in this paper that naively connecting to any available AP, or even multiple APs, may not fully realize the promise of efficiently utilizing the extremely large bandwidths available in this band. This paper holistically addresses these problems through the Multi-AP Association Protocol (MAP) that: (i) indicates ideal durations for beam-searching at the physical/link layers of the protocol stack that will result in minimal interruptions to user-traffic, (ii) devises a Multi-AP association framework based on multipath TCP, which increases the network robustness by allowing immediate redirection of traffic to alternative APs whenever an existing connection is interrupted, and (iii) designs an exploration-exploitation aware flow scheduling algorithm that dynamically activates the sub-flows to optimize transport layer performance. MAP is a lightweight, client side system that needs zero modifications to existing protocol stack, though it interacts with the latter to opportunistically trigger standards-defined functions. The paper also presents convergence analysis of throughput, experimental validation on a 60GHz network testbed, and trace-driven simulation studies that shows MAP achieving 5-7x reduction in re-buffering rate in HD video streaming, rapid convergence to the best possible AP, and fair allocation of resources among clients.},
keywords={millimetre wave communication;resource allocation;telecommunication congestion control;telecommunication scheduling;telecommunication traffic;transport protocols;video streaming;flow control;network architectures;access points;MAP;multiAP association protocol framework;physical-link layers;trigger standards-defined functions;trace-driven simulation;HD video streaming;resource allocation;lightweight client side system;transport layer performance;exploration-exploitation aware flow scheduling algorithm;multipath TCP;user-traffic;beam-searching;line-of-sight related outages;frequency 60.0 GHz;Throughput;Protocols;Streaming media;Bandwidth;Standards;Quality of experience;IP networks},
doi={10.1109/INFOCOM.2018.8486262},
ISSN={},
month={April},}
@INPROCEEDINGS{8486305,
author={S. Sundar and B. Liang},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Offloading Dependent Tasks with Communication Delay and Deadline Constraint},
year={2018},
volume={},
number={},
pages={37-45},
abstract={We study the scheduling decision for an application consisting of dependent tasks, in a generic cloud computing system comprising a network of heterogeneous local processors and a remote cloud server. We formulate an optimization problem to find the offloading decision that minimizes the overall application execution cost, subject to an application completion deadline. Since this problem is NP-hard, we propose a heuristic algorithm termed Individual Time Allocation with Greedy Scheduling (ITAGS) to obtain an efficient solution. ITAGS first uses a binary-relaxed version of the original problem to allocate a completion deadline to each individual task, and then greedily optimizes the scheduling of each task subject to its time allowance. Through trace-based simulation using real applications, as well as various randomly generated task trees, we study the performance of ITAGS, highlighting the effect of the application deadline, communication delay, number of processors, and number of tasks. We further demonstrate the substantial performance advantage of ITAGS over existing alternatives.},
keywords={cloud computing;digital simulation;greedy algorithms;multiprocessing systems;optimisation;resource allocation;scheduling;trees (mathematics);communication delay;deadline constraint;scheduling decision;heterogeneous local processors;remote cloud server;optimization problem;NP-hard;Individual Time Allocation;Greedy Scheduling;ITAGS;randomly generated task trees;cloud computing system;dependent task offloading;Task analysis;Program processors;Cloud computing;Delays;Processor scheduling;Mobile handsets;Scheduling},
doi={10.1109/INFOCOM.2018.8486305},
ISSN={},
month={April},}
@INPROCEEDINGS{8485893,
author={V. Chaganti and J. Kurose and A. Venkataramani},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={A Cross-Architectural Quantitative Evaluation of Mobility Approaches},
year={2018},
volume={},
number={},
pages={639-647},
abstract={Future Internet Architectures must support the rapid growth of traffic generated by mobile endpoints in a manner that is scalable and ensures low latency. We present a quantitative evaluation of three distinct approaches towards handling endpoint mobility: name-based forwarding, indirection and a global name service (GNS). Using a range of parameterized mobility distributions and real ISP topologies, we describe representative instantiations of each approach and evaluate their performance using four key metrics: update cost and update propagation cost in the control plane; and forwarding traffic cost and time-to-connect (TTC) in the data plane. (1) We show that by leveraging the fact that realistic endpoint mobility distributions show a high probability of being at a small subset of visited locations, name-based forwarding strategies can provide up to 60% improvement in control costs over simple best-port forwarding. (2) We show that the TTC in these name-based forwarding strategies is comparable to the TTC in the GNS. (3) Finally we show that a GNS-based approach offers the most suitable balance of total (combined data and control) cost to TTC across all approaches, all endpoint mobility distributions, and all ISP topologies considered.},
keywords={mobile radio;probability;radiowave propagation;telecommunication network topology;telecommunication traffic;propagation cost;forwarding traffic cost;name-based forwarding strategies;control costs;TTC;GNS-based approach;ISP topologies;cross-architectural quantitative evaluation;global name service;parameterized mobility distributions;future Internet architectures;endpoint mobility distributions;time-to-connect;probability;Servers;Measurement;Routing protocols;Network topology;Topology;Conferences;Internet},
doi={10.1109/INFOCOM.2018.8485893},
ISSN={},
month={April},}
@INPROCEEDINGS{8486238,
author={J. Lubars and R. Srikant},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Correcting the Output of Approximate Graph Matching Algorithms},
year={2018},
volume={},
number={},
pages={1745-1753},
abstract={Approximate graph matching refers to the problem of finding the best correspondence between the node labels of two correlated graphs. The problem has been applied to a number of domains, including social network de-anonymization. Recently, a number of algorithms have been proposed for seeded graph matching, which uses a few seed matches between two graphs to determine the remaining correspondence. We adapt the ideas from seeded algorithms to develop a graph matching correction algorithm, which takes a partially correct correspondence as input and returns an improved correspondence. We show that this algorithm can correct all errors in graph matching for stochastic block model graphs with high probability. Finally, we apply our algorithm as a post-processing step for other approximate graph matching algorithms to significantly improve the performance of state-of-the-art algorithms for seedless graph matching.},
keywords={approximation theory;graph theory;probability;social networking (online);stochastic processes;seedless graph matching;approximate graph matching algorithms;node labels;correlated graphs;social network de-anonymization;graph matching correction algorithm;stochastic block model graphs;probability;Approximation algorithms;Social network services;Stochastic processes;Inference algorithms;Mathematical model;Conferences;Measurement},
doi={10.1109/INFOCOM.2018.8486238},
ISSN={},
month={April},}
@INPROCEEDINGS{8486326,
author={Q. Li and X. Feng and R. Wang and Z. Li and L. Sun},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Towards Fine-grained Fingerprinting of Firmware in Online Embedded Devices},
year={2018},
volume={},
number={},
pages={2537-2545},
abstract={An increasing number of embedded devices are connecting to the Internet at a surprising rate. Those devices usually run firmware and are exposed to the public by device search engines. Firmware in embedded devices comes from different manufacturers and product versions. More importantly, many embedded devices are still using outdated versions of firmware due to compatibility and release-time issues, raising serious security concerns. In this paper, we propose generating fine-grained fingerprints based on the subtle differences between the filesystems of various firmware images. We leverage the natural language processing technique to process the file content and the document object model to obtain the firmware fingerprint. To validate the fingerprints, we have crawled 9,716 firmware images from official websites of device vendors and conducted real-world experiments for performance evaluation. The results show that the recall and precision of the firmware fingerprints exceed 90%. Furthermore, we have deployed the prototype system on Amazon EC2 and collected firmware in online embedded devices across the IPv4 space. Our findings indicate that thousands of devices are still using vulnerable firmware on the Internet.},
keywords={firmware;IP networks;natural language processing;search engines;online embedded devices;fine-grained fingerprints;firmware fingerprint;vulnerable firmware;search engines;firmware images;crawled images;fine-grained fingerprinting;natural language processing;document object model;Amazon EC2;IPv4 space;Internet;Microprogramming;Computer security;Natural language processing;Prototypes;Cameras;Conferences},
doi={10.1109/INFOCOM.2018.8486326},
ISSN={},
month={April},}
@INPROCEEDINGS{8486418,
author={J. Lin and M. Li and D. Yang and G. Xue},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Sybil-Proof Online Incentive Mechanisms for Crowdsensing},
year={2018},
volume={},
number={},
pages={2438-2446},
abstract={Crowdsensing leverages the rapid growth of sensor-embedded smartphones and human mobility for pervasive information collection. To incentivize smartphone users to participate in crowdsensing, many auction-based incentive mechanisms have been proposed for both offline and online scenarios. It has been demonstrated that the Sybil attack may undermine these mechanisms. In a Sybil attack, a user illegitimately pretends multiple identities to gain benefits. Sybil-proof incentive mechanisms have been proposed for the offline scenario. However, the problem of designing Sybil-proof online incentive mechanisms for crowdsensing is still open. Compared to the offline scenario, the online scenario provides users one more dimension of flexibility, i.e., active time, to conduct Sybil attacks, which makes this problem more challenging. In this paper, we design Sybil-proof online incentive mechanisms to deter the Sybil attack for crowdsensing. Depending on users' flexibility on performing their tasks, we investigate both single-minded and multi-minded cases and propose SOS and SOM, respectively. SOS achieves computational efficiency, individual rationality, truthfulness, and Sybil-proofness. SOM achieves individual rationality, truthfulness, and Sybil-proofness. Through extensive simulations, we evaluate the performance of SOS and SOM.},
keywords={mobile computing;security of data;smart phones;sensor-embedded smartphones;smartphone users;auction-based incentive mechanisms;Sybil attack;Sybil-proof incentive mechanisms;Sybil-proofness;crowdsensing;Sybil-proof online incentive mechanisms;human mobility;pervasive information collection;computational efficiency;truthfulness;individual rationality;Task analysis;Smart phones;Sensors;Microsoft Windows;Conferences;Privacy;Cost function},
doi={10.1109/INFOCOM.2018.8486418},
ISSN={},
month={April},}
@INPROCEEDINGS{8485965,
author={J. Lee and F. Baccelli},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={On the Effect of Shadowing Correlation on Wireless Network Performance},
year={2018},
volume={},
number={},
pages={1601-1609},
abstract={We propose and analyze a new shadowing field model meant to capture spatial correlations. The interference field associated with this new model is compared to that of the widely used independent shadowing model. Independent shadowing over links is adopted because of the resulting closed forms for performance metrics, and in spite of the well-known fact that the shadowing fields of networks are spatially correlated. The main purpose of this paper is to challenge this independent shadowing approximation. For this, we analyze the interference measured at the origin in networks where 1) nodes which are in the same cell of some random shadowing tessellation share the same shadow, or 2) nodes which share a common mother point in some cluster process share the same shadow. By leveraging stochastic comparison techniques, we give the order relation of the three main user performance metrics, namely coverage probability, Shannon throughput and local delay, under both the correlated and the independent shadowing assumptions. We show that the evaluation of the considered metrics under the independent approximation is systematically pessimistic compared to the correlated shadowing model. The improvement in each metric when adopting the correlated shadow model is quantified and shown to be quite significant.},
keywords={approximation theory;correlation methods;probability;radio networks;stochastic processes;random shadowing tessellation;correlated shadowing model;independent approximation;independent shadowing assumptions;main user performance metrics;independent shadowing approximation;spatial correlations;shadowing field model;wireless network performance;Shadow mapping;Stochastic processes;Interference;Base stations;Measurement;Random variables;Computational modeling},
doi={10.1109/INFOCOM.2018.8485965},
ISSN={},
month={April},}
@INPROCEEDINGS{8486213,
author={A. Roy and C. A. Kamhoua and P. Mohapatra},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Game Theoretic Characterization of Collusive Behavior Among Attackers},
year={2018},
volume={},
number={},
pages={2078-2086},
abstract={Recent observations have shown that most of the attacks are fruits of collaboration among attackers. In this work we have developed a coalition formation game to model the collusive behavior among attackers. The novelty of this work is that we are the first to investigate the coalition formation dynamics among attackers with different efficiency. Most of the related works have modeled the attacker as a single entity. We define a new parameter called friction to represent the unwillingness of an attacker to collude. We have shown that the proportion of attackers in the Maximum Average Payoff Coalition (MAPC) decreases with efficiency. We have also shown that as the friction increases, size and heterogeneity of MAPC decrease. We show, using text analysis on a hacker web forum chat data, that the hacker collaboration network shows a strong small-world characteristics. We identify the leaders in these coalitions. The cluster compositions of the hacker collaboration network agree with our model. We also develop method to estimate the friction parameters for the attackers to decide optimal coalition to join. As this model provides insight into coalition formation among attackers, e.g., leaders, composition, and homogeneity, this model will be helpful to develop better defender strategies.},
keywords={computer crime;game theory;groupware;text analysis;collusive behavior;attacker;coalition formation game;hacker collaboration network;game theoretic characterization;friction;maximum average payoff coalition;MAPC;text analysis;hacker web forum chat data;Computer hacking;Friction;Games;Data models;Collaboration;Computational modeling;Computer crime},
doi={10.1109/INFOCOM.2018.8486213},
ISSN={},
month={April},}
@INPROCEEDINGS{8485832,
author={C. Leet and X. Wang and Y. Richard Yang and J. Aspnes},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Toward the First SDN Programming Capacity Theorem on Realizing High-Level Programs on Low-Level Datapaths},
year={2018},
volume={},
number={},
pages={711-719},
abstract={High-Ievel programming and programmable data paths are two key capabilities of software-defined networking (SDN). A fundamental problem linking these two capabilities is whether a given high-level SDN program can be realized onto a given low-level SDN datapath structure. Considering all high-level programs that can be realized onto a given datapath as the programming capacity of the datapath, we refer to this problem as the SDN data path programming capacity problem. In this paper, we conduct the first study on the SDN datapath programming capacity problem, in the general setting of high-level, datapath oblivious, algorithmic SDN programs and state-of-art multi-table SDN data path pipelines. In particular, considering datapath-oblivious SDN programs as computations and datapath pipelines as computation capabilities, we introduce a novel framework called SDN characterization junctions, to map both SDN programs and datapaths into a unifying space, deriving the first rigorous result on SDN datapath programming capacity. We not only prove our results but also conduct realistic evaluations to demonstrate the tightness of our analysis.},
keywords={pipeline processing;program compilers;programming;software defined networking;low-level datapaths;programmable data paths;low-level SDN datapath structure;SDN data path programming capacity problem;SDN datapath programming capacity problem;datapath-oblivious SDN programs;SDN characterization junctions;high-level programs;SDN programming capacity theorem;multitable SDN data path pipelines;high-level SDN program compilers;Pipelines;Routing;Registers;Programming profession;Conferences;Switches},
doi={10.1109/INFOCOM.2018.8485832},
ISSN={},
month={April},}
@INPROCEEDINGS{8486241,
author={Q. Liu and S. Huang and J. Opadere and T. Han},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={An Edge Network Orchestrator for Mobile Augmented Reality},
year={2018},
volume={},
number={},
pages={756-764},
abstract={Mobile augmented reality (MAR) involves high complexity computation which cannot be performed efficiently on resource limited mobile devices. The performance of MAR would be significantly improved by offloading the computation tasks to servers deployed with the close proximity to the users. In this paper, we design an edge network orchestrator to enable fast and accurate object analytics at the network edge for MAR. The measurement-based analytical models are built to characterize the tradeoff between the service latency and analytics accuracy in edge-based MAR systems. As a key component of the edge network orchestrator, a server assignment and frame resolution selection algorithm named FACT is proposed to mitigate the latency-accuracy tradeoff. Through network simulations, we evaluate the performance of the FACT algorithm and show the insights on optimizing the performance of edge-based MAR systems. We have implemented the edge network orchestrator and developed the corresponding communication protocol. Our experiments validate the performance of the proposed edge network orchestrator.},
keywords={augmented reality;computational complexity;distributed processing;mobile computing;protocols;edge network orchestrator;mobile augmented reality;edge-based MAR systems;FACT algorithm;mobile devices;server assignment;frame resolution selection algorithm;communication protocol;Servers;Mars;Computational modeling;Cloud computing;Analytical models;Wireless communication;Mobile handsets},
doi={10.1109/INFOCOM.2018.8486241},
ISSN={},
month={April},}
@INPROCEEDINGS{8486291,
author={M. Khodak and L. Zheng and A. S. Lan and C. Joe-Wong and M. Chiang},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Learning Cloud Dynamics to Optimize Spot Instance Bidding Strategies},
year={2018},
volume={},
number={},
pages={2762-2770},
abstract={As infrastructure-as-a-service clouds become more popular, cloud providers face the complicated problem of maximizing their resource utilization by handling the dynamics of user demand. Auction-based pricing, such as Amazon EC2 spot pricing, provides an option for users to use idle resources at highly reduced yet dynamic prices; under such a pricing scheme, users place bids for cloud resources, and the provider chooses a threshold “spot” price above which bids are admitted. In this paper, we propose a nonlinear dynamical system model for the time-evolution of the spot price as a function of latent states that characterize user demand in the spot and on-demand markets. This model enables us to adaptively predict future spot prices given past spot price observations, allowing us to derive user bidding strategies for heterogeneous cloud resources that minimize the cost to complete a job with negligible probability of interruption. Along the way, the model also yields novel, empirically verifiable insights into cloud provider behavior. We experimentally validate our model and bidding strategy on two months of Amazon EC2 spot price data and find that our proposed bidding strategy is up to 4 times closer to the optimal strategy in hindsight compared to a baseline regression approach while incurring the same negligible probability of interruption.},
keywords={cloud computing;cost reduction;optimisation;pricing;resource allocation;infrastructure-as-a-service clouds;cloud providers;auction-based pricing;Amazon EC2 spot pricing;idle resources;pricing scheme;threshold spot price;nonlinear dynamical system model;on-demand markets;spot price observations;user bidding strategies;heterogeneous cloud resources;cloud provider behavior;bidding strategy;Amazon EC2 spot price data;resource utilization maximization;dynamic prices;spot price time-evolution;cloud dynamics learning;spot instance bidding strategy optimization;user demand dynamics handling;cost minimization;interruption probability;Cloud computing;Pricing;Hidden Markov models;Predictive models;Computational modeling;Conferences;Resource management},
doi={10.1109/INFOCOM.2018.8486291},
ISSN={},
month={April},}
@INPROCEEDINGS{8486314,
author={P. Madadi and F. Baccelli and G. de Veciana},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={On Spatial and Temporal Variations in Ultra Dense Wireless Networks},
year={2018},
volume={},
number={},
pages={2663-2671},
abstract={Ultra densification along with the use of wider bands at higher frequencies are likely to be key elements towards meeting the throughput/coverage objectives of 5G wireless networks. In addition to increased parallelism, densification leads to improved, but eventually bounded, benefits from proximity of users to base stations, while resulting in increased aggregate interference. Such networks are expected to be interference limited, and in higher frequency regimes, the interference is expected to become spatially variable due to the increased sensitivity of propagation to obstructions and the proximity of active interferers. This paper studies the characteristics of the spatial random fields associated with interference and Shannon capacity in ultra-dense limiting regimes. They rely on the theory of Gaussian random fields which arise as natural limits under densification. Our models show how densification and operation at higher frequencies, could lead to increasingly rough temporal variations in the interference process. This is characterized by the Hölder exponent of the interference field. We show that these fluctuations make it more difficult for mobile users to adapt modulation and coding. We further study how the spatial correlations in users' rates impact backhaul dimensioning. Therefore, this paper identifies and quantifies challenges associated with densification in terms of the resulting unpredictability and the correlation of interference on the achievable rates.},
keywords={5G mobile communication;correlation methods;densification;Gaussian processes;interference suppression;radiofrequency interference;random processes;spatiotemporal phenomena;ultra-dense limiting regimes;Gaussian random fields;interference process;mobile users;spatial correlations;ultra dense wireless networks;5G wireless networks;base stations;spatial random fields;temporal variations;Shannon capacity;Holder exponent;Interference;Base stations;Wireless communication;Modulation;Adaptation models;5G mobile communication;Encoding},
doi={10.1109/INFOCOM.2018.8486314},
ISSN={},
month={April},}
@INPROCEEDINGS{8485810,
author={Y. Zhu and R. A. Berry},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Contracts as Investment Barriers in Unlicensed Spectrum},
year={2018},
volume={},
number={},
pages={1925-1933},
abstract={By not requiring expensive licenses, unlicensed spectrum lowers the barriers for firms to offer wireless services. However, incumbent firms may still try to erect other entry barriers. For example, recent work has highlighted how customer contracts may be used as one such barrier by penalizing customers for switching to a new entrant. However, this work did not account for another potential benefit of unlicensed spectrum, having access to this open resource may incentivize entrants to invest in new and potentially better technology. This paper studies the interaction of contracts and the incentives of firms to invest in developing new technology. We use a game theoretic model to study this and characterize the effect of contracts on economic welfare. The role of subsidies or taxes by a social planner is also considered.},
keywords={game theory;investment;investment barriers;unlicensed spectrum;expensive licenses;wireless services;entry barriers;customer contracts;game theoretic model;Contracts;Investment;Wireless communication;Switches;Liquids;Quality of service;Games},
doi={10.1109/INFOCOM.2018.8485810},
ISSN={},
month={April},}
@INPROCEEDINGS{8486017,
author={Y. Li and H. Wang and K. Sun},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Email as a Master Key: Analyzing Account Recovery in the Wild},
year={2018},
volume={},
number={},
pages={1646-1654},
abstract={Account recovery (usually through a password reset) on many websites has mainly relied on accessibility to a registered email due to its favorable deployability and usability. However, it makes a user's online accounts vulnerable to a single point of failure when the registered email account is compromised. While previous research focuses on strengthening user passwords, the security risk imposed by email-based account recovery has not yet been well studied. In this paper, we investigate the possibility of mounting an email-based account recovery attack. Specifically, we examine the account authentication and recovery protocols in 239 traffic-heavy websites, confirming that most of them use emails for account recovery. We further scrutinize the security policy of major email service providers and show that a significant portion of them take no or marginal effort to protect user email accounts, leaving compromised email accounts readily available for mounting account recovery attacks. Then, we conduct case studies to assess potential losses caused by such attacks. Finally, we propose a lightweight email security enhancement called Secure Email Account Recovery (SEAR) to defend against account recovery attacks as an extra layer of protection to account recovery emails.},
keywords={authorisation;data protection;electronic mail;message authentication;Web sites;registered email account;user passwords;email-based account recovery attack;account authentication;recovery protocols;email service providers;user email accounts;compromised email accounts;Secure Email Account Recovery;email security;websites;account recovery email protection;Electronic mail;Password;Protocols;Authentication;Tools;Google},
doi={10.1109/INFOCOM.2018.8486017},
ISSN={},
month={April},}
@INPROCEEDINGS{8486207,
author={S. Mohanti and E. Bozkaya and M. Yousof Naderi and B. Canberk and K. Chowdhury},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={WiFED: WiFi Friendly Energy Delivery with Distributed Beamforming},
year={2018},
volume={},
number={},
pages={926-934},
abstract={Wireless RF energy transfer for indoor sensors is an emerging paradigm that ensures continuous operation without battery limitations. However, high power radiation within the ISM band interferes with the packet reception for existing WiFi devices. The paper proposes the first effort in merging the RF energy transfer functions within a standards compliant 802.11 protocol to realize practical and WiFi-friendly Energy Delivery (WiFED). The WiFED architecture is composed of a centralized controller that coordinates the actions of multiple distributed energy transmitters (ETs), and a number of deployed sensors that periodically request energy from the ETs. The paper first describes the specific 802.11 supported protocol features that can be exploited by sensors to request energy and for the ETs to participate in the energy delivery process. Second, it devises a controller-driven bipartite matching-based algorithmic solution that assigns the appropriate number of ETs to energy requesting sensors for an efficient energy transfer process. The proposed in-band and protocol supported coexistence in WiFED is validated via simulations and partly in a software defined radio testbed, showing 15% improvement in network lifetime and 31% reduction in the charging delay compared to the classical nearest distance-based charging schemes that do not anticipate future energy needs of the sensors and are not designed to co-exist with WiFi systems.},
keywords={array signal processing;computer network reliability;protocols;radio transmitters;software radio;telecommunication power management;wireless LAN;wireless sensor networks;WiFi systems;distributed beamforming;indoor sensors;WiFED architecture;multiple distributed energy transmitters;ET;wireless RF energy transfer function process;WiFi-friendly energy delivery architecture;ISM band;packet reception;standards compliant 802.11 protocol;software defined radio testbed;nearest distance-based charging scheme;energy requesting sensors;controller-driven bipartite matching-based algorithmic solution;Sensors;Energy exchange;Wireless fidelity;Array signal processing;Receivers;Protocols;Radio frequency},
doi={10.1109/INFOCOM.2018.8486207},
ISSN={},
month={April},}
@INPROCEEDINGS{8485852,
author={X. Wang and R. A. Berry},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={The Impact of Bundling Licensed and Unlicensed Wireless Service},
year={2018},
volume={},
number={},
pages={1394-1402},
abstract={Unlicensed spectrum has been viewed as a way to increase competition in wireless access and promote innovation in new technologies and business models. However, several recent papers have shown that the openness of such spectrum can also lead to it becoming over congested when used by competing wireless service providers (SPs). This in turn can result in the SPs making no profit and may deter them from entering the market. However, this prior work assumes that unlicensed access is a separate service from any service offered using licensed spectrum. Here, we instead consider the more common case were service providers bundle both licensed and unlicensed spectrum as a single service and offer this with a single price. We analyze a model for such a market and show that in this case SPs are able to gain higher profit than the case without bundling. It is also possible to get higher social welfare with bundling. Moreover, we explore the case where SPs are allowed to manage the customers' average percentage of time they receive service on unlicensed spectrum and characterize the social welfare gap between the profit maximizing and social welfare maximizing setting.},
keywords={cognitive radio;game theory;pricing;profitability;radio spectrum management;single service;unlicensed spectrum;profit;unlicensed wireless service;wireless access;technologies;business models;wireless service providers;unlicensed access;separate service;licensed spectrum;service providers bundle both licensed;bundling licensed wireless service;SPs;Wireless communication;Computational modeling;Bandwidth;Conferences;Biological system modeling;Games;Licenses},
doi={10.1109/INFOCOM.2018.8485852},
ISSN={},
month={April},}
@INPROCEEDINGS{8485859,
author={M. Nishino and T. Inoue and N. Yasuda and S. Minato and M. Nagata},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Optimizing Network Reliability via Best-First Search over Decision Diagrams},
year={2018},
volume={},
number={},
pages={1817-1825},
abstract={Communication networks are an essential infrastructure and must be designed carefully to ensure high reliability. Identifying a fully reliable design is, however, computationally very tough since it requires that a reliability evaluation, which is known to be #P-complete, be repeated an exponential number of times. Existing studies, therefore, attempt to avoid exact optimization to reduce the computational burden by applying heuristics. Due to the importance of communication networks and to better assess the accuracy of heuristic approaches, exact optimization remains a key goal. This paper proposes an exact method for two network design problems: reliability maximization under budget constraints and cost minimization with assurance of reliability. Our method employs a common idea to solve these problems, i.e., a best-first search algorithm that runs on decision diagrams. Our method employs just a single binary decision diagram (BDD) to compute the reliability for any solution and is also used as the basis of a novel heuristic function, called the cost-aware BDD heuristic function, as a search guide. Numerical experiments show that our method scales well; it successfully optimizes a network with 189 links. In addition, our method reveals the poor performance of existing heuristic approaches; a well-known existing heuristic method is shown to yield a solution that offers less than half the optimal reliability.},
keywords={binary decision diagrams;computational complexity;decision trees;optimisation;reliability;search problems;best-first search over decision diagrams;communication networks;reliability evaluation;network design problems;reliability maximization;cost minimization;search algorithm;cost-aware BDD heuristic function;network reliability optimization;binary decision diagram;optimal reliability;#P-complete;Boolean functions;Computer network reliability;Reliability engineering;Optimization;Reliability theory;Communication networks},
doi={10.1109/INFOCOM.2018.8485859},
ISSN={},
month={April},}
@INPROCEEDINGS{8486323,
author={S. Im and M. Shadloo and Z. Zheng},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Online Partial Throughput Maximization for Multidimensional Coflow},
year={2018},
volume={},
number={},
pages={2042-2050},
abstract={Coflow has recently been introduced to capture communication patterns that are widely observed in the cloud and massively parallel computing. Coflow consists of a number of flows that each represents data communication from one machine to another. A coflow is completed when all of its flows are completed. Due to its elegant abstraction of the complicated communication processes found in various parallel computing platforms, it has received significant attention. In this paper, we consider coflow for the objective of maximizing partial throughput. This objective seeks to measure the progress made for partially completed coflows before their deadline. Partially processed coflows still could be useful when their flows send out useful data that can be used for the next round computation. In our measure, a coflow is processed by a certain fraction when all of its flows are processed by the same fraction or more. We consider a natural class of greedy algorithms, which we call myopic concurrent. The algorithms seek to maximize the marginal increase of the partial throughput objective at each time. We analyze the performance of our algorithm against the optimal scheduler. In fact, our result is more general as a flow could be extended to demand various heterogeneous resources. Our experiment demonstrates our algorithm's superior performance.},
keywords={data communication;parallel programming;scheduling;parallel computing platforms;partially completed coflows;partially processed coflows;partial throughput objective;online partial throughput maximization;multidimensional coflow;communication patterns;massively parallel computing;data communication;Task analysis;Throughput;Computational modeling;Schedules;Optimal scheduling;Cloud computing;Processor scheduling},
doi={10.1109/INFOCOM.2018.8486323},
ISSN={},
month={April},}
@INPROCEEDINGS{8486427,
author={R. I. Tavares da Costa Filho and W. Lautenschläger and N. Kagami and M. Caggiani Luizelli and V. Roesler and L. Paschoal Gaspary},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Scalable QoE-aware Path Selection in SDN-based Mobile Networks},
year={2018},
volume={},
number={},
pages={989-997},
abstract={To deal with the massive traffic produced by video applications, mobile operators rely on offloading technologies such as Small Cells, Content Delivery Networks and, shortly, Cloud Edge and 5G Device to Device communications. Although these techniques are fundamental for improving network efficiency, they produce a multitude of paths onto which the user traffic can be forwarded. Thus, a critical problem arises about how to handle the increasing video traffic while managing the interplay between infrastructure optimization and the user's Quality of Experience (QoE). Solving this problem is remarkably difficult, and recent investigations do not consider the large-scale context of mobile operator networks. To address this issue, we present a novel QoE-aware path deployment scheme for large-scale SDN-based mobile networks. The scheme relies on both a polynomial-time algorithm for composing multiple QoS metrics and a scalable QoS to QoE translation strategy. Considering real mobile operator network and video traffic traces, we show that the proposed algorithm outperformed state-of-the-art approaches by reducing impaired videos in aggregate MOS by at least 37% and lowering accumulated video stall length four times.},
keywords={5G mobile communication;quality of experience;quality of service;software defined networking;telecommunication traffic;video streaming;5G device to device communications;accumulated video stall length;user quality of experience;QoS to QoE translation strategy;infrastructure optimization;impaired videos;video traffic traces;scalable QoS;large-scale SDN-based mobile networks;novel QoE-aware path deployment scheme;mobile operator network;user traffic;network efficiency;Cloud Edge;Content Delivery Networks;offloading technologies;video applications;massive traffic;scalable QoE-aware path selection;Quality of experience;Quality of service;Streaming media;Delays;Loss measurement;Throughput},
doi={10.1109/INFOCOM.2018.8486427},
ISSN={},
month={April},}
@INPROCEEDINGS{8486370,
author={Y. Cao and A. O. F. Atya and S. Singh and Z. Qian and S. V. Krishnamurthy and T. L. Porta and P. Krishnamurthy and L. Marvel},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={A Framework for MIMO-based Packet Header Obfuscation},
year={2018},
volume={},
number={},
pages={1763-1771},
abstract={Eavesdroppers can exploit exposed packet headers towards attacks that profile clients and their data flows. In this paper, we propose FOG, a framework for effective header blinding using MIMO, to thwart eavesdroppers. FOG effectively tracks header bits as they traverse physical (PHY) layer sub-systems that perform functions like scrambling and interleaving. It combines multiple blinding signals for more effective and less predictable obfuscation, as compared to using a fixed blinding signal. We implement FOG on the WARP platform and demonstrate via extensive experiments that it yields better obfuscation than prior schemes that deploy full packet blinding. It causes a bit error rate (BER) of > 40 % at an eavesdropper if two blinding streams are sent during header transmissions. Furthermore, FOG incurs a very small throughput hit of ≈5 % with one blinding stream (and 9 % with two streams). Full packet blinding incurs much higher throughput hits (25 % with one stream and 50 % with two streams).},
keywords={error statistics;MIMO communication;signal processing;telecommunication security;telecommunication traffic;predictable obfuscation;PHY layer sub-systems;WARP platform;header transmissions;blinding stream;bit error rate;full packet blinding;fixed blinding signal;multiple blinding signals;physical layer sub-systems;header bits;effective header blinding;FOG;data flows;profile clients;eavesdropper;MIMO-based packet header obfuscation;MIMO communication;Encryption;Throughput;Receivers;Transmitting antennas;Wireless communication},
doi={10.1109/INFOCOM.2018.8486370},
ISSN={},
month={April},}
@INPROCEEDINGS{8485982,
author={N. Papadis and S. Borst and A. Walid and M. Grissa and L. Tassiulas},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Stochastic Models and Wide-Area Network Measurements for Blockchain Design and Analysis},
year={2018},
volume={},
number={},
pages={2546-2554},
abstract={The Blockchain paradigm provides a popular mechanism for establishing trust and consensus in distributed environments. While Blockchain technology is currently primarily deployed in crypto-currency systems like Bitcoin, the concept is also expected to emerge as a key component of the Internet-of-Things (IoT), enabling novel applications in digital health, smart energy, asset tracking and smart transportation. As Blockchain networks evolve to industrial deployments with large numbers of geographically distributed nodes, the block transfer and processing delays arise as a critical issue which may create greater potential for forks and vulnerability to adversarial attacks. Motivated by these issues, we develop stochastic network models to capture the Blockchain evolution and dynamics and analyze the impact of the block dissemination delay and hashing power of the member nodes on Blockchain performance in terms of the overall block generation rate and required computational power for launching a successful attack. The results provide useful insight in crucial design issues, e.g., how to adjust the `difficulty-of-work' in the presence of delay so as to achieve a target block generation rate or appropriate level of immunity from adversarial attacks. We employ a combination of analytical calculations and simulation experiments to investigate both stationary and transient performance features, and demonstrate close agreement with measurements on a wide-area network testbed running the Ethereum protocol.},
keywords={cryptographic protocols;Internet of Things;peer-to-peer computing;stochastic processes;wide area networks;stochastic models;blockchain design;blockchain paradigm;transient performance features;stationary performance features;target block generation rate;hashing power;block dissemination delay;stochastic network models;greater potential;processing delays;block transfer;geographically distributed nodes;industrial deployments;smart transportation;asset tracking;smart energy;digital health;IoT;Internet-of-Things;Bitcoin;crypto-currency systems;distributed environments;wide-area network measurements;Delays;Peer-to-peer computing;Analytical models;Stochastic processes;Bitcoin},
doi={10.1109/INFOCOM.2018.8485982},
ISSN={},
month={April},}
@INPROCEEDINGS{8486271,
author={R. Tahir and A. Raza and F. Ahmad and J. Kazi and F. Zaffar and C. Kanich and M. Caesar},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={It's All in the Name: Why Some URLs are More Vulnerable to Typosquatting},
year={2018},
volume={},
number={},
pages={2618-2626},
abstract={Typosquatting is a blackhat practice that relies on human error and low-cost domain registrations to hijack legitimate traffic from well-established websites. The technique is typically used for phishing, driving traffic towards competitors or disseminating indecent or malicious content and as such remains a concern for businesses. We take a fresh new look at this well-studied phenomenon to explore why some URLs are more vulnerable to typing mistakes than others. We explore the relationship between human hand anatomy, keyboard layouts and typing mistakes using various URL datasets. We create an extensive user-centric typographical model and compute a Hardness Quotient (likelihood of mistyping) for each URL using a quantitative measure for finger and hand effort. Furthermore, our model predicts the most likely typos for each URL which can then be defensively registered. Cross-validation against actual URL and DNS datasets suggests that this is a meaningful and effective defense mechanism.},
keywords={computer crime;Internet;keyboards;Web sites;typosquatting;blackhat practice;malicious content;URL datasets;hijack legitimate traffic;websites;user-centric typographical model;hardness quotient;DNS;Uniform resource locators;Keyboards;Layout;Measurement;Phishing;Browsers},
doi={10.1109/INFOCOM.2018.8486271},
ISSN={},
month={April},}
@INPROCEEDINGS{8485935,
author={A. M. Abdelmoniem and B. Bensaou},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={Curbing Timeouts for TCP-Incast in Data Centers via A Cross-Layer Faster Recovery Mechanism},
year={2018},
volume={},
number={},
pages={675-683},
abstract={We first study, at a microscopic level, the effects of various types of packet losses on TCP performance in a small data center. Then based on the findings we propose a simple recovery mechanism to combat the drawbacks of the long retransmission timeout. We emphasize through our empirical study that packet losses that occur at the tail of short-lived flows a nd/or bursty losses that span a large fraction of the congestion window are frequent in data center networks; and, in most cases, especially for short-lived flows, they result in a loss recovery that incurs waiting for a long retransmission timeout (RTO). The negative effect of frequent RTOs on the FCT is dramatic, yet recovery via RTO is merely a symptom of the pathological design of TCP's minimum RTO mechanism (set by default to the Internet scale). We propose the so-called Timely Retransmitted ACKs (T-RACKs), a very simple recovery mechanism for data centers, implemented as a shim layer between the virtual machines layer and the end-host NIC, to bridge the gap between TCP's huge RTO and the actual round trip times experienced in the data center. Compared to alternative solutions such as DCTCP, our T-RACKS has the virtue of not requiring any modification to TCP, which makes it readily deployable in virtualized multi-tenant public data centers. Experimental results show considerable improvements in the FCT distribution. Index Terms-Data Center, Cross Layer, Fast Recovery, Kernel Module, TCP-Incast, Timeouts.},
keywords={computer centres;transport protocols;curbing timeouts;cross-layer faster recovery mechanism;TCP-incast performance;TCP minimum RTO mechanism;index terms-data center networks;congestion window;Internet scale;timely retransmitted ACK;T-RACK;end-host NIC;FCT distribution;index terms-data center;kernel module;virtualized multitenant public data centers;long retransmission timeout;packet losses;Data centers;Packet loss;Microsoft Windows;Delays;Probes;Computational efficiency},
doi={10.1109/INFOCOM.2018.8485935},
ISSN={},
month={April},}
@INPROCEEDINGS{8486369,
author={Y. Jia and Y. Xiao and J. Yu and X. Cheng and Z. Liang and Z. Wan},
booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
title={A Novel Graph-based Mechanism for Identifying Traffic Vulnerabilities in Smart Home IoT},
year={2018},
volume={},
number={},
pages={1493-1501},
abstract={Smart home IoT devices have been more prevalent than ever before but the relevant security considerations fail to keep up with due to device and technology heterogeneity and resource constraints, making IoT systems susceptible to various attacks. In this paper, we propose a novel graph-based mechanism to identify the vulnerabilities in communication of IoT devices for smart home systems. Our approach takes one or more packet capture files as inputs to construct a traffic graph by passing the captured messages, identify the correlated subgraphs by examining the attribute-value pairs associated with each message, and then quantify their vulnerabilities based on the sensitivity levels of different keywords. To test the effectiveness of our approach, we setup a smart home system that can control a smart bulb LB100 via either the smartphone APP for LB100 or the Google Home speaker. We collected and analyzed 58,714 messages and exploited 6 vulnerable correlated sub graphs, based on which we implemented 6 attack cases that can be easily reproduced by attackers with little knowledge of IoT. This study is novel as our approach takes only the collected traffic files as inputs without requiring the knowledge of the device firmware while being able to identify new vulnerabilities. With this approach, we won the third prize out of 20 teams in a hacking competition.},
keywords={graph theory;Internet of Things;message passing;mobile computing;security of data;smart phones;smart home IoT devices;technology heterogeneity;resource constraints;IoT systems;smart home system;packet capture files;traffic graph;smart bulb LB100;Google Home speaker;graph-based mechanism;messages passing;traffic files;traffic vulnerabilities identification;smartphone APP;Google;Servers;Smart homes;Cryptography;Protocols;Sensitivity},
doi={10.1109/INFOCOM.2018.8486369},
ISSN={},
month={April},}

