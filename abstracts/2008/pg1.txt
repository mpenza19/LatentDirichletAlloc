@INPROCEEDINGS{4509604,
author={Y. Shi and Y. T. Hou},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Theoretical Results on Base Station Movement Problem for Sensor Network},
year={2008},
volume={},
number={},
pages={1-5},
abstract={The benefits of using mobile base station to prolong sensor network lifetime have been well recognized. However, due to the complexity of the problem (time-dependent network topology and traffic routing), theoretical performance limit and provably optimal algorithms remain difficult to develop. This paper fills this important gap by contributing theoretical results regarding the optimal movement of a mobile base station. Our main result hinges upon a novel transformation of the joint base station movement and flow routing problem from time domain to space domain. Based on this transformation, we first show that if the base station is allowed to be present only on a set of pre-defined points, then we can find the optimal time span for the base station on each of these points so that the overall network lifetime is maximized. Based on this finding, we show that when the location of the base station is un-constrained (i.e., can move to any point in the two-dimensional plane), we can develop an approximation algorithm for the joint mobile base station location and flow routing problem such that the network lifetime is guaranteed to be at least (1-epsiv) of the maximum network lifetime, where epsiv can be made arbitrarily small depending on required precision.},
keywords={computational complexity;mobile radio;telecommunication network reliability;telecommunication network routing;wireless sensor networks;mobile base station movement problem;sensor network lifetime;joint base station movement;flow routing problem;approximation algorithm;Base stations;Routing;Cost function;Telecommunication traffic;Network topology;Approximation algorithms;Communications Society;USA Councils;Mobile computing;Fasteners},
doi={10.1109/INFOCOM.2008.9},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509605,
author={M. J. Neely},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Delay Analysis for Maximal Scheduling in Wireless Networks with Bursty Traffic},
year={2008},
volume={},
number={},
pages={6-10},
abstract={We consider the delay properties of one-hop networks with general interference constraints and multiple traffic streams with time-correlated arrivals. We first treat the case when arrivals are modulated by independent finite state Markov chains. We show that the well known maximal scheduling algorithm achieves average delay that grows at most logarithmically in the largest number of interferers at any link. Further, in the important special case when each Markov process has at most two states (such as bursty ON/OFF sources), we prove that average delay is independent of the number of nodes and links in the network, and hence is order-optimal. We provide tight delay bounds in terms of the individual auto-correlation parameters of the traffic sources. These are perhaps the first order-optimal delay results for controlled queueing networks that explicitly account for such statistical information.},
keywords={Markov processes;queueing theory;radio networks;scheduling;telecommunication traffic;delay analysis;maximal scheduling algorithm;one-hop wireless network traffic;finite state Markov chain;queueing network;Wireless networks;Telecommunication traffic;Interference constraints;Delay;Throughput;Traffic control;Scheduling algorithm;Markov processes;Transmitters;Packet switching},
doi={10.1109/INFOCOM.2008.10},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509606,
author={Z. Kong and E. M. Yeh},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Connectivity and Latency in Large-Scale Wireless Networks with Unreliable Links},
year={2008},
volume={},
number={},
pages={11-15},
abstract={We study connectivity and transmission latency in wireless networks with unreliable links from a percolation-based perspective. We first examine static models, where each link of the network is functional (active) with some probability, independently of all other links, where the probability may depend on the distance between the two nodes. We obtain analytical upper and lower bounds on the critical density for phase transition in this model. We then examine dynamic models, where each link is active or inactive according to a Markov on- off process. We show that a phase transition also exists in such dynamic networks, and the critical density for this model is the same as the one for static networks under some mild conditions. Furthermore, due to the dynamic behavior of links, a delay is incurred for any transmission even when propagation delay is ignored. We study the behavior of this transmission delay and show that the delay scales linearly with the Euclidean distance between the sender and the receiver when the network is in the subcritical phase, and the delay scales sub-linearly with the distance if the network is in the supercritical phase.},
keywords={electromagnetic wave propagation;Markov processes;percolation;phase transformations;probability;radio links;radio networks;large-scale wireless network;unreliable network links;percolation-based perspective;phase transition;Markov process;electromagnetic propagation delay;Large-scale systems;Wireless networks;Propagation delay;Wireless sensor networks;Fading;Communications Society;USA Councils;Peer to peer computing;Delay lines;Euclidean distance},
doi={10.1109/INFOCOM.2008.11},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509607,
author={Y. Xu and W. Wang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={The Speed of Information Propagation in Large Wireless Networks},
year={2008},
volume={},
number={},
pages={16-20},
abstract={This paper investigates the speed limit of information propagation in large wireless networks, which provides fundamental understanding of the fastest information transportation and delivery that a wireless network is able to accommodate. We show that there is a unified speed upper bound for broadcast and unicast communications in large wireless networks. When network connectivity and successful packet delivery are considered, this speed upper bound is a function of node density. As this bound is unreachable with finite node density, we also quantify the gap between the actually achieved speed and the desired upper bound, which converges to zero exponentially as the node density increases to infinity.},
keywords={radio networks;information propagation;large wireless networks;information transportation;broadcast communication;unicast communication;packet delivery;Wireless networks;Upper bound;Throughput;Transportation;Delay;Working environment noise;Peer to peer computing;Broadcasting;Unicast;Relays},
doi={10.1109/INFOCOM.2008.12},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509608,
author={J. Cao and A. Chen and T. Bu},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Quasi-Likelihood Approach for Accurate Traffic Matrix Estimation in a High Speed Network},
year={2008},
volume={},
number={},
pages={21-25},
abstract={Knowing the traffic matrix, i.e., packet/byte counts between pairs of nodes in a network, is important for network management. The main challenges for accurate traffic matrix estimation in a high speed network are the computation and memory limitations. In this paper, we propose a novel algorithm for traffic matrix estimation that can yield accurate estimates whereas uses small memory and per packet update overhead. Our algorithm constructs a compact probabilistic traffic digest at each network node, and derives a Quasi Maximum Likelihood Estimate (Quasi-MLE) of the traffic matrix by correlating the traffic digests received at a central location. Our new approach is highly efficient, requiring no prior knowledge of the exact packet size distributions. We derive accurate approximation of the relative error distribution of our estimate. For an origin- destination (OD) pair (o,d), we show that by using an array of size M for each traffic digest at o and d, the relative estimation standard error is O(M<sup>-1/2</sup>(sigma<sub>o</sub> <sub>+</sub>sigma<sub>d</sub>)<sup>1/2</sup>), where sigma<sub>o</sub>,sigma<sub>d</sub> are the noise-to-signal ratios, defined as the ratios of non-OD packet/byte counts to OD packet/byte counts at the origin and destination. This is superior to the state-of-the-art algorithms, especially for large sigma<sub>o</sub> and sigma<sub>d</sub>, where the estimation is more challenging. We further demonstrate the effectiveness of our approach using both model and real Internet trace-driven simulations.},
keywords={approximation theory;computational complexity;matrix algebra;maximum likelihood estimation;probability;telecommunication network management;telecommunication traffic;probabilistic traffic matrix estimation;quasi maximum likelihood estimation;high speed network management;approximation theory;Telecommunication traffic;High-speed networks;Yield estimation;Computer network management;Computer networks;Maximum likelihood estimation;Estimation error;Signal to noise ratio;State estimation;Internet},
doi={10.1109/INFOCOM.2008.13},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509609,
author={C. Hu and S. Wang and J. Tian and B. Liu and Y. Cheng and Y. Chen},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Accurate and Efficient Traffic Monitoring Using Adaptive Non-Linear Sampling Method},
year={2008},
volume={},
number={},
pages={26-30},
abstract={Sampling technology has been widely deployed in measurement systems to control memory consumption and processing overhead. However, most of the existing sampling methods suffer from large estimation errors in analyzing small-size flows. To address the problem, we propose a novel adaptive non-linear sampling (ANLS) method for passive measurement. Instead of statically configuring the sampling rate, ANLS dynamically adjusts the sampling rate for a flow depending on the number of packets having been counted. We provide the generic principles guiding the selection of sampling function for sampling rate adjustment. Moreover, we derive the unbiased flow size estimation, the bound of the relative error, and the bound of required counter size for ANLS. The performance of ANLS is thoroughly studied through theoretic analysis and experiments under synthetic/real network data traces, with comparison to several related sampling methods. The results demonstrate that the proposed ANLS can significantly improve the estimation accuracy, particularly for small-size flows, while maintain a memory and processing overhead comparable to existing methods.},
keywords={Internet;measurement systems;sampling methods;telecommunication traffic;traffic monitoring;adaptive nonlinear sampling method;measurement systems;memory consumption;processing overhead;Internet;Sampling methods;Telecommunication traffic;Estimation error;Engineering management;USA Councils;Counting circuits;Random access memory;Remote monitoring;Communications Society;Programmable control},
doi={10.1109/INFOCOM.2008.14},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509610,
author={D. Leonard and D. Loguinov},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Turbo King: Framework for Large-Scale Internet Delay Measurements},
year={2008},
volume={},
number={},
pages={31-35},
abstract={Distance estimation and topological proximity in the Internet have recently emerged as important problems for many distributed applications [1], [10], [11], [19], [29], [31], [40], [41], [44]. Besides deploying tracers and using virtual coordinates, distance is often estimated using end-to-end methods such as King [13] that rely on the existing DNS infrastructure. However, the question of accuracy in such end-to-end estimation and its ability to produce a large-scale map of Internet delays has never been examined. We undertake this task below and show that King suffers from non-negligible error when DNS zones employ geographically diverse authoritative servers or utilize forwarders, both of which are very common in the existing Internet. We also show that King requires insertion of numerous unwanted DNS records in caches of remote servers (which is called cache pollution) and requires large traffic overhead when deployed in large-scale. To overcome these limitations, we propose a new framework we call Turbo King (T-King) that obtains end-to-end delay samples without bias in the presence of distant authoritative servers and forwarders, while consuming half the bandwidth needed by King and reducing the impact of cache pollution by several orders of magnitude. We finish the paper by evaluating Turbo King in several experiments.},
keywords={delay estimation;distance measurement;Internet;network servers;telecommunication network topology;telecommunication traffic;large-scale Internet delay measurement;Turbo King latency estimation framework;distance estimation;network topological proximity;distant authoritative remote server;network traffic overhead;domain name system;Large-scale systems;Internet;Delay estimation;Pollution measurement;Web server;Extraterrestrial measurements;Network servers;Communications Society;Computer science;Application software},
doi={10.1109/INFOCOM.2008.15},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509611,
author={J. Ni and H. Xie and S. Tatikonda and Y. R. Yang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Network Routing Topology Inference from End-to-End Measurements},
year={2008},
volume={},
number={},
pages={36-40},
abstract={Inference of the routing topology and link performance from a node to a set of other nodes is an important component of network monitoring and application design. In this paper we propose a general framework for designing topology inference algorithms based on additive metrics. Our framework allows the integration of both end-to-end packet probing measurements and traceroute type measurements. Based on this framework we design several computationally efficient topology inference algorithms. In particular, we propose a novel sequential topology inference algorithm to address the probing scalability problem and handle dynamic node joining and leaving. We provide sufficient conditions for the correctness of our algorithms and derive lower bounds on the probability of correct topology inference. We conduct Internet experiments to evaluate and demonstrate the effectiveness of our algorithms.},
keywords={monitoring;probability;telecommunication network management;telecommunication network routing;telecommunication network topology;network routing topology inference algorithms;end-to-end packet probing measurements;network monitoring;application design;additive metrics;traceroute type measurements;dynamic node joining;dynamic node leaving;probability;Routing;Network topology;Peer to peer computing;Inference algorithms;Maximum likelihood detection;Tomography;Algorithm design and analysis;Scalability;Internet;Streaming media},
doi={10.1109/INFOCOM.2008.16},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509612,
author={X. Hu and T. Park and K. G. Shin},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Attack-Tolerant Time-Synchronization in Wireless Sensor Networks},
year={2008},
volume={},
number={},
pages={41-45},
abstract={Achieving secure time-synchronization in wireless sensor networks (WSNs) is a challenging, but very important problem that has not yet been addressed effectively. This paper proposes an attack-tolerant time-synchronization protocol (ATSP) in which sensor nodes cooperate to safeguard the time- synchronization service against malicious attacks. ATSP exploits the high temporal correlation existing among adjacent nodes in a WSN to achieve (1) adaptive management of the profile of each sensor's normal behavior, (2) distributed, cooperative detection of falsified clock values advertised by attackers or compromised nodes, and (3) significant improvement of synchronization accuracy and stability by effectively compensating the clock drifts with the calibrated clock. To reduce the risk of losing time-synchronization due to attacks on the reference node, ATSP utilizes distributed, mutual synchronization and confines the impact of attacks to a local area (where attacks took place). Furthermore, by maintaining an accurate profile of sensors' normal synchronization behaviors, ATSP detects various critical attacks while incurring only reasonable communication and computation overheads, making ATSP attack-tolerant and ideal for resource-constrained WSNs.},
keywords={protocols;synchronisation;telecommunication security;wireless sensor networks;wireless sensor network;attack-tolerant time-synchronization protocol;temporal correlation;Wireless sensor networks;Synchronization;Clocks;Protocols;Peer to peer computing;Military computing;Security;Broadcasting;Communications Society;Computer networks},
doi={10.1109/INFOCOM.2008.17},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509613,
author={B. Sheng and Q. Li},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Verifiable Privacy-Preserving Range Query in Two-Tiered Sensor Networks},
year={2008},
volume={},
number={},
pages={46-50},
abstract={We consider a sensor network that is not fully trusted and ask the question how we preserve privacy for the collected data and how we verify the data reply from the network. We explore the problem in the context of a network augmented with storage nodes and target at range query. We use bucketing scheme to mix the data for a range, use message encryption for data integrity, and employ encoding numbers to prevent the storage nodes from dropping data.},
keywords={cryptography;data integrity;data privacy;distributed sensors;telecommunication network management;telecommunication security;verifiable privacy-preserving range query;two-tiered sensor networks;message encryption;storage nodes;range query;bucketing scheme;data integrity;Data privacy;Data security;Secure storage;Peer to peer computing;Encoding;Information security;Base stations;Communications Society;Computer science;Educational institutions},
doi={10.1109/INFOCOM.2008.18},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509614,
author={M. Shao and Y. Yang and S. Zhu and G. Cao},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Towards Statistically Strong Source Anonymity for Sensor Networks},
year={2008},
volume={},
number={},
pages={51-55},
abstract={For sensor networks deployed to monitor and report real events, event source anonymity is an attractive and critical security property, which unfortunately is also very difficult and expensive to achieve. This is not only because adversaries may attack against sensor source privacy through traffic analysis, but also because sensor networks are very limited in resources. As such, a practical tradeoff between security and performance is desirable. In this paper, for the first time we propose the notion of statistically strong source anonymity, under a challenging attack model where a global attacker is able to monitor the traffic in the entire network. We propose a scheme called FitProbRate, which realizes statistically strong source anonymity for sensor networks. We also demonstrate the robustness of our scheme under various statistical tests that might be employed by the attacker to detect real events. Our analysis and simulation results show that our scheme, besides providing source anonymity, can significantly reduce real event reporting latency compared to two baseline schemes.},
keywords={statistical testing;telecommunication network management;telecommunication security;telecommunication traffic;wireless sensor networks;sensor networks;event source anonymity;traffic analysis;sensor source privacy;statistically strong source anonymity;attack model;traffic monitoring;FitProbRate;statistical test;Monitoring;Telecommunication traffic;Traffic control;Privacy;Robustness;Testing;Event detection;Analytical models;Discrete event simulation;Delay},
doi={10.1109/INFOCOM.2008.19},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509615,
author={T. Feng and C. Wang and W. Zhang and L. Ruan},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Confidentiality Protection for Distributed Sensor Data Aggregation},
year={2008},
volume={},
number={},
pages={56-60},
abstract={Efficiency and security are two basic requirements for sensor network design. However, these requirements could be sharply contrary to each other in some scenarios. For example, in- network data aggregation can significantly reduce communication overhead and thus has been adopted widely as a means to improve network efficiency; however, the adoption of in-network data aggregation may prevent data from being encrypted since it is a prerequisite for aggregation that data be accessible during forwarding. In this paper, we address this dilemma by proposing a family of secret perturbation-based schemes that can protect sensor data confidentiality without disrupting additive data aggregation. Extensive simulations are also conducted to evaluate the proposed schemes. The results show that our schemes provide confidentiality protection for both raw and aggregated data items with an overhead lower than that of existing related schemes.},
keywords={telecommunication security;wireless sensor networks;confidentiality protection;distributed sensor data aggregation;network security;secret perturbation-based scheme;wireless sensor network;Protection;Intelligent sensors;Wireless sensor networks;Sensor phenomena and characterization;Data security;Cryptography;Temperature sensors;Communications Society;Computer science;Computer security},
doi={10.1109/INFOCOM.2008.20},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509616,
author={D. Gokhale and S. Sen and K. Chebrolu and B. Raman},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={On the Feasibility of the Link Abstraction in (Rural) Mesh Networks},
year={2008},
volume={},
number={},
pages={61-65},
abstract={Outdoor community mesh networks based on 802.11 have seen tremendous growth in the recent past. The current understanding is that wireless link performance in these settings in inherently unpredictable, due to multipath delay spread. Consequently, researchers have focused on developing intelligent routing techniques to achieve the best possible performance. In this paper, we are specifically interested in mesh networks in rural locations. We first present detailed measurements to show that the PHY layer in these settings is indeed stable and predictable. There is a strong correlation between the error rate and the received signal strength. We show that interference, and not multipath fading, is the primary cause of unpredictable performance. This is in sharp contrast with current widespread knowledge from prior studies. Furthermore, we corroborate our view with a fresh analysis of data presented in these prior studies. Based on our results, we argue that outdoor rural mesh networks can indeed be built with the link abstraction being valid. This has several design implications, and opens up a fresh perspective on a wide range of technical issues in this domain.},
keywords={radio links;telecommunication network routing;telecommunication network topology;wireless LAN;outdoor community mesh networks;802.11 standard;wireless link abstraction;multipath delay;intelligent routing technique;Mesh networks;Error analysis;Delay;Interference;Pollution measurement;Routing;Physical layer;Data analysis;Signal to noise ratio;Communications Society},
doi={10.1109/INFOCOM.2008.21},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509617,
author={R. Alimi and L. Li and R. Ramjee and H. Viswanathan and Y. R. Yang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={iPack: in-Network Packet Mixing for High Throughput Wireless Mesh Networks},
year={2008},
volume={},
number={},
pages={66-70},
abstract={A major barrier for the adoption of wireless mesh networks is severe limits on throughput. Many in-network packet mixing techniques at the network layer [1], [2], [3] as well as the physical layer [4], [5], [6] have been shown to substantially improve throughput. However, the optimal mixing algorithm that maximizes throughput is still unknown. In this paper, we propose iPack, an algorithm for in-network generation of composite packets that integrates coding at two different layers of the protocol stack: XOR-based network coding and physical layer superposition coding. Using extensive simulations, we find that the throughput gain of the joint coding iPack algorithm is 30% more than the better performer of network coding and superposition coding in a wide range of scenarios, and automatically takes advantage of the best available coding opportunities. In a typical wireless mesh network when more traffic is between the clients and access points, the average throughput improvement of iPack, our joint optimization scheduler, can be 324%, while there can be little gain (less than 10%) if network coding alone is used. We also validate our results by implementing iPack on a small-scale testbed based on GNU Radio.},
keywords={encoding;optimisation;packet radio networks;protocols;scheduling;telecommunication traffic;iPack in-network packet mixing;high throughput wireless mesh network;optimal mixing algorithm;XOR-based network coding;protocol stack;physical layer superposition coding;joint coding iPack algorithm;network traffic;joint optimization scheduler;Throughput;Wireless mesh networks;Network coding;Physical layer;Testing;USA Councils;Protocols;Performance gain;Telecommunication traffic;Communications Society},
doi={10.1109/INFOCOM.2008.22},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509618,
author={L. Dai and Y. Xue and B. Chang and Y. Cao and Y. Cui},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Integrating Traffic Estimation and Routing Optimization for Multi-Radio Multi-Channel Wireless Mesh Networks},
year={2008},
volume={},
number={},
pages={71-75},
abstract={Traffic routing plays a critical role in determining the performance of a wireless mesh network. To investigate the best solution, existing work proposes to formulate the mesh network routing problem as an optimization problem. In this problem formulation, traffic demand is usually implicitly assumed as static and known a priori. Contradictorily, recent studies of wireless network traces show that the traffic demand, even being aggregated at access points, is highly dynamic and hard to estimate. Thus, in order to apply the optimization-based routing solution into practice, one must take into account the dynamic and unpredictable nature of wireless traffic demand. This paper presents an integrated framework for network routing in multi-radio multi-channel wireless mesh networks under dynamic traffic demand. This framework consists of two important components: traffic estimation and routing optimization. By analyzing the traces collected at wireless access points, the traffic estimation component predicts future traffic demand based on its historical value using time-series analysis, and represents the prediction result in two forms - mean value and statistical distribution. The optimal mesh network routing strategies then take these two forms of traffic demand estimations as inputs. In particular, two routing algorithms are proposed based on linear programming which consider the mean value and the statistical distribution of the predicted traffic demands, respectively. The trace-driven simulation study demonstrates that our integrated traffic estimation and routing optimization framework can effectively incorporate traffic dynamics in mesh network routing, where both algorithms outperform the shortest path algorithm in about 80% of the test cases.},
keywords={optimisation;radio networks;statistical distributions;telecommunication network routing;telecommunication network topology;telecommunication traffic;time series;wireless channels;traffic demand estimation;traffic routing optimization;multiradio multichannel wireless mesh network;optimization problem;wireless access point;time-series analysis;statistical distribution;mean value;optimal mesh network routing strategy;trace-driven simulation;shortest path algorithm;Telecommunication traffic;Routing;Wireless mesh networks;Traffic control;Mesh networks;Time series analysis;Statistical distributions;IP networks;Wireless communication;Algorithm design and analysis},
doi={10.1109/INFOCOM.2008.23},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509619,
author={J. Camp and V. Mancuso and O. Gurewitz and E. W. Knightly},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Measurement Study of Multiplicative Overhead Effects in Wireless Networks},
year={2008},
volume={},
number={},
pages={76-80},
abstract={In this paper, we perform an extensive measurement study on a multi-tier mesh network serving 4,000 users. Such dense mesh deployments have high levels of interaction across heterogeneous wireless links. We find that this heterogeneous backhaul consisting of data-carrying (forwarding) links <i>and</i> non- data-carrying (non-forwarding) links creates two key effects on performance. First, we show that low-rate management and control packets can produce a disproportionally large degradation in data throughput. We define a metric for this effect called Wireless Overhead Multiplier and use it to quantify the impact of MAC and PHY mechanisms on the the throughput degradation. Surprisingly, we show that these multiplicative effects are primarily driven by the non-forwarding links where, in the worst case, data packets lose physical layer capture to the overhead, yielding disproportionate throughput degradation. Finally, we show that when data flows contend in this worst-case scenario, the loss-based autorate policy is unnecessarily triggered, causing throughput imbalance and poor network utilization.},
keywords={radio links;radio networks;telecommunication network topology;telecommunication traffic;wireless network;multiplicative overhead effect;multitier mesh network;heterogeneous wireless link;low-rate management;PHY mechanism;MAC mechanism;Wireless networks;Throughput;Mesh networks;Topology;Degradation;Physical layer;Communication system traffic control;Routing protocols;Communications Society;Wireless mesh networks},
doi={10.1109/INFOCOM.2008.24},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509620,
author={A. Khreishah and C. -. Wang and N. B. Shroff},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Optimization Based Rate Control for Communication Networks with Inter-Session Network Coding},
year={2008},
volume={},
number={},
pages={81-85},
abstract={In this paper we develop a distributed rate control algorithm for multiple-unicast-sessions when network coding is allowed. Building on our recent flow-based characterization of network coding, we formulate the problem as a convex optimization problem. The formulation exploits pairwise coding possibilities between any pair of sessions, where the objective function is the sum of the utilities based on the rates supported by each session. With some manipulation on the Lagrangian of the formulated problem, a distributed algorithm is developed with no interaction between intermediate nodes, and each source having the freedom to choose its own utility function. The only information required by the source is the weighted sum of the queue length updates of each link, which can be piggy-backed on the acknowledgment messages. In addition to the optimal rate control algorithm, we propose a decentralized pairwise random coding scheme (PRC) that is optimal when a sufficiently large finite field is used for network coding. The convergence of the rate control algorithm is proved analytically and verified by extensive simulations. Simulations also demonstrate the advantage of our algorithm over the state-of-the-art in terms of throughput and fairness.},
keywords={convex programming;distributed algorithms;distributed control;queueing theory;random codes;telecommunication control;optimization based rate control;communication networks;inter-session network coding;distributed rate control algorithm;multiple-unicast-sessions;convex optimization problem;queue length;decentralized PRC scheme;pairwise random coding scheme;Communication system control;Communication networks;Network coding;Distributed control;Lagrangian functions;Distributed algorithms;Optimal control;Partial response channels;Galois fields;Convergence},
doi={10.1109/INFOCOM.2008.25},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509621,
author={A. Lakshmikantha and R. Srikant and C. Beck},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Impact of File Arrivals and Departures on Buffer Sizing in Core Routers},
year={2008},
volume={},
number={},
pages={86-90},
abstract={Traditionally, it had been assumed that the efficiency requirements of TCP dictate that the buffer size at the router must be of the order of the bandwidth (C)-delay (RTT) product. Recently this assumption was questioned in a number of papers and the rule was shown to be conservative for certain traffic models. In particular, by appealing to statistical multiplexing it was shown that on a router with N long-lived connections, buffers of size O(CxRTT)/radic(N) or even O(1) are sufficient. In this paper, we reexamine the buffer size requirements of core routers when flows arrive and depart. Our conclusion is as follows: if the core to access speed ratio is large, then O(1) buffers are sufficient at the core routers; otherwise, larger buffer sizes do improve the flow-level performance of the users. From a modeling point of view, our analysis offers two new insights. First, it may not be appropriate to derive buffer-sizing rules by studying a network with a fixed number of users. In fact, depending upon the core-to-access speed ratio, the buffer size itself may affect the number of flows in the system, so these two parameters (buffer size and number of flows in the system) should not be treated as independent quantities. Second, in the regime where the core-to- access speed ratio is large, we note that the O(1) buffer sizes are sufficient for good performance and that no loss of utilization results, as previously believed.},
keywords={buffer storage;multiplexing;statistical analysis;telecommunication network routing;telecommunication traffic;transport protocols;TCP;buffer sizing;core router;network traffic;statistical multiplexing;Traffic control;Communications Society;Computer industry;Electrical products industry;Systems engineering and theory;Bandwidth;Performance loss;Throughput;Smoothing methods;Measurement},
doi={10.1109/INFOCOM.2008.26},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509622,
author={A. Tang and L. L. H. Andrew and K. Jacobsson and K. H. Johansson and S. H. Low and H. Hjalmarsson},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Window Flow Control: Macroscopic Properties from Microscopic Factors},
year={2008},
volume={},
number={},
pages={91-95},
abstract={This paper studies window flow control focusing on bridging the gap between microscopic factors such as burstiness in sub-RTT timescales, and observable macroscopic properties such as steady state bandwidth sharing and flow level stability. Using new models, we analytically capture notable effects of microscopic behavior on macroscopic quantities. For loss-based protocols, we calculate the loss synchronization rate for different flows and use it to quantitatively explain the unfair bandwidth sharing between paced and unpaced TCP flows. For delay-based protocols, we show that the ratios of round trip delays are critical to the stability of the system. These results deepen the fundamental understanding of congestion control systems. Packet level simulations are used to verify our theoretical claims.},
keywords={delays;stability;synchronisation;telecommunication congestion control;transport protocols;window flow control;macroscopic properties;microscopic factors;sub-RTT timescales;steady state bandwidth sharing;flow level stability;loss-based protocols;loss synchronization rate;TCP flows;delay-based protocols;round trip delays;congestion control systems;Microscopy;Protocols;Stability;Delay;Internet;Steady-state;USA Councils;Bandwidth;Network topology;Channel allocation},
doi={10.1109/INFOCOM.2008.27},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509623,
author={I. A. Qazi and T. Znati},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={On the Design of Load Factor based Congestion Control Protocols for Next-Generation Networks},
year={2008},
volume={},
number={},
pages={96-100},
abstract={Load factor based congestion control schemes have shown to enhance network performance, in terms of utilization, packet loss and delay. In these schemes, using more accurate representation of network load levels is likely to lead to a more efficient way of communicating congestion information to hosts. Increasing the amount of congestion information, however, may end up adversely affecting the performance of the network. This paper focuses on this trade-off and addresses two important and challenging questions: (i) How many congestion levels should be represented by the feedback signal to provide near-optimal performance? and (ii) What window adjustment policies must be in place to ensure robustness in the face of congestion and achieve efficient and fair bandwidth allocations in high bandwidth-delay product (BDP) networks, while keeping low queues and negligible packet drop rates? Based on theoretical analysis and simulations, our results show that 3-bit feedback is sufficient for achieving near-optimal rate convergence to an efficient bandwidth allocation. While the performance gap between 2-bit and 3-bit schemes is large, gains follow the law of diminishing returns when more than 3 bits are used. Further, we show that using multiple levels for the multiplicative decrease policy enables the protocol to adjust its rate of convergence to fairness, rate variations and responsiveness to congestion based on the degree of congestion at the bottleneck. Based on these fundamental insights, we design multi-level feedback congestion control protocol (MLCP). In addition to being efficient, MLCP converges to a fair bandwidth allocation in the presence of diverse RTT flows while maintaining near-zero packet drop rate and low persistent queue length. These features coupled with MLCP's smooth rate variations make it a viable choice for many real-time applications. Using extensive packet- level simulations we show that the protocol is stable across a diverse range of network scenarios. A fluid model for the protocol shows that MLCP remains globally stable for the case of a single bottleneck link shared by identical round-trip time flows.},
keywords={computer networks;feedback;protocols;telecommunication congestion control;transport protocols;load factor based congestion control protocols;next-generation networks;congestion information;feedback signal;bandwidth-delay product networks;near-zero packet drop rate;low persistent queue length;multi-level feedback congestion control protocol;Protocols;Next generation networking;Feedback;Channel allocation;Convergence;Performance loss;Robustness;Analytical models;Performance gain;Delay},
doi={10.1109/INFOCOM.2008.28},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509624,
author={S. Kumar and J. Turner and P. Crowley},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Peacock Hashing: Deterministic and Updatable Hashing for High Performance Networking},
year={2008},
volume={},
number={},
pages={101-105},
abstract={Hash tables are extensively used in networking to implement data-structures that associate a set of keys to a set of values, as they provide O(1), query, insert and delete operations. However, at moderate or high loads collisions are quite frequent which not only increases the access time, but also induces non- determinism in the performance. Due to this non-determinism, the performance of these hash tables degrades sharply in the multi-threaded network processor based environments, where a collection of threads perform the hashing operations in a loosely synchronized manner. In such systems, it is critical to keep the hash operations more deterministic. A recent series of papers have been proposed, which employs a compact on-chip memory to enable deterministic and fast hash queries. While effective, these schemes require substantial on- chip memory, roughly 10-bits for every entry in the hash table. This limits their general usability; specifically in the network processor context, where on-chip resources are scarce. In this paper, we propose a novel hash table construction called <i>Peacock</i> <i>hash</i>, which reduces the on-chip memory by more than 10-folds while keeping a high degree of determinism in performance. This significantly reduced on-chip memory not only makes Peacock hashing much more appealing for the general use but also makes it an attractive choice for the implementation of a hash hardware accelerator on a network processor.},
keywords={data structures;IP networks;multiprocessor interconnection networks;multi-threading;network-on-chip;Peacock hash;deterministic hashing;updatable hashing;high performance networking;data-structures;multithreaded network processor based environments;on-chip memory;hash table construction;IP network;Yarn;Network-on-a-chip;Filters;Degradation;Costs;System performance;Communications Society;Computer science;Data engineering;Usability},
doi={10.1109/INFOCOM.2008.29},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509625,
author={A. Kirsch and M. Mitzenmacher},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={The Power of One Move: Hashing Schemes for Hardware},
year={2008},
volume={},
number={},
pages={106-110},
abstract={In a standard multiple choice hashing scheme, each item is stored in one of d ges 2 hash table buckets. The availability of choice in where items are stored improves space utilization. These schemes are often very amenable to a hardware implementation, such as in a router. Recently, however, researchers have discovered powerful variants where items already in the hash table may be moved during the insertion of a new item. Unfortunately, these schemes occasionally require a large number of items to be moved during an insertion operation, making them inappropriate for a hardware implementation. We show that it is possible to significantly increase the space utilization of a multiple choice hashing scheme by allowing at most one item to be moved during an insertion. Furthermore, our schemes can be effectively analyzed, optimized, and compared using numerical methods based on fluid limit arguments, without resorting to much slower simulations.},
keywords={file organisation;numerical analysis;hash table buckets;choice availability;space utilization;hardware implementation;multiple choice hashing scheme;fluid limit arguments;Hardware;Design optimization;Communications Society;Power engineering and energy;Performance analysis;Bridges;Optimization methods;Analytical models;Peer to peer computing;Load management},
doi={10.1109/INFOCOM.2008.30},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509626,
author={A. X. Liu and C. R. Meiners and Y. Zhou},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={All-Match Based Complete Redundancy Removal for Packet Classifiers in TCAMs},
year={2008},
volume={},
number={},
pages={111-115},
abstract={Packet classification is the core mechanism that enables many networking services on the Internet such as firewall packet filtering and traffic accounting. Using Ternary Content Addressable Memories (TCAMs) to perform high-speed packet classification has become the de facto standard in industry. TCAMs classify packets in constant time by comparing a packet with all classification rules of ternary encoding in parallel. Despite their high speed, TCAMs suffer from the well-known interval expansion problem. As packet classification rules usually have fields specified as intervals, converting such rules to TCAM- compatible rules may result in an explosive increase in the number of rules. This is not a problem if TCAMs have large capacities. Unfortunately, TCAMs have very limited capacity, and more rules means more power consumption and more heat generation for TCAMs. Even worse, the number of rules in packet classifiers have been increasing rapidly with the growing number of services deployed on the internet. The interval expansion problem of TCAMs can be addressed by removing redundant rules in packet classifiers. This equivalent transformation can significantly reduce the number of TCAM entries needed by a packet classifier. Our experiments on real- life packet classifiers show an average reduction of 58.2% in the number of TCAM entries by removing redundant rules. In this paper, we propose an all-match based complete redundancy removal algorithm. This is the first algorithm that attempts to solve first-match problems from an all-match perspective. We formally prove that our redundancy removal algorithm guarantees no redundant rules in resulting packet classifiers. We conducted extensive experiments on both real-life and synthetic packet classifiers. These experimental results show that our redundancy removal algorithm is both effective in terms of reducing TCAM entries and efficient in terms of running time.},
keywords={content-addressable storage;Internet;redundancy;ternary codes;trees (mathematics);packet classification;Internet;firewall packet filtering;traffic accounting;ternary content addressable memory;ternary encoding;all-match tree based complete redundancy removal algorithm;Web and internet services;IP networks;Information filtering;Information filters;Telecommunication traffic;Associative memory;Encoding;Explosives;Energy consumption;Power generation},
doi={10.1109/INFOCOM.2008.31},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509627,
author={M. Andrews and L. Zhang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Satisfying Arbitrary Delay Requirements in Multihop Networks},
year={2008},
volume={},
number={},
pages={116-120},
abstract={We consider the problem of scheduling in a multi- hop packet network so as to satisfy a set of arbitrary end-to-end delay requirements. A number of scheduling protocols are known for which end-to-end delay bounds can be derived. However, it is often the case that these end-to-end delay bounds are large for flows with small rate. This is clearly inappropriate for traffic types such as VoIP for which tight delay bounds are required but session rates are typically small. In this paper we study the problem of satisfying an arbitrary set of end-to-delay requirements. For a single server in isolation, precise conditions on whether or not a set of delay requirements can be satisfied are known. In contrast, for sessions in a multihop network, we show that deciding whether or not a set of delay requirements can be met is NP-hard. However, if the delay requirements satisfy a simple set of per-server and per- session conditions necessary for schedulability, we show that our proposed protocol can meet all the requirements up to some logarithmic factor. On the negative side, we construct examples in which some delay bound must be violated by a logarithmic factor even if the necessary conditions hold. We further demonstrate through simulation the advantage of our protocol against a counterpart that does not take delay requirements into consideration. We conclude the paper by extending our results to the problem of satisfying arbitrary delay bounds in an input-queued switch.},
keywords={optimisation;protocols;queueing theory;scheduling;multihop packet network;arbitrary end-to-end delay requirement;scheduling protocol;VOIP;NP-hard problem;logarithmic factor;input-queued switch;Spread spectrum communication;Delay;Protocols;Network servers;Traffic control;Telecommunication traffic;Switches;Communication networks;Communications Society;Communication switching},
doi={10.1109/INFOCOM.2008.32},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509628,
author={V. Sharma and S. Kalyanaraman and K. Kar and K. K. Ramakrishnan and V. Subramanian},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={MPLOT: A Transport Protocol Exploiting Multipath Diversity Using Erasure Codes},
year={2008},
volume={},
number={},
pages={121-125},
abstract={In this paper, we propose a novel transport protocol that effectively utilizes available bandwidth and diversity gains provided by heterogeneous, highly lossy paths. Our Multi-Path LOss-Tolerant (MPLOT) protocol can be used to provide significant gains in the goodput of wireless mesh networks, subject to bursty, correlated losses with average loss-rates as high as 50%, and random outage events. MPLOT makes intelligent use of erasure codes to guard against packets losses, and a Hybrid-ARQ/FEC scheme to reduce packet recovery latency, where the redundancy is adaptively provisioned into both proactive and reactive FECs. MPLOT uses dynamic packet mapping based on current path characteristics, and does not require packets to be delivered in sequence to ensure reliability. We present a theoretical analysis of the different design choices of MPLOT and show that MPLOT makes an optimal trade-off between goodput and delay constraints. We test MPLOT, through simulations, under a variety of test scenarios and show that it effectively exploits path diversity in addition to aggregating path bandwidths. We also show that MPLOT is fair to single-path protocols like TCP-SACK.},
keywords={bandwidth allocation;codes;diversity reception;radio networks;transport protocols;transport protocol;multipath diversity;erasure codes;bandwidth utilization;multipath loss-tolerant protocol;wireless mesh network;dynamic packet mapping;Transport protocols;Wireless mesh networks;Delay;Aggregates;Forward error correction;Bandwidth;Counting circuits;Diversity methods;Testing;Wireless networks},
doi={10.1109/INFOCOM.2008.33},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509629,
author={H. X. Nguyen and D. R. Figueiredo and M. Grossglauser and P. Thiran},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Balanced Relay Allocation on Heterogeneous Unstructured Overlays},
year={2008},
volume={},
number={},
pages={126-130},
abstract={Due to the increased usage of NAT boxes and firewalls, it has become harder for applications to establish direct connections seamlessly among two end-hosts. A recently adopted proposal to mitigate this problem is to use relay nodes, end-hosts that act as intermediary points to bridge connections. Efficiently selecting a relay node is not a trivial problem, specially in a large-scale unstructured overlay system where end-hosts are heterogeneous. In such environment, heterogeneity among the relay nodes comes from the inherent differences in their capacities and from the way overlay networks are constructed. Despite this fact, good relay selection algorithms should effectively balance the aggregate load across the set of relay nodes. In this paper, we address this problem using algorithms based on the two random choices method. We first prove that the classic load-based algorithm can effectively balance the load even when relays are heterogeneous, and that its performance depends directly on relay heterogeneity. Second, we propose an utilization-based random choice algorithm to distribute load in order to balance relay utilization. Numerical evaluations through simulations illustrate the effectiveness of this algorithm, indicating that it might also yield provable performance (which we conjecture). Finally, we support our theoretical findings through simulations of various large-scale scenarios, with realistic relay heterogeneity.},
keywords={computer networks;probability;resource allocation;trees (mathematics);balanced relay allocation;heterogeneous unstructured overlay networks;relay selection algorithms;load-based algorithm;utilization-based random choice algorithm;witness tree;probability;Relays;Peer to peer computing;Large-scale systems;Network address translation;Proposals;Numerical simulation;Internet;Application software;Communications Society;Bridges},
doi={10.1109/INFOCOM.2008.34},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509630,
author={F. Baccelli and G. Carofiglio and S. Foss},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Proxy Caching in Split TCP: Dynamics, Stability and Tail Asymptotics},
year={2008},
volume={},
number={},
pages={131-135},
abstract={The split of a multihop, point to point TCP connection consists in replacing a plain, end-to-end TCP connection by a cascade of TCP connections. In such a cascade, connection n feeds connection n+1 through some proxy node n. This technique is used in a variety of contexts. In overlay networks, proxies are often peers of the underlying peer to peer network. Split TCP is also already proposed and largely adopted in wireless networks at the wired/wireless interface to separate links with vastly different characteristics. In order to avoid losses in the proxies, a backpressure mechanism is often used in this context. In this paper we develop a model for such a split TCP connection aimed at the analysis of the throughput dynamics on both links and that of the buffer occupancy in the proxy for long file transfers. The two main variants of Split TCP are considered: that with backpressure and that without. The study consists of two parts: the first part is purely experimental and is based on ns2 simulations. It allows us to identify complex interaction phenomena between TCP flow rates and proxy buffer occupancy, which seem to have been ignored by previous work on Split TCP. The second part of the paper is of mathematical nature. We establish the basic equations that govern the evolution of such a cascade and prove some of the experimental observations made in the first part. In particular, we give the conditions for system stability and we show the possibility of heavy tail asymptotics for proxy buffer occupancy and delays in the stationary regime.},
keywords={cache storage;peer-to-peer computing;transport protocols;proxy caching;split TCP connection;multihop point to point TCP connection;overlay networks;peer to peer network;file transfers;backpressure mechanism;tail asymptotics;stability;Asymptotic stability;Tail;Throughput;Wireless networks;Peer to peer computing;Degradation;Propagation delay;Communications Society;Context;Buffer overflow},
doi={10.1109/INFOCOM.2008.35},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509631,
author={W. de Bruijn and H. Bos},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Beltway Buffers: Avoiding the OS Traffic Jam},
year={2008},
volume={},
number={},
pages={136-140},
abstract={Beltway buffers are operating system I/O paths optimised for high-throughput network applications. The key architectural feature of Beltway buffers is that all I/O takes place in long-lived, allocation-free, shared ringbuffers. Advantages of this design are (1) improved throughput through system-wide copy, context-switch and allocation avoidance and judicious use of the data cache, (2) transparent integration of peripheral hardware and (3) simplicity and familiarity due to comprehensive use of the POSIX file interface for accessing streams.},
keywords={application program interfaces;cache storage;network operating systems;Beltway buffer;operating system I/O path;OS traffic jam;high-throughput network application;data cache;peripheral hardware;POSIX file interface;ring buffer;Operating systems;Splicing;Telecommunication traffic;Hardware;Sockets;Kernel;Costs;Network servers;Random access memory;Linux},
doi={10.1109/INFOCOM.2008.36},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509632,
author={G. Smaragdakis and N. Laoutaris and P. Michiardi and A. Bestavros and J. W. Byers and M. Roussopoulos},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Swarming on Optimized Graphs for n-Way Broadcast},
year={2008},
volume={},
number={},
pages={141-145},
abstract={In an n-way broadcast application each one of n overlay nodes wants to push its own distinct large data file to all other n-1 destinations as well as download their respective data files. BitTorrent-like swarming protocols are ideal choices for handling such massive data volume transfers. The original BitTorrent targets one-to-many broadcasts of a single file to a very large number of receivers and thus, by necessity, employs an almost random overlay topology, n-way broadcast applications on the other hand, owing to their inherent n-squared nature, are realizable only in small to medium scale networks. In this paper, we show that we can leverage this scale constraint to construct optimized overlay topologies that take into consideration the end-to-end characteristics of the network and as a consequence deliver far superior performance compared to random and myopic (local) approaches. We present the Max-Min and Max- Sum peer-selection policies used by individual nodes to select their neighbors. The first one strives to maximize the available bandwidth to the slowest destination, while the second maximizes the aggregate output rate. We design a swarming protocol suitable for n-way broadcast and operate it on top of overlay graphs formed by nodes that employ Max-Min or Max-Sum policies. Using trace-driven simulation and measurements from a PlanetLab prototype implementation, we demonstrate that the performance of swarming on top of our constructed topologies is far superior to the performance of random and myopic overlays. Moreover, we show how to modify our swarming protocol to allow it to accommodate selfish nodes.},
keywords={peer-to-peer computing;protocols;telecommunication network topology;optimized graphs;n-way broadcast;n-1 destinations;BitTorrent-like swarming protocols;massive data volume transfers;max-min peer-selection policies;max-sum peer-selection policies;Broadcasting;Peer to peer computing;Network topology;Protocols;Inductors;Communications Society;Constraint optimization;Bandwidth;Aggregates;Extraterrestrial measurements},
doi={10.1109/INFOCOM.2008.37},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509633,
author={Z. Yao and D. Loguinov},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Link Lifetimes and Randomized Neighbor Selection in DHTs},
year={2008},
volume={},
number={},
pages={146-150},
abstract={Several models of user churn, resilience, and link lifetime have recently appeared in the literature [12], [13], [34], [35]; however, these results do not directly apply to classical distributed hash tables (DHTs) in which neighbor replacement occurs not only when current users die, but also when new user arrive into the system, and where replacement choices are often restricted to the successor of the failed zone in the DHT space. To understand neighbor churn in such networks, this paper proposes a simple, yet accurate, model for capturing link dynamics in structured P2P systems and obtains the distribution of link lifetimes for fairly generic DHTs. Similar to [8], our results show that deterministic networks (e.g., Chord [28], CAN [24]) unfortunately do not extract much benefit from heavy-tailed user lifetimes since link durations are dominated by small remaining lifetimes of newly arriving users that replace the more reliable existing neighbors. We also examine link lifetimes in randomized DHTs equipped with multiple choices for each link and show that users in such systems should prefer neighbors with smaller zones rather than larger age as suggested in prior work [13], [30]. We finish the paper by demonstrating the effectiveness of the proposed min-zone neighbor selection for heavy-tailed user lifetime distributions with the shape parameter alpha obtained from recent measurements [4], [31].},
keywords={file organisation;peer-to-peer computing;randomized neighbor selection;distributed hash table;deterministic network;min-zone neighbor selection;heavy-tailed user lifetime distribution;peer-to-peer network;Switches;Resilience;Peer to peer computing;Shape measurement;Delay;Routing;Communications Society;Computer science;USA Councils;Space stations},
doi={10.1109/INFOCOM.2008.38},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509634,
author={X. Zheng and C. Cho and Y. Xia},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Optimal Peer-to-Peer Technique for Massive Content Distribution},
year={2008},
volume={},
number={},
pages={151-155},
abstract={A distinct trend has emerged that the Internet is used to transport data on a more and more massive scale. Capacity shortage in the backbone networks has become a genuine possibility, which will be more serious with fiber-based access. The problem addressed in this paper is how to conduct massive content distribution efficiently in the future network environment where the capacity limitation can equally be at the core or the edge. We propose a novel peer-to-peer technique as a main content transport mechanism to achieve efficient network resource utilization. The technique uses multiple trees for distributing different file pieces, which at the heart is a version of swarming. In this paper, we formulate an optimization problem for determining an optimal set of distribution trees as well as the rate of distribution on each tree under bandwidth limitation at arbitrary places in the network. The optimal solution can be found by a distributed algorithm. The results of the paper not only provide stand-alone solutions to the massive content distribution problem, but should also help the understanding of existing distribution techniques such as BitTorrent or FastReplica.},
keywords={content management;distributed algorithms;Internet;optimisation;peer-to-peer computing;resource allocation;trees (mathematics);optimal peer-to-peer technique;massive content distribution;Internet;capacity shortage;backbone network;network resource utilization;multiple trees;optimization problem;distributed algorithm;Peer to peer computing;Spine;Telecommunication traffic;Distributed computing;Internet;Resource management;Bandwidth;Distributed algorithms;Optical fiber subscriber loops;TV},
doi={10.1109/INFOCOM.2008.39},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509635,
author={H. Yan and U. Irmak and T. Suel},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Algorithms for Low-Latency Remote File Synchronization},
year={2008},
volume={},
number={},
pages={156-160},
abstract={The remote file synchronization problem is how to update an outdated version of a file located on one machine to the current version located on another machine with a minimal amount of network communication. It arises in many scenarios including Web site mirroring, file system backup and replication, or web access over slow links. A widely used open-source tool called rsync uses a single round of messages to solve this problem (plus an initial round for exchanging meta information). While research has shown that significant additional savings in bandwidth are possible by using multiple rounds, such approaches are often not desirable due to network latencies, increased protocol complexity, and higher I/O and CPU overheads at the endpoints. In this paper, we study single-round synchronization techniques that achieve savings in bandwidth consumption while preserving many of the advantages of the rsync approach. In particular, we propose a new and simple algorithm for file synchronization based on set reconciliation techniques. We then show how to integrate sampling techniques into our approach in order to adaptively select the most suitable algorithm and parameter setting for a given data set. Experimental results on several data sets show that the resulting protocol gives significant benefits over rsync, particularly on data sets with high degrees of redundancy between the versions.},
keywords={client-server systems;peer-to-peer computing;sampling methods;synchronisation;low-latency remote file synchronization problem;network communication;Web site mirroring;file system backup;file system replication;rsync open-source tool;single-round synchronization technique;set reconciliation technique;client-server system;file sharing;sampling technique;Bandwidth;Costs;Access protocols;Computational Intelligence Society;File systems;Open source software;Delay;Sampling methods;Communications Society;Educational institutions},
doi={10.1109/INFOCOM.2008.40},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509636,
author={P. Tague and D. Slater and J. Rogers and R. Poovendran},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Vulnerability of Network Traffic under Node Capture Attacks Using Circuit Theoretic Analysis},
year={2008},
volume={},
number={},
pages={161-165},
abstract={We investigate the impact of node capture attacks on the confidentiality and integrity of network traffic. We map the compromise of network traffic to the flow of current through an electric circuit and propose a metric for quantifying the vulnerability of the traffic using the circuit mapping. We compute the vulnerability metric as a function of the routing and the cryptographic protocols used to secure the network traffic. We formulate the minimum cost node capture attack problem as a nonlinear integer programming problem. Due to the NP-hardness of the minimization problem, we provide a greedy heuristic that approximates the minimum cost attack. We provide examples of node capture attacks using our vulnerability metric and show that the adversary can expend significantly less resources to compromise target traffic by exploiting information leakage from the routing and cryptographic protocols.},
keywords={approximation theory;computational complexity;cryptographic protocols;integer programming;networks (circuits);nonlinear programming;radio networks;routing protocols;telecommunication security;telecommunication traffic;wireless network traffic vulnerability;electric circuit theory analysis;node capture attack;cryptographic protocol;routing protocol;nonlinear integer programming problem;NP-hard problem;minimization problem;greedy heuristic solution;approximation theory;Telecommunication traffic;Circuit analysis;Peer to peer computing;Communication system security;Cryptographic protocols;Costs;US Government;Wireless sensor networks;Routing protocols;Information analysis},
doi={10.1109/INFOCOM.2008.41},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509637,
author={T. Song and W. Zhang and D. Wang and Y. Xue},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Memory Efficient Multiple Pattern Matching Architecture for Network Security},
year={2008},
volume={},
number={},
pages={166-170},
abstract={Pattern matching is one of the most important components for the content inspection based applications of network security, and it requires well designed algorithms and architectures to keep up with the increasing network speed. For most of the solutions, AC and its derivative algorithms are widely used. They are based on the DFA model but utilize large amount of memory because of so many transition rules. An algorithm, called ACC, is presented in this paper for multiple pattern matching. It uses a novel model, namely cached deterministic finite automate (CDFA). In ACC, by using CDFA, only 4.1% transition rules for ClamAV (20.8% for Snort) are needed to represent the same function using DFA built by AC. This paper also proposes a new scheme named next-state addressing (NSA) to store and access transition rules of DFA in memory. Using this method, transition rules can be efficiently stored and directly accessed. Finally the architecture for multiple pattern matching is optimized by several approaches. Experiments show our architecture can achieve matching speed faster than 10 Gbps with very efficient memory utilization, i.e., 81KB memory for 1.8 K Snort rules with total 29 K characters, and 9.5 MB memory for 50 K ClamAV rules with total 4.44 M characters. A single architecture is memory efficient for large pattern set, and it is possible to support more than 10 M patterns with at most half amount of the memory utilization compared to the state-of-the-art architectures.},
keywords={cache storage;computer networks;deterministic automata;finite automata;pattern matching;telecommunication security;memory efficient multiple pattern matching architecture;computer network security;content inspection based application;cached deterministic finite automate;ACC algorithm;next-state addressing scheme;Pattern matching;Doped fiber amplifiers;Inspection;Intrusion detection;Computer architecture;Computer security;Communications Society;Computer science;Application software;Algorithm design and analysis},
doi={10.1109/INFOCOM.2008.42},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509638,
author={S. Khattab and S. Gobriel and R. Melhem and D. Mosse},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Live Baiting for Service-Level DoS Attackers},
year={2008},
volume={},
number={},
pages={171-175},
abstract={Denial-of-service (DoS) attacks remain a challenging problem in the Internet. By making resources unavailable to intended legitimate clients, DoS attacks have resulted in significant loss of time and money for many organizations, thus, many DoS defense mechanisms have been proposed. In this paper we propose live baiting, a novel approach for detecting the identities of DoS attackers. Live baiting leverages group-testing theory, which aims at discovering defective members in a population using the minimum number of dasiadasiatestspsilapsila. This leverage allows live baiting to detect attackers using low state overhead without requiring models of legitimate requests nor anomalous behavior. The amount of state needed by live baiting is in the order of number of attackers not number of clients. This saving allows live baiting to scale to large services with millions of clients. We analyzed the coverage, effectiveness (detection time, false positive and false negative probabilities), and efficiency (memory, message overhead, and computational complexity) of our approach. We validated our analysis using NS-2 simulations modeled after real Web traces.},
keywords={Internet;telecommunication security;denial-of-service attack;Internet;live baiting;group-testing theory;Computer crime;Testing;Network servers;Communications Society;Computer science;Web and internet services;Computational complexity;Computational modeling;Analytical models;Computer security},
doi={10.1109/INFOCOM.2008.43},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509639,
author={A. X. Liu and E. Torng and C. R. Meiners},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Firewall Compressor: An Algorithm for Minimizing Firewall Policies},
year={2008},
volume={},
number={},
pages={176-180},
abstract={A firewall is a security guard placed between a private network and the outside Internet that monitors all incoming and outgoing packets. The function of a firewall is to examine every packet and decide whether to accept or discard it based upon the firewall's policy. This policy is specified as a sequence of (possibly conflicting) rules. When a packet comes to a firewall, the firewall searches for the first rule that the packet matches, and executes the decision of that rule. With the explosive growth of Internet-based applications and malicious attacks, the number of rules in firewalls have been increasing rapidly, which consequently degrades network performance and throughput. In this paper, we propose Firewall Compressor, a framework that can significantly reduce the number of rules in a firewall while keeping the semantics of the firewall unchanged. We make three major contributions in this paper. First, we propose an optimal solution using dynamic programming techniques for compressing one-dimensional firewalls. Second, we present a systematic approach to compressing multi-dimensional firewalls. Last, we conducted extensive experiments to evaluate Firewall Compressor. In terms of effectiveness, Firewall Compressor achieves an average compression ratio of 52.3% on real- life rule sets. In terms of efficiency, Firewall Compressor runs in seconds even for a large firewall with thousands of rules. Moreover, the algorithms and techniques proposed in this paper are not limited to firewalls. Rather, they can be applied to other rule-based systems such as packet filters on Internet routers.},
keywords={authorisation;data compression;dynamic programming;Internet;telecommunication security;firewall compressor;private network;Internet;dynamic programming;IP networks;Communications Society;Computer science;Computer security;Computer displays;Explosives;Degradation;Throughput;Dynamic programming;Knowledge based systems},
doi={10.1109/INFOCOM.2008.44},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509640,
author={S. S. Ahuja and S. Ramasubramanian and M. Krunz},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={SRLG Failure Localization in All-Optical Networks Using Monitoring Cycles and Paths},
year={2008},
volume={},
number={},
pages={700-708},
abstract={We introduce the concepts of monitoring paths (MPs) and monitoring cycles (MCs) for unique localization of shared risk linked group (SRLG) failures in all-optical networks. An SRLG failure is a failure of multiple links due to a failure of a common resource. MCs (MPs) start and end at same (distinct) monitoring location(s). They are constructed such that any SRLG failure results in the failure of a unique combination of paths and cycles. We derive necessary and sufficient conditions on the set of MCs and MPs needed for localizing an SRLG failure in an arbitrary graph. When a single monitoring location is employed, we show that a network must be (k + 2)-edge connected for localizing all SRLG failures with up to k links. For networks that are less than (k + 2)-edge connected, we derive necessary and sufficient condition on the placement of monitoring locations for unique localization of any SRLG failure of up to k links. We use these conditions to develop an algorithm for the placement of monitoring locations. We show a graph transformation technique that converts the problem of identifying MCs and MPs with multiple monitoring locations to a problem of identifying MCs with single monitoring location. We provide an integer linear program and a heuristic to identify MCs for networks with one monitoring location. Through extensive simulations, we demonstrate the effectiveness of the proposed monitoring technique.},
keywords={graph theory;integer programming;linear programming;monitoring;optical fibre networks;telecommunication network management;telecommunication network reliability;SRLG failure localization;all-optical networks;monitoring paths;monitoring cycles;shared risk linked group failures;arbitrary graph;graph transformation technique;integer linear program;All-optical networks;Condition monitoring;Probes;Optical fiber networks;Fault diagnosis;Sufficient conditions;Wavelength division multiplexing;Optical fiber devices;Communications Society;Optical losses},
doi={10.1109/INFOCOM.2008.45},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509641,
author={D. Xu and E. Anshelevich and M. Chiang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={On Survivable Access Network Design: Complexity and Algorithms},
year={2008},
volume={},
number={},
pages={186-190},
abstract={With economic constraints and limited routing capability, the structure of an access network is typically a "fat tree", where the terminal has to relay the traffic from another terminal of the same or higher level. New graph theory problems naturally arise from such features of access network models, different from those targeted towards survivable backbone (mesh) networks. We model the important problem of provisioning survivability to an existing single-level fat tree through two graph theory problem formulations: the Terminal Backup problem and the simplex cover problem, which we show to be equivalent. We then develop two polynomial-time approaches, indirect and direct, for the simplex cover problem. The indirect approach of solving the matching version of simplex cover is convenient in proving polynomial-time solvability though it is prohibitively slow in practice. In contrast, leveraging the special properties of simplex cover itself, we demonstrate that the direct approach can solve the simplex cover problem very efficiently even for large networks. Extensive numerical results of applying our algorithms are also reported for designing survivable access networks over different types of topologies.},
keywords={graph theory;polynomials;subscriber loops;telecommunication network routing;survivable access network design;fat tree;graph theory;survivable backbone network;terminal backup problem;simplex cover problem;polynomial-time approach;telecommunication network routing;Algorithm design and analysis;Tree graphs;Graph theory;Polynomials;Routing;Relays;Telecommunication traffic;Traffic control;Spine;Network topology},
doi={10.1109/INFOCOM.2008.46},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509642,
author={S. Huang and B. Mukherjee and C. Martel},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Survivable Multipath Provisioning with Differential Delay Constraint in Telecom Mesh Networks},
year={2008},
volume={},
number={},
pages={191-195},
abstract={Multipath provisioning is a key feature of next-generation SONET/SDH networks (which can be used on top of optical WDM) and they can support virtual concatenation (VCAT); thus, multipath provisioning can significantly outperform single-path provisioning in resource efficiency, service resilience, and flexibility. However, in multipath provisioning, differential delay is an important constraint which should not be ignored. We investigate survivability of service paths based on differential-delay constraint (DDC) and multipath provisioning together in a telecom backbone mesh network. We present a DDC-based K link-disjoint paths algorithm (DDCKDP) for multipath provisioning subject to DDC. We also compare it with the minimum-cost-flow (MCF) and K shortest link-disjoint paths (KDP) algorithm, using Shared Protection of the Largest Individual Traversed link (SPLIT), under dynamic service request with several different DDCs. We find that (1) exploiting link-disjoint paths is very efficient for survivable multipath provisioning; and (2) SPLIT-DDCKDP is resource efficient, has low signaling overhead, and has fast fault-recovery for survivable multipath provisioning with DDC. For a 5 ms DDC, DDCKDP can decrease the Bandwidth Blocking Ratio (BBR) by more than 100% compared with KDP in a typical US backbone network.},
keywords={fault tolerance;graph theory;minimisation;optical fibre networks;SONET;synchronous digital hierarchy;telecommunication network reliability;telecommunication network topology;wavelength division multiplexing;survivable multipath provisioning;differential delay constraint;telecom backbone mesh network;next-generation SONET/SDH network;optical WDM;virtual concatenation;K shortest link-disjoint path algorithm;minimum-cost-flow;largest individual traversed link shared protection;fault recovery;bandwidth blocking ratio;Telecommunications;Mesh networks;Spine;Next generation networking;SONET;Synchronous digital hierarchy;Optical fiber networks;Wavelength division multiplexing;Resilience;Delay},
doi={10.1109/INFOCOM.2008.47},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509643,
author={M. Tornatore and D. Lucerna and A. Pattavina},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Improving Efficiency of Backup Reprovisioning in WDM Networks},
year={2008},
volume={},
number={},
pages={196-200},
abstract={Protection techniques for optical networks mainly rely on pre-allocated backup bandwidth, which may not be able to provide full protection guarantee when multiple failures occur in a network. To protect against multiple concurrent potential failures and to utilize the available resources more efficiently, strategies such as backup reprovisioning rearrange backups of protected connections after one failure occurs or, more generally, whenever the network state changes, e.g., when a new request arrives or terminates. Recently, new solutions for automatized management in Optical networks promise to allow customers to specify on-demand the terms of the service level agreement (SLA) to be guaranteed by the service provider. In this paper we show that is possible to further reduce the capacity requirements of backup reprovisioning techniques exploiting the knowledge, among the other service level specifications (SLS), of the connection holding-time. Our main contributions are as follows. First, we prove that the problem of backup reprovisioning for all the lightpaths requiring shared-path protection under a current network state is NP-complete. Second, we provide a mathematical ILP model of the reprovisioning problem considering the additional holding-time information. Third, since the problem is NP-complete and we can not efficiently rely on exact approaches, a new global reprovisioning algorithm called Ph-GBR is proposed which can significantly reduce the resource overbuild exploiting the information about connection durations. By means of simulative experiments, we compare capacity requirement and computational complexity of Ph-GBR to those of another holding-time unaware, yet efficient algorithm, called GBR, considering a dynamic traffic in a wavelength-convertible WDM mesh network scenario.},
keywords={computational complexity;integer programming;linear programming;optical fibre networks;quality of service;wavelength division multiplexing;optical network;pre-allocated backup bandwidth;resource utilization;WDM network;service level agreement;generalized backup reprovisioning technique;service level specification;NP-complete problem;mathematical ILP model;computational complexity;WDM networks;Protection;Optical fiber networks;Bandwidth;Laser sintering;Mathematical model;Computational modeling;Computational complexity;Telecommunication traffic;Traffic control},
doi={10.1109/INFOCOM.2008.48},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509644,
author={L. Grokop and D. N. C. Tse},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Spectrum Sharing Between Wireless Networks},
year={2008},
volume={},
number={},
pages={201-205},
abstract={We consider the problem of two wireless networks operating on the same (presumably unlicensed) frequency band. Pairs within a given network cooperate with one another, but between networks there is competition for spectrum. To make the problem tractable, we assume transmissions are scheduled according to a random access protocol where each network chooses an access probability for its users. In this vein a game between the two networks is defined. We characterize the Nash Equilibrium behavior of the system. Three regimes are identified; a full-spread regime, where both networks choose to simultaneously schedule all transmissions; a partial-spread regime, where one network schedules all transmissions and the other only schedules a fraction; and a joint-spread regime, where both networks schedule a fraction of their transmissions. The regime of operation depends on the pathloss exponent alpha. The joint-spread regime, which has a cooperative flavor, is attainable only for alpha &gt; 4, which suggests that in certain environments there may be a natural incentive for rival wireless networks to cooperate.},
keywords={access protocols;game theory;radio networks;wireless network;random access protocol;Nash equilibrium;spectrum sharing;Wireless networks;Access protocols;Interference;Throughput;Frequency;Radio spectrum management;Communications Society;Computer networks;USA Councils;Veins},
doi={10.1109/INFOCOM.2008.49},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509645,
author={M. Garetto and P. Giaccone and E. Leonardi},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Capacity Scaling of Sparse Mobile Ad Hoc Networks},
year={2008},
volume={},
number={},
pages={206-210},
abstract={We provide the scaling laws for the transport capacity of a wide class of mobile wireless ad hoc networks. Our analysis generalizes previous results obtained under restrictive assumptions on the node mobility process and overall node density over the network area. The broader family of mobile networks that we consider is able to account for many important characteristics usually recognized in real traces of both human and vehicular mobility. In particular, we consider clustered, sparse networks of heterogeneous nodes, in which the shape of the spatial distribution of each node around one or more home-points plays a fundamental role in determining the overall transport capacity. We identify different operational regimes that arise within our general class of mobile networks, and for each regime we propose optimal scheduling and routing strategies achieving the maximum asymptotic capacity.},
keywords={ad hoc networks;graph theory;mobile radio;scheduling;telecommunication network routing;transport capacity scaling laws;sparse mobile ad hoc networks;clustered sparse networks;heterogeneous mobile nodes;operational regimes;optimal scheduling strategies;optimal routing strategies;asymptotic capacity;graph theory;Mobile ad hoc networks;Peer to peer computing;Disruption tolerant networking;Throughput;Delay effects;Communication switching;Character recognition;Humans;Routing;Relays},
doi={10.1109/INFOCOM.2008.50},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509646,
author={Z. Wang and H. R. Sadjadpour and J. J. Garcia-Luna-Aceves},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Unifying Perspective on the Capacity of Wireless Ad Hoc Networks},
year={2008},
volume={},
number={},
pages={211-215},
abstract={We present the first unified modeling framework for the computation of the throughput capacity of random wireless ad hoc networks in which information is disseminated by means of unicast routing, multicast routing, broadcasting, or different forms of anycasting. We introduce (n,m, k)-casting as a generalization of all forms of one-to-one, one-to-many and many-to-many information dissemination in wireless networks. In this context, n, m, and k denote the total number of nodes in the network, the number of destinations for each communication group, and the actual number of communication-group members that receive information (i.e., k lesm), respectively. We compute upper and lower bounds for the (n, m, k)- cast throughput capacity in random wireless networks. When m = k = ominus(1), the resulting capacity equals the well-known capacity result for multi-pair unicasting by Gupta and Kumar. We demonstrate that ominus(1/radic(mnlogn)) bits per second constitutes a tight bound for the capacity of multicasting (i.e., m = k &lt; n) when m les ominus (n/(log n)). We show that the multicast capacity of a wireless network equals its capacity for multi-pair unicasting when the number of destinations per multicast source is not a function of n. We also show that the multicast capacity of a random wireless ad hoc network is ominus (1/n), which is the broadcast capacity of the network, when m ges ominus(n/ log n). Furthermore, we show that ominus (radicm/(kradic(n log n))),ominus(1/(k log n)) and ominus(1/n) bits per second constitutes a tight bound for the throughput capacity of multicasting (i.e., k &lt; m &lt; n) when ominus(1) les m les ominus (n/ log n), k les ominus(n / log n) les m les n and ominus (n/ log n) les k les m les n respectively.},
keywords={ad hoc networks;communication complexity;telecommunication network routing;wireless ad hoc networks;unicast routing;multicast routing;multi-pair unicasting;information dissemination;Mobile ad hoc networks;Wireless networks;Broadcasting;Throughput;Computer networks;Unicast;Routing;USA Councils;Context;Ad hoc networks},
doi={10.1109/INFOCOM.2008.51},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509647,
author={S. -. Ng and W. K. G. Seah},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Game-Theoretic Model for Collaborative Protocols in Selfish, Tariff-Free, Multihop Wireless Networks},
year={2008},
volume={},
number={},
pages={216-220},
abstract={Traditional networks are built on the assumption that network entities cooperate based on a mandatory network communication semantic to achieve desirable qualities such as efficiency and scalability. Over the years, this assumption has been eroded by the emergence of users that alter network behavior in a way to benefit themselves at the expense of others. At one extreme, a malicious user/node may eavesdrop on sensitive data or deliberately inject packets into the network to disrupt network operations. The solution to this generally lies in encryption and authentication. In contrast, a rational node acts only to achieve an outcome that he desires most. In such a case, cooperation is still achievable if the outcome is to the best interest of the node. The node misbehaviour problem would be more pronounced in multihop wireless networks like mobile ad hoc and sensor networks, which are typically made up of wireless battery-powered devices that must cooperate to forward packets for one another. But, cooperation may be hard to maintain as it consumes scarce resources such as bandwidth, computational power and battery power. This paper applies game theory to achieve collusive networking behavior in such network environments. In this work, pricing, promiscuous listening and mass punishments are avoided altogether. Our model builds on recent work in the field of Economics on the theory of imperfect private monitoring for the dynamic Bertrand oligopoly, and adapts it to the wireless multihop network. The model derives conditions for collusive packet forwarding, truthful routing broadcasts and packet acknowledgments under a lossy, wireless, multi-hop environment, thus capturing many important characteristics of the network layer and link layer in one integrated analysis that has not been achieved previously. Finally, we provide a proof of the viability of the model under a theoretical wireless environment.},
keywords={cryptography;game theory;message authentication;packet switching;protocols;radio networks;telecommunication network routing;telecommunication security;game-theoretic model;collaborative protocol;selfish tariff-free multihop wireless network;malicious user/node;sensitive data eavesdrop;encryption;data authentication;wireless battery-powered device;packet forwarding;collusive networking behavior;dynamic Bertrand oligopoly;truthful routing broadcast;Collaboration;Wireless application protocol;Spread spectrum communication;Wireless networks;Wireless sensor networks;Scalability;Cryptography;Authentication;Bandwidth;Batteries},
doi={10.1109/INFOCOM.2008.52},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509648,
author={A. Caruso and F. Paparella and L. F. M. Vieira and M. Erol and M. Gerla},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={The Meandering Current Mobility Model and its Impact on Underwater Mobile Sensor Networks},
year={2008},
volume={},
number={},
pages={221-225},
abstract={Underwater mobile acoustic sensor networks are promising tools for the exploration of the oceans. These networks require new robust solutions for fundamental issues such as: localization service for data tagging and networking protocols for communication. All these tasks are closely related with connectivity, coverage and deployment of the network. A realistic mobility model that can capture the physical movement of the sensor nodes with ocean currents gives better understanding on the above problems. In this paper, we propose a novel physically-inspired mobility model which is representative of underwater environments. We study how the model affects a range-based localization protocol, and its impact on the coverage and connectivity of the network under different deployment scenarios.},
keywords={mobile radio;protocols;underwater acoustic communication;wireless sensor networks;meandering current mobility model;underwater mobile sensor networks;localization service;data tagging;networking protocols;Acoustic sensors;Lagrangian functions;Biosensors;Sea measurements;Mobile computing;Protocols;Remote monitoring;Ocean temperature;Temperature measurement;Communications Society},
doi={10.1109/INFOCOM.2008.53},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509649,
author={M. C. Vuran and I. F. Akyildiz},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Cross-Layer Packet Size Optimization for Wireless Terrestrial, Underwater, and Underground Sensor Networks},
year={2008},
volume={},
number={},
pages={226-230},
abstract={In this paper, a cross-layer solution for packet size optimization in wireless sensor networks (WSN) is introduced such that the effects of multi-hop routing, the broadcast nature of the physical wireless channel, and the effects of error control techniques are captured. A key result of this paper is that contrary to the conventional wireless networks, in wireless sensor networks, longer packets reduce the collision probability. Consequently, an optimization solution is formalized by using three different objective functions, i.e., packet throughput, energy consumption, and resource utilization. Furthermore, the effects of end-to-end latency and reliability constraints are investigated that may be required by a particular application. As a result, a generic, cross-layer optimization framework is developed to determine the optimal packet size in WSN. This framework is further extended to determine the optimal packet size in underwater and underground sensor networks. From this framework, the optimal packet sizes under various network parameters are determined.},
keywords={optimisation;probability;telecommunication network routing;underground communication;wireless channels;wireless sensor networks;cross-layer packet size optimization;wireless sensor networks;multihop routing;wireless channel;error control technique;collision probability;energy consumption;resource utilization;terrestrial sensor network;underground sensor network;underwater sensor network;Wireless sensor networks;Spread spectrum communication;Routing;Broadcasting;Error correction;Sensor phenomena and characterization;Throughput;Energy consumption;Delay;Acoustic sensors},
doi={10.1109/INFOCOM.2008.54},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509650,
author={A. A. Syed and W. Ye and J. Heidemann},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={T-Lohi: A New Class of MAC Protocols for Underwater Acoustic Sensor Networks},
year={2008},
volume={},
number={},
pages={231-235},
abstract={This paper introduces T-Lohi, a new class of distributed and energy-efficient media-access protocols (MAC) for underwater acoustic sensor networks (UWSN). MAC design for UWSN faces significant challenges. For example, acoustic communication suffers from latencies five orders-of-magnitude larger than radio communication, so a naive CSMA MAC would require very long listen time resulting in low throughput and poor energy efficiency. In this paper, we first identify unique characteristics in underwater networking that may affect all MACs, such as space-time uncertainty and deafness conditions. We then develop T-Lohi employing a novel tone-based contention resolution mechanism that exploits space-time uncertainty and high latency to detect collisions and count contenders, achieving good throughput across all offered loads. Lohi uses our low-power wake-up receiver to significantly reduce energy consumption. Finally, we evaluate design choices and protocol performance through extensive simulation. The results show that the energy cost of packet transmission is within 3-9 % of optimal, and that Lohi achieves good channel utilization, within 30% utilization of the theoretical maximum. We also show that Lohi is stable and fair under both low and very high offered loads.},
keywords={access protocols;channel allocation;underwater acoustic communication;wireless sensor networks;MAC protocol;underwater acoustic sensor network;T-Lohi;energy-efficient media-access protocol;space-time uncertainty;tone-based contention resolution mechanism;channel utilization;Media Access Protocol;Underwater acoustics;Acoustic sensors;Energy efficiency;Delay;Throughput;Uncertainty;Sensor phenomena and characterization;Radio communication;Multiaccess communication},
doi={10.1109/INFOCOM.2008.55},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509651,
author={W. Cheng and A. Y. Teymorian and L. Ma and X. Cheng and X. Lu and Z. Lu},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Underwater Localization in Sparse 3D Acoustic Sensor Networks},
year={2008},
volume={},
number={},
pages={236-240},
abstract={We study the localization problem in sparse 3D underwater sensor networks. Considering the fact that depth information is typically available for underwater sensors, we transform the 3D underwater positioning problem into its two- dimensional counterpart via a projection technique and prove that a non-degenerative projection preserves network localizability. We further prove that given a network and a constant <i>k</i>, all of the geometric <i>k</i>-lateration localization methods are equivalent. Based on these results, we design a purely distributed localization framework termed USP. This framework can be applied with any ranging method proposed for 2D terrestrial sensor networks. Through theoretical analysis and extensive simulation, we show that USP preserves the localizability of the original 3D network via a simple projection and improves localization capabilities when bilateration is employed. USP has low storage and computation requirements, and predictable and balanced communication overhead.},
keywords={distributed sensors;underwater acoustic communication;underwater localization;sparse 3D acoustic sensor networks;sparse 3D underwater sensor networks;3D underwater positioning problem;geometric k-lateration localization methods;2D terrestrial sensor networks;Acoustic sensors;Sensor phenomena and characterization;Computer science;Global Positioning System;Communications Society;USA Councils;Partial response channels;Computational modeling;Analytical models;Collaboration},
doi={10.1109/INFOCOM.2008.56},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509652,
author={P. Venkitasubramaniam and L. Tong},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Throughput Anonymity Trade-off in Wireless Networks under Latency Constraints},
year={2008},
volume={},
number={},
pages={241-245},
abstract={Providing anonymity to routes in a wireless ad hoc network from passive eavesdroppers is considered. Using Shannon's equivocation as an information theoretic measure of anonymity, scheduling strategies are designed for wireless nodes using receiver directed signaling. The achievable rate region for multiaccess relays are characterized under constraints on average packet latency. The relationship between overall network throughput and the route anonymity is obtained by drawing a connection to the rate-distortion tradeoff in information theory. A decentralized implementation of the relaying strategy is proposed, and the corresponding performance analyzed.},
keywords={ad hoc networks;multi-access systems;radio receivers;rate distortion theory;wireless ad hoc network;passive eavesdroppers;Shannon equivocation;information theoretic measure;receiver directed signaling;multiaccess relays;average packet latency;rate-distortion tradeoff;Throughput;Wireless networks;Delay;Telecommunication traffic;Peer to peer computing;Relays;Internet;Cryptography;Cryptographic protocols;Communications Society},
doi={10.1109/INFOCOM.2008.57},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509653,
author={C. Zhang and R. Lu and X. Lin and P. -. Ho and X. Shen},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={An Efficient Identity-Based Batch Verification Scheme for Vehicular Sensor Networks},
year={2008},
volume={},
number={},
pages={246-250},
abstract={With the adoption of state-of-the-art telecommunication technologies for sensing and collecting traffic related information, Vehicular Sensor Networks (VSNs) have emerged as a new application scenario that is envisioned to revolutionize the human driving experiences and traffic flow control systems. To avoid any possible malicious attack and resource abuse, employing a digital signature scheme is widely recognized as the most effective approach for VSNs to achieve authentication, integrity, and validity. However, when the number of signatures received by a Roadside Unit (RSU) becomes large, a scalability problem emerges immediately, where the RSU could be difficult to sequentially verify each received signature within 300 ms interval according to the current Dedicated Short Range Communications (DSRC) broadcast protocol. In this paper, we introduce an efficient batch signature verification scheme for communications between vehicles and RSUs (or termed vehicle- to-Infrastructure (V2I) communications), in which an RSU can verify multiple received signatures at the same time such that the total verification time can be dramatically reduced. We demonstrate that the proposed scheme can achieve conditional privacy preservation that is essential in VSNs, where each message launched by a vehicle is mapped to a distinct pseudo identity, while a trust authority can always retrieve the real identity of a vehicle from any pseudo identity. With the proposed scheme, since identity-based cryptography is employed in generating private keys for pseudo identities, certificates are not needed and thus transmission overhead can be significantly reduced.},
keywords={ad hoc networks;digital signatures;private key cryptography;road traffic;telecommunication security;traffic engineering computing;wireless sensor networks;identity-based batch signature verification scheme;vehicular sensor networks;digital signature scheme;roadside unit;identity-based cryptography;private keys;VANET;vehicular ad hoc networks;ITS;traffic related information;Vehicles;Communication system traffic control;Sensor systems and applications;Humans;Control systems;Telecommunication control;Digital signatures;Authentication;Scalability;Broadcasting},
doi={10.1109/INFOCOM.2008.58},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509654,
author={C. Zhang and Y. Song and Y. Fang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Modeling Secure Connectivity of Self-Organized Wireless Ad Hoc Networks},
year={2008},
volume={},
number={},
pages={251-255},
abstract={Wireless ad hoc networks (WANETs) offer communications over a shared wireless channel without any pre-existing infrastructure. Forming peer-to-peer security associations in self-organized WANETs is more challenging than in conventional networks due to the lack of central authorities. In this paper, we propose a generic model to evaluate the relationship of connectivity, memory size, communication overhead and security in fully self-organized WANETs. Based on some reasonable assumptions on node deployment and mobility, we show that when the average number of authenticated neighbors of each node is Theta(1), with respect to the network size n, most of the nodes can be securely connected, forming a connected secure backbone, i.e., the secure network percolates. This connected secure backbone can be utilized to break routing-security dependency loop, and provide enough derived secure links connecting isolated nodes with the secure backbone in a multi-hop fashion, which leads to the secure connectivity of the whole network.},
keywords={ad hoc networks;message authentication;telecommunication network routing;telecommunication security;wireless channels;self-organized wireless ad hoc network;shared wireless channel;peer-to-peer security;Mobile ad hoc networks;Peer to peer computing;Spine;Communication system security;Network servers;National security;Authentication;Communications Society;Ad hoc networks;Joining processes},
doi={10.1109/INFOCOM.2008.59},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509655,
author={M. Srivatsa and A. Iyengar and A. Iyengar and Jian Yin and Ling Liu},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Scalable Method for Access Control in Location-Based Broadcast Services},
year={2008},
volume={},
number={},
pages={256-260},
abstract={One important problem for public broadcast Location-Based Services (LBS) is to enforce access control on a large number of subscribers. In such a system a user typically subscribes to a LBS for a time interval (a, b) and a spatial region (x<sub>bl</sub>, y<sub>bl</sub>, x<sub>ir</sub>, y<sub>tr</sub>) according to a 3-dimensional spatial-temporal authorization model. In this paper, we argue that current approaches to access control using group key management protocols are not scalable. Our proposal STauth minimizes the number of keys which needs to be distributed and is thus scalable to a much higher number of subscribers and the dimensionality of the authorization model. We analytically and experimentally demonstrate the performance and scalability benefits of our approach against other group key management protocols.},
keywords={authorisation;broadcasting;cryptographic protocols;mobile radio;public key cryptography;telecommunication network management;telecommunication security;public location-based broadcast service;access control;3-dimensional spatial-temporal authorization model;group key management protocol;Access control;Broadcasting;Authorization;Protocols;Counting circuits;Subscriptions;Scalability;Communication system security;Cryptography;Communications Society},
doi={10.1109/INFOCOM.2008.60},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509656,
author={D. Bonfiglio and M. Mellia and M. Meo and N. Ritacca and D. Rossi},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Tracking Down Skype Traffic},
year={2008},
volume={},
number={},
pages={261-265},
abstract={Skype is beyond any doubt the most popular VoIP application in the current Internet application spectrum. Its amazing success drawn the attention of telecom operators and the research community, both interested in knowing Skype's internal mechanisms, characterizing traffic and understanding users' behavior. In this paper, we dissect the following fundamental components: data traffic generated by voice and video communication, and signaling traffic generated by Skype. We use both active and passive measurement techniques to gather a deep understanding on the traffic Skype generates. From extensive testbed experiments, we devise a source model which takes into account: (i) the service type, i.e., voice or video calls (ii) the selected source Codec, (iii) the adopted transport-layer protocol, and (iv) network conditions. Furthermore, leveraging on the use of an accurate Skype classification engine that we recently proposed, we study and characterize Skype traffic based on extensive passive measurements collected from our campus LAN.},
keywords={Internet telephony;pattern classification;telecommunication signalling;telecommunication traffic;tracking;transport protocols;video communication;voice communication;Skype traffic tracking;VoIP application;Internet application spectrum;data traffic;signaling traffic;active measurement techniques;passive measurement techniques;voice communication;video communication;source codec;transport-layer protocol;network conditions;Skype classification engine;Telecommunication traffic;Traffic control;Signal generators;Internet;Measurement techniques;Testing;Codecs;Transport protocols;Search engines;Local area networks},
doi={10.1109/INFOCOM.2008.61},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509657,
author={X. Meng and G. Jiang and H. Zhang and H. Chen and K. Yoshihira},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Automatic Profiling of Network Event Sequences: Algorithm and Applications},
year={2008},
volume={},
number={},
pages={266-270},
abstract={The behavior of network entities, such as flows, sessions, hosts, and users, can often be described by communication event sequences in the time domain. For the purpose of many network measurement and monitoring tasks, it is desirable to have an accurate yet information-compact profiling of the behavior of massive event sequences. This paper proposes a new method to achieve this goal. On a given set of event sequences, the proposed method automatically learns a mixture model which fully captures the sequence behavior including both event pattern and duration between events. The learned mixture model is information-compact as it classifies sequences into a set of behavior templates, each of which is described by a Markov Chain. The model parameters are estimated in an iterative procedure which is developed from the Expectation Maximization algorithm. Two network management applications are proposed based on the method: a visualization tool for network administrators to conduct exploratory traffic analysis, and an efficient anomaly detection mechanism. In the evaluation, we validate the method accuracy as well as the usefulness of the two applications by using three networking datasets with different types: TCP packet traces, VoIP calls, and syslog traces in wireless networks.},
keywords={data visualisation;expectation-maximisation algorithm;Markov processes;telecommunication computing;telecommunication network management;telecommunication security;telecommunication traffic;automatic network event sequence profiling;network measurement;network monitoring;Markov chain;iterative procedure;expectation maximization algorithm;network management application;visualization tool;network traffic analysis;anomaly detection mechanism;TCP packet trace;VoIP call;syslog trace;wireless network;Monitoring;Wireless networks;Protocols;Communications Society;National electric code;Laboratories;Parameter estimation;Iterative algorithms;Visualization;Telecommunication traffic},
doi={10.1109/INFOCOM.2008.62},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509658,
author={Y. Li and J. -. Cui and D. Maggiorini and M. Faloutsos},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Characterizing and Modelling Clustering Features in AS-Level Internet Topology},
year={2008},
volume={},
number={},
pages={271-275},
abstract={The AS-level Internet topology has shown significant clustering features. In this paper, we propose a new set of clustering metrics and conduct extensive measurement on the AS- level Internet topology. We give a thorough characterization on the clustering features and their evolution. We also study the clustering features of different topological structures by comparing the Internet with various topology models. Due to the limitation of existing topology models on capturing clustering features, we design a new topology model based on clustering. Through extensive evaluations, we claim that our model can closely capture the clustering features as well as other common topological properties.},
keywords={Internet;statistical analysis;telecommunication network topology;AS-level Internet topology;clustering metrics;Internet;Network topology;Clustering algorithms;Computer science;USA Councils;Robustness;Partitioning algorithms;Communications Society;Peer to peer computing;Routing protocols},
doi={10.1109/INFOCOM.2008.63},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509659,
author={V. Arya and N. G. Duffield and D. Veitch},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Temporal Delay Tomography},
year={2008},
volume={},
number={},
pages={276-280},
abstract={Multicast-based network tomography enables inference of average loss rates and delay distributions of internal network links from end-to-end measurements of multicast probes. Recent work showed that this method, based on correlating observations of multicast receivers, also supports the inference of temporal loss characteristics of network links. In this paper, we show that temporal characteristics can, in fact, be estimated even for link delay processes. Knowledge of temporal delay characteristics has applications for delay sensitive services such as VoIP as well as for characterizing the queueing behavior of bottleneck links. By assuming mutually independent, but arbitrary link delay processes, we develop estimators which can infer, in addition to delay distributions, the probabilities of arbitrary patterns of delay, means and full distributions of delay-run periods at chosen delay levels, for each link in the multicast tree. By applying the recently proposed principle of subtree-partitioning, the estimator is made scalable to multicast trees of large degree. Estimation error and convergence rates are evaluated using simulations.},
keywords={delays;multicast communication;parameter estimation;statistical distributions;telecommunication network topology;trees (mathematics);temporal delay tomography;multicast-based network tomography;link delay processes;delay distributions;probability;multicast tree;subtree-partitioning;estimation error;convergence rates;temporal parameter estimation;Tomography;Delay estimation;Added delay;Virtual private networks;Delay effects;Statistical distributions;Communications Society;USA Councils;Loss measurement;Probes},
doi={10.1109/INFOCOM.2008.64},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509660,
author={S. Misra and S. D. Hong and G. Xue and J. Tang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Constrained Relay Node Placement in Wireless Sensor Networks to Meet Connectivity and Survivability Requirements},
year={2008},
volume={},
number={},
pages={281-285},
abstract={The relay node placement problem for wireless sensor networks is concerned with placing a minimum number of relay nodes into a wireless sensor network to meet certain connectivity and survivability requirements. In this paper, we study constrained versions of the relay node placement problem, where relay nodes can only be placed at a subset of candidate locations. In the connected relay node placement problem, we want to place a minimum number of relay nodes to ensure the connectivity of the sensor nodes and the base stations. In the survivable relay node placement problem, we want to place a minimum number of relay nodes to ensure the biconnectivity of the sensor nodes and the base stations. For each of the two problems, we discuss its computational complexity, and present a framework of polynomial time O(1) -approximation algorithms with small approximation ratios.},
keywords={approximation theory;computational complexity;wireless sensor networks;constrained relay node placement;wireless sensor network;sensor node biconnectivity;computational complexity;polynomial time approximation algorithm;Relays;Wireless sensor networks;Peer to peer computing;Tin;Base stations;Polynomials;Routing;Computer science;Protective relaying;Communications Society},
doi={10.1109/INFOCOM.2008.65},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509661,
author={J. Wu and S. Yang and M. Cardei},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={On Maintaining Sensor-Actor Connectivity in Wireless Sensor and Actor Networks},
year={2008},
volume={},
number={},
pages={286-290},
abstract={In wireless sensor and actor networks (WSANs), a group of sensors and actors are connected by a wireless medium to perform distributed sensing and acting tasks. Sensors usually gather information in an event area and pass it on to actors, which are resource-rich devices that make decisions and perform necessary actions. Therefore, it is vital to maintain connections between sensors and actors for effective sensor- actor coordination. In this paper, we first define several sensor- actor connection requirements, including weak and strong actor-connectivity, and then propose several local solutions that put as many sensors as possible to sleep for energy saving purposes, while meeting different actor-connectivity requirements. We also prove the relationship between the proposed actor-connectivity and the connectivity in regular graphs, which helps with the implementation of the proposed solutions. Comprehensive performance analysis is conducted through simulations.},
keywords={telecommunication network routing;telecommunication network topology;wireless sensor networks;sensor-actor connectivity;wireless sensor networks;wireless actor networks;distributed sensing task;acting tasks;Wireless sensor networks;Sensor phenomena and characterization;Routing;Computer science;Communications Society;Maintenance engineering;Sleep;Performance analysis;Analytical models;Actuators},
doi={10.1109/INFOCOM.2008.66},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509662,
author={Y. Bejerano},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Simple and Efficient k-Coverage Verification without Location Information},
year={2008},
volume={},
number={},
pages={291-295},
abstract={Wireless sensor networks (WSNs) have recently emerged as a prominent technology for environmental monitoring and hazardous event detection. Yet, their success depends considerably on their ability to ensure reliable event detection. Such guarantees can be provided only if the target field monitored by a WSN does not contain coverage holes that are not monitored by any sensor. Currently, the coverage-holes detection solutions require accurate knowledge of the sensors locations, which cannot be easily obtained, or they cannot provide guarantees on the coverage quality. In this study we address the challenge of designing an accurate k-coverage verification scheme, without using location information, for a predefined kges1. To this end, we present an efficient, distributed and localized k-coverage verification scheme with proven guarantees on its coverage detection quality. Our simulations show that the scheme accurately detects coverage holes of various sizes.},
keywords={wireless sensor networks;wireless sensor networks;environmental monitoring;hazardous event detection;coverage hole detection;distributed-localized k-coverage verification scheme;Wireless sensor networks;Event detection;Monitoring;Peer to peer computing;Quality of service;Global Positioning System;Communications Society;Wireless communication;Collaboration;Ad hoc networks},
doi={10.1109/INFOCOM.2008.67},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509663,
author={X. Bai and Z. Yun and D. Xuan and T. H. Lai and W. Jia},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Deploying Four-Connectivity and Full-Coverage Wireless Sensor Networks},
year={2008},
volume={},
number={},
pages={296-300},
abstract={We study the issue of optimal deployment to achieve four connectivity and full coverage for wireless sensor networks (WSNs) under different ratios of sensors' communication range (denoted by r<sub>c</sub>) to their sensing range (denoted by r<sub>s</sub>). We propose a "Diamond" pattern, which can be viewed as a series of different evolving patterns. When r<sub>c</sub>/r<sub>s</sub> ges radic3, the Diamond pattern coincides with the well-known triangle lattice pattern; when r<sub>c</sub>/r<sub>s</sub> ges radic2, it degenerates to a "Square" pattern. We prove the Diamond pattern to be asymptotically optimal when r<sub>c</sub>/r<sub>s</sub> ges radic2- Our work is the first to propose an asymptotically optimal deployment pattern to achieve four connectivity and full coverage for WSNs. We hope our work will provide some insights on how optimal patterns evolve and how to search for them.},
keywords={wireless sensor networks;full-coverage wireless sensor network;four-connectivity wireless sensor network;diamond pattern;triangle lattice pattern;Wireless sensor networks;Computer science;Lattices;Acoustic sensors;Communications Society;USA Councils;Mathematics;Costs;Heuristic algorithms;Topology},
doi={10.1109/INFOCOM.2008.68},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509664,
author={S. Jagabathula and V. Doshi and D. Shah},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Fair Scheduling through Packet Election},
year={2008},
volume={},
number={},
pages={301-305},
abstract={In this paper, we consider the problem of designing a scheduling algorithm for input queued switches, that is both fair as well as throughput optimal. Most of the existing literature on input-queued switch fairness criteria concentrates on flow-based fairness. Since a large fraction of network traffic is about "short- flows" there is a need for packet-based fairness criterion. The significant body of literature developed over the past two decades for packet-based scheduling algorithms is primarily concerned with throughput and delay, but not fairness. One of the reasons for such a state of affairs is the lack of a proper definition for packet-based fairness. The difficulty in defining fair stems from the fact that any reasonable notion of fairness must combine the well-known notion of fairness for a single-queue with the scheduling constraint of an input queued switch in an appropriate manner. As one of the main results of this paper, we define a notion of packet-based fair scheduling by identifying it as the selection of a winner in the following ranked election: packets are voters; schedules are candidates and each packet ranks different schedules based on their priorities. Drawing upon the seminal work of Goodman and Markowitz (1952) on ranked elections, we obtain a unique characterization of the fair schedule. Another important contribution of this paper is proving that the thus obtained fair scheduling algorithm is throughput optimal. There is no a priori reason why this should be true, and we introduce some non-standard proof techniques to prove the result. Our results suggest a framework for defining fair scheduling algorithm for a constrained packet network; a nonstandard method to prove throughput stability for algorithms, such as ours, that are not based on queue-sizes.},
keywords={packet switching;queueing theory;scheduling;telecommunication traffic;fair scheduling;packet election;input queued switches;flow-based fairness;network traffic;packet-based fairness criterion;Nominations and elections;Switches;Scheduling algorithm;Throughput;Packet switching;Round robin;Communication switching;Channel allocation;Algorithm design and analysis;Communications Society},
doi={10.1109/INFOCOM.2008.69},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509665,
author={C. -. Chang and J. Cheng and D. -. Lee and C. -. Wu},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Quasi-Output-Buffered Switches},
year={2008},
volume={},
number={},
pages={306-310},
abstract={Output-buffered switches are known to have better performance than other switch architectures. However, output- buffered switches also suffer from the notorious scalability problem, and direct constructions of large output-buffered switches are difficult. In this paper, we study the problem of constructing scalable switches that have comparable performance to output- buffered switches. For this, we propose a new concept, called quasi-output-buffered switch. Like an output-buffered switch, a quasi-output-buffered switch is a deterministic switch that delivers packets in the FIFO order and achieves 100% throughput. Using the three-stage Clos network, we show that one can recursively construct a larger quasi-output-buffered switch with a set of smaller quasi-output-buffered switches. By recursively expanding the three-stage Clos network, we obtain a quasi-output-buffered switch with only 2 x 2 switches. Such a switch is called a packet- pair switch as it always transmits packets in pairs. By computer simulations, we show that packet-pair switches have better delay performance than most load-balanced switches with comparable construction complexity.},
keywords={digital simulation;multiprocessor interconnection networks;packet switching;quasi-output-buffered switch;notorious scalability problem;three-stage Clos network;packet-pair switch;computer simulation;load-balanced switch;construction complexity;Switches;Communication switching;Packet switching;Throughput;Traffic control;Delay;Calculus;Computer simulation;Communications Society;Scalability},
doi={10.1109/INFOCOM.2008.70},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509666,
author={D. Ficara and S. Giordano and G. Procissi and F. Vitucci},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={MultiLayer Compressed Counting Bloom Filters},
year={2008},
volume={},
number={},
pages={311-315},
abstract={Bloom filters are efficient randomized data structures for membership queries on a set with a certain known false positive probability. Counting bloom filters (CBFs) allow the same operation on dynamic sets that can be updated via insertions and deletions with larger memory requirements. This paper first presents a new upper bound for counters overflow probability in CBFs. This bound is much tighter than that usually adopted in literature and it allows for designing more efficient CBFs. Three novel data structures are proposed, which introduce the idea of a hierarchical structure as well as the use of Huffman code. Our algorithms improve standard CBFs in terms of fast access and limited memory consumption (up to 50% of memory saving): the target could be the implementation of the compressed data structures in the small (but fast) local memory or "on-chip SRAM" of devices such as network processors .},
keywords={data structures;filters;Huffman codes;SRAM chips;compressed multilayer;counting bloom filters;data structures;counters overflow probability;Huffman code;on-chip SRAM;network processors;Nonhomogeneous media;Data structures;Counting circuits;Information filtering;Information filters;Upper bound;Network-on-a-chip;Random access memory;Huffman coding;Communications Society},
doi={10.1109/INFOCOM.2008.71},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509667,
author={W. Lu and S. Sahni},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Low Power TCAMs for Very Large Forwarding Tables},
year={2008},
volume={},
number={},
pages={316-320},
abstract={Ternary content-addressable memories (TCAMs) may be used to obtain a simple and very fast implementation of a router's forwarding engine. The applicability of TCAMs is, however, limited by their size and high power requirement. Zane et al. (2003) proposed a method and associated algorithms to reduce the power needed to search a forwarding table using a TCAM. We improve on both the algorithms proposed by them. Additionally, we show how to couple TCAMs and high bandwidth SRAMs so as to overcome both the power and size limitations of a pure TCAM forwarding engine.},
keywords={content-addressable storage;IP networks;table lookup;telecommunication network routing;transport protocols;low-power ternary content-addressable memory;IPV4 router forwarding table;high-bandwidth SRAM;TCAM forwarding engine;Random access memory;Partitioning algorithms;Engines;Energy consumption;Communications Society;Power engineering computing;Information science;Power engineering and energy;Bandwidth;Associative memory},
doi={10.1109/INFOCOM.2008.72},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509668,
author={T. Lan and X. Lin and M. Chiang and R. Lee},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={How Bad is Suboptimal Rate Allocation?},
year={2008},
volume={},
number={},
pages={321-325},
abstract={Not too bad. A rate allocation that is suboptimal with respect to a utility maximization formulation still maintains the maximum flow-level stability when the utility gap is sufficiently small, and provides a minimum size of stability region otherwise. Utility-suboptimal allocation may also enhance other network performance metrics, e.g., it may increase network throughput and reduce link saturation. Quantifying these intuitions, this paper provides a theoretical support for turning attention from optimal but complex solutions of network optimization to those that are simple even though suboptimal.},
keywords={optimisation;performance evaluation;resource allocation;suboptimal rate allocation;network utility maximization;NUM formulation;flow-level stability;network optimization;resource allocation;network performance metrics;Stability;Resource management;Stochastic processes;USA Councils;Measurement;Throughput;Utility programs;Time sharing computer systems;Communications Society;Maintenance engineering},
doi={10.1109/INFOCOM.2008.73},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509669,
author={K. Ma and R. Mazumdar and J. Luo},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={On the Performance of Primal/Dual Schemes for Congestion Control in Networks with Dynamic Flows},
year={2008},
volume={},
number={},
pages={326-330},
abstract={Stability and fairness are two design objectives of congestion control mechanisms; they have traditionally been analyzed for long-lived flows (or elephants). It is only recently that short-lived flows (or mice) have received attention. Whereas stability has been established for the existing primal-dual based control mechanisms, the performance issue has been largely overlooked. In this paper, we study utility maximization problems for networks with dynamic flows. In particular, we consider the case where sessions of each class results in flows that arrive according to a Poisson process and have a length given by a general distribution. The goal is to maximize the long-term expected system utility that is a function of the number of flows and the rate (identical within a given class) allocated to each flow. Our results show that, as long as the average amount of work brought by the flows is strictly within the network stability region, the rate allocation and stability issues are decoupled. While stability can be guaranteed by, for example, a FIFO policy, utility maximization becomes an unconstrained optimization that results in a static rate allocation for flows. We also provide a queueing interpretation of this seemingly surprising result and show that not all utility functions make sense for dynamic flows. Finally, we use simulation results to show that indeed the open-loop algorithm maximizes the expected system utility.},
keywords={stability;stochastic processes;telecommunication congestion control;congestion control;dynamic flows;primal-dual based control mechanisms;Poisson process;system utility;rate allocation;stability;Communication system control;Iterative algorithms;Game theory;Communications Society;Multiaccess communication;USA Councils;Stability analysis;Performance analysis;Mice;Communication networks},
doi={10.1109/INFOCOM.2008.74},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509670,
author={K. Miller and T. Harks},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Utility Max-Min Fair Congestion Control with Time-Varying Delays},
year={2008},
volume={},
number={},
pages={331-335},
abstract={We present a framework for designing delay- independent end-to-end congestion control algorithms, where each end-user may have a different utility function. We only require that utility functions are strictly increasing. In this framework, we design an algorithm that maximizes the minimum utility value in the network, that is, the resulting resource allocation is utility max-min fair. To achieve this, we first extend the congestion control algorithm EMKC proposed by Zhang et al. [1], which aims at max-min fair bandwidth allocation. Our extension (xMKC) allows for arbitrary rate allocations in the steady state. We investigate xMKC analytically and prove local asymptotic stability with heterogeneous time-varying feedback delays in multi-link networks and global asymptotic stability with homogeneous time-varying feedback delays in single-link networks. Then, we propose uMKC (Utility Max-Min Fair Kelly Control), which achieves utility max-min fairness in its steady state. Based on the analysis of xMKC, we establish stability results for uMKC in the presence of time-varying feedback delays. Finally, we evaluate the performance of uMKC using NS-2 simulations [2].},
keywords={asymptotic stability;bandwidth allocation;delays;feedback;minimax techniques;telecommunication congestion control;telecommunication networks;time-varying systems;utility theory;utility max-min fair congestion control;time-varying feedback delays;delay-independent end-to-end congestion control algorithm design;utility function;minimum utility value;local asymptotic stability;multi link network;max-min fair bandwidth allocation;Delay;Algorithm design and analysis;Feedback;Communication system control;Distributed algorithms;Steady-state;Asymptotic stability;Traffic control;Aggregates;Jacobian matrices},
doi={10.1109/INFOCOM.2008.75},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509671,
author={M. Enachescu and M. Wang and A. Goel},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Reducing Maximum Stretch in Compact Routing},
year={2008},
volume={},
number={},
pages={336-340},
abstract={It is important in communication networks to use routes that are as short as possible (i.e have low stretch) while keeping routing tables small. Recent advances in compact routing show that a stretch of 3 can be achieved while maintaining a sub- linear (in the size of the network) space at each node [14]. It is also known that no routing scheme can achieve stretch less than 3 with sub-linear space for arbitrary networks. In contrast, simulations on real-life networks have indicated that stretch less than 3 can indeed be obtained using sub-linear sized routing tables[6]. In this paper, we further investigate the space-stretch tradeoffs for compact routing by analyzing a specific class of graphs and by presenting an efficient algorithm that (approximately) finds the optimum space-stretch tradeoff for any given network. We first study a popular model of random graphs, known as Bernoulli random graphs or Erds-Renyi graphs, and prove that stretch less than 3 can be obtained in conjunction with sub- linear routing tables. In particular, stretch 2 can be obtained using routing tables that grow roughly as n<sup>3/4</sup> where n is the number of nodes in the network. Compact routing schemes often involve the selection of landmarks. We present a simple greedy scheme for landmark selection that takes a desired stretch s and a budget L on the number of landmarks as input, and produces a set of at most 0(L logn) landmarks that achieve stretch s. Our scheme produces routing tables that use no more than O(logn) more space than the optimum scheme for achieving stretch s with L landmarks. This may be a valuable tool for obtaining near-optimum stretch-space tradeoffs for specific graphs. We simulate this greedy scheme (and other heuristics) on multiple classes of random graphs as well as on Internet like graphs.},
keywords={graph theory;network theory (graphs);telecommunication network routing;communication network routing;Bernoulli random graph;Erds-Renyi graph;greedy scheme;landmark selection;Routing;Peer to peer computing;Communications Society;Computer science;Computer network management;Engineering management;Maintenance engineering;Communication networks;Algorithm design and analysis;Internet},
doi={10.1109/INFOCOM.2008.76},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509672,
author={M. Kalantari and M. Haghpanahi and M. Shayman},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A p-norm Flow Optimization Problem in Dense Wireless Sensor Networks},
year={2008},
volume={},
number={},
pages={341-345},
abstract={In a network with a high density of wireless nodes, we model flow of information by a continuous vector field known as the information flow vector field. We use a mathematical model that translates a communication network composed of a large but finite number of sensors into a continuum of nodes on which information flow is formulated by a vector field. The magnitude of this vector field is the intensity of the communication activity, and its orientation is the direction in which the traffic is forwarded. The information flow vector field satisfies a set of Neumann boundary conditions and a partial differential equation (PDE) involving the divergence of information, but the divergence constraint and Neumann boundary conditions do not specify the information flow vector field uniquely, and leave us freedom to optimize certain measures within their feasible set. Therefore, we introduce a <i>p-norm</i> flow optimization problem in which we minimize the <i>p-norm</i> of information flow vector field over the area of the network. This problem is a convex optimization problem, and we use sequential quadratic programming (SQP) to solve it. SQP is known for numerical stability and fast convergence to the optimal solution in convex optimization problems. By using standard SQP on <i>p-norm</i> flow optimization, we prove that the solution of each iteration of SQP is uniquely specified by an elliptic PDE with generalized Neumann boundary conditions. The <i>p-norm</i> flow optimization shows interesting properties for different values of <i>p</i>. For example, if p is close to one, the information routes resemble the geometric shortest paths of the sources and sinks, and for <i>p</i> = 2, the information flow shows an analogy to electrostatics. For infinitely large values of <i>p</i>, the problem minimizes the maximum magnitude of the information vector field over the network, and hence it achieves maximum load balancing.},
keywords={boundary-value problems;numerical stability;partial differential equations;quadratic programming;telecommunication network routing;wireless sensor networks;p-norm flow optimization;dense wireless sensor networks;information flow vector field;Neumann boundary conditions;partial differential equation;sequential quadratic programming;numerical stability;information routes;maximum load balancing;Wireless sensor networks;Boundary conditions;Mathematical model;Communication networks;Telecommunication traffic;Partial differential equations;Constraint optimization;Fluid flow measurement;Quadratic programming;Numerical stability},
doi={10.1109/INFOCOM.2008.77},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509673,
author={H. Kalosha and A. Nayak and S. Ruhrup and I. Stojmenovic},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Select-and-Protest-Based Beaconless Georouting with Guaranteed Delivery in Wireless Sensor Networks},
year={2008},
volume={},
number={},
pages={346-350},
abstract={Recently proposed beaconless georouting algorithms are fully reactive, with nodes forwarding packets without prior knowledge of their neighbors. However, existing approaches for recovery from local minima can either not guarantee delivery or they require the exchange of complete neighborhood information. We describe two general methods that enable completely reactive face routing with guaranteed delivery. The Beaconless Forwarder Planarization (BFP) scheme finds correct edges of a local planar subgraph at the forwarder node without hearing from all neighbors. Face routing then continues properly. Angular Relaying determines directly the next hop of a face traversal. Both schemes are based on the Select and Protest principle. Neighbors respond according to a delay function, if they do not violate the condition for a planar subgraph construction. Protest messages are used to remove falsely selected neighbors that are not in the planar subgraph. We show that a correct beaconless planar subgraph construction is not possible without protests. We also show the impact of the chosen planar subgraph construction on the message complexity. This leads to the definition of the Circlunar Neighborhood Graph (CNG), a new proximity graph, that enables BFP with a bounded number of messages in the worst case, which is not possible when using the Gabriel graph (GG). The CNG is sparser than the GG, but this does not lead to a performance degradation. Simulation results show similar message complexities in the average case when using CNG and GG. Angular Relaying uses a delay function that is based on the angular distance to the previous hop. Simulation results show that in comparison to BFP more protests are used, but overall message complexity can be further reduced.},
keywords={computational complexity;graph theory;telecommunication network routing;wireless sensor networks;select-and-protest-based beaconless georouting;wireless sensor networks;guaranteed delivery;beaconless forwarder planarization;local planar subgraph;angular relaying;message complexity;circlunar neighborhood graph;proximity graph;Gabriel graph;delay function;Wireless sensor networks;Routing;Peer to peer computing;Delay;Planarization;Relays;Communications Society;Information technology;Broadcasting;Auditory system},
doi={10.1109/INFOCOM.2008.78},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509674,
author={C. -. Lin and B. -. Liu and H. -. Yang and C. -. Kao and M. -. Tsai},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Virtual-Coordinate-Based Delivery-Guaranteed Routing Protocol in Wireless Sensor Networks with Unidirectional Links},
year={2008},
volume={},
number={},
pages={351-355},
abstract={A wireless sensor network has unidirectional links because sensors can have different transmission ranges, sensors have unstable transmission ranges, and a hidden terminal problem exists. In this paper, we introduce a virtual coordinate assignment protocol (ABVCap_Uni) to assign virtual coordinates to nodes that have no geographic information in wireless sensor networks with unidirectional links, and we propose a routing protocol based on the ABVCap_Uni virtual coordinates. Our routing protocol guarantees packet delivery without computation and storage of global topology features in a discrete domain. Using simulation, we evaluate the performance of the proposed routing protocol (ABVCap_Uni routing), the greedy landmark- descent routing protocol (GLDR+VLM routing), and the greedy routing protocol based on physical coordinates (Euclidean routing). The simulations demonstrate that our routing protocol ensures moderate routing path length cost overhead.},
keywords={routing protocols;telecommunication network topology;wireless sensor networks;virtual-coordinate-based delivery-guaranteed routing protocol;unidirectional link wireless sensor network;unstable transmission range;hidden terminal problem;virtual coordinate assignment protocol;global network topology feature;greedy landmark-descent routing protocol;Euclidean routing;Routing protocols;Wireless sensor networks;Network topology;Computational modeling;Euclidean distance;Communications Society;Computer science;Electronic mail;Wireless application protocol;Peer to peer computing},
doi={10.1109/INFOCOM.2008.79},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509675,
author={Y. Wu and S. Fahmy and N. B. Shroff},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={On the Construction of a Maximum-Lifetime Data Gathering Tree in Sensor Networks: NP-Completeness and Approximation Algorithm},
year={2008},
volume={},
number={},
pages={356-360},
abstract={Energy efficiency is critical for wireless sensor networks. The data gathering process must be carefully designed to conserve energy and extend the network lifetime. For applications where each sensor continuously monitors the environment and periodically reports to a base station, a tree-based topology is often used to collect data from sensor nodes. In this work, we study the construction of a data gathering tree to maximize the network lifetime, which is defined as the time until the first node depletes its energy. The problem is shown to be NP-complete. We design an algorithm which starts from an arbitrary tree and iteratively reduces the load on bottleneck nodes (nodes likely to soon deplete their energy due to high degree or low remaining energy). We show that the algorithm terminates in polynomial time and is provably near optimal.},
keywords={communication complexity;telecommunication network reliability;telecommunication network topology;trees (mathematics);wireless sensor networks;maximum-lifetime data gathering tree;wireless sensor network;NP-complete algorithm;approximation algorithm;energy efficiency;tree-based topology;polynomial time algorithm;Approximation algorithms;Network topology;Peer to peer computing;Wireless sensor networks;Base stations;Iterative algorithms;Monitoring;Polynomials;Aggregates;Communications Society},
doi={10.1109/INFOCOM.2008.80},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509676,
author={T. Cui and L. Chen and T. Ho},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Energy Efficient Opportunistic Network Coding for Wireless Networks},
year={2008},
volume={},
number={},
pages={361-365},
abstract={We consider energy efficient network coding design in wireless networks with multiple unicast sessions. Our approach decomposes multiple unicast sessions into a superposition of multicast and unicast sessions, with coding occurring only within each session. We give an optimization approach that is more general than the existing poison-remedy optimization formulation. For the case of wireless, we consider XOR coding and give an achievable rate region for a primary interference model. To simplify network operation, we give an oblivious backpressure algorithm which does not optimize overhearing of transmissions, and a practical protocol called COPR based on the oblivious backpressure algorithm. Simulation experiments show that COPR largely reduces network power consumption over existing algorithms.},
keywords={encoding;optimisation;protocols;radio networks;radiofrequency interference;energy efficient opportunistic network coding;wireless network;multiple unicast session;poison-remedy optimization formulation;XOR coding;primary interference model;oblivious backpressure algorithm;COPR protocol;Energy efficiency;Network coding;Wireless networks;Unicast;Scheduling algorithm;Interference;Algorithm design and analysis;Routing;Protocols;Multicast algorithms},
doi={10.1109/INFOCOM.2008.81},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509677,
author={R. Kumar and H. Choi and J. Shin and T. La Porta},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Channelization for Network Coding in Wireless Networks},
year={2008},
volume={},
number={},
pages={366-370},
abstract={Network coding is increasingly being investigated as an alternative to routing to increase throughput in packet networks. Like most data transfer schemes, the effectiveness of network coding may be limited by extreme congestion. When using network coding, these congested conditions are mitigated somewhat, but may still occur. We propose a selective channelization scheme in which links that experience congestion at a level that cannot be overcome by network coding are given reserved communication resources. This method has the following benefits. First, the algorithm proposed allows network coding full opportunity to overcome congestion before performing channelization, thus reducing the number of reserved resources used. Second, when triggered, the channelization of severely congested links greatly improves the end-to-end performance of flows that traverse the channelized link. To determine the point at which channelization should be triggered, we perform a thorough analysis of potential coding gains in a network facing errors due to collisions, and determine the point at which network coding loses its effectiveness.},
keywords={encoding;packet radio networks;telecommunication network routing;wireless channels;network coding;packet networks;wireless networks;data transfer schemes;selective channelization scheme;reserved communication resource;potential coding;network facing errors;Network coding;Wireless networks;Routing;Error analysis;Telecommunication traffic;Road accidents;Communications Society;Computer science;Throughput;Telecommunication congestion control},
doi={10.1109/INFOCOM.2008.82},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509678,
author={J. Le and J. C. S. Lui and D. M. Chiu},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={How Many Packets Can We Encode? - An Analysis of Practical Wireless Network Coding},
year={2008},
volume={},
number={},
pages={371-375},
abstract={While the practical coding scheme has been shown to be able to improve throughput of wireless networks, there still lacks fundamental understanding on how the coding scheme works under realistic settings, namely, when it operates on a realistic physical layer and the medium access is controlled by some random access methods. In this paper, we provide a formal analysis on the performance of the practical coding scheme under such realistic settings. The key performance measure is the encoding number, i.e., the number of packets that can be encoded by a coding node in each transmission. We provide an upper bound on the encoding number for the general coding topology, and derive the average encoding number and system throughput for a general class of random access mechanisms. Based on the practical coding scheme, we also derive a tighter upper bound on the throughput gain for a general wireless network. Our results can be particularly useful for coding-related MAC/Routing protocol design and analysis.},
keywords={access protocols;encoding;radio networks;routing protocols;telecommunication network topology;wireless network coding;random access method;medium access control protocol;formal analysis;encoding number;routing protocol;network topology;Wireless networks;Throughput;Encoding;Upper bound;Decoding;Bandwidth;Relays;Communications Society;Computer science;Physical layer},
doi={10.1109/INFOCOM.2008.83},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509679,
author={A. Keshavarz-Haddad and R. Riedi},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Bounds on the Benefit of Network Coding: Throughput and Energy Saving in Wireless Networks},
year={2008},
volume={},
number={},
pages={376-384},
abstract={In this paper we establish fundamental limitations to the benefit of network coding in terms of energy and throughput in multihop wireless networks. Thereby we adopt two well accepted scenarios in the field: single multicast session and multiple unicast sessions. Most of our results apply to arbitrary wireless network and are, in particular, not asymptotic in kind. In terms of throughput and energy saving we prove that the gain of network coding of a single multicast session is at most a constant factor. Also, we present a lower bound on the expected number of transmissions of multiple unicast sessions under an arbitrary network coding. We identify scenarios for which the network coding gain for energy saving becomes surprisingly close to 1, in some cases even exactly 1, corresponding to no benefit at all. Interestingly, we prove that the gain of network coding in terms of transport capacity is bounded by a constant factor pi in any arbitrary wireless network and for all traditional channel models. This shows that the traditional bounds on the transport capacity [1]-[4] do not change more than constant factor pi if we employ network coding. As a corollary, we find that the gain of network coding on the throughput of large scale homogeneous wireless networks is asymptotically bounded by a constant. Note that our result is more general than the previous work [5] and it is obtained by a different technique. In conclusion, we show that in contrast to wired networks, the network coding gain in wireless networks is constraint by fundamental limitations.},
keywords={channel capacity;encoding;multicast communication;radio networks;network coding;multihop wireless network;multicast session;arbitrary wireless network;transport capacity;channel capacity;Network coding;Throughput;Wireless networks;Unicast;Energy consumption;Wireless sensor networks;Wireless mesh networks;Mesh networks;Communications Society;Statistics},
doi={10.1109/INFOCOM.2008.84},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509680,
author={J. Liu and Y. T. Hou},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Weighted Proportional Fairness Capacity of Gaussian MIMO Broadcast Channels},
year={2008},
volume={},
number={},
pages={385-393},
abstract={Recently, there has been tremendous interest in exploring the capacity region of multiple-input multiple-output broadcast channels (MIMO-BC). However, fairness, a very important performance measure of multi-user communications systems and networks, has not been addressed for MIMO-BC in the literature. In this paper, we study how to determine the weighted proportional fairness (WPF) capacity of MIMO-BC. The difficulty of finding the WPF capacity of MIMO-BC lies in that it contains two difficult subproblems: 1) a complex combinatorial optimization problem to determine the optimal decoding order in the dual MIMO multiple access channel (MEMO-MAC) and 2) a nonconvex optimization problem in computing the optimal input covariance matrices to achieve WPF capacity. To circumvent the difficulty in the first subproblem, we derive a set of optimality conditions that the optimal decoding order must satisfy. Based on these optimality conditions, we design an efficient algorithm called iterative gradient sorting (IGS) to determine the optimal decoding order by iteratively sorting the gradient entries and moving across corner points. We also show that this method can be geometrically interpreted as sequential gradient projections. For the second subproblem, we propose an efficient algorithm based on conjugate gradient projection (CGP) technique, which employs the concept of Hessian conjugate. We also develop a polynomial time algorithm to solve the projection subproblem.},
keywords={broadcast channels;channel capacity;channel coding;computational complexity;concave programming;conjugate gradient methods;covariance matrices;Gaussian channels;iterative decoding;MIMO communication;multiuser channels;sorting;weighted proportional fairness capacity;Gaussian MIMO broadcast channels;multiple-input multiple-output broadcast channels;multiuser communications systems;WPF capacity;combinatorial optimization problem;optimal decoding order;dual MIMO multiple access channel;nonconvex optimization problem;optimal input covariance matrices;iterative gradient sorting algorithm;IGS algorithm;conjugate gradient projection technique;CGP technique;polynomial time algorithm;MIMO;Broadcasting;Communication systems;Covariance matrix;Iterative decoding;Iterative algorithms;Sorting;Information theory;Communications Society;Algorithm design and analysis},
doi={10.1109/INFOCOM.2008.85},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509681,
author={I. Menache and N. Shimkin},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Decentralized Rate Regulation in Random Access Channels},
year={2008},
volume={},
number={},
pages={394-402},
abstract={We consider a time-slotted multipacket reception channel, shared by a finite number of mobile users who transmit to a common base station. Each user is allocated a fixed data rate, which may be imposed by the base station or self-determined. For sustaining the required rate over time, each user may adjust a single parameter which determines the individual transmission probability in a given slot. An equilibrium point is attained when the assigned data rates are met with equality. This paper analyzes the equilibrium points which result in this system, with a focus on power efficiency of the solution. While multiple equilibrium points exist in general, we establish that one of these equilibria is best for all users, in the sense that the transmission probability (hence the power investment) of each user is minimal. Further to the existence of worse equilibrium points, we point to the possibility of a partial- equilibrium with starvation, where stronger users (in terms of received power) satisfy their data rates, while preventing weaker ones from obtaining their respective rates. To avoid these sub- optimal working points, we suggest a distributed mechanism that converges to the best equilibrium point. Further analysis is provided for a specific channel model which involves perfect capture.},
keywords={access protocols;data communication;radio networks;wireless channels;decentralized rate regulation;random access channels;time-slotted multipacket reception channel;transmission probability;power investment;data rates;Resource management;Base stations;Processor scheduling;Throughput;Investments;Fading;Scheduling algorithm;Wireless application protocol;Communications Society;WiMAX},
doi={10.1109/INFOCOM.2008.86},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509682,
author={I. Menache and N. Shimkin},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Efficient Rate-Constrained Nash Equilibrium in Collision Channels with State Information},
year={2008},
volume={},
number={},
pages={403-411},
abstract={We consider a wireless collision channel, shared by a finite number of users who transmit to a common base station. Users are self-optimizing, and each wishes to minimize its average transmission rate (or power investment), subject to minimum- throughput demand. The channel quality between each user and the base station is time-varying, and partially observed by the user in the form of channel state information (CSI) signals. We assume that each user can transmit at a fixed power level and that its transmission decision at each time slot is stationary in the sense that it can depend only on the current CSI. We are interested in properties of the Nash equilibrium of the resulting game between users. We define the feasible region of user's throughput demands, and show that when the demands are within this region, there exist exactly two Nash equilibrium points, with one strictly better than the other (in terms of invested power) for all users. We further provide some lower bounds on the channel capacity that can be obtained, both in the symmetric and non-symmetric case. Finally, we show that a simple greedy mechanism converges to the best equilibrium point without requiring any coordination between the users.},
keywords={channel capacity;decision theory;game theory;wireless channels;rate-constrained Nash equilibrium;wireless collision channels;channel state information;CSI signals;user game;user throughput demands;channel capacity;Nash equilibrium;Throughput;Base stations;Wireless networks;Investments;Channel state information;Wireless application protocol;Quality of service;Power system modeling;Communications Society},
doi={10.1109/INFOCOM.2008.87},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509683,
author={I. Broustis and A. Vlavianos and P. Krishnamurthy and S. V. Krishnamurthy},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={CTU: Capturing Throughput Dependencies in UWB Networks},
year={2008},
volume={},
number={},
pages={412-420},
abstract={The inherent channel characteristics of impulse-based UWB networks affect the MAC layer performance significantly. Previous studies on evaluating MAC protocols are based on prolonged simulations, and do not account for the multiple-access interference that arises due to multipath delay spread. In this work, we develop CTU, an analytical framework that captures the performance of MAC protocols, while taking into account the underlying PHY layer effects. The key attributes that make CTU novel are: (a) It is modular and therefore flexible; it can be easily modified to provide a basis for characterizing and evaluating a wide range of MAC protocols designed for impulse-based UWB networks. The only requirements are that the MAC protocol under study be based on time-hopping, and the modulation scheme be pulse position modulation; these are common design decisions in most impulse based UWB networks, (b) It considers the channel characteristics in addition to MAC layer effects; in particular, CTU correlates probabilistically the multipath delay profile of the channel with the packet error rate. We employ CTU to evaluate the performance of a generic medium access procedure. We compare the results with those from extensive simulations and show the high accuracy of CTU. We use CTU to assess the impact of various system parameters on the MAC layer performance; we make several interesting observations that are discussed in depth.},
keywords={access protocols;ultra wideband communication;wireless channels;UWB networks;throughput dependencies;MAC protocols;multipath delay spread;wireless communication;Throughput;Media Access Protocol;Physical layer;Pulse modulation;Access protocols;Delay effects;Multiple access interference;Analytical models;Computational modeling;Performance analysis},
doi={10.1109/INFOCOM.2008.88},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509684,
author={X. Zhong and C. -. Xu},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Online Energy Efficient Packet Scheduling with Delay Constraints in Wireless Networks},
year={2008},
volume={},
number={},
pages={421-429},
abstract={We study online energy-efficient packet scheduling in a multi-user AWGN channel. The objective is to minimize the overall energy consumption in both transmission and circuit by adapting packet transmission rates with individual packet delay constraints. The scheduling is online in the sense that the transmission rates are determined without any assumption about future packet arrivals. We consider a transmitter with a general input and the transmitter communicates with multiple receivers with different distances, which means packets may have different power characteristics and delay constraints. We propose to conduct online rate assignment based on backlogged packets and derive the optimal algorithm in transmitting the packets. We compare the proposed scheduler with existing online algorithms via simulation in terms of energy consumption and algorithms efficiency. Results demonstrate the effectiveness of the online algorithms in striking a better energy-delay trade-off.},
keywords={AWGN channels;multiuser channels;radio networks;scheduling;online energy efficient packet scheduling;delay constraints;wireless networks;multiuser AWGN channel;energy consumption;Energy efficiency;Scheduling algorithm;Wireless networks;Delay;Energy consumption;Circuits;Transmitters;Timing;Communications Society;AWGN channels},
doi={10.1109/INFOCOM.2008.89},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509685,
author={S. Kwon and N. B. Shroff},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Unified Energy-Efficient Routing for Multi-Hop Wireless Networks},
year={2008},
volume={},
number={},
pages={430-438},
abstract={In this paper, we develop an energy-efficient routing scheme that takes into account three key wireless system elements: transmission power; interference; and residual energy. Since energy is a scarce resource, many energy-aware routing algorithms have been proposed to improve network performance. However, previous algorithms have been designed for a subset of these three main elements, which could limit their applicability. Thus, our contribution is here to develop a unified routing algorithm called the Energy-efficient Unified Routing (EURo) algorithm that accommodates any combination of these above key elements. We show via simulations that EURo outperforms the state-of-the-art.},
keywords={radio networks;radiofrequency interference;telecommunication network routing;unified energy-efficient routing scheme;multihop wireless network;wireless system element;Energy efficiency;Routing;Spread spectrum communication;Wireless networks;Interference;Wireless mesh networks;Batteries;Communications Society;Signal to noise ratio;Wireless sensor networks},
doi={10.1109/INFOCOM.2008.90},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509686,
author={D. Jung and A. Savvides},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={An Energy Efficiency Evaluation for Sensor Nodes with Multiple Processors, Radios and Sensors},
year={2008},
volume={},
number={},
pages={439-447},
abstract={This paper constructs a model for studying the energy efficiency of sensor node architectures featuring a pair of low-end, low-power processor and radio and a pair of high-end, energy efficient processor and radio. Such nodes can have a highly dynamic range of operation ranging from the collection of simple temperature measurements or motion detection all the way up to sophisticated signal processing of sensor data. Our model explores the energy efficiency tradeoffs related to the decision of which processor and which radio should be used for each task. For this we derive a general Semi-Markov Decision process model for maximizing the asymptotic lifetime of two alternate designs, one with dynamic and one with static interconnect. The resulting models are validated with simulation and are applied to the reported measurements from an existing platform with two radios and two processors. Our results show how to quantify the gains of such design with respect to the power consumption properties of each component. Furthermore, based on our power budget calculation, we conclude that the deisgn of reconfigurable interconnect between multiple processors and radios would result in efficiency gains despite the energy overhead such device may incur.},
keywords={energy conservation;Markov processes;wireless sensor networks;energy efficiency evaluation;sensor nodes;low end processor;low power processor;energy efficient processor;energy efficient radio;semi-Markov decision process;power budget calculation;Energy efficiency;Peer to peer computing;Temperature sensors;Dynamic range;Energy consumption;Costs;Hardware;Communications Society;Temperature measurement;Motion detection},
doi={10.1109/INFOCOM.2008.91},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509687,
author={R. Banner and A. Orda},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Multi-Objective Topology Control in Wireless Networks},
year={2008},
volume={},
number={},
pages={448-456},
abstract={Topology control is the task of establishing an efficient underlying graph for ad-hoc networks over which high level routing protocols are implemented. The following design goals are of fundamental importance for wireless topologies: (1) low level of interference; (2) minimum energy consumption; (3) high spatial reuse; (4) connectivity; (5) planarity; (6) sparseness; (7) symmetry; (8) small nodal degree; (9) communication-efficient and localized construction. Previous topology control algorithms have usually been designed to provide good performance guarantees in the worst case. Yet, since design goals often conflict, it is usually impossible to construct a single structure that concurrently addresses a large number of goals efficiently. On the other hand, in this paper we show that a substantially larger number of design goals can concurrently be addressed when "pathological" worst case scenarios are ignored. Accordingly, we focus on average performance and establish a protocol that satisfies all the above design goals. Specifically, we formally prove the efficiency of our protocol with respect to all design goals except for high spatial reuse and small nodal degree, for which this is demonstrated by way of simulations. We note that minimum energy consumption and low level of interference have been the main targets of topology control, and our protocol is proven to offer salient performance guarantees with respect to both.},
keywords={ad hoc networks;graph theory;routing protocols;telecommunication network topology;multi-objective topology control;wireless networks;ad-hoc networks;minimum energy consumption;high spatial reuse;connectivity;planarity;sparseness;symmetry;small nodal degree;communication-efficient and localized construction;Network topology;Wireless networks;Interference;Energy consumption;Communication system control;Communications Society;Ad hoc networks;Routing protocols;Algorithm design and analysis;Pathology},
doi={10.1109/INFOCOM.2008.92},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509688,
author={J. Chabarek and J. Sommers and P. Barford and C. Estan and D. Tsiang and S. Wright},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Power Awareness in Network Design and Routing},
year={2008},
volume={},
number={},
pages={457-465},
abstract={Exponential bandwidth scaling has been a fundamental driver of the growth and popularity of the Internet. However, increases in bandwidth have been accompanied by increases in power consumption, and despite sustained system design efforts to address power demand, significant technological challenges remain that threaten to slow future bandwidth growth. In this paper we describe the power and associated heat management challenges in today's routers. We advocate a broad approach to addressing this problem that includes making power-awareness a primary objective in the design and configuration of networks, and in the design and implementation of network protocols. We support our arguments by providing a case study of power demands of two standard router platforms that enables us to create a generic model for router power consumption. We apply this model in a set of target network configurations and use mixed integer optimization techniques to investigate power consumption, performance and robustness in static network design and in dynamic routing. Our results indicate the potential for significant power savings in operational networks by including power-awareness.},
keywords={Internet;power aware computing;protocols;telecommunication network routing;power awareness;network design;network routing;exponential bandwidth scaling;Internet;power consumption;network protocols;Routing;Bandwidth;Energy consumption;Power demand;Internet;Energy management;Power system management;Protocols;Design optimization;Robustness},
doi={10.1109/INFOCOM.2008.93},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509689,
author={D. Xu and M. Chiang and J. Rexford},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Link-State Routing with Hop-by-Hop Forwarding Can Achieve Optimal Traffic Engineering},
year={2008},
volume={},
number={},
pages={466-474},
abstract={Link-state routing with hop-by-hop forwarding is widely used in the Internet today. The current versions of these protocols, like OSPF, split traffic evenly over shortest paths based on link weights. However, optimizing the link weights for OSPF to the offered traffic is an NP-hard problem, and even the best setting of the weights can deviate significantly from an optimal distribution of the traffic. In this paper, we propose a new link-state routing protocol, PEFT, that splits traffic over multiple paths with an exponential penalty on longer paths. Unlike its predecessor, DEFT, our new protocol provably achieves optimal traffic engineering while retaining the simplicity of hop-by-hop forwarding. A gain of 15 % in capacity utilization over OSPF is demonstrated using the Abilene topology and traffic traces. The new protocol also leads to significant reduction in the time needed to compute the best link weights. Both the protocol and the computational methods are developed in a new conceptual framework, called network entropy maximization, which is used to identify the traffic distributions that are not only optimal but also realizable by link-state routing.},
keywords={computational complexity;Internet;routing protocols;telecommunication traffic;link-state routing;hop-by-hop forwarding;optimal traffic engineering;Internet;NP-hard problem;link-state routing protocol;Abilene topology;traffic traces;network entropy maximization;Telecommunication traffic;Tellurium;Routing protocols;Computer networks;Cost function;Entropy;Multiprotocol label switching;Communications Society;Internet;NP-hard problem},
doi={10.1109/INFOCOM.2008.94},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509690,
author={H. Zlatokrilov and H. Levy},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Area Avoidance Routing in Distance-Vector Networks},
year={2008},
volume={},
number={},
pages={475-483},
abstract={Network routing may be required, under certain applications, to avoid certain areas (or nodes). These areas can be of potential security threat, possess poor quality or have other undesired characteristics. Thus, protocols that can perform area avoidance routing can be beneficial for many objectives. Such routing is particularly challenging in distance-vector networks, where only the shortest-distance information is available to the nodes. We address this challenge by algorithms that retrieve distance-vector information from other nodes in the network, referred to as reference nodes, and exploit it for computing guaranteed area-avoiding paths. Having these paths, the source can direct the packets using loose source routing towards the destination. We lay out the model for area avoidance routing and study several algorithms for calculating area-avoiding paths. In addition, we address the problem of dynamically selecting reference nodes. We show, through analysis and extensive simulation, that in many cases a small number of reference nodes are sufficient for area avoidance routing.},
keywords={ad hoc networks;telecommunication network routing;telecommunication security;area avoidance routing;distance-vector networks;shortest-distance information;Peer to peer computing;Routing protocols;Information security;Information retrieval;Computer networks;Terminology;Communications Society;Computer science;Application software;Computational modeling},
doi={10.1109/INFOCOM.2008.95},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509691,
author={L. Ying and R. Srikant and D. Towsley},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Cluster-Based Back-Pressure Routing Algorithm},
year={2008},
volume={},
number={},
pages={484-492},
abstract={We study scalable, distributed, and adaptive routing algorithms for communication networks. The back-pressure algorithm introduced in [21] is a well-known distributed and adaptive routing/scheduling algorithm where nodes only need the queue length information of neighboring nodes to make routing decisions, and packets are adaptively routed in the network according to congestion information, which makes the algorithm resilient to traffic and topology changes. However, the back-pressure algorithm requires routers to maintain a separate queue for each destination, which prevents its implementation in large-scale networks like the Internet. In this paper, we propose a cluster-based back-pressure routing algorithm, which retains the distributability and adaptability of back-pressure routing, while significantly reducing the number of queues that have to be maintained at each node. Since the cluster-based algorithm performs adaptive load-balancing in the network, it has the potential to eliminate the need for off-line traffic engineering in the Internet.},
keywords={Internet;resource allocation;telecommunication network routing;workstation clusters;cluster-based back-pressure routing algorithm;communication networks;queue length information;congestion information;adaptive load-balancing;Internet;Routing;Clustering algorithms;Scheduling algorithm;Telecommunication traffic;Traffic control;IP networks;Communication networks;Network topology;Large-scale systems;Maintenance engineering},
doi={10.1109/INFOCOM.2008.96},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509692,
author={Q. Gao and J. Zhang and S. V. Hanly},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Cross-Layer Rate Control in Wireless Networks with Lossy Links: Leaky-Pipe Flow, Effective Network Utility Maximization and Hop-by-Hop Algorithms},
year={2008},
volume={},
number={},
pages={493-501},
abstract={Due to multi-path fading and co-channel interference, wireless links are lossy in nature. As a result, the data rate of a given flow becomes "thinner and thinner" along its routing path, and the data rate received successfully at the destination node (<i>the</i> <i>effective</i> <i>rate</i>) is typically lower than the transmission rate at the source node (<i>the</i> <i>injection</i> <i>rate</i>). In light of this observation, each flow is treated as a "leaky-pipe" model in this study. Moreover, we introduce the notion of "effective utility" associated with the effective rate (not the injection rate) for each flow, and explore rate control mechanisms through effective network utility maximization (ENUM). We focus on two network models: (1) ENUM with link outage constraints with a maximum error rate at each link; (2) ENUM with path outage constraints where there exists an end-to-end outage requirement for each flow. For both problems, we explicitly take into account the "thinning" feature of data flows and devise distributed hop-by-hop rate control algorithms accordingly. Our numerical examples corroborate that higher effective network utility and better fairness among effective flow rates can be achieved by the ENUM algorithms than the standard NUM.},
keywords={cochannel interference;fading channels;multipath channels;radio links;telecommunication network routing;cross-layer rate control;wireless network utility maximization;lossy links;leaky-pipe flow;hop-by-hop algorithm;multipath fading;co-channel interference;Wireless networks;Utility programs;Fading;Peer to peer computing;Internet;Transport protocols;Scalability;Communications Society;Communication system control;Interchannel interference},
doi={10.1109/INFOCOM.2008.97},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509693,
author={B. Rengarajan and G. de Veciana},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Architecture and Abstractions for Environment and Traffic Aware System-Level Coordination of Wireless Networks: The Downlink Case},
year={2008},
volume={},
number={},
pages={502-510},
abstract={Two ways to substantially enhance wireless broadband capacity are full frequency reuse and smaller cells, both of which result in operational regimes that are highly dynamic and interference limited. This paper presents a system-level approach to interference management, that has reasonable backhaul communication and computation requirements. The basis for the approach is clustering and aggregation of measurements of the spatial diversity in sensitivity to interference associated with average user populations. This enables the system to exchange information and optimize coordinated transmission schedules using only coarse grained data. The paper explores various ways of optimizing such schedules: from a static, decoupled version to a dynamic version capturing user-level scheduling, fluctuating loads and inter-cell interference that couples base stations' performance. Based on extensive system-level simulations, we demonstrate reductions in file transfer delay ranging from 20-80%, from light to heavy loads, as compared to a simple baseline not unlike those in the field today. This improvement is achieved while providing more uniform coverage, and reducing base station power consumption by up to 45%.},
keywords={broadband networks;cellular radio;diversity reception;interference suppression;optimisation;scheduling;telecommunication network management;telecommunication traffic;traffic aware system-level coordination;wireless network;wireless broadband capacity;full frequency reuse;interference management;backhaul communication;clustering approach;spatial diversity;coordinated transmission schedule optimization;dynamic version capturing user-level scheduling;inter-cell interference;file transfer delay;Telecommunication traffic;Wireless networks;Downlink;Interference;Dynamic scheduling;Base stations;Frequency;Processor scheduling;Couplings;Delay},
doi={10.1109/INFOCOM.2008.98},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509694,
author={R. Cohen and L. Katzir},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Computational Analysis and Efficient Algorithms for Micro and Macro OFDMA Scheduling},
year={2008},
volume={},
number={},
pages={511-519},
abstract={OFDMA is one of the most important modulation and access methods for the future mobile networks. Before transmitting a frame on the downlink, an OFDMA base station has to invoke an algorithm that determines which of the pending packets will be transmitted, what modulation should be used for each of them, and how to construct the complex OFDMA frame matrix as a collection of rectangles that fit into a single matrix with fixed dimensions. We propose efficient, and theoretically best possible, algorithms that solves this intricate OFDMA scheduling problem by breaking it down into two sub-problems, referred to as macro and micro scheduling. We analyze the computational complexity of these sub-problems and develop efficient algorithms for solving them.},
keywords={computational complexity;mobile radio;OFDM modulation;scheduling;OFDMA scheduling;mobile networks;OFDMA base station;computational complexity;Algorithm design and analysis;Scheduling algorithm;Processor scheduling;Downlink;Base stations;OFDM modulation;Throughput;Frequency conversion;Forward error correction;Bandwidth},
doi={10.1109/INFOCOM.2008.99},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509695,
author={U. C. Kozat},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={On the Throughput Capacity of Opportunistic Multicasting with Erasure Codes},
year={2008},
volume={},
number={},
pages={520-528},
abstract={In this paper, we concentrate on opportunistic scheduling for multicast information. We pose the problem as a multicast throughput optimization problem. As a solution we present how one can jointly utilize fixed-rate and rateless erasure coding along with simple rate adaptation techniques in order to achieve the optimal multicast throughput per user. We first investigate the performance of the proposed system under i.i.d. channel conditions. Our analysis shows a linear gain for the multicast capacity over i.i.d. Rayleigh fading channels with respect to the number of users. Since the established results require coding over large number of blocks and hence induce large decoding delays, we extend our analysis to the cases where we code over shorter block lengths and thus quantify the delay-capacity tradeoffs under a simple setting. We further look into non-i.i.d. channel conditions and show achievable gains by modifying a scheduling heuristic whose fairness is well- established for opportunistic scheduling of unicast flows. Our overall evaluations demonstrate that under both i.i.d. and non-i.i.d. channel conditions, opportunistic multicasting with erasure coding can significantly improve the performance over the traditional techniques used in today's communication systems.},
keywords={block codes;cellular radio;channel capacity;channel coding;decoding;diversity reception;multicast communication;multiuser channels;optimisation;Rayleigh channels;scheduling;opportunistic scheduling;multicast throughput optimization problem;fixed-rate erasure coding;rateless erasure coding;rate adaptation technique;multicast channel capacity;Rayleigh fading channels;decoding;multiuser diversity;cellular network;block code;Throughput;Base stations;Decoding;Delay;Unicast;Communications Society;USA Councils;Gain;Fading;Wireless networks},
doi={10.1109/INFOCOM.2008.100},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509696,
author={S. Khanna and S. S. Venkatesh and O. Fatemieh and F. Khan and C. A. Gunter},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Adaptive Selective Verification},
year={2008},
volume={},
number={},
pages={529-537},
abstract={We consider Denial of Service (DoS) attacks within the province of a shared channel model in which attack rates may be large but are bounded and client request rates vary within fixed bounds. In this setting it is shown that the clients can respond effectively to an attack by using bandwidth as a payment scheme and time-out windows to adaptively boost request rates. The server will be able to process client requests with high probability while pruning out most of the attack by selective random sampling. Our protocol, which we call Adaptive Selective Verification (ASV) is shown to be efficient in terms of bandwidth consumption using both a theoretical model and network simulations. It differs from previously-investigated adaptive mechanisms for bandwidth-based payment by requiring very limited state on the server.},
keywords={bandwidth allocation;client-server systems;formal verification;probability;protocols;telecommunication security;adaptive selective verification;DoS attacks;denial-of-service attacks;client-server system;probability;ASV protocol;bandwidth consumption;Bandwidth;Protocols;Computer crime;Network servers;Sampling methods;Web server;Costs;Protection;Performance analysis;Communications Society},
doi={10.1109/INFOCOM.2008.101},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509697,
author={X. Zou and Y. -. Dai and E. Bertino},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Practical and Flexible Key Management Mechanism For Trusted Collaborative Computing},
year={2008},
volume={},
number={},
pages={538-546},
abstract={Trusted collaborative computing (TCC) is a new research and application paradigm. Two important challenges in such a context are represented by secure information transmission among the collaborating parties and selective differentiated access to data among members of collaborating groups. Addressing such challenges requires, among other things, developing techniques for secure group communication (SGQ), secure dynamic conferencing (SDC), differential access control (DIF-AC), and hierarchical access control (HAC). Cryptography and key management have been intensively investigated and widely applied in order to secure information. However, there is a lack of key management mechanisms which are general and flexible enough to address all requirements arising from information transmission and data access. This paper proposes the first holistic group key management scheme which can directly support all these functions yet retain efficiency. The proposed scheme is based on the innovative concept of access control polynomial (ACP) that can efficiently and effectively support full dynamics, flexible access control with fine-tuned granularity, and anonymity. The new scheme is immune from various attacks from both external and internal malicious parties.},
keywords={authorisation;cryptography;groupware;flexible key management mechanism;trusted collaborative computing;secure information transmission;secure group communication;secure dynamic conferencing;differential access control;hierarchical access control;cryptography;access control polynomial;Collaboration;Access control;Computer science;USA Councils;Cryptography;Context;Communication system security;Resource management;Collaborative work;Data security},
doi={10.1109/INFOCOM.2008.102},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509698,
author={T. Xu and Y. Cai},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Exploring Historical Location Data for Anonymity Preservation in Location-Based Services},
year={2008},
volume={},
number={},
pages={547-555},
abstract={We present a new approach for if-anonymity protection in Location-Based Services (LBSs). Specifically, we depersonalize location information by ensuring that each location reported for LBSs is a cloaking area that contains K different footprints-historical locations of different mobile nodes. Therefore, the exact identity and location of the service requestor remain anonymous from LBS service providers. Existing techniques, on the other hand, compute the cloaking area using current locations of K neighboring hosts of the service requestor. Because of this difference, our approach significantly reduces the cloaking area, which in turn decreases query processing and communication overhead for returning query results to the requesting host. In addition, existing techniques also require frequent location updates from all nodes, regardless of whether or not these nodes are requesting LBSs. Most importantly, our approach is the first practical solution that provides K-anonymity trajectory protection needed to ensure anonymity when a mobile host requests LBSs continuously as it moves. Our solution depersonalizes a user's trajectory (a time-series of the user's locations) based on the historical trajectories of other users. We evaluate our techniques under various conditions using location data synthetically generated based on real road maps. The results show that our techniques can provide K-anonymity trajectory protection using a minimized cloaking area.},
keywords={mobile computing;security of data;anonymity preservation;location-based services;mobile node;historical location data;query processing;minimized cloaking area;K-anonymity trajectory protection;Protection;Peer to peer computing;Computer science;Network servers;Communications Society;Query processing;Roads;Large-scale systems;Tracking;Mobile communication},
doi={10.1109/INFOCOM.2008.103},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509699,
author={Z. Zhong and L. Ramaswamy and K. Li},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={ALPACAS: A Large-Scale Privacy-Aware Collaborative Anti-Spam System},
year={2008},
volume={},
number={},
pages={556-564},
abstract={While the concept of collaboration provides a natural defense against massive spam emails directed at large numbers of recipients, designing effective collaborative anti-spam systems raises several important research challenges. First and foremost, since emails may contain confidential information, any collaborative anti-spam approach has to guarantee strong privacy protection to the participating entities. Second, the continuously evolving nature of spam demands the collaborative techniques to be resilient to various kinds of camouflage attacks. Third, the collaboration has to be lightweight, efficient, and scalable. Towards addressing these challenges, this paper presents ALPACAS - a privacy-aware framework for collaborative spam filtering. In designing the ALPACAS framework, we make two unique contributions. The first is a feature-preserving message transformation technique that is highly resilient against the latest kinds of spam attacks. The second is a privacy-preserving protocol that provides enhanced privacy guarantees to the participating entities. Our experimental results conducted on a real email dataset shows that the proposed framework provides a 10 fold improvement in the false negative rate over the Bayesian-based Bogofilter when faced with one of the recent kinds of spam attacks. Further, the privacy breaches are extremely rare. This demonstrates the strong privacy protection provided by the ALPACAS system.},
keywords={data privacy;groupware;information filtering;protocols;unsolicited e-mail;large-scale privacy-aware collaborative anti spam system;ALPACAS;massive spam emails;data privacy protection;collaborative spam filtering;feature-preserving message transformation technique;privacy-preserving protocol;Large-scale systems;Collaboration;Privacy;Fingerprint recognition;Information filtering;Information filters;Protection;Collaborative work;Computer science;Bayesian methods},
doi={10.1109/INFOCOM.2008.104},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509700,
author={G. Zhou and J. Lu and C. -. Wan and M. D. Yarvis and J. A. Stankovic},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={BodyQoS: Adaptive and Radio-Agnostic QoS for Body Sensor Networks},
year={2008},
volume={},
number={},
pages={565-573},
abstract={As wireless devices and sensors are increasingly deployed on people, researchers have begun to focus on wireless body-area networks. Applications of wireless body sensor networks include healthcare, entertainment, and personal assistance, in which sensors collect physiological and activity data from people and their environments. In these body sensor networks, quality of service is needed to provide reliable data communication over prioritized data streams. This paper proposes BodyQoS, the first running QoS system demonstrated on an emulated body sensor network. BodyQoS adopts an asymmetric architecture, in which most processing is done on a resource rich aggregator, minimizing the load on resource limited sensor nodes. A virtual MAC is developed in BodyQoS to make it radio-agnostic, allowing a BodyQoS to schedule wireless resources without knowing the implementation details of the underlying MAC protocols. Another unique property of BodyQoS is its ability to provide adaptive resource scheduling. When the effective bandwidth of the channel degrades due to RF interference or body fading effect, BodyQoS adaptively schedules remaining bandwidth to meet QoS requirements. We have implemented BodyQoS in NesC on top of TinyOS, and evaluated its performance on MicaZ devices. Our system performance study shows that BodyQoS delivers significantly improved performance over conventional solutions in combating channel impairment.},
keywords={access protocols;personal area networks;quality of service;wireless sensor networks;BodyQoS;adaptive QoS;Radio-Agnostic QoS;wireless body sensor networks;quality of service;virtual MAC;data communication;Body sensor networks;Wireless sensor networks;Quality of service;Media Access Protocol;Bandwidth;Medical services;Telecommunication network reliability;Data communication;Wireless application protocol;Adaptive scheduling},
doi={10.1109/INFOCOM.2008.105},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509701,
author={L. Huang and S. Setia},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={CORD: Energy-Efficient Reliable Bulk Data Dissemination in Sensor Networks},
year={2008},
volume={},
number={},
pages={574-582},
abstract={This paper presents CORD, a reliable bulk data dissemination protocol for propagating a large data object to all the nodes in a large scale sensor network. Unlike well- known reliable data dissemination protocols such as Deluge whose primary design criterion is to reduce the latency of object propagation, CORD's primary goal is to minimize energy consumption. To achieve its goals CORD employs a two phase approach in which the object is delivered to a subset of nodes in the network that form a connected dominating set in the first phase, and to the remaining nodes in the second phase. Further, CORD installs a coordinated sleep schedule on the nodes in the network whereby nodes that are not involved in receiving or transmitting data can turn off their radios to reduce their energy consumption. We evaluated the performance of CORD experimentally on both an indoor and outdoor sensor network testbed and via extensive simulations. Our results show that in comparison to Deluge (the de facto network reprogramming protocol for TinyOS) CORD significantly reduces the energy consumption for reliable data dissemination while achieving a comparable latency.},
keywords={data communication;protocols;wireless sensor networks;CORD;energy-efficient reliable bulk data dissemination protocol;sensor networks;coordinated sleep schedule;core based reliable dissemination;Energy efficiency;Protocols;Peer to peer computing;Energy consumption;Computer network reliability;Sleep;Telecommunication network reliability;Large-scale systems;Delay;Communications Society},
doi={10.1109/INFOCOM.2008.106},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509702,
author={J. Jeong and S. Guo and T. He and D. Du},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={APL: Autonomous Passive Localization for Wireless Sensors Deployed in Road Networks},
year={2008},
volume={},
number={},
pages={583-591},
abstract={In road networks, sensors are deployed sparsely (hundreds of meters apart) to save costs. This makes the existing localization solutions based on the ranging be ineffective. To address this issue, this paper introduces an autonomous passive localization scheme, called APL. Our work is inspired by the fact that vehicles move along routes with a known map. Using binary vehicle-detection timestamps, we can obtain distance estimates between any pair of sensors on roadways to construct a virtual graph composed of sensor identifications (i.e., vertices) and distance estimates (i.e., edges). The virtual graph is then matched with the topology of road map, in order to identify where sensors are located in roadways. We evaluate our design outdoor in Minnesota roadways and show that our distance estimate method works well despite of traffic noises. In addition, we show that our localization scheme is effective in a road network with eighteen intersections, where we found no location matching error, even with a maximum sensor time synchronization error of 0.3 sec and the vehicle speed deviation of 10 km/h.},
keywords={graph theory;road vehicles;telecommunication network topology;wireless sensor networks;autonomous passive localization scheme;wireless sensor network;road network topology;binary vehicle-detection timestamp;virtual graph;distance estimation method;location matching error;maximum sensor time synchronization error;Wireless sensor networks;Costs;Road vehicles;Vehicle detection;Surveillance;Communications Society;Helium;Computer science;Remotely operated vehicles;Network topology},
doi={10.1109/INFOCOM.2008.107},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509703,
author={Y. Zhu and L. M. Ni},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Probabilistic Approach to Provisioning Guaranteed QoS for Distributed Event Detection},
year={2008},
volume={},
number={},
pages={592-600},
abstract={It has been of significant importance to provision network-wide guaranteed QoS for a wide range of event detection applications in wireless sensor networks (WSNs). This paper investigates solutions to this QoS provision problem. For event detection applications, there are two key performance metrics, i.e., detection probability and detection latency. This paper focuses on dual-objective QoS provision, taking both metrics into account. This is very challenging due to the stringent resource constraint of sensor nodes and unpredictable randomness of physical events. We propose a novel probabilistic approach to provisioning due-objective QoS. Following a unified framework, we design a distributed algorithm that determines the active probability of every sensor node. The probability is minimized while being sufficient for QoS provision. Our approach is flexible and supports different requirements that may be posed by different applications. Theoretical analysis and comprehensive simulation experiments have been conducted, which jointly demonstrate that our approach is able to deliver guaranteed QoS for distributed event detection while prolonging the system lifetime significantly compared with other alternative schemes.},
keywords={distributed algorithms;probability;quality of service;wireless sensor networks;distributed event detection;wireless sensor network;probability;dual-objective QoS provision;distributed algorithm;Event detection;Quality of service;Peer to peer computing;Wireless sensor networks;Energy consumption;Delay;Sensor systems;Energy conservation;Algorithm design and analysis;Distributed algorithms},
doi={10.1109/INFOCOM.2008.108},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509704,
author={M. Durvy and O. Dousse and P. Thiran},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Border Effects, Fairness, and Phase Transition in Large Wireless Networks},
year={2008},
volume={},
number={},
pages={601-609},
abstract={We characterize the fairness of decentralized medium access control protocols based on CSMA/CA, such as IEEE 802.11, in large multi-hop wireless networks. In particular, we show that the widely observed unfairness of the protocol in small network topologies does not always persist in large topologies. This unfairness is essentially due to the unfair advantage of nodes at the border of the network, which have a restricted neighborhood and thus a higher probability to access the communication channel. In large one-dimensional networks these border effects do not propagate inside the network, and nodes sufficiently far away from the border have equal access to the channel; as a result the protocol is long-term fair. In two-dimensional networks, we observe a phase transition. If the access intensity of the protocol is small, the border effects remain local and the protocol behaves similarly as in one- dimensional networks. However, if the access intensity of the protocol is large enough, the border effects persist independently of the size of the network and the protocol is strongly unfair. Finally, in situations where the protocol is long-term fair, we provide a characterization of its short-term fairness.},
keywords={access protocols;radio networks;phase transition;large wireless networks;decentralized medium access control protocols;multihop wireless networks;network topology;Wireless networks;Access protocols;Network topology;Media Access Protocol;Spread spectrum communication;Peer to peer computing;Communication channels;Physics;Communications Society;Laboratories},
doi={10.1109/INFOCOM.2008.109},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509705,
author={S. Merlin and N. Vaidya and M. Zorzi},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Resource Allocation in Multi-Radio Multi-Channel Multi-Hop Wireless Networks},
year={2008},
volume={},
number={},
pages={610-618},
abstract={A joint congestion control, channel allocation and scheduling algorithm for multi-channel multi-interface multi- hop wireless networks is discussed. The goal of maximizing a utility function of the injected traffic, while guaranteeing queue stability, is defined as an optimization problem where the input traffic intensity, channel loads, interface to channel binding and transmission schedules are jointly optimized by a dynamic algorithm. Due to the inherent NP-Hardness of the scheduling problem, a simple centralized heuristic is used to define a lower bound for the performance of the whole optimization algorithm. The behavior of the algorithm for different numbers of channels, interfaces and traffic flows is shown through simulations.},
keywords={channel allocation;computational complexity;optimisation;queueing theory;radio networks;resource allocation;scheduling;stability;telecommunication congestion control;telecommunication traffic;utility theory;resource allocation;multi radio multi channel multi hop wireless network;joint congestion control;channel allocation;scheduling algorithm;utility function maximization;network traffic queue stability;NP-hardness;optimization algorithm;Resource management;Spread spectrum communication;Wireless networks;Scheduling algorithm;Traffic control;Communication system traffic control;Channel allocation;Stability;Dynamic scheduling;Heuristic algorithms},
doi={10.1109/INFOCOM.2008.110},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509706,
author={U. Akyol and M. Andrews and P. Gupta and J. Hobby and I. Saniee and A. Stolyar},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Joint Scheduling and Congestion Control in Mobile Ad-Hoc Networks},
year={2008},
volume={},
number={},
pages={619-627},
abstract={In this paper we study the problem of jointly performing scheduling and congestion control in mobile ad-hoc networks so that network queues remain bounded and the resulting flow rates satisfy an associated network utility maximization problem. In recent years a number of papers have presented theoretical solutions to this problem that are based on combining differential-backlog scheduling algorithms with utility-based congestion control. However, this work typically does not address a number of issues such as how signaling should be performed and how the new algorithms interact with other wireless protocols. In this paper we address such issues. In particular: ldr We define a specific network utility maximization problem that we believe is appropriate for mobile adhoc networks. ldr We describe a wireless greedy primal dual (wGPD) algorithm for combined congestion control and scheduling that aims to solve this problem. ldr We show how the wGPD algorithm and its associated signaling can be implemented in practice with minimal disruption to existing wireless protocols. ldr We show via OPNET simulation that wGPD significantly outperforms standard protocols such as 802.11 operating in conjunction with TCP. This work was supported by the DARPA CBMANET program.},
keywords={ad hoc networks;greedy algorithms;mobile radio;optimisation;protocols;queueing theory;scheduling;telecommunication congestion control;telecommunication traffic;scheduling;congestion control;mobile ad-hoc networks;network queue;network utility maximization problem;wireless protocol;wireless greedy primal dual algorithm;Ad hoc networks;Utility programs;Scheduling algorithm;Communication system control;Wireless application protocol;Network servers;Web server;Aggregates;Wireless networks;Communications Society},
doi={10.1109/INFOCOM.2008.111},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509707,
author={H. -. Dai and K. -. Ng and R. -. Wong and M. -. Wu},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={On the Capacity of Multi-Channel Wireless Networks Using Directional Antennas},
year={2008},
volume={},
number={},
pages={628-636},
abstract={The capacity of wireless ad hoc networks is affected by two key factors: the interference among concurrent transmissions and the number of simultaneous transmissions on a single interface. Recent studies found that using multiple channels can separate concurrent transmissions and greatly improve network throughput. However, those studies only consider that wireless nodes are equipped with only omnidirectional antennas, which cause high collisions. On the other hand, some researchers found that directional antennas bring more benefits such as reduced interference and increased spatial reuse compared with omnidirectional antennas. But, they only focused on a single-channel network which only allows finite concurrent transmissions. Thus, combining the two technologies of multiple channels and directional antennas together potentially brings more benefits. In this paper, we propose a multi-channel network architecture (called MC-MDA) that equips each wireless node with multiple directional antennas. We derive the capacity bounds of MC-MDA networks under arbitrary and random placements. We will show that deploying directional antennas to multi-channel networks can greatly improve the network capacity due to increased network connectivity and reduced interference. We have also found that even a multi-channel network with a single directional antenna only at each node can give a significant improvement on the throughput capacity. Besides, using multiple channels mitigates interference caused by directional antennas. MC-MDA networks integrate benefits from multi-channel and directional antennas and thus have significant performance improvement.},
keywords={ad hoc networks;directive antennas;interference suppression;multipath channels;telecommunication network management;multichannel wireless networks;directional antennas;wireless ad hoc networks;interference;finite concurrent transmissions;multichannel network architecture;network capacity;Wireless networks;Directional antennas;Throughput;Interference;Directive antennas;Peer to peer computing;Mobile ad hoc networks;Network interfaces;Receiving antennas;Transmitting antennas},
doi={10.1109/INFOCOM.2008.112},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509708,
author={D. -. Lee and C. -. Chang and J. Cheng and H. -. Yan},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Queueing Analysis of Loss Systems with Variable Optical Delay Lines},
year={2008},
volume={},
number={},
pages={637-645},
abstract={A new optical device called variable optical delay line (VODL) has been proposed in the literature. As suggested by its name, the delay of a VODL can be dynamically set within a certain range. Once set, a VODL behaves like a traditional fiber delay line and can admit packets requiring the same delay as that set by the VODL. As in the queueing context, a VODL can thus be viewed as a server that serves packets with the service times equal to the required delays. In this paper, we consider loss systems with parallel VODLs subject to various classes of packet arrivals. Such loss systems are different from the classical loss systems as a VODL, even when occupied, can still admit new packets with the same delay. For the case with an infinite number of VODLs, we show that the number of VODLs occupied by different classes of packets still has a product form solution. However, the analysis for the case with a finite number of VODLs is much more difficult. For this, we propose an approximation method based on state truncation. We show that the packet loss probabilities derived from our approximation are very close to those generated from simulations. In order to minimize the packet loss probabilities in such loss systems, we also consider the problem of assigning dedicated VODLS to various classes of packets. We show under the light traffic condition, the complete sharing policy, i.e., the policy that does not assign any dedicated VODLs, is optimal. For the general traffic condition, we propose a greedy search algorithm to find a suboptimal assignment of dedicated VODLS. Simulation results show that our greedy algorithm yields very good assignments when comparing with the optimal ones.},
keywords={approximation theory;optical delay lines;optical fibre losses;optical fibre networks;probability;queueing theory;search problems;variable optical delay line;queueing analysis;loss system;approximation method;state truncation;packet loss probability;greedy search algorithm;traffic condition;Queueing analysis;Optical losses;Delay lines;Optical buffering;Optical packet switching;Optical signal processing;High speed optical techniques;Optical devices;Optical fiber networks;Communication switching},
doi={10.1109/INFOCOM.2008.113},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509709,
author={B. Moon},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Emergency Handling in Ethernet Passive Optical Networks Using Priority-Based Dynamic Bandwidth Allocation},
year={2008},
volume={},
number={},
pages={646-654},
abstract={Ethernet Passive Optical Networks (EPON) is a prominent solution for broadband access networks. However, little work has been done on emergency handling in EPON. In this paper, to manage emergency cases causing severe network traffic, the network state is divided into two states, i.e., the <i>normal</i> and the <i>emergency</i> states. The authors apply new priority- based Dynamic Bandwidth Allocation (DBA) algorithms to the emergency state, each of which gives high priority to some Optical Network Units (ONUs) corresponding to important agencies. The proposed algorithms are analyzed in terms of utilization of the uplink bandwidth and the packet delay. Simulation and theoretical results show that the proposed algorithms are working properly at the emergency state and can satisfy more stringent QoS requirements than the conventional Weighted Round Robin (WRR)-based algorithms.},
keywords={bandwidth allocation;broadband networks;optical fibre LAN;optical fibre subscriber loops;quality of service;telecommunication network management;telecommunication traffic;Ethernet passive optical network;priority-based dynamic bandwidth allocation;emergency handling;broadband access network;network traffic;optical network unit;uplink bandwidth utilization;quality of service;QoS requirement;Channel allocation;Optical network units;EPON;Traffic control;Passive optical networks;Round robin;Ethernet networks;Communication system traffic control;Time division multiplexing;Bandwidth},
doi={10.1109/INFOCOM.2008.114},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509710,
author={F. Solano and R. Van Caenegem and D. Colle and J. L. Marzo and M. Pickavet and R. Fabregat and P. Demeester},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={All-Optical Label Stacking: Easing the Trade-offs Between Routing and Architecture Cost in All-Optical Packet Switching},
year={2008},
volume={},
number={},
pages={655-663},
abstract={All-optical label swapping (AOLS) forms a key technology towards the implementation of all-optical packet switching nodes (AOPS) for the future optical Internet. The capital expenditures of the deployment of AOLS increases with the size of the label spaces (i.e. the number of used labels), since a special optical device is needed for each recognized label on every node. Label space sizes are affected by the way in which demands are routed. For instance, while shortest-path routing leads to the usage of fewer labels but high link utilization, minimum interference routing leads to the opposite. This paper studies all-optical label stacking (AOLStack), which is an extension of the AOLS architecture. AOLStack aims at reducing label spaces while easing the compromise with link utilization. In this paper, an integer lineal program is proposed with the objective of analyzing the softening of the aforementioned trade-off due to AOLStack. Furthermore, a heuristic aiming at finding good solutions in polynomial-time is proposed as well. Simulation results show that AOLStack either a) reduces the label spaces with a low increase in the link utilization or, similarly, b) uses better the residual bandwidth to decrease the number of labels even more.},
keywords={computational complexity;integer programming;optical communication;packet switching;telecommunication network routing;telecommunication network topology;all-optical label stacking;routing cost;architecture cost;all-optical packet switching;integer lineal program;polynomial-time;all-optical label swapping;Stacking;Routing;Costs;Packet switching;Space technology;Multiprotocol label switching;Optical packet switching;Internet;Optical devices;Interference},
doi={10.1109/INFOCOM.2008.115},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509711,
author={J. Cheng and C. -. Chang and T. -. Chao and D. -. Lee and C. -. Lien},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={On Constructions of Optical Queues with a Limited Number of Recirculations},
year={2008},
volume={},
number={},
pages={664-672},
abstract={Recently, there has been a lot of attention on the constructions of optical queues by using optical Switches and fiber Delay Lines (SDL). In this paper, we consider the constructions of optical queues with a limited number of recirculations through the fibers in such SDL constructions. Such a limitation on the number of recirculations comes from practical feasibility considerations, such as crosstalk, power loss, amplified spontaneous emission (ASE) from the Erbium doped fiber amplifiers (EDFA), and the pattern effect of the optical switches. We first transform the design of the fiber delays in such SDL constructions to an equivalent integer representation problem. Specifically, given 1 les k les M, we seek for an M-sequence d<sup>M</sup> <sub>1</sub> = (d<sub>1</sub>,d<sub>2</sub>,...,d<sub>m</sub>) of positive integers to maximize the number of consecutive integers (starting from 0) that can be represented by the C-transform relative to d<sup>M</sup> <sub>1</sub> such that there are at most k 1-entries in their C-transforms. Then we give a class of greedy constructions so that d<sub>1</sub>, d<sub>2</sub>,..., d<sub>M</sub> are obtained recursively and the maximum number of representable consecutive integers by using d<sub>1</sub>,d<sub>2</sub>,...,d<sub>i</sub> is larger than that by using d<sub>1</sub>,d<sub>2</sub>,...,d<sub>i-1</sub> for all i. Furthermore, we obtain an explicit recursive expression for d<sub>1</sub>, d<sub>2</sub>,..., d<sub>M</sub> given by a greedy construction. Finally, we show that an optimal M-sequence (in the sense of achieving the maximum number of representable consecutive integers) can be given by a greedy construction. The solution of such an integer representation problem can be applied to the construction of optical 2-to-l FIFO multiplexers with a limited number of recirculations. We show that the complexity of searching for an optimal construction under our routing policy can be greatly reduced from exponential time to polynomial time by only considering the greedy constructions instead of performing an exhaustive search. Similar results can be obtained for linear compressors and linear decompressors with a limited number of recirculations.},
keywords={m-sequences;multiplexing equipment;optical fibre communication;optical switches;queueing theory;telecommunication network routing;optical queues;optical switches;fiber delay lines;SDL constructions;recirculation limitation;integer representation problem;optimal M-sequence;optical 2-to-1 FIFO multiplexers;search complexity;routing policy;Optical crosstalk;Optical switches;Optical fibers;Delay lines;Stimulated emission;Optical fiber losses;Spontaneous emission;Erbium;Doped fiber amplifiers;Erbium-doped fiber amplifier},
doi={10.1109/INFOCOM.2008.116},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509712,
author={E. Altman and K. Avrachenkov and A. Garnaev},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Closed Form Solutions for Symmetric Water Filling Games},
year={2008},
volume={},
number={},
pages={673-681},
abstract={We study power control in optimization and game frameworks. In the optimization framework there is a single decision maker who assigns network resources and in the game framework users share the network resources according to Nash equilibrium. The solution of these problems is based on so-called water-filling technique, which in turn uses bisection method for solution of non-linear equations for Lagrange multipliers. Here we provide a closed form solution to the water-filling problem, which allows us to solve it in a finite number of operations. Also, we produce a closed form solution for the Nash equilibrium in symmetric Gaussian interference game with an arbitrary number of users. Even though the game is symmetric, there is an intrinsic hierarchical structure induced by the quantity of the resources available to the users. We use this hierarchical structure to perform a successive reduction of the game. In addition to its mathematical beauty, the explicit solution allows one to study limiting cases when the crosstalk coefficient is either small or large. We provide an alternative simple proof of the convergence of the iterative water filling algorithm. Furthermore, it turns out that the convergence of Iterative water filling algorithm slows down when the crosstalk coefficient is large. Using the closed form solution, we can avoid this problem. Finally, we compare the non-cooperative approach with the cooperative approach and show that the non-cooperative approach results in a more fair resource distribution.},
keywords={game theory;nonlinear equations;power control;radio access networks;symmetric water filling games;power control;Nash equilibrium;nonlinear equations;Lagrange multipliers;intrinsic hierarchical structure;iterative water filling algorithm;Closed-form solution;Filling;Water resources;Nash equilibrium;Crosstalk;Iterative algorithms;Power control;Nonlinear equations;Lagrangian functions;Interference},
doi={10.1109/INFOCOM.2008.117},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509713,
author={H. Mutlu and M. Alanyali and D. Starobinski},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Spot Pricing of Secondary Spectrum Usage in Wireless Cellular Networks},
year={2008},
volume={},
number={},
pages={682-690},
abstract={Recent deregulation initiatives enable cellular providers to sell excess spectrum for secondary usage. In this paper, we investigate the problem of optimal spot pricing of spectrum by a provider in the presence of both non-elastic primary users, with long-term commitments, and opportunistic, elastic secondary users. We first show that optimal pricing can be formulated as an infinite horizon average reward problem and solved using stochastic dynamic programming. Next, we investigate the design of efficient single pricing policies. We provide numerical and analytical evidences that static pricing policies do not perform well in such settings (in sharp contrast to settings where all the users are elastic). On the other hand, we prove that deterministic threshold pricing achieves optimal profit amongst all single-price policies and performs close to global optimal pricing. We characterize the profit regions of static and threshold pricing, as a function of the arrival rate of primary users. Under certain reasonable assumptions on the demand function, we show that the profit region of threshold pricing can be far larger than that of static pricing. Moreover, we also show that these profit regions critically depend on the support of the demand function rather than specific form of it. We prove that the profit function of threshold pricing is unimodal in price and determine a restricted interval in which the optimal threshold lies. These two properties enable very efficient computation of the optimal threshold policy that is far faster than that of the global optimal policy.},
keywords={cellular radio;dynamic programming;infinite horizon;pricing;radio networks;radio spectrum management;stochastic programming;optimal spot pricing;secondary spectrum usage;wireless cellular networks;nonelastic primary users;opportunistic elastic secondary users;infinite horizon average reward problem;stochastic dynamic programming;deterministic threshold pricing;static pricing;Pricing;Land mobile radio cellular systems;Quality of service;Infinite horizon;Stochastic processes;Dynamic programming;Communications Society;Electronic mail;Contracts;Traffic control},
doi={10.1109/INFOCOM.2008.118},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509714,
author={A. L. Stolyar and H. Viswanathan},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Self-Organizing Dynamic Fractional Frequency Reuse in OFDMA Systems},
year={2008},
volume={},
number={},
pages={691-699},
abstract={We describe an algorithm for sub-carrier and power allocation that achieves out-of-cell interference avoidance through dynamic fractional frequency reuse (FFR) in downlink of cellular systems based on orthogonal frequency division multiple access (OFDMA). The focus in on the constant-bit-rate (CBR) traffic type flows (e.g., VoIP). Our approach is based on the continuous "selfish" optimization of resource allocation by each sector. No a priori frequency planning and/or inter-cell coordination is required. We show, both analytically (on a simple illustrative example) and by simulations (of a more realistic system), that the algorithm leads the system to "self-organize" into efficient frequency reuse patterns.},
keywords={frequency division multiple access;interference suppression;OFDM modulation;optimisation;resource allocation;telecommunication traffic;self-organizing dynamic fractional frequency reuse;OFDMA system;sub-carrier allocation;power allocation;out-of-cell interference avoidance;dynamic fractional frequency reuse;cellular system;orthogonal frequency division multiple access;constant-bit-rate;traffic type flow;selfish optimization;resource allocation;priori frequency planning;Interference;Frequency conversion;Base stations;Geometry;Radio spectrum management;Throughput;Communications Society;Downlink;Traffic control;Resource management},
doi={10.1109/INFOCOM.2008.119},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509715,
author={D. Amzallag and R. Bar-Yehuda and D. Raz and G. Scalosub},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Cell Selection in 4G Cellular Networks},
year={2008},
volume={},
number={},
pages={700-708},
abstract={Cell selection is the process of determining the cells that provide service to each mobile station. Optimizing these processes is an important step towards maximizing the utilization of current and future cellular networks. In this paper we study the potential benefit of global cell selection versus the current local mobile SNR-based decision protocol. In particular, we study the new possibility that is feasible in OFDMA-based systems, of satisfying the minimal demand of a mobile station simultaneously by more than one base station. We formalize the problem as an optimization problem, called the all-or-nothing demand maximization problem, and show that when the demand of a single mobile station can exceed the capacity of a base station, this problem is not only NP-hard but also cannot be approximated within any reasonable factor. In contrast, under the very practical assumption that the maximum required bandwidth of a single mobile station is at most an r-fraction of the capacity of a base station, we present two different algorithms for cell selection. The first algorithm guarantees a satisfaction of at least a 1- r r fraction of an optimal assignment, where a mobile station can be covered simultaneously by more than one base station. The second algorithm guarantees a satisfaction of at least a 1-r/1-r fraction of an optimal assignment, while every mobile station is covered by at most one base station. Using an extensive simulation study we show that the cell selections determined by our algorithms achieve a better utilization of high-loaded capacity-constrained future 4G networks than the current SNR- based scheme. Specifically, our algorithms are shown to obtain up to 20% better usage of the network's capacity, in comparison with the current cell selection algorithms.},
keywords={4G mobile communication;cellular radio;frequency division multiple access;OFDM modulation;optimisation;cell selection;4G cellular networks;mobile station;local mobile SNR-based decision protocol;OFDMA-based systems;optimization problem;all-or-nothing demand maximization problem;NP-hard problem;Land mobile radio cellular systems;Base stations;Microcell networks;Computer science;Mobile computing;Communications Society;Programmable control;Protocols;Bandwidth;Costs},
doi={10.1109/INFOCOM.2008.120},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509716,
author={M. Nassiri and M. Heusse and A. Duda},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Novel Access Method for Supporting Absolute and Proportional Priorities in 802.11 WLANs},
year={2008},
volume={},
number={},
pages={709-717},
abstract={Many researchers have addressed the problem of QoS differentiation in 802.11 wireless networks, however no method proposed so far benefits from all desirable properties: high aggregate throughput even for a large number of contending stations, fair allocation to all stations in the same class, fast adaptation to changing conditions, and support for absolute priorities. If we consider the IEEE 802.11e standard, its EDCA (enhanced distributed channel access) access method suffers from an increased collision rate when the number of stations increases. In this paper, we propose a novel access method that supports both relative proportional throughput allocation and absolute priorities in 802.11 wireless networks. The method is efficient, scalable, and fair. It builds on the idea of the Idle Sense method that provides the optimal throughput and fairness for 802.11 WLANs [1]: each station adjusts its contention window based on the observed average number of idle slots. We achieve absolute priority differentiation by setting the target value for the number of idle slots to a small value, so that the absolute priority class gains all the available throughput. The method also supports relative proportional throughput allocation in which several classes share the available throughput according to desired ratios. Our simulations show that the proposed method achieves its objectives of relative and absolute differentiation both with respect to the aggregated throughput and the speed of convergence. Unlike 802.11e EDCA, it presents very good scalability - the throughput remains almost constant in function of the number of contending stations.},
keywords={wireless channels;wireless LAN;wireless LAN;IEEE 802.11e standard;enhanced distributed channel access method;idle sense method;Throughput;Telecommunication traffic;Wireless networks;Quality of service;Multimedia communication;Communications Society;Informatics;Degradation;Teleconferencing;Broadcasting},
doi={10.1109/INFOCOM.2008.121},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509717,
author={A. P. Subramanian and P. Deshpande and J. Gao and S. R. Das},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Drive-By Localization of Roadside WiFi Networks},
year={2008},
volume={},
number={},
pages={718-725},
abstract={We use a steerable beam directional antenna mounted on a moving vehicle to localize roadside WiFi access points (APs), located outdoors or inside buildings. Localizing APs is an important step towards understanding the topologies and network characteristics of large scale WiFi networks that are deployed in a chaotic fashion in urban areas. The idea is to estimate the angle of arrival of frames transmitted from the AP using signal strength information on different directional beams of the antenna - as the beam continuously rotates while the vehicle is moving. This information together with the GPS locations of the vehicle are used in a triangulation approach to localize the APs. We show how this method must be extended using a clustering approach to account for multi-path reflections in cluttered environments. Our technique is completely passive requiring minimum effort beyond driving the vehicle around in the neighborhood where the APs need to be localized, and is able to improve the localization accuracy by an order of magnitude compared with trilateration approaches using omnidirectional antennas, and by a factor of two relative to other known techniques using directional antennas.},
keywords={beam steering;directive antennas;Global Positioning System;mobile radio;road vehicles;wireless LAN;drive-by localization;roadside WiFi networks;steerable beam directional antenna;WiFi access points;GPS locations;multi-path reflections;moving vehicle;Directional antennas;Directive antennas;Road vehicles;Network topology;Large-scale systems;Chaos;Urban areas;Direction of arrival estimation;Transmitting antennas;Global Positioning System},
doi={10.1109/INFOCOM.2008.122},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509718,
author={S. Shin and H. Schulzrinne},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Call Admission Control in IEEE 802.11 WLANs Using QP-CAT},
year={2008},
volume={},
number={},
pages={726-734},
abstract={As IEEE 802.11 networks in a BSS are increasingly used to carry VoIP, concerns about QoS arise. The overall delay of all VoIP flows drastically increases when the number of VoIP sources approaches the capacity, due to the characteristics of CSMA/CA. We propose a novel call admission control with the Queue size Prediction using Computation of Additional Transmission (QP-CAT) to avoid admitting an excessive number of simultaneous calls. In QP-CAT, an AP can accurately predict the effect of new VoIP flows on the delay of existing VoIP flows by predicting its queue size, before the new VoIP flow is actually admitted. It can be easily extended to support 802.11e so that the AP can predict the effect even when background traffic exists with VoIP traffic under 802.11e.},
keywords={carrier sense multiple access;Internet telephony;quality of service;queueing theory;telecommunication congestion control;telecommunication traffic;wireless LAN;call admission control;IEEE 802.11 WLAN;QP-CAT;QoS;CSMA/CA;queue size prediction;VoIP traffic flow;Call admission control;Quality of service;Downlink;Traffic control;Wireless networks;Telecommunication traffic;Delay estimation;Delay effects;Protection;Admission control},
doi={10.1109/INFOCOM.2008.123},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509719,
author={S. Rayanchu and A. Mishra and D. Agrawal and S. Saha and S. Banerjee},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Diagnosing Wireless Packet Losses in 802.11: Separating Collision from Weak Signal},
year={2008},
volume={},
number={},
pages={735-743},
abstract={It is well known that a packet loss in 802.11 can happen either due to collision or an insufficiently strong signal. However, discerning the exact cause of a packet loss, once it occurs, is known to be quite difficult. In this paper we take a fresh look at this problem of wireless packet loss diagnosis for 802.11-based communication and propose a promising technique called COLLIE. COLLIE performs loss diagnosis by using newly designed metrics that examine error patterns within a physical-layer <i>symbol</i> in order to expose statistical differences between collision and weak signal based losses. We implement COLLIE through custom driver-level modifications in Linux and evaluate its performance experimentally. Our results demonstrate that it has an accuracy ranging between 60-95% while allowing a false positive rate of up to 2%. We also demonstrate the use of COLLIE in subsequent link adaptations in both static and mobile wireless usage scenarios through measurements on regular laptops and the Netgear SPH101 Voice-over-WiFi phone. In these experiments, COLLIE led to throughput improvements of 20- 60% and reduced retransmission related costs by 40% depending upon the channel conditions.},
keywords={carrier sense multiple access;telecommunication congestion control;wireless LAN;collision;wireless packet loss diagnosis;802.11-based communication;COLLIE;carrier-sense multiple access protocol;CSMA protocol;Multiaccess communication;Propagation losses;Throughput;Access protocols;Ethernet networks;Face detection;Feedback;Communications Society;USA Councils;Signal design},
doi={10.1109/INFOCOM.2008.124},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509720,
author={G. Jakllari and S. Eidenbenz and N. Hengartner and S. V. Krishnamurthy and M. Faloutsos},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Link Positions Matter: A Noncommutative Routing Metric for Wireless Mesh Network},
year={2008},
volume={},
number={},
pages={744-752},
abstract={We revisit the problem of computing the path with the minimum cost in terms of the expected number of link layer transmissions (including retransmissions) in wireless mesh networks. Unlike previous efforts, such as the popular ETX, we account for the fact that MAC protocols (including the IEEE 802.11 MAC) incorporate a finite number of transmission attempts per packet. This in turn leads to our key observation: the performance of a path depends not only on the number of the links on the path and the quality of its links, but also, on the relative positions of the links on the path. Based on this observation, we propose ETOP, a path metric that accurately captures the expected number of link layer transmissions required for reliable end-to-end packet delivery. We analytically compute ETOP, which is not trivial, since ETOP is a noncommutative function of the link success probabilities. Although ETOP is a more involved metric, we show that the problem of computing paths with the minimum ETOP cost can be solved by a greedy algorithm. We implement and evaluate a routing approach based on ETOP on a 25-node indoor mesh network. Our experiments show that the path selection with ETOP consistently results in superior TCP goodput (by over 50% in many cases) compared to path selection based on ETX. We also perform an in-depth analysis of the measurements to better understand why the paths selected by ETOP improve the TCP performance.},
keywords={access protocols;greedy algorithms;indoor radio;radio networks;telecommunication network routing;noncommutative routing metric;wireless mesh network;MAC protocol;end-to-end packet delivery;greedy algorithm;indoor mesh network;ETOP;Routing;Wireless mesh networks;Costs;Throughput;Computer networks;Media Access Protocol;Mesh networks;Collaborative work;Government;Communications Society},
doi={10.1109/INFOCOM.2008.125},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509721,
author={H. Nama and N. Mandayam and R. Yates},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Network Formation Among Selfish Energy-Constrained Wireless Devices},
year={2008},
volume={},
number={},
pages={753-761},
abstract={We study the formation of ad-hoc networks among selfish energy-constrained wireless devices that are primarily interested in being <i>connected</i> with other devices. We use a non-cooperative bilateral connection game (BCG) framework to study network formation. For a BCG in which devices choose their individual strategies to remain connected by minimizing only their direct transmission power costs, we show that the price-of-anarchy is unbounded in the network size. We propose a BCG with an alternate cost structure in which each device additionally <i>pays</i> the transmission power costs incurred by other devices for its own traffic. We show that a unique network structure emerges in this game that is stable as well as socially efficient. We then study the achievable throughput for random point-to-point traffic in this stable energy-efficient network. When the nodes of a network are located in a bounded planar region the distribution of point- to-point flows through the nodes exhibits a scale-free behavior.},
keywords={ad hoc networks;game theory;mobile radio;telecommunication traffic;ad-hoc network formation;selfish energy-constrained wireless device;noncooperative bilateral connection game framework;price-of-anarchy;random point-to-point traffic;scale-free network behavior;Peer to peer computing;Costs;IP networks;Energy efficiency;IEEE news;Telecommunication traffic;Internet;Communications Society;Mobile handsets;Personal digital assistants},
doi={10.1109/INFOCOM.2008.126},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509722,
author={E. Aryafar and O. Gurewitz and E. W. Knightly},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Distance-1 Constrained Channel Assignment in Single Radio Wireless Mesh Networks},
year={2008},
volume={},
number={},
pages={762-770},
abstract={This paper addresses channel assignment and random medium access design for single-radio multi-channel mesh networks. Two prior approaches include: (i) designing MAC protocols that dynamically select channels based on local information and (ii) partitioning the mesh into subnetworks with different channels and using 802.11 as the medium access protocol. Both of these approaches suffer from limited throughput improvement; the first approach due to wrong or incomplete channel state information that inherently arises in a multi-hop wireless environment, while the second approach due to high interference within each subnetwork. In this paper, we first introduce D1C-CA, Distance-1 Constrained Channel Assignment. D1C-CA statically assigns channels to a set of links as a function of physical connectivity, contention, and the unique gateway functionality of mesh networks, i.e, all Internet (non-local) traffic has a gateway node as its source or destination. To design D1C-CA, we model the channel assignment problem as a new form of graph edge coloring in which edges at distance one are constrained. We prove that the problem is NP-complete and design an efficient heuristic solution for mesh networks. Second, we design an asynchronous control-channel-based MAC protocol that solves multi-channel coordination problems and employs the proposed channel assignment algorithm. Finally, we investigate the performance of our approach through extensive simulations and show considerable performance improvements compared to alternate schemes.},
keywords={access protocols;channel allocation;graph colouring;radio networks;wireless channels;single radio wireless mesh network;distance-1 constrained channel assignment;random medium access control design;MAC protocol;graph edge coloring;NP-complete problem;Wireless mesh networks;Mesh networks;Media Access Protocol;Access protocols;Throughput;Channel state information;Interference constraints;IP networks;Telecommunication traffic;Traffic control},
doi={10.1109/INFOCOM.2008.127},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509723,
author={D. Panigrahi and P. Dutta and S. Jaiswal and K. V. M. Naidu and R. Rastogi},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Minimum Cost Topology Construction for Rural Wireless Mesh Networks},
year={2008},
volume={},
number={},
pages={771-779},
abstract={IEEE 802.11 WiFi equipment based wireless mesh networks have recently been proposed as an inexpensive approach to connect far-flung rural areas. Such networks are built using high-gain directional antennas that can establish long-distance wireless point-to-point links. Some nodes in the network (called gateway nodes) are directly connected to the wired internet, and the remaining nodes connect to the gateway(s) using one or more hops. The dominant cost of constructing such a mesh network is the cost of constructing antenna towers at nodes. The cost of a tower depends on its height, which in turn depends on the length of its links and the physical obstructions along those links. We investigate the problem of selecting which links should be established such that all nodes are connected, while the cost of constructing the antenna towers required to establish the selected links is minimized. We show that this problem is NP-hard and that a better than O(log n) approximation cannot be expected, where n is the number of vertices in the graph. We then present the first algorithm in the literature, for this problem, with provable performance bounds. More precisely, we present a greedy algorithm that is an O(log n) approximation algorithm for this problem. Finally, through simulations, we compare our approximation algorithm with both the optimal solution, and a naive heuristic.},
keywords={computational complexity;directive antennas;greedy algorithms;internetworking;telecommunication network topology;wireless LAN;minimum cost topology construction;rural wireless mesh networks;IEEE 802.11 WiFi equipment;gateway nodes;link selection problem;NP-hard problem;greedy algorithm;approximation algorithm;high-gain directional antennas;wireless point-to-point links;wired Internet;Costs;Network topology;Wireless mesh networks;Poles and towers;Peer to peer computing;Mesh networks;Buildings;Approximation algorithms;WiMAX;Internet},
doi={10.1109/INFOCOM.2008.128},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509724,
author={A. Anandkumar and L. Tong and A. Swami and A. Ephremides},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Minimum Cost Data Aggregation with Localized Processing for Statistical Inference},
year={2008},
volume={},
number={},
pages={780-788},
abstract={The problem of minimum cost in-network fusion of measurements, collected from distributed sensors via multihop routing is considered. A designated fusion center performs an optimal statistical-inference test on the correlated measurements, drawn from a Markov random field. Conditioned on the delivery of a sufficient statistic for inference to the fusion center, the structure of optimal routing and fusion is shown to be a Steiner tree on a transformed graph. This Steiner-tree reduction preserves the approximation ratio, which implies that any Sterner- tree approximation can be employed for minimum cost fusion with the same approximation ratio. The proposed fusion scheme involves routing packets of two types viz., raw measurements sent for local processing, and aggregates obtained on combining these processed values. The performance of heuristics for minimum cost fusion are evaluated through theory and simulations, showing a significant saving in routing costs, when compared to routing all the raw measurements to the fusion center.},
keywords={distributed sensors;Markov processes;random processes;sensor fusion;statistical analysis;telecommunication network routing;trees (mathematics);minimum cost data aggregation;statistical inference;distributed sensor fusion;multihop routing;Markov random field;Steiner tree;Routing;Statistics;Statistical distributions;Cost function;Sensor fusion;Markov random fields;Collaborative work;Government;Power generation economics;Communications Society},
doi={10.1109/INFOCOM.2008.129},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509725,
author={S. Lederer and Y. Wang and J. Gao},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Connectivity-Based Localization of Large Scale Sensor Networks with Complex Shape},
year={2008},
volume={},
number={},
pages={789-797},
abstract={We study the problem of localizing a large sensor network having a complex shape, possibly with holes. A major challenge with respect to such networks is to figure out the correct network layout, i.e., avoid global flips where a part of the network folds on top of another. Our algorithm first selects landmarks on network boundaries with sufficient density, then constructs the landmark Voronoi diagram and its dual combinatorial Delaunay complex on these landmarks. The key insight is that the combinatorial Delaunay complex is provably globally rigid and has a unique realization in the plane. Thus an embedding of the landmarks by simply gluing the Delaunay triangles properly recovers the faithful network layout. With the landmarks nicely localized, the rest of the nodes can easily localize themselves by trilateration to nearby landmark nodes. This leads to a practical and accurate localization algorithm for large networks using only network connectivity. Simulations on various network topologies show surprisingly good results. In comparison, previous connectivity-based localization algorithms such as multi-dimensional scaling and rubberband representation generate globally flipped or distorted localization results.},
keywords={computational geometry;distributed sensors;graph theory;telecommunication network topology;connectivity-based localization algorithm;large scale sensor network layout;landmark Voronoi graph diagram;dual combinatorial Delaunay complex;network topology;Large-scale systems;Shape;Iterative algorithms;Peer to peer computing;Communications Society;Computer science;Network topology;Global Positioning System;Information geometry;Computer networks},
doi={10.1109/INFOCOM.2008.130},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509726,
author={F. Zhang and A. Jiang and J. Chen},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Robust Planarization of Unlocalized Wireless Sensor Networks},
year={2008},
volume={},
number={},
pages={798-806},
abstract={Wireless sensor networks need very efficient network protocols due to the sensors' limited communication and computation capabilities. Network planarization - finding a planar subgraph of the network that contains all the nodes - has been a very important technique for many network protocols. It first became the foundation of various well known routing protocols, including GPSR, GOAFR and several other protocols. Since then, it has also been used in numerous other applications, including data-centric storage, network localization, topology discovery, etc. However, an important problem remains: network planarization itself is very difficult. So far, efficient planarization algorithms exist only for very restrictive models: the network must be a unit-disk graph, and accurate measurements related to the node locations (e.g., node positions or angles between adjacent links) need to be known. For more practical network models, where the transmission ranges are usually not uniform and sensors cannot obtain their accurate location information via expensive localization devices, no efficient planarization algorithm is available. In this paper, we present a novel method that robustly planarizes sensor networks of a realistic model: networks with non-uniform transmission ranges and unlocalized sensors (that is, static sensors whose locations are unknown). Our method starts with a simple shortest path between two nodes, and progressively planarizes the whole network. It achieves both efficiency and a good planarization result. We present two planarization algorithms for different settings. Our results not only solve the planarization problem, but also outperform some known results in the graph drawing research field. We demonstrate the practical performance of our method - as well as its application in topology discovery, - through extensive simulations.},
keywords={graph theory;network theory (graphs);routing protocols;wireless sensor networks;unlocalized wireless sensor network;robust planarization;planar subgraph;routing protocols;nonuniform transmission range;Robustness;Planarization;Wireless sensor networks;Network topology;Routing protocols;Semiconductor device modeling;Peer to peer computing;Computer networks;Position measurement;Geometry},
doi={10.1109/INFOCOM.2008.131},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509727,
author={J. Kim and X. Lin and N. B. Shroff and P. Sinha},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={On Maximizing the Lifetime of Delay-Sensitive Wireless Sensor Networks with Anycast},
year={2008},
volume={},
number={},
pages={807-815},
abstract={Sleep-wake scheduling is an effective mechanism to prolong the lifetime of energy-constrained wireless sensor networks. However, it incurs an additional delay for packet delivery when each node needs to wait for its next-hop relay node to wake up, which could be unacceptable for delay-sensitive applications. Prior work in the literature has proposed to reduce this delay using anycast, where each node opportunistically selects the first neighboring node that wakes up among multiple candidate nodes. In this paper, we study the joint control problem of how to optimally control the sleep-wake schedule, the anycast candidate set of next-hop neighbors, and anycast priorities, to maximize the network lifetime subject to a constraint on the expected end-to-end delay. We provide an efficient solution to this joint control problem. Our numerical results indicate that the proposed solution can substantially outperform prior heuristic solutions in the literature, especially under the practical scenarios where there are obstructions in the coverage area of the wireless sensor network.},
keywords={optimal control;optimisation;scheduling;set theory;telecommunication control;telecommunication network reliability;wireless sensor networks;delay-sensitive wireless sensor network lifetime maximization;anycast candidate set;sleep-wake scheduling mechanism;next-hop relay node;joint control problem;optimal control;Wireless sensor networks;Peer to peer computing;Optimal control;Protocols;Routing;Relays;Delay effects;Added delay;Scheduling;Communications Society},
doi={10.1109/INFOCOM.2008.132},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509728,
author={K. Zeng and W. Lou and H. Zhai},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={On End-to-End Throughput of Opportunistic Routing in Multirate and Multihop Wireless Networks},
year={2008},
volume={},
number={},
pages={816-824},
abstract={Routing in multi-hop wireless networks presents a great challenge mainly due to unreliable wireless links and interference among concurrent transmissions. Recently, a new routing paradigm, opportunistic routing (OR), is proposed to cope with the unreliable transmissions by exploiting the broadcast nature and spatial diversity of the wireless medium. Previous studies on OR focused on networks with a single channel rate. The performance of OR in a multi-rate scenario is not carefully studied. In addition, although simulation and practical implementation have shown that OR achieves better throughput performance than that of traditional routing, there is no theoretical results on capacity enhancement provided by OR or network capacity bounds of OR. In this paper, we bridge these gaps by carrying out a comprehensive study on the impacts of multiple rates, interference, candidate selection and prioritization on the maximum end-to-end throughput or capacity of OR. Taking into consideration of wireless interference, we propose a new method of constructing transmission conflict graphs - we propose transmitter based conflict graph in contrast to link conflict graph. Then, we introduce the concept of concurrent transmitter sets to represent the constraints imposed by the transmission conflicts of OR, and formulate the maximum end-to-end throughput problem as a maximum-flow linear programming problem subject to the transmission conflict constraints. We also propose a rate selection scheme, and compare the throughput capacity of multi- rate OR with single-rate ones. We validate the analysis results by simulation, and show that OR has great potential to improve end- to-end throughput and system operating at multi-rates achieves higher throughput than that operating at any single rate.},
keywords={radio links;radio networks;telecommunication network routing;opportunistic routing;multihop wireless network;wireless interference;transmitter based conflict graph;concurrent transmitter;wireless links;Throughput;Routing;Spread spectrum communication;Wireless networks;Transmitters;Broadcasting;Bridges;Interference constraints;Linear programming;Analytical models},
doi={10.1109/INFOCOM.2008.133},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509729,
author={Z. Jiang and J. Ma and W. Lou and J. Wu},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={An Information Model for Geographic Greedy Forwarding in Wireless Ad-Hoc Sensor Networks},
year={2008},
volume={},
number={},
pages={825-833},
abstract={In wireless ad-hoc sensor networks, an important issue often faced in geographic greedy forwarding routing is the "local minimum phenomenon" which is caused by deployment holes and blocks the forwarding process. In this paper, we provide a new information model for the geographic greedy forwarding routing that only forwards the packet within the so-called request zone. Under this new information model, the hole and its affected area are identified easily and quickly in an unsafe area with a labeling process. The greedy forwarding will be blocked if and only if a node inside the unsafe area is used. Due to the shape of the request zone, an unsafe area can be estimated as a rectangular region in the local view of unsafe nodes. With such estimate information, the new routing method proposed in this paper will avoid blocking by holes and achieve better performance in routing time while the cost of information construction is greatly reduced compared with the best results known to date.},
keywords={ad hoc networks;telecommunication network routing;wireless sensor networks;information model;wireless ad-hoc sensor networks;geographic greedy forwarding routing;local minimum phenomenon;Wireless sensor networks;Routing;Shape;Costs;USA Councils;Computer networks;Peer to peer computing;Communications Society;Computer science;Sensor phenomena and characterization},
doi={10.1109/INFOCOM.2008.134},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509730,
author={R. Flury and R. Wattenhofer},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Randomized 3D Geographic Routing},
year={2008},
volume={},
number={},
pages={834-842},
abstract={We reconsider the problem of geographic routing in wireless ad hoc networks. We are interested in local, memoryless routing algorithms, i.e. each network node bases its routing decision solely on its local view of the network, nodes do not store any message state, and the message itself can only carry information about O(1) nodes. In geographic routing schemes, each network node is assumed to know the coordinates of itself and all adjacent nodes, and each message carries the coordinates of its target. Whereas many of the aspects of geographic routing have already been solved for 2D networks, little is known about higher-dimensional networks. It has been shown only recently that there is in fact no local memoryless routing algorithm for 3D networks that delivers messages deterministically. In this paper, we show that a cubic routing stretch constitutes a lower bound for any local memoryless routing algorithm, and propose and analyze several randomized geographic routing algorithms which work well for 3D network topologies. For unit ball graphs, we present a technique to locally capture the surface of holes in the network, which leads to 3D routing algorithms similar to the greedy-face-greedy approach for 2D networks.},
keywords={ad hoc networks;greedy algorithms;telecommunication network routing;telecommunication network topology;randomized 3D geographic routing;wireless ad hoc networks;memoryless routing algorithms;3D network topologies;greedy-face-greedy approach;Peer to peer computing;Ad hoc networks;Computer networks;Laboratories;Mobile ad hoc networks;IP networks;Routing protocols;Hardware;Communications Society;Algorithm design and analysis},
doi={10.1109/INFOCOM.2008.135},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509731,
author={X. Wang and J. J. Garcia-Luna-Aceves},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Embracing Interference in Ad Hoc Networks Using Joint Routing and Scheduling with Multiple Packet Reception},
year={2008},
volume={},
number={},
pages={843-851},
abstract={We present an approach that takes advantage of multi-packet reception (MPR) to reduce the negative effects of multiple access interference and therefore increase the capacity of an ad hoc network. We analyze the performance upper bound of joint routing and scheduling for ad hoc networks that embrace interference by using MPR. We formulate the optimization problem under a deterministic model and seek to maximize the aggregate network throughput subject to minimum rate requirements. We then propose a polynomial-time heuristic algorithm aimed at approximating the optimal solution to the joint routing and channel access problem under MPR. We show the effectiveness of our heuristic algorithm by comparing its performance with the upper bound.},
keywords={ad hoc networks;computational complexity;optimisation;radiofrequency interference;scheduling;telecommunication network routing;ad hoc networks;joint routing scheduling optimization problem;multiple packet reception;multiple access interference;deterministic model;polynomial-time heuristic algorithm;channel access problem;Ad hoc networks;Routing;Access protocols;Multiple access interference;Decoding;Upper bound;Wireless application protocol;Scheduling;USA Councils;Throughput},
doi={10.1109/INFOCOM.2008.136},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509732,
author={E. -. Park and H. Kim and J. -. Kim and H. -. Kim},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Dynamic Bandwidth Request-Allocation Algorithm for Real-Time Services in IEEE 802.16 Broadband Wireless Access Networks},
year={2008},
volume={},
number={},
pages={852-860},
abstract={The emerging broadband wireless access (BWA) technology based on IEEE 802.16 is one of the most promising solutions to provide ubiquitous wireless access to the broadband service at low cost. This paper proposes an efficient uplink bandwidth request-allocation algorithm for variable-rate realtime services in IEEE 802.16 BWA networks. In order to minimize bandwidth wastage without degrading quality of service (QoS), we introduce a notion of target delay and propose dual feedback architecture. The proposed algorithm calculates the amount of bandwidth request such that the delay is regulated around the desired level to minimize delay violation and delay jitter for real-time services. Also, it can maximize utilization of wireless channel by making use of dual feedback, where the bandwidth request is adjusted based on the information about the backlogged amount of traffic in the queue and the rate mismatch between packet arrival and service rates. Due to the dual feedback architecture, the proposed scheme responds quickly to the variation of traffic load and is robust to the change of network condition. We analyze the stability of the proposed algorithm from a control-theoretic viewpoint and derive a simple design guideline based on the analysis. By implementing the algorithm in OPNET simulator, we evaluate its performance in terms of queue regulation, optimal bandwidth allocation, delay controllability, and robustness to traffic characteristics.},
keywords={bandwidth allocation;broadband networks;channel allocation;jitter;quality of service;queueing theory;radio access networks;radio links;telecommunication traffic;wireless channels;dynamic uplink bandwidth request-allocation algorithm;IEEE 802.16 broadband wireless access networks;ubiquitous wireless access;variable-rate real-time services;quality of service;QoS;dual feedback architecture;delay jitter;wireless channel utilization;network traffic;queueing theory;Bandwidth;Heuristic algorithms;Wireless networks;Delay;Feedback;Algorithm design and analysis;Quality of service;Communication system traffic control;Traffic control;Ubiquitous computing},
doi={10.1109/INFOCOM.2008.137},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509733,
author={M. Andrews and L. Zhang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Creating Templates to Achieve Low Delay in Multi-Carrier Frame-Based Wireless Data Systems},
year={2008},
volume={},
number={},
pages={861-869},
abstract={We consider the problem of creating template-based schedules for multi-carrier frame-based wireless data systems such as 802.16 (Wimax). A template consists of an assignment of carriers to users over a fixed set of time slots. This schedule can then be repeated multiple times. The aim is to assign the (time slot, carrier) pairs to the users in such a way that the service to each user is as smooth as possible. This in turn ensures that the users experience low delay. A number of elegant template scheduling algorithms exist for the single-carrier case. However, the case of multi-carrier systems where the channel rates can be different on different carriers has received much less attention. We present a general framework for studying the delay performance of a multi-carrier template. We then describe a number of deterministic and randomized scheduling algorithms for template creation and study their delay performance via analysis and simulation. We also show that the delay bounds can sometimes be improved by randomly shifting the schedule on each carrier and by scheduling in a hierarchical manner.},
keywords={deterministic algorithms;randomised algorithms;scheduling;WiMax;multi-carrier frame-based wireless data systems;template-based schedules;802.16;Wimax;time slot;channel rates;deterministic scheduling algorithms;randomized scheduling algorithms;Delay;Data systems;OFDM;Bandwidth;WiMAX;Scheduling algorithm;Feedback;Communications Society;Performance analysis;Algorithm design and analysis},
doi={10.1109/INFOCOM.2008.138},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509734,
author={L. Cao and H. Zheng},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Stable and Efficient Spectrum Access in Next Generation Dynamic Spectrum Networks},
year={2008},
volume={},
number={},
pages={870-878},
abstract={Future wireless infrastructure networks will dynamically access spectrum for maximum utilization. However, the fundamental challenge is how to provide stable spectrum access required for most applications. Using dynamic spectrum access, each node's spectrum usage is inherently unpredictable and unstable. We propose to address this challenge by integrating interference-aware statistical admission control with stability- driven spectrum allocation. Specifically, we propose to proactively regulate nodes' spectrum demand to allow efficient statistical multiplexing while minimizing outages. Admitted nodes coordinate to adapt instantaneous spectrum allocation to match time- varying demand. While the optimization problem is NP-hard, we develop computational-efficient algorithms with strong analytical guarantees. Experimental results show that the proposed approach can provide stable spectrum usage while improving its utilization by 80-100% compared to conventional solutions.},
keywords={computational complexity;optimisation;radio access networks;spectral analysis;telecommunication congestion control;time-varying systems;dynamic spectrum network;wireless infrastructure network;interference-aware statistical admission control;stability-driven spectrum allocation;statistical multiplexing;time-varying demand;computational-efficient algorithm;Next generation networking;Peer to peer computing;Admission control;Stability;Algorithm design and analysis;Interference constraints;Communications Society;Computer science;Application software;Technological innovation},
doi={10.1109/INFOCOM.2008.139},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509735,
author={X. Zhang and W. Jiao and M. Tao},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={End-to-End Resource Allocation in OFDM Based Linear Multi-Hop Networks},
year={2008},
volume={},
number={},
pages={879-887},
abstract={We study the end-to-end resource allocation in an OFDM based multi-hop network consisting of a one-dimensional chain of nodes including a source, a destination, and multiple relays. The problem is to maximize the end-to-end average transmission rate under a long-term total power constraint by adapting the transmission power on each subcarrier over each hop and the transmission time used by each hop in every time frame. The solution to the problem is derived by decomposing it into two subproblems: short-term time and power allocation given an arbitrary total power constraint for each channel realization, and total power distribution over all channel realizations. We show that the optimal solution has the following features: the power allocation on subcarriers over each hop has the water-filling structure and a higher water level is given to the hop with relatively poor channel condition; meanwhile, the fraction of transmission time allocated to each hop is adjusted to keep the instantaneous rates over all hops equal. To tradeoff between performance, computational complexity and signalling overhead, three suboptimal resource allocation algorithms are also proposed. Numerical results are illustrated under different network settings and channel environments.},
keywords={OFDM modulation;radio networks;resource allocation;wireless channels;end-to-end resource allocation;OFDM;linear multi-hop networks;end-to-end average transmission rate;computational complexity;water-filling structure;Resource management;OFDM;Spread spectrum communication;Relays;Wireless LAN;Communications Society;Computer networks;Peer to peer computing;Power distribution;Water resources},
doi={10.1109/INFOCOM.2008.140},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509736,
author={P. Maille and B. Tuffin},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Analysis of Price Competition in a Slotted Resource Allocation Game},
year={2008},
volume={},
number={},
pages={888-896},
abstract={Pricing telecommunication networks has become a highly regarded topic during the last decade, in order to cope with congestion by controlling demand, or to yield proper incentives for a fair sharing of resources. On the other hand, another important factor has to be brought in: there is a rise of competition between service providers in telecommunication networks such as for instance the Internet, and the impact of this competition has to be carefully analyzed. The present paper pertains to this recent stream of works. We consider a slotted resource allocation game with several providers, each of them having a fixed capacity during each time slot, and a fixed access price. Each provider serves its demand up to its capacity, demand in excess being dropped. Total user demand is therefore split among providers according to Wardrop's principle, depending on price and loss probability. Using the characterization of the resulting equilibrium, we prove, under mild conditions, the existence and uniqueness of a Nash equilibrium in the pricing game between providers. We also show that, remarkably, this equilibrium actually corresponds to the socially optimal situation obtained when both users and providers cooperate to maximize the sum of all utilities, this even if providers have the opportunity to artificially reduce their capacity.},
keywords={game theory;radio networks;slotted resource allocation game;telecommunication networks pricing;Nash equilibrium;Resource management;Pricing;IP networks;Web and internet services;Quality of service;WiMAX;Bandwidth;Communications Society;Telecommunication congestion control;Telecommunication control},
doi={10.1109/INFOCOM.2008.141},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509737,
author={D. Comer and M. Martynov},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Design and Analysis of Hybrid Packet Schedulers},
year={2008},
volume={},
number={},
pages={897-905},
abstract={This paper addresses the problem of scheduling variable-size packets from large number of distinct traffic flows. Although the problem of fair packet scheduling in computer networks has received thorough theoretical consideration, practical high-speed packet scheduling systems remain elementary. The disparity arises because algorithms with theoretically favorable delay and fairness characteristics have unacceptably high computational cost, while algorithms of low complexity have substantially inferior properties. A variety of hybrid scheduling methods have been proposed as a compromise, but we argue that existing hybrid algorithms are either impractical, or do not provide full spectrum of required features. We then formally define requirements for a packet scheduler and propose a hybrid scheduling algorithm which guarantees these requirements. We develop a theoretical framework to prove that our algorithm indeed provides all the properties claimed.},
keywords={computer networks;scheduling;telecommunication traffic;hybrid packet schedulers;distinct traffic flows;computer networks;high-speed packet scheduling systems;Scheduling algorithm;Delay;Telecommunication traffic;Algorithm design and analysis;Processor scheduling;Computer networks;Bandwidth;Application software;Communications Society;Computational efficiency},
doi={10.1109/INFOCOM.2008.142},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509738,
author={C. Guo},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Improved Smoothed Round Robin Schedulers for High-Speed Packet Networks},
year={2008},
volume={},
number={},
pages={906-914},
abstract={The smoothed round robin (SRR) packet scheduler is attractive for use in high-speed networks due to its very low time complexity, but it is not suitable for realtime applications since it cannot provide tight delay bound. In this paper, we present two improved algorithms based on SRR, namely SRR<sup>c</sup> and SRR<sup>#</sup>, which are based on novel matrix transform techniques. By transforming the irregular Weight Matrix of SRR into triangular and diagonal ones, SRR<sup>+</sup> and SRR<sup>#</sup> are able to evenly interleave flows based on their reserved rates even in skewed weight distributions. SRR<sup>+</sup> and SRR<sup>#</sup> provide bounded delay, whereas are still of low space and time complexities and are simple to implement in high-speed networks. The properties of SRR<sup>+</sup> and SRR<sup>#</sup> are addressed in detail through analysis and simulations. SRR<sup>+</sup> and SRR<sup>#</sup>, together with SRR and the recently developed G-3 scheduler [9] form a full spectrum of schedulers that provide tradeoffs among delay, time complexity, and space complexity.},
keywords={computational complexity;Internet;matrix algebra;transforms;smoothed round robin schedulers;high-speed packet networks;time complexity;SRR+;SRR#;matrix transform techniques;space complexity;Internet;Round robin;Scheduling algorithm;Delay effects;Bandwidth;Network interfaces;High-speed networks;Web and internet services;Data structures;Communications Society;Analytical models},
doi={10.1109/INFOCOM.2008.143},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509739,
author={E. H. Watanabe and D. S. Menasche and E. de Souza e Silva and R. M. M. Leao},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Modeling Resource Sharing Dynamics of VoIP Users over a WLAN Using a Game-Theoretic Approach},
year={2008},
volume={},
number={},
pages={915-923},
abstract={We consider a scenario in which users share an access point and are mainly interested in VoIP applications. Each user is allowed to adapt to varying network conditions by choosing the transmission rate at which VoIP traffic is received. We denote this adaptation process by end-user congestion control, our object of study. The two questions that we ask are: (1) what are the performance consequences of letting the users to freely choose their rates? and (2) how to explain the adaptation process of the users? We set a controlled lab experiment having students as subject to answer the first question, and we extend an evolutionary game-theoretic model to address the second. Our partial answers are the following: (1) free users with local information can reach an equilibrium which is close to optimal from the system perspective. However, the equilibrium can be unfair; (2) the adaptation of the users can be explained using a game theoretic model. We propose a methodology to parameterize the latter, which involves active network measurements, simulations and an artificial neural network to estimate the QoS perceived by the users in each of the states of the model.},
keywords={game theory;Internet telephony;quality of service;telecommunication congestion control;wireless LAN;resource sharing dynamics;VoIP;WLAN;transmission rate;end-user congestion control;evolutionary game-theoretic model;QoS;Resource management;Wireless LAN;Automatic control;Internet telephony;Codecs;Aggregates;Communications Society;Systems engineering and theory;Computer science;Application software},
doi={10.1109/INFOCOM.2008.144},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509740,
author={I. Rhee and M. Shin and S. Hong and K. Lee and S. Chong},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={On the Levy-Walk Nature of Human Mobility},
year={2008},
volume={},
number={},
pages={924-932},
abstract={We report that human walks performed in outdoor settings of tens of kilometers resemble a truncated form of Levy walks commonly observed in animals such as monkeys, birds and jackals. Our study is based on about one thousand hours of GPS traces involving 44 volunteers in various outdoor settings including two different college campuses, a metropolitan area, a theme park and a state fair. This paper shows that many statistical features of human walks follow truncated power-law, showing evidence of scale-freedom and do not conform to the central limit theorem. These traits are similar to those of Levy walks. It is conjectured that the truncation, which makes the mobility deviate from pure Levy walks, comes from geographical constraints including walk boundary, physical obstructions and traffic. None of commonly used mobility models for mobile networks captures these properties. Based on these findings, we construct a simple Levy walk mobility model which is versatile enough in emulating diverse statistical patterns of human walks observed in our traces. The model is also used to recreate similar power-law inter-contact time distributions observed in previous human mobility studies. Our network simulation indicates that the Levy walk features are important in characterizing the performance of mobile network routing performance.},
keywords={mobile radio;telecommunication network routing;human mobility;human walks;geographical constraints;walk boundary;physical obstructions;traffic;Levy walk mobility model;power-law inter-contact time distributions;mobile network routing performance;Humans;Biological system modeling;Global Positioning System;Gaussian distribution;Animals;Urban areas;Communications Society;Computer science;Birds;Educational institutions},
doi={10.1109/INFOCOM.2008.145},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509741,
author={W. Wang and M. Zhao},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Joint Effects of Radio Channels and Node Mobility on Link Dynamics in Wireless Networks},
year={2008},
volume={},
number={},
pages={933-941},
abstract={In this paper, we study link properties over dynamic radio channels based on analytical models and simulations. Specifically, channel variability and mobility are investigated through two quantities: effective transmission range and node-pair distance, respectively. We find that the PDF of link lifetime can be approximated by exponential distribution with parameter characterized by the ratio of average node speed to effective transmission range. Moreover, we show that average link lifetime for slower mobile nodes is mainly determined by radio channel characteristics, whereas for faster mobile nodes, it is dominated by node mobility. Through analysis and simulations, we find that the impacting factors on residual link lifetime are in the decreasing order of average node speed, effective transmission range, and node-pair distance on the fly. We further present the implication and application of link properties to path lifetime, network connectivity, and routing performance.},
keywords={ad hoc networks;mobile radio;telecommunication network routing;telecommunication network topology;wireless channels;radio channels;node mobility;link dynamics;wireless networks;network connectivity;routing performance;mobile ad hoc networks;MANET;Wireless networks;Peer to peer computing;Exponential distribution;Analytical models;Fading;Network topology;Communications Society;Computational modeling;Computer simulation;Routing},
doi={10.1109/INFOCOM.2008.146},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509742,
author={H. Zhu and Y. Zhu and M. Li and L. M. Ni},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={HERO: Online Real-Time Vehicle Tracking in Shanghai},
year={2008},
volume={},
number={},
pages={942-950},
abstract={Intelligent transportation systems have become increasingly important for the public transportation in Shanghai. In response, ShanghaiGrid (SG) aims to provide abundant intelligent transportation services to improve the traffic condition. A challenging service in SG is to accurately locate the positions of moving vehicles in real time. In this paper we present an innovative scheme HERO to tackle this problem. In SG, the location information of individual vehicles is actively logged in local nodes which are distributed throughout the city. For each vehicle, HERO dynamically maintains an advantageous hierarchy on the overlay network of local nodes to conservatively update the location information only in nearby nodes. By bounding the maximum number of hops the query is routed, HERO guarantees to meet the real-time constraint associated with each vehicle. Extensive simulations based on the real road network and trace data of vehicle movements from Shanghai demonstrate the efficacy of HERO.},
keywords={automated highways;computer networks;road vehicles;online real-time vehicle tracking;intelligent public transportation system;ShanghaiGrid project;traffic condition;overlay network;road network;hierarchical exponential region organization;Peer to peer computing;Real time systems;Intelligent transportation systems;Cities and towns;Councils;Communications Society;Computational modeling;Road vehicles;Spatiotemporal phenomena;Radiofrequency identification},
doi={10.1109/INFOCOM.2008.147},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509743,
author={X. Hu and L. Li and Z. M. Mao and Y. R. Yang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Wide-Area IP Network Mobility},
year={2008},
volume={},
number={},
pages={951-959},
abstract={IP network mobility is emerging as a major paradigm for providing continuous Internet access while a set of users are on the move in a transportation system. The intense interest on its support has led to the establishment of the NEMO IETF working group and a test-deployment by a major airline equipment vendor - Boeing - on major airline routes. However, the previously proposed solutions are either inefficient or may cause instability to the global Internet. In this paper, we propose WINMO, a simple, systematic, novel solution for wide-area IP network mobility using techniques including route aggregation, scoped update propagation, and packet mobility states. Our solution provides efficient routing when users travel both across autonomous systems (ASes) and within a single AS, generates minimal global routing overhead to prevent global instability, ensures good location privacy, and helps to defend against denial-of-service attacks. Furthermore, our basic scheme (without packet mobility state) is transparent to both clients and servers. Our extensive evaluations demonstrate the effectiveness of our mobility solution.},
keywords={Internet;IP networks;mobility management (mobile radio);telecommunication network routing;telecommunication security;telecommunication services;wide-area IP network mobility;Internet;Boeing;transportation system;WINMO;route aggregation;scoped update propagation;packet mobility states;autonomous systems;global routing overhead;location privacy;denial-of-service attacks;IP networks;Routing;Internet;Protocols;Testing;Computer crime;Web server;Air transportation;Rail transportation;Airplanes},
doi={10.1109/INFOCOM.2008.148},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509744,
author={R. Sarkar and X. Zhu and J. Gao and L. J. Guibas and J. S. B. Mitchell},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Iso-Contour Queries and Gradient Descent with Guaranteed Delivery in Sensor Networks},
year={2008},
volume={},
number={},
pages={960-967},
abstract={We study the problem of data-driven routing and navigation in a distributed sensor network over a continuous scalar field. Specifically, we address the problem of searching for the collection of sensors with readings within a specified range. This is named the <i>iso-contour</i> <i>query</i> problem. We develop a gradient based routing scheme such that from any query node, the query message follows the signal field gradient or derived quantities and successfully discovers <i>all</i> iso-contours of interest. Due to the existence of local maxima and minima, the guaranteed delivery requires preprocessing of the signal field and the construction of a <i>contour</i> <i>tree</i> in a distributed fashion. Our approach has the following properties: (i) the gradient routing uses only local node information and its message complexity is close to optimal, as shown by simulations; (ii) the preprocessing message complexity is linear in the number of nodes and the storage requirement for each node is a small constant. The same preprocessing also facilitates route computation between any pair of nodes where the the route lies within any user supplied range of values.},
keywords={communication complexity;gradient methods;query processing;telecommunication network routing;wireless sensor networks;iso-contour queries;gradient descent;guaranteed delivery;data-driven routing;distributed sensor network;query message;contour tree;message complexity;wireless sensor networks;Routing;Peer to peer computing;Wireless sensor networks;Navigation;Chemical sensors;Intelligent sensors;Monitoring;Computer science;Military computing;Telecommunication traffic},
doi={10.1109/INFOCOM.2008.149},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509745,
author={X. Lu and M. Spear and K. Levitt and S. F. Wu},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={iBubble: Multi-Keyword Routing Protocol for Heterogeneous Wireless Sensor Networks},
year={2008},
volume={},
number={},
pages={968-976},
abstract={Many tasks require multiple sensing capabilities; in wireless sensor networks (WSNs), it is expensive to deploy a homogeneous network wherein every sensor has the same functionality. Instead, it is economical to deploy a heterogeneous network wherein sensors differ in their capabilities; in such a network, efficient data querying is essential. We propose a multi-keyword routing protocol, iBubble, for heterogeneous wireless sensor networks (HWSNs) where keywords describe sensor functionalities. iBubble provides an efficient query interface for locating data; queries are routed only along paths with nodes matching the query. iBubble utilizes an intelligent bubbling mechanism to propagate keywords to the base-station (BS). The keywords are aggregated via a novel use of lattices to reduce network cost. We show that iBubble can emulate diffusion and generally produce less traffic by restricting the query dissemination based upon both application type and data value. Our study analytically compares iBubble and diffusion, and formally characterizes the conditions required for iBubble to outperform diffusion in both static (fixed) and dynamic (mobile) networks. We did extensive simulations, our results match our theory and show that iBubble can outperform diffusion in many heterogeneous deployments when keyword distributions are "clustered" enough to satisfy the fraction of the network involved in a query/update defined by our analytical bound. Additionally, iBubble handles mobility, fault-tolerance, and provides network diagnosis via keyword bubbling. By utilizing keywords, iBubble bridges many routing and energy problems prevalent in WSNs, and provides a simple, uniform solution.},
keywords={query processing;routing protocols;wireless sensor networks;iBubble;multi-keyword routing protocol;heterogeneous wireless sensor networks;data querying;query interface;intelligent bubbling mechanism;Routing protocols;Wireless sensor networks;Intelligent sensors;Power generation economics;Sensor phenomena and characterization;Intelligent networks;Lattices;Costs;Telecommunication traffic;Traffic control},
doi={10.1109/INFOCOM.2008.150},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509746,
author={L. Ying and Z. Liu and D. Towsley and C. H. Xia},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Distributed Operator Placement and Data Caching in Large-Scale Sensor Networks},
year={2008},
volume={},
number={},
pages={977-985},
abstract={Recent advances in computer technology and wireless communications have enabled the emergence of stream-based sensor networks. In such sensor networks, real-time data are generated by a large number of distributed sources. Queries are made that may require sophisticated processing and filtering of the data. A query is represented by a query graph. In order to reduce the data transmission and to better utilize resources, it is desirable to place operators of the query graph inside the network, and thus to perform in-network processing. Moreover, given that various queries occur with different frequencies and that only a subset of sensor data may actually be queried, caching intermediate data objects inside the network can help improve query efficiency. In this paper, we consider the problem of placing both operators and intermediate data objects inside the network for a set of queries so as to minimize the total cost of storage, computation, and data transmission. We propose distributed algorithms that achieve optimal solutions for tree-structured query graph topologies and general network topologies. The algorithms converge in L<sub>max</sub>(.H<sub>Q</sub> + 1) iterations, where L<sub>max</sub> is the order of the diameter of the sensor network, and Hq represents the depth of the query graph, defined as the maximum number of operations needed for a raw data to become a final data. For a regular grid network and complete binary tree query graph, the complexity is 0(radic(N)log<sub>2</sub> M), where N is the number of nodes in the sensor network and M is the number of data objects in a query graph. The most attractive features of these algorithms are that they require only information exchanges between neighbors, can be executed asynchronously, are adaptive to cost change and topology change, and are resilient to node or link failures.},
keywords={communication complexity;data analysis;distributed algorithms;telecommunication network topology;tree data structures;trees (mathematics);wireless sensor networks;distributed operator placement;data caching;large-scale wireless sensor network;computer technology;wireless communication;stream-based sensor network;in-network processing;distributed algorithm;tree-structured query graph topology;network topology;regular grid network;complete binary tree query graph;Large-scale systems;Network topology;Computer networks;Data communication;Communications technology;Wireless communication;Wireless sensor networks;Filtering;Frequency;Computational efficiency},
doi={10.1109/INFOCOM.2008.151},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509747,
author={P. Denantes and F. Benezit and P. Thiran and M. Vetterli},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Which Distributed Averaging Algorithm Should I Choose for my Sensor Network?},
year={2008},
volume={},
number={},
pages={986-994},
abstract={Average consensus and gossip algorithms have recently received significant attention, mainly because they constitute simple and robust algorithms for distributed information processing over networks. Inspired by heat diffusion, they compute the average of sensor networks measurements by iterating local averages until a desired level of convergence. Confronted with the diversity of these algorithms, the engineer may be puzzled in his choice for one of them. As an answer to his/her need, we develop precise mathematical metrics, easy to use in practice, to characterize the convergence speed and the cost (time, message passing, energy...) of each of the algorithms. In contrast to other works focusing on time-invariant scenarios, we evaluate these metrics for ergodic <i>time-varying</i> networks. Our study is based on Oseledec's theorem, which gives an almost- sure description of the convergence speed of the algorithms of interest. We further provide upper bounds on the convergence speed. Finally, we use these tools to make some experimental observations illustrating the behavior of the convergence speed with respect to network topology and reliability in both average consensus and gossip algorithms.},
keywords={distributed algorithms;telecommunication network reliability;telecommunication network topology;time-varying networks;wireless sensor networks;distributed averaging algorithm;average consensus algorithm;gossip algorithm;distributed information processing;ergodic time-varying network;network topology;network reliability;wireless sensor network;Convergence;Robustness;Information processing;Computer networks;Thermal sensors;Power engineering and energy;Costs;Message passing;Upper bound;Network topology},
doi={10.1109/INFOCOM.2008.152},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509748,
author={R. Gummadi and K. Jung and D. Shah and R. Sreenivas},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Feasible Rate Allocation in Wireless Networks},
year={2008},
volume={},
number={},
pages={995-1003},
abstract={Rate allocation is a fundamental problem in the operation of a wireless network because of the necessity to schedule the operation of mutually interfering links between the nodes. Among the many reasons behind the importance of efficiently determining the membership of an arbitrary rate vector in the feasibility region, is its high relevance in optimal cross layer design. A key feature in a wireless network is that links without common nodes can also conflict (secondary interference constraints). While the node exclusive model problem has efficient algorithms, it has long been known that this is a hard problem with these additional secondary constraints. However, wireless networks are usually deployed in geographic areas that do not span the most general class of all graphs possible. This is the underlying theme of this paper, where we provide algorithms for two restricted instances of wireless network topologies. In the first tractable instance, we consider nodes placed arbitrarily in a region such that (a) the node density is bounded, and (b) a node can only transmit or interfere with other nodes that are within a certain limited radius. We obtain a simple (1 - epsi) polynomial-time approximation scheme for checking feasibility (for any epsi &gt; 0). The second instance considers the membership problem of an arbitrary rate-vector in the feasible set, where the nodes are distributed within a slab of fixed width (there are no density assumptions). Specifically, the results in [13] are shown to extend to a much more general class of graphs, which we call the (d<sub>min</sub>,d<sub>max</sub>) class of graphs, and this generalization is used to obtain a strongly polynomial time algorithm that decides membership of a rate-vector where the hosts are distributed within an infinite corridor with fixed cross-section.},
keywords={computational complexity;radio links;radio networks;scheduling;telecommunication network topology;feasible rate allocation;wireless network topologies;mutually interfering link scheduling;polynomial-time approximation scheme;membership problem;arbitrary rate-vector;Wireless networks;Peer to peer computing;Polynomials;Interference constraints;Scheduling algorithm;Communications Society;Cross layer design;Network topology;Slabs;Engineering profession},
doi={10.1109/INFOCOM.2008.153},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509749,
author={L. Li and M. Pal and Y. R. Yang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Proportional Fairness in Multi-Rate Wireless LANs},
year={2008},
volume={},
number={},
pages={1004-1012},
abstract={In multi-rate wireless LANs, throughput-based fair bandwidth allocation can lead to drastically reduced aggregate throughput. To balance aggregate throughput while serving users in a fair manner, proportional fair or time-based fair scheduling has been proposed to apply at each access point (AP). However, since a realistic deployment of wireless LANs can consist of a network of APs, this paper considers proportional fairness in this much wider setting. Our technique is to intelligently associate users with APs to achieve optimal proportional fairness in a network of APs. We propose two approximation algorithms for periodical offline optimization. Our algorithms are the first approximation algorithms in the literature with a tight worst-case guarantee for the NP-hard problem. Our simulation results demonstrate that our algorithms can obtain an aggregate throughput which can be as much as 2.3 times more than that of the max-min fair allocation in 802.11b. While maintaining aggregate throughput, our approximation algorithms outperform the default user-AP association method in the 802.11b standard significantly in terms of fairness.},
keywords={approximation theory;bandwidth allocation;scheduling;wireless LAN;multi-rate wireless LAN;throughput-based fair bandwidth allocation;time-based fair scheduling;approximation algorithms;NP-hard problem;802.11b;Wireless LAN;Approximation algorithms;Channel allocation;Throughput;Aggregates;Vectors;Optimal control;Wireless networks;Bandwidth;Proportional control},
doi={10.1109/INFOCOM.2008.154},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509750,
author={Y. Gao and J. C. Hou and H. Nguyen},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Topology Control for Maintaining Network Connectivity and Maximizing Network Capacity under the Physical Model},
year={2008},
volume={},
number={},
pages={1013-1021},
abstract={In this paper we study the issue of topology control under the physical signal-to-interference-noise-ratio (SINR) model, with the objective of maximizing network capacity. We show that existing graph-model-based topology control captures interference inadequately under the physical SINR model, and as a result, the interference in the topology thus induced is high and the network capacity attained is low. Towards bridging this gap, we propose a centralized approach, called spatial reuse maximizer (MaxSR), that combines a power control algorithm T2P with a topology control algorithm P2T. T2P optimizes the assignment of transmit power given a fixed topology, where by optimality we mean that the transmit power is so assigned that it minimizes the average interference degree (defined as the number of interfering nodes that may interfere with the ongoing transmission on a link) in the topology. P2T, on the other hand, constructs, based on the power assignment made in T2P, a new topology by deriving a spanning tree that gives the minimal interference degree. By alternately invoking the two algorithms, the power assignment quickly converges to an operational point that maximizes the network capacity. We formally prove the convergence of MaxSR. We also show via simulation that the topology induced by MaxSR outperforms that derived from existing topology control algorithms by 50%-110% in terms of maximizing the network capacity.},
keywords={graph theory;radio networks;telecommunication network topology;network connectivity;network capacity;signal-to-interference-noise-ratio;graph-model-based topology control;spatial reuse maximizer;power control algorithm;Network topology;Interference;Communication system control;Signal to noise ratio;Transmitters;Communications Society;Computer science;Power control;Centralized control;Peer to peer computing},
doi={10.1109/INFOCOM.2008.155},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509751,
author={C. Hua and R. Zheng},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Starvation Modeling and Identification in Dense 802.11 Wireless Community Networks},
year={2008},
volume={},
number={},
pages={1022-1030},
abstract={With the growing number of spontaneously deployed WiFi hotspots and home networks, end-users often experience significant performance degradation or even starvation. However, we observe that tuning individual system parameters (channel, Tx power, carrier sense (CS) threshold, and transmit rate etc.) is insufficient and in some cases may lead to starvation. In this paper, we develop a comprehensive analytical model to characterize throughput of individual flows in dense IEEE 802.11 wireless community networks. The proposed model subsumes existing models for the IEEE 802.11 MAC in multihop wireless networks by accounting for heterogeneous transmission power levels and CS thresholds, as well as various sources of packet collisions. Based on the insight from the theoretical analysis and simulation results, we propose a simple identification mechanism that determines the sources of starvation using local measurements. Both the theoretical model and the identification algorithm are validated using ns-2 simulations.},
keywords={access protocols;home computing;wireless LAN;starvation modeling;dense 802.11 wireless community networks;WiFi hotspots;home networks;multihop wireless networks;heterogeneous transmission power levels;packet collisions;Analytical models;Wireless sensor networks;Throughput;Spread spectrum communication;Wireless LAN;Home automation;Degradation;Power system modeling;Wireless networks;Energy management},
doi={10.1109/INFOCOM.2008.156},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509752,
author={B. Li and S. Xie and Y. Qu and G. Y. Keung and C. Lin and J. Liu and X. Zhang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Inside the New Coolstreaming: Principles, Measurements and Performance Implications},
year={2008},
volume={},
number={},
pages={1031-1039},
abstract={The peer-to-peer (P2P) based video streaming has emerged as a promising solution for Internet video distribution. Leveraging the resource available at end users, this approach poses great potential to scale in the Internet. We have now seen the commercial P2P streaming systems that are orders of magnitude larger than the earlier academic systems. We believe understanding its basic principles and limitations are important in the design of future systems. The Coolstreaming, first released in summer 2004, arguably represented the first successful large-scale P2P live streaming system. Since then, the system has been significantly modified and commercially launched. This paper takes an inside look at the new Coolstreaming system by exposing its design options and rationale behind them, and examines their implications on streaming performance. Specifically, by leveraging a large set of traces obtained from recent live event broadcast and extensive simulations, we study the workload characteristics, system dynamics, and impact from a variety of system parameters. We demonstrate that there is a highly skewed resource distribution in such systems and the performance is mostly affected by the system dynamics. In addition, we show that there are inherent correlations and fundamental trade-off among different system parameters, which can be further explored to enhance the system performance.},
keywords={Internet;peer-to-peer computing;video streaming;Coolstreaming system;peer-to-peer based video streaming;Internet video distribution;live event broadcast;resource distribution;Peer to peer computing;Streaming media;Contracts;Internet;Topology;Large-scale systems;Proposals;Delay;Communications Society;Broadcasting},
doi={10.1109/INFOCOM.2008.157},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509753,
author={T. G. Papaioannou and G. D. Stamoulis},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Achieving Honest Ratings with Reputation-Based Fines in Electronic Markets},
year={2008},
volume={},
number={},
pages={1040-1048},
abstract={The effectiveness of online feedback mechanisms for rating the performance of providers in electronic markets is vulnerable to the submission of dishonest ratings. In this paper, we deal with how to elicit honest such ratings in a competitive electronic market where each participant can occasionally act both as provider and as client. We assume that each service provision is rated by both parties involved; only upon agreement, this rating is included in the calculation of reputation for the provider's performance. We first study as a single-shot game the effectiveness of inducing, upon evidence of lying (i.e. disagreement of the submitted feedback), fixed fines to both transacted parties, yet different ones for the provider and the client. We prove that the submission of honest feedback can be a stable equilibrium for the whole market under certain initial system conditions. Then, we refine our game-model for repeated transactions and calculate proper different reputation-based fines for lying. These fines enable the submission of honest feedback as a stable Nash equilibrium of the repeated game and reduce the social losses due to unfair punishments. Finally, we argue that our model is appropriate for analyzing actual electronic markets, and we investigate the impact of employing our approach to the feedback schemes of such markets.},
keywords={electronic commerce;game theory;security of data;reputation-based fines;electronic markets;online feedback mechanisms;honest ratings;single-shot game;lying evidence;stable Nash equilibrium;Consumer electronics;Feedback;Games;Peer to peer computing;Computer science;Communications Society;Nash equilibrium;Quality of service;Context-aware services;Programming profession},
doi={10.1109/INFOCOM.2008.158},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509754,
author={Z. Yao and D. Loguinov},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Understanding Disconnection and Stabilization of Chord},
year={2008},
volume={},
number={},
pages={1049-1057},
abstract={Previous analytical work [15], [16] on the resilience of P2P networks has been restricted to disconnection arising from simultaneous failure of all neighbors in routing tables of participating users. In this paper, we focus on a different technique for maintaining consistent graphs - Chord's successor sets and periodic stabilizations - under both static and dynamic node failure. We derive closed-form models for the probability that Chord remains connected under both types of node failure and show the effect of using different stabilization interval lengths (i.e., exponential, uniform, and constant) on the probability of partitioning in Chord.},
keywords={graph theory;network theory (graphs);peer-to-peer computing;probability;peer-to-peer network;consistent graph;Chord successor set;periodic stabilization;dynamic node failure;static node failure;probability;Peer to peer computing;Routing;Resilience;Failure analysis;Communications Society;Computer science;USA Councils;IP networks;Computer networks;Delay},
doi={10.1109/INFOCOM.2008.159},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509755,
author={D. Ren and Y. -. Li and S. -. Chan},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={On Reducing Mesh Delay for Peer-to-Peer Live Streaming},
year={2008},
volume={},
number={},
pages={1058-1066},
abstract={Peer-to-peer (P2P) technology has emerged as a promising scalable solution for live streaming to large group. In this paper, we address the design of overlay which achieves low source-to-peer delay, is robust to user churn, accommodates of asymmetric and diverse uplink bandwidth, and continuously improves based on existing user pool. A natural choice is the use of mesh, where each peer is served by multiple parents. Since the peer delay in a mesh depends on its longest path through its parents, we study how to optimize such delay while meeting a certain streaming rate requirement. We first formulate the minimum delay mesh problem and show that it is NP-hard. Then we propose a centralized heuristic based on complete knowledge which serves as our benchmark and optimal solution for all the other schemes under comparison. Our heuristic makes use of the concept of power in network given by the ratio of throughput and delay. By maximizing the network power, our heuristic achieves very low delay. We then propose a simple distributed algorithm where peers select their parents based on the power concept. The algorithm makes continuous improvement on delay until some minimum delay is reached. Simulation results show that our distributed protocol performs close to the centralized one, and substantially outperforms traditional and state-of-the-art approaches.},
keywords={computational complexity;optimisation;peer-to-peer computing;protocols;video streaming;peer-to-peer live streaming;mesh delay;diverse uplink bandwidth;asymmetric uplink bandwidth;NP-hard problem;distributed algorithm;distributed protocol;Delay;Peer to peer computing;Bandwidth;Streaming media;Robustness;Protocols;Communications Society;Computer science;Throughput;Distributed algorithms},
doi={10.1109/INFOCOM.2008.160},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509756,
author={F. Hao and M. Kodialam and T. V. Lakshman},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Incremental Bloom Filters},
year={2008},
volume={},
number={},
pages={1067-1075},
abstract={A bloom filter is a randomized data structure for performing approximate membership queries. It is being increasingly used in networking applications ranging from security to routing in peer to peer networks. In order to meet a given false positive rate, the amount of memory required by a bloom filter is a function of the number of elements in the set. We consider the problem of minimizing the memory requirements in cases where the number of elements in the set is not known in advance but the distribution or moment information of the number of elements is known. We show how to exploit such information to minimize the expected amount of memory required for the filter. We also show how this approach can significantly reduce memory requirement when bloom filters are constructed for multiple sets in parallel. We show analytically as well as experiments on synthetic and trace data that our approach leads to one to three orders of magnitude reduction in memory compared to a standard bloom filter.},
keywords={data structures;peer-to-peer computing;query processing;incremental bloom filters;randomized data structure;membership queries;peer to peer networks;memory requirements;Information filtering;Information filters;Upper bound;Data structures;Routing;Distributed databases;Testing;Communications Society;USA Councils;Data security},
doi={10.1109/INFOCOM.2008.161},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509757,
author={M. H. Gunes and K. Sarac},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Resolving Anonymous Routers in Internet Topology Measurement Studies},
year={2008},
volume={},
number={},
pages={1076-1084},
abstract={Internet measurement studies utilize traceroute to collect path traces from the Internet. A router that does not respond to a traceroute query is referred to as an anonymous router and is represented by a '*' in the traceroute output. Anonymous router resolution refers to the task of identifying the occurrences of '*'s that belong to the same router in the underlying network. This task is an important step in building traceroute-based topology maps and obtaining an optimum solution is shown to be NP-complete. In this paper, we use a novel technique from graph data mining field to build an efficient solution. The results of our experiments on both synthetic and genuine topologies show a significant improvement in accuracy and effectiveness over the existing approaches.},
keywords={computational complexity;Internet;optimisation;telecommunication network routing;telecommunication network topology;anonymous routers;Internet topology measurement;traceroute query;traceroute-based topology maps;NP-complete;graph data mining;Internet;Network topology;Computational complexity;IP networks;Communications Society;Computer science;Probes;Spine;Data mining;Character generation},
doi={10.1109/INFOCOM.2008.162},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509758,
author={P. Fraigniaud and E. Lebhar and L. Viennot},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={The Inframetric Model for the Internet},
year={2008},
volume={},
number={},
pages={1085-1093},
abstract={A large amount of algorithms has recently been designed for the Internet under the assumption that the distance defined by the round-trip delay (RTT) is a metric. Moreover, many of these algorithms (e.g., overlay network construction, routing scheme design, sparse spanner construction) rely on the assumption that the metric has bounded ball growth or bounded doubling dimension. This paper analyzes the validity of these assumptions and proposes a tractable model matching experimental observations. On the one hand, based on Skitter data collected by CAIDA and King matrices of Meridian and P2PSim projects, we verify that the ball growth of the Internet, as well as its doubling dimension, can actually be quite large. Nevertheless, we observed that the doubling dimension is much smaller when restricting the measures to balls of large enough radius. Moreover, by computing the number of balls of radius r required to cover balls of radius R &gt; r, we observed that this number grows with R much slower than what is predicted by a large doubling dimension. On the other hand, based on data collected on the PlanetLab platform by the All-Sites-Pings project, we confirm that the triangle inequality does not hold for a significant fraction of the nodes. Nevertheless, we demonstrate that RTT measures satisfy a weak version of the triangle inequality: there exists a small constant p such that for any triple u, v, w, we have RTT(u,v) les rho-max{RTT(u,w),RTT(w,v)}. (Smaller bounds on p can even be obtained when the triple u, v, w is skewed). We call inframetric a distance function satisfying this latter inequality. Inframetrics subsume standard metrics and ultrametrics. Based on inframetrics and on our observations concerning the doubling dimension, we propose an analytical model for Internet RTT latencies. This model is tuned by a small set of parameters concerning the violation of the triangle inequality and the geometrical dimension of the network. We demonstrate the tractability of our model by designing a simple and efficient compact routing scheme with low stretch. Precisely, the scheme has constant multiplicative stretch and logarithmic additive stretch.},
keywords={delays;Internet;performance evaluation;telecommunication network routing;inframetric model;Internet;round-trip delay;compact routing scheme;triangle inequality;Internet;Algorithm design and analysis;Delay;Routing;Extraterrestrial measurements;Communications Society;Sparse matrices;Peer to peer computing;Analytical models;Solid modeling},
doi={10.1109/INFOCOM.2008.163},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509759,
author={X. Wang and X. Liu and D. Loguinov},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Modeling the Evolution of Degree Correlation in Scale-Free Topology Generators},
year={2008},
volume={},
number={},
pages={1094-1102},
abstract={In this paper, we examine the asymptotic behavior of degree correlation (i.e., the joint degree distribution of adjacent nodes) in several scale-free topology generators GED [13], PLRG [1], GLP [10], BA [3], AB [2]. We present a unifying analytical framework that allows tractable analysis of degree correlation in all studied models and derive asymptotic formulas of two degree correlation metrics - assortativity and clustering. Our results indicate that all studied generators become uncorrelated as graph size increases, which is inconsistent with time-invariance of these metrics in real networks such as the Internet [36], [48], [50]. Since the class of degree-based generators is incapable of reproducing evolving characteristics of the Internet, we study three other models that evolve graphs using different rules than preference of degree (e.g., based on random walks [50], optimization [17], and geometry [23]) and show using simulations that these models are much more viable alternatives for replicating the complex structure of Internet-like graphs.},
keywords={correlation methods;graph theory;Internet;telecommunication network topology;scale-free topology generator;degree correlation evolution;assortativity degree correlation metric;clustering degree correlation metric;Internet-like graph;Topology;Internet;Solid modeling;Communications Society;Peer to peer computing;IP networks;Character generation;Geometry;Collaboration;Couplings},
doi={10.1109/INFOCOM.2008.164},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509760,
author={C. Joo and X. Lin and N. B. Shroff},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Understanding the Capacity Region of the Greedy Maximal Scheduling Algorithm in Multi-Hop Wireless Networks},
year={2008},
volume={},
number={},
pages={1103-1111},
abstract={In this paper, we characterize the performance of an important class of scheduling schemes, called greedy maximal scheduling (GMS), for multi-hop wireless networks. While a lower bound on the throughput performance of GMS is relatively well-known in the simple node-exclusive interference model, it has not been thoroughly explored in the more general K-hop interference model. Moreover, empirical observations suggest that the known bounds are quite loose, and that the performance of GMS is often close to optimal. In this paper, we provide a number of new analytic results characterizing the performance limits of GMS. We first provide an equivalent characterization of the efficiency ratio of GMS through a topological property called the local-pooling factor of the network graph. We then develop an iterative procedure to estimate the local-pooling factor under a large class of network topologies and interference models. We use these results to study the worst-case efficiency ratio of GMS on two classes of network topologies. First, we show how these results can be applied to tree networks to prove that GMS achieves the full capacity region in tree networks under the K-hop interference model. Second, we show that the worst-case efficiency ratio of GMS in geometric network graphs is between 1/6 and 1/3.},
keywords={greedy algorithms;radio networks;radiofrequency interference;scheduling;telecommunication network topology;greedy maximal scheduling algorithm;multi-hop wireless networks;node-exclusive interference model;k-hop interference model;network graph;network topologies;Scheduling algorithm;Spread spectrum communication;Wireless networks;Interference;Network topology;Throughput;Tree graphs;Solid modeling;Communications Society;Peer to peer computing},
doi={10.1109/INFOCOM.2008.165},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509761,
author={M. Li and H. Zhu and Y. Xiao and I. Chlamtac and B. Prabhakaran},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Adaptive Frame Concatenation Mechanisms for QoS in Multi-Rate Wireless Ad Hoc Networks},
year={2008},
volume={},
number={},
pages={1112-1120},
abstract={Providing quality of service (QoS) to users in a wireless ad-hoc network is a key concern for service providers. With the availability of multiple rates in IEEE 802.11a/b/g wireless LANs, it is desirable to improve the network capacity and temporal fairness by sending multiple consecutive frames (also referred as frame concatenation mechanism) over high rate links, as proposed in opportunistic auto rate (OAR). However, OAR does not consider the effect of frame sizes and may yield unsatisfactory performance for high priority multimedia flows transmitting over low rate links. Therefore, a more appropriate frame concatenation strategy and a corresponding service differentiation scheme should be devised to provide better performance for high priority voice/video flows than low priority data flows, under various channel rate scenarios. In this paper, we first analyze the effect of frame size on the performance of OAR. Then, we propose a general concatenation mechanism (GCM), a more accurate frame concatenation mechanism for multi-rate MAC with better fairness. Finally, we propose two mechanisms: adaptive weighted fair frame concatenation mechanism (AWFCM) and adaptive QoS aware frame concatenation mechanism (AQCM), for supporting service differentiation and QoS in multi-rate wireless ad hoc networks. The primary idea is to adjust the number of concatenated frames based on flow weights/priorities, frame sizes, link rates, and network traffic. Simulation results show that the proposed mechanisms achieve desirable performance on supporting multimedia applications in multi-rate wireless ad-hoc networks.},
keywords={ad hoc networks;quality of service;telecommunication traffic;wireless LAN;multirate wireless ad hoc networks;quality of service;IEEE 802.11a/b/g;wireless LAN;general concatenation mechanism;adaptive weighted fair frame concatenation mechanism;adaptive QoS aware frame concatenation mechanism;network traffic;Mobile ad hoc networks;Quality of service;USA Councils;Telecommunication traffic;Computer science;Concatenated codes;Ad hoc networks;Wireless LAN;Streaming media;Communications Society},
doi={10.1109/INFOCOM.2008.166},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509762,
author={S. Deb and K. Mangla and K. V. M. Naidu},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Fast and Distributed Computation of Schedules in Wireless Networks},
year={2008},
volume={},
number={},
pages={1121-1129},
abstract={In a wireless network with <i>node</i> <i>exclusive</i> <i>spectrum</i> <i>sharing</i>, two popular schedules are maximum weight matching (MWM) schedule and maximum size matching (MSM) schedule. The former has been proved to be throughput optimal and has superior delay properties, and the latter schedules as many links, with packets to transmit, as possible. However, it is challenging to design algorithms for computing these schedules that (i) are distributed, (i.e., only local message exchanges between neighboring nodes are permitted) (ii) have low running times (iii) exchanges a small number of messages. In this paper, we develop algorithms that satisfy these properties and also provide good approximations to MWM and MSM schedules. We also note that constant approximation to MWM leads to improved delay properties. We refer to a round as a length of time over which every node in the network can make at most one message-transmission attempt. We propose distributed algorithms for computing (i) 1/2 - epsi e approximation to MWM schedule in O(log(1/epsi) log<sup>2</sup> n) rounds, and (ii) 2/3 - epsi approximation to MSM schedule in O((1/epsi) log<sup>2</sup> n) rounds, where n is the network size. Simulation results with a popular model for wireless ad-hoc networks demonstrate that (i) our algorithms perform within 85% - 95% of the optimal in many scenarios, and (ii) the time-complexity of the algorithms can be reduced considerably in practice. The number of message transmissions for both our algorithms scale as O(n log<sup>2</sup> n). In summary, ours is the first work to (i) provide half (two-third) approximate distribute algorithms for computing MWM (MSM) schedule with logarithmic time- complexity and quasi-linear message exchanges (ii) demonstrate that the algorithms are close to optimal for realistic topologies.},
keywords={ad hoc networks;computational complexity;data communication;scheduling;node exclusive spectrum sharing;maximum weight matching schedule;maximum size matching schedule;message-transmission;distributed algorithms;wireless ad-hoc networks;time-complexity;quasi-linear message exchanges;Computer networks;Distributed computing;Processor scheduling;Wireless networks;Scheduling algorithm;Delay;Throughput;Algorithm design and analysis;Distributed algorithms;Ad hoc networks},
doi={10.1109/INFOCOM.2008.167},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509763,
author={A. Srinivas and E. Modiano},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Joint Node Placement and Assignment for Throughput Optimization in Mobile Backbone Networks},
year={2008},
volume={},
number={},
pages={1130-1138},
abstract={We study the novel hierarchical architecture of Mobile Backbone Networks. In such networks, a set of mobile backbone nodes (MBNs) are deployed to provide an end-to-end communications capability for the regular nodes (RNs). In this work, we address the joint problem of placing a fixed number K MBNs in the plane, and assigning each RN to exactly one MBN. We formulate and solve two problems under a general communications model. The first is the maximum fair placement and assignment (MFPA) problem in which the objective is to maximize the throughput of the minimum throughput RN. The second is the maximum throughput placement and assignment (MTPA) problem, in which the objective is to maximize the aggregate throughput of the RNs. Our main result is a novel optimal polynomial time algorithm for the MFPA problem for fixed K. For a restricted version of the MTPA problem, we develop an optimal polynomial time algorithm for Kles2. We also develop two heuristic algorithms for both problems, including an approximation algorithm for which we bound the worst case performance loss. Finally, we present simulation results comparing the performance of the various algorithms developed in the paper.},
keywords={computational complexity;mobile radio;optimisation;joint node placement assignment;throughput optimization;mobile backbone network hierarchical architecture;end-to-end communications capability;regular nodes;maximum fair placement assignment problem;MFPA problem;maximum throughput placement assignment problem;MTPA problem;optimal polynomial time algorithm;Throughput;Spine;Peer to peer computing;Mobile communication;Aggregates;Polynomials;Approximation algorithms;Wireless sensor networks;Mobile ad hoc networks;Telecommunication network reliability},
doi={10.1109/INFOCOM.2008.168},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509764,
author={G. Zussman and A. Brzezinski and E. Modiano},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Multihop Local Pooling for Distributed Throughput Maximization in Wireless Networks},
year={2008},
volume={},
number={},
pages={1139-1147},
abstract={Efficient operation of wireless networks requires distributed routing and scheduling algorithms that take into account interference constraints. Recently, a few algorithms for networks with primary- or secondary-interference constraints have been developed. Due to their distributed operation, these algorithms can achieve only a guaranteed fraction of the maximum possible throughput. It was also recently shown that if a set of conditions (known as Local Pooling) is satisfied, simple distributed scheduling algorithms achieve 100% throughput. However, previous work regarding Local Pooling focused mostly on obtaining abstract conditions and on networks with single-hop interference or single-hop traffic. In this paper, we identify several graph classes that satisfy the Local Pooling conditions, thereby enabling the use of such graphs in network design algorithms. Then, we study the multihop implications of Local Pooling. We show that in many cases, as the interference degree increases, the Local Pooling conditions are more likely to hold. Consequently, although increased interference reduces the maximum achievable throughput of the network, it tends to enable distributed algorithms to achieve 100% of this throughput. Regarding multihop traffic, we show that if the network satisfies only the single-hop Local Pooling conditions, distributed joint routing and scheduling algorithms are not guaranteed to achieve maximum throughput. Therefore, we present new conditions for Multihop Local Pooling, under which distributed algorithms achieve 100% throughout. Finally, we identify network topologies in which the conditions hold and discuss the algorithmic implications of the results.},
keywords={distributed algorithms;graph theory;optimisation;radio networks;scheduling;telecommunication network routing;telecommunication network topology;telecommunication traffic;multihop local pooling;distributed throughput maximization;wireless network topology;distributed routing algorithm;distributed scheduling algorithm;single-hop interference;single-hop traffic;network design algorithm;graph theory;Spread spectrum communication;Throughput;Wireless networks;Scheduling algorithm;Routing;Interference constraints;Telecommunication traffic;Distributed algorithms;Algorithm design and analysis;Network topology},
doi={10.1109/INFOCOM.2008.169},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509765,
author={D. Chafekar and D. Levin and V. S. A. Kumar and M. V. Marathe and S. Parthasarathy and A. Srinivasan},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Capacity of Asynchronous Random-Access Scheduling in Wireless Networks},
year={2008},
volume={},
number={},
pages={1148-1156},
abstract={We study the throughput capacity of wireless networks which employ (asynchronous) random-access scheduling as opposed to deterministic scheduling. The central question we answer is: how should we set the channel-access probability for each link in the network so that the network operates close to its optimal throughput capacity? We design simple and distributed channel-access strategies for random-access networks which are provably competitive with respect to the optimal scheduling strategy, which is deterministic, centralized, and computationally infeasible. We show that the competitiveness of our strategies are nearly the best achievable via random-access scheduling, thus establishing fundamental limits on the performance of random- access. A notable outcome of our work is that random access compares well with deterministic scheduling when link transmission durations differ by small factors, and much worse otherwise. The distinguishing aspects of our work include modeling and rigorous analysis of asynchronous communication, asymmetry in link transmission durations, and hidden terminals under arbitrary link-conflict based wireless interference models.},
keywords={probability;radio links;radio networks;scheduling;wireless channels;asynchronous random-access scheduling;wireless networks;throughput capacity;channel access probability;radio link;Wireless networks;Optimal scheduling;Throughput;Processor scheduling;Computer science;Computer networks;Educational institutions;Interference;Routing;Peer to peer computing},
doi={10.1109/INFOCOM.2008.170},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509766,
author={K. Yang and Y. Wu and J. Huang and X. Wang and S. Verdu},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Distributed Robust Optimization for Communication Networks},
year={2008},
volume={},
number={},
pages={1157-1165},
abstract={Robustness of optimization models for networking problems has been an under-explored area. Yet most existing algorithms for solving robust optimization problems are centralized, thus not suitable for many communication networking problems that demand distributed solutions. This paper represents the first step towards building a framework for designing distributed robust optimization algorithms. We first discuss several models for describing parameter uncertainty sets that can lead to decomposable problem structures. These models include general polyhedron, D-norm, and ellipsoid. We then apply these models to solve robust power control in wireless networks and robust rate control in wireline networks. In both applications, we propose distributed algorithms that converge to the optimal robust solution. Various tradeoffs among performance, robustness, and distributiveness are illustrated both analytically and through simulations.},
keywords={optimisation;radio networks;set theory;telecommunication network management;distributed robust optimization;communication networks;parameter uncertainty sets;general polyhedron;D-norm;ellipsoid;robust power control;wireless networks;robust rate control;wireline networks;Robustness;Communication networks;Robust control;Buildings;Algorithm design and analysis;Design optimization;Uncertain systems;Ellipsoids;Power control;Wireless networks},
doi={10.1109/INFOCOM.2008.171},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509767,
author={D. Chafekar and V. S. A. Kumar and M. V. Marathe and S. Parthasarathy and A. Srinivasan},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Approximation Algorithms for Computing Capacity of Wireless Networks with SINR Constraints},
year={2008},
volume={},
number={},
pages={1166-1174},
abstract={A fundamental problem in wireless networks is to estimate its throughput capacity - given a set of wireless nodes, and a set of connections, what is the maximum rate at which data can be sent on these connections. Most of the research in this direction has focused on either random distributions of points, or has assumed simple graph-based models for wireless interference. In this paper, we study capacity estimation problem using the more general Signal to Interference Plus Noise Ratio (SINR) model for interference, on arbitrary wireless networks. The problem becomes much harder in this setting, because of the non-locality of the SINR model. Recent work by Moscibroda et al. (2006) has shown that the throughput in this model can differ from graph based models significantly. We develop polynomial time algorithms to provably approximate the total throughput in this setting.},
keywords={approximation theory;graph theory;radio networks;radiofrequency interference;approximation algorithm;wireless network;graph-based model;wireless interference;signal to interference plus noise ratio;polynomial time algorithm;Approximation algorithms;Computer networks;Wireless networks;Signal to noise ratio;Throughput;Computer science;Protocols;Interference constraints;Algorithm design and analysis;Propagation losses},
doi={10.1109/INFOCOM.2008.172},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509768,
author={X. Zhu and R. Sarkar and J. Gao and J. S. B. Mitchell},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Light-Weight Contour Tracking in Wireless Sensor Networks},
year={2008},
volume={},
number={},
pages={1175-1183},
abstract={We study the problem of contour tracking with binary sensors, an important problem for monitoring spatial signals and tracking group targets. In particular, we track the boundaries of the blobs of interest and capture the topological changes as the blobs merge or split. Only the nodes on the boundaries of these deformable blobs stay active and the repair cost is proportional to the size of the contour changes. Our algorithm is completely distributed, requires only local information, and yet captures the global topological properties. The algorithm performs a fundamental monitoring function and is a foundation for further information processing of spatial sensor data.},
keywords={radio tracking;target tracking;wireless sensor networks;wireless sensor network;light-weight contour tracking;spatial signal monitoring;group target tracking;Wireless sensor networks;Monitoring;Chemical sensors;Target tracking;Pollution;Sensor phenomena and characterization;Peer to peer computing;Costs;Signal processing algorithms;Signal processing},
doi={10.1109/INFOCOM.2008.173},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509769,
author={T. He and J. A. Stankovic and R. Stoleru and Y. Gu and Y. Wu},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Essentia: Architecting Wireless Sensor Networks Asymmetrically},
year={2008},
volume={},
number={},
pages={1184-1192},
abstract={In this paper, we advocate asymmetric function placement as one of guiding principles to architect sensor network systems. We demonstrate its generic applicability and effectiveness by applying this principle to three typical sensor network technologies, namely, localization (<i>Spotlight</i>), sensing (<i>uSense</i>) and communication (<i>mNets</i>). These technologies have very dissimilar features, representing a wide spectrum of system design requirements. We have invested significant effort to design, implement and evaluate our techniques on TinyOS/Mote testbeds. The results from several running systems indicate that asymmetric function placement is a powerful guiding principle to achieve <i>efficiency</i> and <i>high-performance</i> simultaneously in wireless sensor networks. At the end, we exam the system features that discourage the use of asymmetric function placement and approaches to address them.},
keywords={telecommunication network planning;wireless sensor networks;wireless sensor network system design;asymmetric function placement;TinyOS/Mote testbed;Wireless sensor networks;Sensor systems;Computer science;Internet;IP networks;Computer architecture;Communications Society;Testing;Surveillance;Power system protection},
doi={10.1109/INFOCOM.2008.174},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509770,
author={Y. Wu and J. A. Stankovic and T. He and S. Lin},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Realistic and Efficient Multi-Channel Communications in Wireless Sensor Networks},
year={2008},
volume={},
number={},
pages={1193-1201},
abstract={This paper demonstrates how to use multiple channels to improve communication performance in Wireless Sensor Networks (WSNs). We first investigate multi-channel realities in WSNs through intensive empirical experiments with Micaz motes. Our study shows that current multi-channel protocols are not suitable for WSNs, because of the small number of available channels and unavoidable time errors found in real networks. With these observations, we propose a novel tree-based multichannel scheme for data collection applications, which allocates channels to disjoint trees and exploits parallel transmissions among trees. In order to minimize interference within trees, we define a new channel assignment problem which is proven NP- complete. Then we propose a greedy channel allocation algorithm which outperforms other schemes in dense networks with a small number of channels.We implement our protocol, called TMCP, in a real testbed. Through both simulation and real experiments, we show that TMCP can significantly improve network throughput and reduce packet losses. More importantly, evaluation results show that TMCP better accommodates multi-channel realities found in WSNs than other multi-channel protocols.},
keywords={access protocols;channel allocation;greedy algorithms;wireless sensor networks;multi-channel communications;wireless sensor networks;tree-based multichannel scheme;data collection applications;parallel transmissions;channel assignment;greedy channel allocation algorithm;TMCP;Wireless sensor networks;Interference;Throughput;Media Access Protocol;Computer science;Testing;Telecommunication network reliability;Frequency synchronization;Communications Society;Helium},
doi={10.1109/INFOCOM.2008.175},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509771,
author={B. Blaszczyszyn and B. Radunovic},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Using Transmit-Only Sensors to Reduce Deployment Cost of Wireless Sensor Networks},
year={2008},
volume={},
number={},
pages={1202-1210},
abstract={We consider a hybrid wireless sensor network with regular and transmit-only sensors. The transmit-only sensors do not have the receiver circuit (or have a very low data-rate one), hence are cheaper and less energy consuming, but their transmissions cannot be coordinated. Regular sensors, also called cluster-heads, are responsible for receiving information from the transmit-only sensors and forwarding it to sinks. The main goal of such a hybrid network is to reduce the cost of deployment while achieving some performance goals (minimum coverage, sensing rate, etc). In this paper we are interested in the communication between the transmit-only sensors and the cluster-heads. Since the sensors have no feedback, their transmission schedule is random. The cluster-heads, on the contrary, adapt their reception policy to achieve the performance goals. Using a mathematical model of random access networks developed in [1] we define and evaluate packet admission policies for different performance criteria. We show that the proposed hybrid network architecture, using the optimal policies, can achieve substantial dollar cost and power consumption savings as compared to conventional architectures while providing the same performance guarantees.},
keywords={wireless sensor networks;transmit-only sensors;deployment cost reduction;wireless sensor networks;regular sensors;mathematical model;random access network;packet admission policy;power consumption;Wireless sensor networks;Energy consumption;Telecommunication traffic;Mathematical model;Throughput;Communications Society;Circuits;Feedback;Cost function;Monitoring},
doi={10.1109/INFOCOM.2008.176},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509772,
author={J. T. Chiang and Y. -. Hu},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Dynamic Jamming Mitigation for Wireless Broadcast Networks},
year={2008},
volume={},
number={},
pages={1211-1219},
abstract={Wireless communications are inherently symmetric; that is, it takes an attacker the same amount of power to modulate a signal as it does for a legitimate node to modulate the same signal. As a result, wireless communications are often susceptible to the jamming attack in which the attacker injects a high level of noise into the system. Spread spectrum has long been used to resist jamming attacks in unicast environments, or when the jammer has less information than the other receivers. Recently, we proposed a scheme for broadcast jamming mitigation based on spread spectrum and a binary key tree and showed some improvements over a multiple-unicast system. In this paper, we extend our previous work in five ways. First, we provide a theoretical result that under our scheme, jammers can cause only a limited number of losses. Second, we develop a dynamic tree-remerging scheme that achieves higher power efficiency than previously proposed schemes, and scales to an arbitrary number of receivers without increasing the number of codes in use; in particular, we send each transmission on at most 2j + 1 codes, where j is the number of jammers. Third, we show that our scheme is close to optimal, demonstrating that under certain realistic restrictions, the system cannot escape jamming without using at least j +1 codes. Fourth, we provide a detailed analysis of false alarm rates, showing both experimental and theoretical results. Finally, we perform a more extensive analysis of our system using both a chip-accurate MATLAB simulation and a bit-accurate event-driven simulation in the ns-2 network simulator; these simulations demonstrate that our scheme approaches the best possible performance.},
keywords={jamming;radio networks;spread spectrum communication;dynamic jamming mitigation;wireless broadcast networks;spread spectrum;binary key tree;multiple-unicast system;dynamic tree-remerging scheme;MATLAB simulation;Jamming;Broadcasting;Analytical models;Discrete event simulation;Wireless communication;Spread spectrum communication;Working environment noise;Noise level;Resists;Unicast},
doi={10.1109/INFOCOM.2008.177},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509773,
author={L. Ma and A. Y. Teymorian and X. Cheng},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Hybrid Rogue Access Point Protection Framework for Commodity Wi-Fi Networks},
year={2008},
volume={},
number={},
pages={1220-1228},
abstract={We develop a practical and comprehensive hybrid rogue access point (AP) detection framework for commodity Wi- Fi networks. It is the first scheme that combines the distributed wireless media surveillance and the centralized wired end socket level traffic "fingerprinting" The former is designed not only to detect various types of rogue APs, but also to discover suspicious activities so as to prevent the adversaries from turning victim APs into rogue devices. Moreover, the socket level traffic fingerprinting helps our frame work to achieve a finer granularity on rogue AP detection among the existing schemes. This framework has the following nice properties: i) it requires neither specialized hardware nor modification to existing standards; ii) the proposed mechanism greatly improves the rogue AP detection probability so that network resilience is improved; iii) it provides a cost-effective solution to Wi-Fi network security enhancement by incorporating free but mature software tools; iv) it can protect the network from adversaries capable of using customized equipment and/or violating the IEEE 802.11 standard; v) its open architecture allows extra features to be easily added on in the future. Our analysis and evaluation demonstrate that this hybrid rogue AP protection framework is capable of reliably revealing rogue devices and preempting potential attacks.},
keywords={IEEE standards;security of data;wireless LAN;hybrid rogue access point protection framework;commodity Wi-Fi networks;wireless media surveillance;centralized wired end socket level traffic fingerprinting;Wi-Fi network security;IEEE 802.11 standard;Protection;Sockets;Telecommunication traffic;Fingerprint recognition;Software standards;Surveillance;Turning;Mechanical factors;Hardware;Resilience},
doi={10.1109/INFOCOM.2008.178},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509774,
author={R. Lu and X. Lin and H. Zhu and P. -. Ho and X. Shen},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={ECPP: Efficient Conditional Privacy Preservation Protocol for Secure Vehicular Communications},
year={2008},
volume={},
number={},
pages={1229-1237},
abstract={In this paper, we introduce an efficient conditional privacy preservation (ECPP) protocol in vehicular ad hoc networks (VANETs) to address the issue on anonymous authentication for safety messages with authority traceability. The proposed protocol is characterized by the generation of on-the-fly short-time anonymous keys between on-board units (OBUs) and roadside units (RSUs), which can provide fast anonymous authentication and privacy tracking while minimizing the required storage for short-time anonymous keys. We demonstrate the merits gained by the proposed protocol through extensive analysis.},
keywords={ad hoc networks;cryptography;data privacy;message authentication;mobile radio;road safety;telecommunication security;ECPP protocol;efficient conditional privacy preservation protocol;secure vehicular communications;vehicular ad hoc networks;VANET;anonymous message authentication;authority traceability;on-the-fly short-time anonymous keys;on-board units;roadside units;MANET;road safety;Privacy;Protocols;Safety;Message authentication;Target tracking;Ad hoc networks;Secure storage;Performance analysis;Road vehicles;Protection},
doi={10.1109/INFOCOM.2008.179},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509775,
author={M. Raya and P. Papadimitratos and V. D. Gligor and J. -. Hubaux},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={On Data-Centric Trust Establishment in Ephemeral Ad Hoc Networks},
year={2008},
volume={},
number={},
pages={1238-1246},
abstract={We argue that the traditional notion of trust as a relation among entities, while useful, becomes insufficient for emerging data-centric mobile ad hoc networks. In these systems, setting the data trust level equal to the trust level of the data- providing entity would ignore system salient features, rendering applications ineffective and systems inflexible. This would be even more so if their operation is ephemeral, i.e., characterized by short-lived associations in volatile environments. In this paper, we address this challenge by extending the traditional notion of trust to data-centric trust: trustworthiness attributed to node-reported data per se. We propose a framework for data-centric trust establishment: First, trust in each individual piece of data is computed; then multiple, related but possibly contradictory, data are combined; finally, their validity is inferred by a decision component based on one of several evidence evaluation techniques. We consider and evaluate an instantiation of our framework in vehicular networks as a case study. Our simulation results show that our scheme is highly resilient to attackers and converges stably to the correct decision.},
keywords={ad hoc networks;mobile radio;security of data;data centric trust establishment;ephemeral ad hoc networks;data centric mobile ad hoc networks;system salient features;evidence evaluation techniques;vehicular networks;Ad hoc networks;Peer to peer computing;Vehicles;Logic;Government;Communications Society;Computer networks;Mobile communication;Mobile ad hoc networks;Computational modeling},
doi={10.1109/INFOCOM.2008.180},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509776,
author={J. Robinson and M. Uysal and R. Swaminathan and E. Knightly},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Adding Capacity Points to a Wireless Mesh Network Using Local Search},
year={2008},
volume={},
number={},
pages={1247-1255},
abstract={Wireless mesh network deployments are popular as a cost-effective means to provide broadband connectivity to large user populations. As the network usage grows, network planners need to evolve an existing mesh network to provide additional capacity. In this paper, we study the problem of adding new capacity points (e.g., gateway nodes) to an existing mesh network. We first present a new technique for calculating gateway-limited fair capacity as a function of the contention at each gateway. Then, we present two online gateway placement algorithms that use local search operations to maximize the capacity gain on an existing network. A key challenge is that each gateway's capacity depends on the locations of other gateways and cannot be known in advance of determining a gateway placement. We address this challenge with two placement algorithms with different approaches to estimating the unknown gateway capacities. Our first placement algorithm, MinHopCount, is adapted from a solution to the facility location problem. MinHopCount minimizes path lengths and iteratively estimates the wireless capacity of each gateway location. Our second algorithm, MinContention, is adapted from a solution to the uncapacitated k-median problem and minimizes average contention on mesh nodes, i.e. the number of links in contention range of a mesh node and the number of routes using each link. We show that our gateway placement algorithms outperform a greedy heuristic by up to 64% on realistic topologies. For an example topology, we study the set of all possible gateway placements and find that there is large capacity gain between near-optimal and optimal placements, but the near-optimal placements found by local search are similar in configuration to the optimal.},
keywords={broadband networks;internetworking;optimisation;radio networks;search problems;telecommunication network planning;telecommunication network topology;wireless mesh network capacity point;broadband connectivity;network planning;gateway-limited fair capacity;online gateway placement algorithm;local search operation;capacity gain maximization;MinHopCount algorithm;facility location problem;MinContention algorithm;uncapacitated k-median problem;greedy heuristic;network topology;Wireless mesh networks;Iterative algorithms;Peer to peer computing;Network topology;IP networks;Mesh networks;Telecommunication traffic;Communications Society;Cities and towns;Throughput},
doi={10.1109/INFOCOM.2008.181},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509777,
author={M. Kim and Z. Liu and S. Parthasarathy and D. Pendarakis and H. Yang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Association Control in Mobile Wireless Networks},
year={2008},
volume={},
number={},
pages={1256-1264},
abstract={As mobile nodes roam in a wireless network, they continuously associate with different access points and perform handoff operations. However, frequent handoffs can potentially incur unacceptable delays and even interruptions for interactive applications. To alleviate these negative impacts, we present novel association control algorithms that can minimize the frequency of handoffs occurred to mobile devices. Specifically, we show that a greedy LookAhead algorithm is optimal in the offline setting, where the user's future mobility is known. Inspired by such optimality, we further propose two online algorithms, namely LookBack and Track, that operate without any future mobility information. Instead, they seek to predict the lifetime of an association using randomization and statistical approaches, respectively. We evaluate the performance of these algorithms using both analysis and trace-driven simulations. The results show that the simple LookBack algorithm has surprisingly a competitive ratio .of (log k + 2), where k is the maximum number of APs that a user can hear at any time, and the Track algorithm can achieve near-optimal performance in practical scenarios.},
keywords={greedy algorithms;mobile radio;quality of service;association control;mobile wireless networks;handoff operations;greedy LookAhead algorithm;LookBack algorithms;Track algorithms;Wireless networks;Delay;Frequency;Communication system control;Algorithm design and analysis;Communications Society;Peer to peer computing;Performance analysis;Analytical models;Quality of service},
doi={10.1109/INFOCOM.2008.182},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509778,
author={E. Bayraktaroglu and C. King and X. Liu and G. Noubir and R. Rajaraman and B. Thapa},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={On the Performance of IEEE 802.11 under Jamming},
year={2008},
volume={},
number={},
pages={1265-1273},
abstract={In this paper, we study the performance of the IEEE 802.11 MAC protocol under a range of jammers that covers both channel-oblivious and channel-aware jamming. We study two channel-oblivious jammers: a periodic jammer that jams deterministically at a specified rate, and a memoryless jammer whose signals arrive according to a Poisson process. We also develop new models for channel-aware jamming, including a reactive jammer that only jams non-colliding transmissions and an omniscient jammer that optimally adjusts its strategy according to current states of the participating nodes. Our study comprises of a theoretical analysis of the saturation throughput of 802.11 under jamming, an extensive simulation study, and a testbed to conduct real world experimentation of jamming IEEE 802.11 using GNU Radio and USRP platform. In our theoretical analysis, we use a discrete-time Markov chain analysis to derive formulae for the saturation throughput of IEEE 802.11 under memoryless, reactive and omniscient jamming. One of our key results is a characterization of optimal omniscient jamming that establishes a lower bound on the saturation throughput of 802.11 under arbitrary jammer attacks. We validate the theoretical analysis by means of Qualnet simulations. Finally, we measure the real-world performance of periodic and memoryless jammers using our GNU radio jammer prototype.},
keywords={access protocols;Markov processes;telecommunication traffic;wireless channels;wireless LAN;IEEE 802.11 MAC protocol;channel-aware jamming;channel-oblivious jamming;Poisson process;GNU radio;USRP platform;discrete-time Markov chain analysis;Jamming;Throughput;Media Access Protocol;Analytical models;Physical layer;Virtual prototyping;Testing;Radio control;Performance analysis;Wireless communication},
doi={10.1109/INFOCOM.2008.183},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509779,
author={M. U. Ilyas and H. Radha},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Measurement Based Analysis and Modeling of the Error Process in IEEE 802.15.4 LR-WPANs},
year={2008},
volume={},
number={},
pages={1274-1282},
abstract={Knowledge of the error process and related channel parameters in wireless networks is invaluable and highly instrumental in a broad range of applications. Under the IEEE 802.15.4 Low Rate-Wireless Personal Area Networks (LR- WPAN) standard, compliant devices are capable of providing two pieces of information about the channel conditions along with each received packet, the Link Quality Indication (LQI) and Received Signal Strength Indication (RSSI). Together they constitute a form of Channel State Information (CSI). This work is based on statistical and information theoretic analysis of a very extensive data set of wireless channel traffic between a transmitter and receiver, called packet traces. Data is collected in a variety of documented environments. To our knowledge, this is the first detailed trace collection effort for this type of network. The traces distinguish themselves from data sets of other studies in that they record individual bit errors as well as packets that are never detected by receivers. First, we provide a detailed analysis of the IEEE 802.15.4 wireless channel. More specifically, we provide a detailed analysis of the Bit Error Rate (BER) process at individual bit and on a packet-by-packet basis. We explore the relationship between the packet-level BER process and the LQI and RSSI processes (also observable on a packet-by- packet). The analysis shows that measurements of both LQI and RSSI provide information that allows us to reduce uncertainty about the BER. Secondly, we develop a model of the BER process that is driven by observable CSI parameters. Thirdly, we continue our analysis with measurements of channel memory at the packet and bit level. We determine that the wireless channel 2 bits memory. At the packet level we observe that the amount of channel memory is more varied.},
keywords={error statistics;personal area networks;radio links;statistical analysis;telecommunication traffic;wireless channels;wireless sensor networks;IEEE 802.15.4 LR-WPAN;low rate-wireless personal area networks;link quality indication;received signal strength indication;channel state information;statistical analysis;information theoretic analysis;wireless channel traffic;bit error rate;wireless sensor network;Bit error rate;Wireless sensor networks;Testing;Computer errors;Personal area networks;Channel state information;Information analysis;Communication standards;Failure analysis;Cyclic redundancy check},
doi={10.1109/INFOCOM.2008.184},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509780,
author={M. Zhao and M. Ma and Y. Yang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Mobile Data Gathering with Space-Division Multiple Access in Wireless Sensor Networks},
year={2008},
volume={},
number={},
pages={1283-1291},
abstract={Recent years have witnessed a surge of interest in efficient data gathering schemes in wireless sensor networks (WSNs). In this paper, we address this important issue in WSNs by adopting mobility and space-division multiple access (SDMA) technique to optimize system performance. Specifically, a mobile data collector, for convenience, called SenCar in this paper, is deployed in a WSN. It works like a mobile base station and polls each sensor while traversing its transmission range. Each sensor directly sends data to the SenCar without any relay so that the lifetime of sensors can be prolonged. We also consider applying SDMA technique to data gathering by equipping the SenCar with two antennas. With SDMA, two distinct compatible sensors may successfully make concurrent data uploading to the SenCar. Intuitively, if the SenCar can always simultaneously communicate with two compatible sensors, data uploading time can be cut into half in the ideal case. We focus on the problem of minimizing the total time of a data gathering tour which consists of two parts: data uploading time and moving time. To better enjoy the benefit of SDMA, the SenCar may have to visit some specific locations where more sensors are compatible, which may adversely prolong the moving path. Hence, an optimum solution should be a tradeoff between the shortest moving path and full utilization of SDMA. We refer to this optimization problem as mobile data gathering problem with SDMA, or MDG-SDMA for short. We formalize the MDG-SDMA problem into an integer program (IP) and then propose three heuristic algorithms that provide practically good solutions to the problem. Our simulation results demonstrate that the proposed algorithms can greatly reduce the total data gathering time compared to the non-SDMA algorithm with only minimum overhead.},
keywords={data handling;integer programming;space division multiple access;wireless sensor networks;mobile data gathering;space-division multiple access;wireless sensor networks;SenCar;data gathering tour;integer program;Wireless sensor networks;Multiaccess communication;Relays;Sensor phenomena and characterization;Routing;Energy consumption;Communications Society;Mobile computing;USA Councils;Surges},
doi={10.1109/INFOCOM.2008.185},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509781,
author={Y. Shi and Y. T. Hou},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Distributed Optimization Algorithm for Multi-Hop Cognitive Radio Networks},
year={2008},
volume={},
number={},
pages={1292-1300},
abstract={Cognitive radio (CR) is a revolution in radio technology and is viewed as an enabling technology for dynamic spectrum access. This paper investigates how to design distributed algorithm for a future multi-hop CR network, with the objective of maximizing data rates for a set of user communication sessions. We study this problem via a cross-layer optimization approach, with joint consideration of power control, scheduling, and routing. The main contribution of this paper is the development of a distributed optimization algorithm that iteratively increases data rates for user communication sessions. During each iteration, there are two separate processes, a Conservative Iterative Process (CIP) and an Aggressive Iterative Process (AIP). For both CIP and AIP, we describe our design of routing, minimalist scheduling, and power control/scheduling modules. To evaluate the performance of the distributed optimization algorithm, we compare it to an upper bound of the objective function, since the exact optimal solution to the objective function cannot be obtained via its mixed integer nonlinear programming (MINLP) formulation. Since the achievable performance via our distributed algorithm is close to the upper bound and the optimal solution (unknown) lies between the upper bound and the feasible solution obtained by our distributed algorithm, we conclude that the results obtained by our distributed algorithm are very close to the optimal solution.},
keywords={cognitive radio;distributed algorithms;integer programming;iterative methods;nonlinear programming;multihop cognitive radio network;distributed optimization algorithm;dynamic spectrum access;user communication session;cross-layer optimization approach;conservative iterative process;aggressive iterative process;minimalist scheduling;mixed integer nonlinear programming;Spread spectrum communication;Cognitive radio;Distributed algorithms;Upper bound;Chromium;Power control;Routing;Iterative algorithms;Algorithm design and analysis;Functional programming},
doi={10.1109/INFOCOM.2008.186},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509782,
author={R. Urgaonkar and M. J. Neely},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Opportunistic Scheduling with Reliability Guarantees in Cognitive Radio Networks},
year={2008},
volume={},
number={},
pages={1301-1309},
abstract={We develop opportunistic scheduling policies for cognitive radio networks that maximize the throughput utility of the secondary (unlicensed) users subject to maximum collision constraints with the primary (licensed) users. We consider a cognitive network with static primary users and potentially mobile secondary users. We use the technique of Lyapunov Optimization to design an online flow control, scheduling and resource allocation algorithm that meets the desired objectives and provides explicit performance guarantees.},
keywords={cognitive radio;Lyapunov methods;mobile radio;optimisation;radio networks;resource allocation;scheduling;telecommunication congestion control;telecommunication network reliability;opportunistic scheduling policies;reliability guarantees;cognitive radio networks;mobile secondary users;unlicensed users;static primary users;licensed users;maximum collision constraints;Lyapunov optimization technique;online flow control;resource allocation algorithm;Cognitive radio;Throughput;Resource management;Scheduling algorithm;Communications Society;Design optimization;Algorithm design and analysis;Wireless mesh networks;Stochastic processes;Telecommunication network reliability},
doi={10.1109/INFOCOM.2008.187},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509783,
author={M. Kurth and A. Zubow and J. -. Redlich},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Cooperative Opportunistic Routing Using Transmit Diversity in Wireless Mesh Networks},
year={2008},
volume={},
number={},
pages={1310-1318},
abstract={We propose a new high performance forwarding scheme, denoted transmit diversity based cooperative opportunistic routing (TDiCOR), that efficiently exploits multiuser and transmit diversity to improve the overall throughput in wireless multi-hop networks. Neighboring nodes cooperate through simultaneous transmissions in a MISO fashion in order to overcome the destructive effects of fading. TDiCOR uses distributed transmit diversity to increase the robustness of acknowledgements as well as data transmissions while preserving the opportunistic nature by using multiple candidates for packet relaying. We present measurements from a wireless testbed which suggest that cooperation using transmit diversity is feasible even with today's off-the-shelf hardware. At the instance of TDiCOR, we demonstrate how to realize the idea of cooperative opportunistic routing as an operational protocol and present very promising simulation results. TDiCOR outperforms traditional routing (i.e. DSR) in typical outdoor scenarios in terms of throughput by 30% and by 50% in indoor scenarios with high shadow fading, without consuming additional bandwidth or additional hardware resources.},
keywords={packet radio networks;routing protocols;telecommunication network topology;cooperative opportunistic routing protocol;transmit diversity;wireless multihop mesh network;high performance forwarding scheme;packet relaying;Routing;Wireless mesh networks;Throughput;Fading;Hardware;Spread spectrum communication;Robustness;Data communication;Relays;Testing},
doi={10.1109/INFOCOM.2008.188},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509784,
author={N. Bansal and R. Bhagwan and N. Jain and Y. Park and D. Turaga and C. Venkatramani},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Towards Optimal Resource Allocation in Partial-Fault Tolerant Applications},
year={2008},
volume={},
number={},
pages={1319-1327},
abstract={We introduce Zen, a new resource allocation framework that assigns application components to node clusters to achieve high availability for partial-fault tolerant (PFT) applications. These applications have the characteristic that under partial failures, they can still produce useful output though the output quality may be reduced. Thus, the primary goal of resource allocation for PFT applications is to prevent, delay, or minimize the impact of failures on the application output quality. This paper is the first to approach this resource allocation problem from a theoretical perspective, and obtains a series of results regarding component assignments that provide the highest service availability under the constraints imposed by the application data flow graph and the hosting clusters. We show that (1) even simple versions of this resource allocation problem are NP-Hard, (2) a 2-approximate polynomial-time algorithm works for tree topologies, and (3) a simple greedy component placement performs well in practice for general application topologies. We implement a system prototype to study the application availability achieved by Zen compared to failure-oblivious placement, replication, and Zen+replication. Our experimental results show that three PFT applications achieve significant data output quality and availability benefits using Zen.},
keywords={computational complexity;data flow graphs;fault tolerant computing;polynomial approximation;resource allocation;trees (mathematics);optimal resource allocation;partial-fault tolerant application;data flow graph;NP-hard;polynomial-time algorithm work;tree topology;failure-oblivious placement;greedy component placement;Resource management;Availability;Topology;Delay;Constraint theory;Flow graphs;Polynomials;Clustering algorithms;Tree graphs;Prototypes},
doi={10.1109/INFOCOM.2008.189},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509785,
author={S. Subramanian and S. Shakkottai and P. Gupta},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Optimal Geographic Routing for Wireless Networks with Near-Arbitrary Holes and Traffic},
year={2008},
volume={},
number={},
pages={1328-1336},
abstract={We consider the problem of throughput-optimal routing over large-scale wireless ad-hoc networks. Gupta and Kumar (2000) showed that a throughput capacity (a uniform rate over all source-destination pairs) of thetas( 1/radicn log n ) is achievable in random planar networks, and the capacity is achieved by straight-line routes. In reality, both the network model and the traffic demands are likely to be highly non-uniform. In this paper, we first propose a randomized forwarding strategy based on geographic routing that achieves near-optimal throughput over random planar networks with an arbitrary number of routing holes (regions devoid of nodes) of varying sizes. Next, we study a random planar network with arbitrary source-destination pairs with arbitrary traffic demands. For such networks, we demonstrate a randomized local load-balancing algorithm that supports any traffic load that is within a poly-logarithmic factor of the throughput region. Our algorithms are based on geographic routing and hence inherit their advantageous properties of low- complexity, robustness and stability.},
keywords={ad hoc networks;resource allocation;telecommunication network routing;telecommunication traffic;optimal geographic routing;large-scale wireless ad-hoc networks;random planar networks;throughput capacity;network traffic;randomized forwarding strategy;load balancing algorithm;near-arbitrary hole;Routing;Wireless networks;Telecommunication traffic;Throughput;Robust stability;Wireless sensor networks;Communications Society;Large-scale systems;Ad hoc networks;Traffic control},
doi={10.1109/INFOCOM.2008.190},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509786,
author={D. Wang and A. A. Abouzeid},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Link State Routing Overhead in Mobile Ad Hoc Networks: A Rate-Distortion Formulation},
year={2008},
volume={},
number={},
pages={1337-1345},
abstract={In this paper an information-theoretic formulation is used for characterizing the minimum overhead of maintaining link state information across a mobile ad hoc network. The minimum overhead problem is formulated a rate-distortion problem. Lower bounds are derived for the minimum overhead incurred by maintaining link state information when link state routing protocols are designed with guaranteed delivery ratio for data packets. The deficit caused by the this overhead on the overall transport capacity of a mobile network is characterized. Further a threshold value is derived for the delivery error ratio, and it is shown that no link state routing protocol can achieve a delivery error ratio smaller than this threshold.},
keywords={ad hoc networks;mobile radio;rate distortion theory;routing protocols;link state routing overhead;mobile ad hoc networks;rate-distortion formulation;information-theoretic formulation;link state information;link state routing protocol;Mobile ad hoc networks;Rate-distortion;Routing protocols;Computer networks;Peer to peer computing;Delay;Bit rate;Communications Society;Maintenance engineering;Systems engineering and theory},
doi={10.1109/INFOCOM.2008.191},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509787,
author={F. Li and Y. Wang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Circular Sailing Routing for Wireless Networks},
year={2008},
volume={},
number={},
pages={1346-1354},
abstract={Routing in wireless networks has been heavily studied in the last decade and numerous routing protocols were proposed in literature. The packets usually follow the shortest paths between sources and destinations in routing protocols to achieve smallest traveled distance. However, this leads to the uneven distribution of traffic load in a network. For example, wireless nodes in the center of the network will have heavier traffic since most of the shortest routes go through them. In this paper, we first describe a novel routing method, called circular sailing routing (CSR), which can distribute the traffic more evenly in the network. The proposed method first maps the network onto a sphere via a simple stereographic projection, and then the route decision is made by the distance on the sphere instead of the Euclidean distance in the plane. We theoretically prove that for a network the distance traveled by the packets using CSR is no more than a small constant factor of the minimum (the distance of the shortest path). We then extend CSR to a localized version, Localized CSR, by modifying the greedy routing without any additional communication overhead. Finally, we further propose CSR protocols for 3D networks where nodes are distributed in a 3D space instead of a 2D plane. For all proposed methods, we conduct simulations to study their performances and compare them with global shortest path routing or greedy routing.},
keywords={routing protocols;wireless sensor networks;circular sailing routing;wireless networks;routing protocols;stereographic projection;route decision;global shortest path routing;greedy routing;Wireless networks;Telecommunication traffic;Routing protocols;Load management;Peer to peer computing;Network topology;Traffic control;Communications Society;USA Councils;Euclidean distance},
doi={10.1109/INFOCOM.2008.192},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509788,
author={C. Wu and B. Li and S. Zhao},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Multi-Channel Live P2P Streaming: Refocusing on Servers},
year={2008},
volume={},
number={},
pages={1355-1363},
abstract={Due to peer instability and time-varying peer upload bandwidth availability in live peer-to-peer (P2P) streaming channels, it is preferable to provision adequate levels of stable upload capacities at dedicated streaming servers, in order to guarantee the streaming quality in all channels. Most commercial P2P streaming systems have resorted to the practice of over-provisioning upload capacities on streaming servers. In this paper, we have performed a detailed analysis on 400 GB and 7 months of run-time traces from UUSee, a commercial P2P streaming system, and observed that available capacities on streaming servers are not able to keep up with the increasing demand imposed by hundreds of channels. We propose a novel online server capacity provisioning algorithm that proactively adjusts the server capacities available to each of the concurrent channels, such that the supply of server bandwidth in each channel dynamically adapts to the forecasted demand, taking into account the number of peers, the streaming quality, and the priorities of channels. The algorithm is able to learn over time, and has full ISP awareness to maximally constrain P2P traffic within ISP boundaries. To evaluate the effectiveness of our solution, our experimental studies are based on an implementation of the algorithm with actual channels of P2P streaming traffic, with real-world traces replayed within a server cluster.},
keywords={bandwidth allocation;channel capacity;peer-to-peer computing;multi-channel live P2P streaming;peer instability;online server capacity provisioning algorithm;time-varying peer up-load bandwidth availability;peer-to-peer system;Bandwidth;Streaming media;Demand forecasting;Communications Society;Peer to peer computing;Performance analysis;Runtime;Clustering algorithms;Multimedia communication;Topology},
doi={10.1109/INFOCOM.2008.193},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509789,
author={F. Wang and J. Liu and Y. Xiong},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Stable Peers: Existence, Importance, and Application in Peer-to-Peer Live Video Streaming},
year={2008},
volume={},
number={},
pages={1364-1372},
abstract={This paper presents a systematic in-depth study on the existence, importance, and application of stable nodes in peer- to-peer live video streaming. Using traces from a real large-scale system as well as analytical models, we show that, while the number of stable nodes is small throughout a whole session, their longer lifespans make them constitute a significant portion in a per-snapshot view of a peer-to-peer overlay. As a result, they have substantially affected the performance of the overall system. Inspired by this, we propose a tiered overlay design, with stable nodes being organized into a tier-1 backbone for serving tier-2 nodes. It offers a highly cost-effective and deployable alternative to proxy-assisted designs. We develop a comprehensive set of algorithms for stable node identification and organization. Specifically, we present a novel structure, <i>Labeled</i> <i>Tree</i>, for the tier-1 overlay, which, leveraging stable peers, simultaneously achieves low overhead and high transmission reliability. Our tiered framework flexibly accommodates diverse existing overlay structures in the second tier. Our extensive simulation results demonstrated that the customized optimization using selected stable nodes boosts the streaming quality and also effectively reduces the control overhead. This is further validated through prototype experiments over the PlanetLab network.},
keywords={peer-to-peer computing;trees (mathematics);video streaming;peer-to-peer live video streaming;stable node identification;labeled tree structure;overlay network;Peer to peer computing;Streaming media;Large-scale systems;Analytical models;Bandwidth;Delay;Communications Society;Asia;Spine;Prototypes},
doi={10.1109/INFOCOM.2008.194},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509790,
author={S. -. Hu and T. -. Huang and S. -. Chang and W. -. Sung and J. -. Jiang and B. -. Chen},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={FLoD: A Framework for Peer-to-Peer 3D Streaming},
year={2008},
volume={},
number={},
pages={1373-1381},
abstract={Interactive 3D content on Internet has yet become popular due to its typically large volume and the limited network bandwidth. Progressive content transmission, or 3D streaming, thus is necessary to enable real-time content interactions. However, the heavy data and processing requirements of 3D streaming challenge the scalability of client-server delivery methods. We propose the use of peer-to-peer (P2P) networks for 3D streaming, and argue that due to the non-linear access patterns of 3D content, P2P 3D streaming is a new class of applications apart from existing media streaming and requires new investigations. We also present FLoD, the first P2P 3D streaming framework that allows clients of 3D virtual globe or virtual environment (VE) applications to obtain relevant data from other clients while minimizing server resource usage. To demonstrate how FLoD applies to real-world scenarios, we build a prototype system that adapts JPEG 2000-based 3D mesh streaming for P2P delivery. Experiments show that server-side bandwidth usage can thus be reduced, while simulations indicate that P2P 3D streaming is fundamentally more scalable than client-server approaches.},
keywords={client-server systems;media streaming;peer-to-peer computing;virtual reality;peer-to-peer network;Internet;client-server delivery methods;nonlinear access pattern;3D media streaming;virtual environment application;JPEG 2000;Peer to peer computing;Streaming media;Layout;Bandwidth;Computer science;Virtual environment;Navigation;Earth;Communications Society;Information management},
doi={10.1109/INFOCOM.2008.195},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509791,
author={H. Wang and C. C. Tan and Q. Li},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Snoogle: A Search Engine for the Physical World},
year={2008},
volume={},
number={},
pages={1382-1390},
abstract={Hardware advances will allow us to embed small devices into everyday objects such as toasters and coffee mugs, thus naturally form a wireless object network that connects the object with each other. This paper presents Snoogle, a search engine for such a network. Snoogle uses information retrieval techniques to index information and process user queries, and compression schemes such as Bloom filters to reduce communication overhead. Snoogle also considers security and privacy protections for sensitive data. We have implemented the system prototype on off-the-shelf sensor motes, and conducted extensive experiments to evaluate the system performance.},
keywords={data compression;data privacy;public key cryptography;query processing;search engines;security of data;wireless sensor networks;Snoogle search engine;wireless object network;information retrieval technique;information indexing;user query processing;information compression scheme;data security;data privacy protection;sensor network;Search engines;Communication system security;Hardware;Information retrieval;Information filtering;Information filters;Data security;Information security;Data privacy;Protection},
doi={10.1109/INFOCOM.2008.196},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509792,
author={S. Zhong and M. Jadliwala and S. Upadhyaya and C. Qiao},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Towards a Theory of Robust Localization Against Malicious Beacon Nodes},
year={2008},
volume={},
number={},
pages={1391-1399},
abstract={Localization in the presence of malicious beacon nodes is an important problem in wireless networks. Although significant progress has been made on this problem, some fundamental theoretical questions still remain unanswered: in the presence of malicious beacon nodes, what are the necessary and sufficient conditions to guarantee a bounded error during 2-dimensional location estimation? Under these necessary and sufficient conditions, what class of localization algorithms can provide that error bound? In this paper, we try to answer these questions. Specifically, we show that, when the number of malicious beacons is greater than or equal to some threshold, there is no localization algorithm that can have a bounded error. Furthermore, when the number of malicious beacons is below that threshold, we identify a class of localization algorithms that can ensure that the localization error is bounded. We also outline two algorithms in this class, one of which is guaranteed to finish in polynomial time (in the number of beacons providing information) in the worst case, while the other is based on a heuristic and is practically efficient. For completeness, we also extend the above results to the 3-dimensional case. Experimental results demonstrate that our solution has very good localization accuracy and computational efficiency.},
keywords={radio networks;malicious beacon nodes;wireless networks;2-dimensional location estimation;localization algorithm;polynomial time;Robustness;Peer to peer computing;Wireless networks;Sufficient conditions;Polynomials;Computational efficiency;Communications Society;Computer science;Broadcasting;Computational complexity},
doi={10.1109/INFOCOM.2008.197},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509793,
author={K. Premkumar and A. Kumar},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Optimal Sleep-Wake Scheduling for Quickest Intrusion Detection Using Wireless Sensor Networks},
year={2008},
volume={},
number={},
pages={1400-1408},
abstract={We consider the problem of <i>quickest</i> <i>detection</i> <i>of</i> <i>an</i> <i>intrusion</i> using a sensor network, keeping only a minimal number of sensors <i>active</i>. By using a minimal number of sensor devices, we ensure that the energy expenditure for <i>sensin</i>g, <i>computation</i> and <i>communication</i> is minimized (and the lifetime of the network is maximized). We model the intrusion detection (or change detection) problem as a <i>Markov</i> <i>decision</i> <i>process</i> (MDP). Based on the theory of MDP, we develop the following closed loop <i>sleep/wake</i> scheduling algorithms: (1) optimal control of M<sub>k+1</sub>, the number of sensors in the wake state in time slot k + 1, (2) optimal control of q<sub>k+1</sub>, the probability of a sensor in the wake state in time slot k + 1, and an open loop <i>sleep/wake</i> scheduling algorithm which (3) computes q, the optimal probability of a sensor in the wake state (which does not vary with time), based on the sensor observations obtained until time slot k. Our results show that an optimum closed loop control on M<sub>k+1</sub> significantly decreases the cost compared to keeping any number of sensors active all the time. Also, among the three algorithms described, we observe that the total cost is minimum for the optimum control on M<i>k+1</i> and is maximum for the optimum open loop control on q.},
keywords={closed loop systems;distributed sensors;Markov processes;open loop systems;scheduling;security of data;telecommunication control;telecommunication security;intrusion detection;sensor networks;Markov decision process;closed loop sleep-wake scheduling algorithms;optimal control;open loop control;Intrusion detection;Wireless sensor networks;Intelligent sensors;Sensor fusion;Scheduling algorithm;Open loop systems;Testing;Optimal control;Cost function;Sleep},
doi={10.1109/INFOCOM.2008.198},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509794,
author={Z. Yu and Y. Wei and B. Ramkumar and Y. Guan},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={An Efficient Signature-Based Scheme for Securing Network Coding Against Pollution Attacks},
year={2008},
volume={},
number={},
pages={1409-1417},
abstract={Network coding provides the possibility to maximize network throughput and receives various applications in traditional computer networks, wireless sensor networks and peer-to-peer systems. However, the applications built on top of network coding are vulnerable to pollution attacks, in which the compromised forwarders can inject polluted or forged messages into networks. Existing schemes addressing pollution attacks either require an extra secure channel or incur high computation overhead. In this paper, we propose an efficient signature-based scheme to detect and filter pollution attacks for the applications adopting linear network coding techniques. Our scheme exploits a novel homomorphic signature function to enable the source to delegate its signing authority to forwarders, that is, the forwarders can generate the signatures for their output messages without contacting the source. This nice property allows the forwarders to verify the received messages, but prohibit them from creating the valid signatures for polluted or forged ones. Our scheme does not need any extra secure channels, and can provide source authentication and batch verification. Experimental results show that it can improve computation efficiency up to ten times compared to some existing one. In addition, we present an alternate lightweight scheme based on a much simpler linear signature function. This alternate scheme provides a tradeoff between computation efficiency and security.},
keywords={digital signatures;encoding;formal verification;signature-based scheme;pollution attacks;network throughput;linear network coding;source authentication;batch verification;Network coding;Pollution;Application software;Throughput;Computer networks;Wireless sensor networks;Peer to peer computing;Nonlinear filters;Authentication;Security},
doi={10.1109/INFOCOM.2008.199},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509795,
author={W. Zhang and N. Subramanian and G. Wang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Lightweight and Compromise-Resilient Message Authentication in Sensor Networks},
year={2008},
volume={},
number={},
pages={1418-1426},
abstract={Numerous authentication schemes have been proposed in the past for protecting communication authenticity and integrity in wireless sensor networks. Most of them however have following limitations: high computation or communication overhead, no resilience to a large number of node compromises, delayed authentication, lack of scalability, etc. To address these issues, we propose in this paper a novel message authentication approach which adopts a perturbed polynomial-based technique to simultaneously accomplish the goals of lightweight, resilience to a large number of node compromises, immediate authentication, scalability, and non-repudiation. Extensive analysis and experiments have also been conducted to evaluate the scheme in terms of security properties and system overhead.},
keywords={wireless sensor networks;message authentication;wireless sensor networks;communication authenticity;immediate authentication;node compromises;Message authentication;Peer to peer computing;Polynomials;Resilience;Scalability;Wireless sensor networks;Computer science;Delay;Public key;Digital signatures},
doi={10.1109/INFOCOM.2008.200},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509796,
author={S. Huang and X. Liu and Z. Ding},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Opportunistic Spectrum Access in Cognitive Radio Networks},
year={2008},
volume={},
number={},
pages={1427-1435},
abstract={Driven by regulatory initiatives and radio technology advances, opportunistic spectrum access has the potential to mitigate spectrum scarcity and meet the increasing demand for spectrum. In this paper, we consider a scenario where secondary users can opportunistically access unused spectrum vacated by idle primaries. We introduce two metrics to protect primary performance, namely collision probability and overlapping time. We present three spectrum access schemes using different sensing, back-off, and transmission mechanisms. We show that they achieve indistinguishable secondary performance under given primary constraints. We provide closed form analysis on secondary user performance, present a tight capacity upper bound, and reveal the impact of various design options, such as sensing, packet length distribution, back-off time, packet overhead, and grouping. Our work sheds light on the fundamental properties and design criteria on opportunistic spectrum access.},
keywords={cognitive radio;probability;radio networks;radio spectrum management;telecommunication congestion control;opportunistic spectrum access scheme;cognitive radio network;regulatory initiative;collision probability;Cognitive radio;Protection;Wireless sensor networks;Hardware;Quality of service;Communications Society;Collision mitigation;Performance analysis;Upper bound;Capacitive sensors},
doi={10.1109/INFOCOM.2008.201},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509797,
author={G. D. Celik and G. Zussman and W. F. Khan and E. Modiano},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={MAC for Networks with Multipacket Reception Capability and Spatially Distributed Nodes},
year={2008},
volume={},
number={},
pages={1436-1444},
abstract={The physical layer of future wireless networks will be based on novel radio technologies such as UWB and MIMO. One of the important capabilities of such technologies is the ability to capture a few packets simultaneously. This capability has the potential to improve the performance of the MAC layer. However, we show that in networks with spatially distributed nodes, reusing backoff mechanisms originally designed for narrow-band systems (e.g. CSMA/CA) is inefficient. It is well known that when networks with spatially distributed nodes operate with such MAC protocols, the channel may be captured by nodes that are near the destination, leading to unfairness. We show that when the physical layer enables multipacket reception, the negative implications of reusing the legacy protocols include not only such unfairness but also a significant throughput reduction. We present alternative backoff mechanisms and evaluate their performance via Markovian analysis and simulation. We show that our alternative backoff mechanisms can improve both overall throughput and fairness.},
keywords={access protocols;Markov processes;packet radio networks;wireless channels;multipacket reception capability;wireless network;radio technology;UWB;MIMO;reusing backoff mechanism;narrow-band system;alternative backoff mechanism;Markovian analysis;multipacket reception;Media Access Protocol;Peer to peer computing;Physical layer;Wireless networks;Multiaccess communication;Access protocols;Wireless communication;Throughput;Communications technology;Wireless application protocol},
doi={10.1109/INFOCOM.2008.202},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509798,
author={Z. Zeng and Y. Yang and J. C. Hou},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={How Physical Carrier Sense Affects System Throughput in IEEE 802.11 Wireless Networks},
year={2008},
volume={},
number={},
pages={1445-1453},
abstract={In recent years, wireless ad hoc networks have become increasingly popular in both military and civilian applications due to their capability of building networks without the need for a pre-existing infrastructure. While a number of studies have been carried out to study the performance of IEEE 802.11 DCF in single-cell wireless LANs, the analysis of IEEE 802.11 DCF in multi-hop wireless networks with consideration of the effects of physical carrier sense, SINR, and collision caused by accumulative interference has been lacking. In this paper, we substantially extend Cali's work and rigorously model, with all these effects considered, channel activities governed by IEEE 802.11 DCF in multi-hop wireless networks. We show that as in single-cell WLANs, the choice of the contention window size can impact the system throughput in multi-hop wireless networks. However, the optimal value of the contention window size is quite different. Moreover, the optimal attempt probability p (the optimal window size) derived for multi-hop wireless networks with respect to maximizing the system throughput is larger (smaller) than that for single-cell WLANs. We also show that, given the minimal SINR threshold beta, the optimal carrier sense range is smaller than the conventional value used (provided that the contention window size is tuned accordingly).},
keywords={ad hoc networks;IEEE standards;wireless LAN;system throughput;IEEE 802.11 wireless networks;wireless ad hoc networks;IEEE 802.11 DCF;single-cell wireless LAN;multi-hop wireless networks;physical carrier sense;Throughput;Wireless networks;Spread spectrum communication;Signal to noise ratio;Mobile ad hoc networks;Wireless LAN;Cause effect analysis;Performance analysis;Interference;IEEE activities},
doi={10.1109/INFOCOM.2008.203},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509799,
author={D. Zheng and M. Cao and J. Zhang and P. R. Kumar},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Channel Aware Distributed Scheduling for Exploiting Multi-Receiver Diversity and Multiuser Diversity in Ad-Hoc Networks: A Unified PHY/MAC Approach},
year={2008},
volume={},
number={},
pages={1454-1462},
abstract={We study channel aware distributed scheduling in ad hoc networks where many links contend for the common channel using random access, and the focus here is on the model where each transmitter node has multiple intended receivers. In such a network, channel probing takes place in two phases: 1) in phase I, all transmitters contend for the channel using random access to reserve the channel, and the probing to accomplish a successful channel contention takes a random duration; and 2) in phase II, subsequent probings are carried out to estimate the link conditions from the successful transmitter in phase I to its intended receivers, according to specific probing mechanisms, and the probing for each receiver takes a constant duration. In this paper, we shall study various probing mechanisms for utilizing multi-receiver diversity in phase II and multiuser diversity in phase I for ad hoc (peer-to-peer) communications. Clearly, further probing increases the likelihood of seeing better channel conditions for exploiting diversities, but at the cost of additional time. Therefore, channel probing must be done efficiently to balance the tradeoff between the throughput gain from better channel conditions and the probing cost. One main objective of this study is to characterize this tradeoff in a stochastic decision making framework. Specifically, we cast network throughput optimization as an optimal stopping problem, and then explore channel aware distributed scheduling to leverage multi-receiver diversity and multiuser diversity in a joint manner. We show that the optimal scheduling policies for all proposed probing mechanisms exhibit threshold structures, indicating that they are amenable to easy distributed implementation. We show that the optimal thresholds and the maximum network throughput can be obtained off-line by solving fixed point equations. We further develop iterative algorithms to compute the optimal thresholds and the throughput.},
keywords={access protocols;ad hoc networks;decision making;diversity reception;iterative methods;multiuser channels;optimisation;radio receivers;radio transmitters;random processes;scheduling;stochastic processes;channel aware distributed scheduling;multi receiver diversity;ad-hoc network;PHY/MAC approach;random access protocol;multiple intended receiver;channel contention;multiuser diversity;stochastic decision making;network throughput optimization;optimal stopping problem;iterative algorithm;radio transmitters;Ad hoc networks;Physical layer;Throughput;Optimal scheduling;Transmitters;Peer to peer computing;Costs;Phase estimation;Stochastic processes;Decision making},
doi={10.1109/INFOCOM.2008.204},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509800,
author={Y. Xi and E. M. Yeh},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Pricing, Competition, and Routing for Selfish and Strategic Nodes in Multi-Hop Relay Networks},
year={2008},
volume={},
number={},
pages={1463-1471},
abstract={We study pricing games in multi-hop relay networks where nodes price their services and route their traffic selfishly and strategically. Each node (1) makes a bid to each of its customers, specifying a charging function and a proposed traffic share, and (2) allocates its received traffic to its service providers. A node aims to maximize its profit from forwarding traffic. We show that the socially optimal routing can always be induced by an equilibrium where no node can increase its profit by unilaterally changing its bids. Inefficient equilibria arise in oligopolies due to the monopolistic pricing power of a superior relay. It results in finite price of anarchy if marginal cost functions are concave, but unbounded price of anarchy when they are convex. Pricing games of general topology suffer from the intrinsic multi-hop network structure, which gives rise to infinite price of anarchy.},
keywords={game theory;pricing;telecommunication network routing;telecommunication traffic;routing;selfish nodes;strategic nodes;multi-hop relay networks;pricing games;charging function;traffic share;oligopolies;Pricing;Routing;Spread spectrum communication;Relays;Peer to peer computing;Telecommunication traffic;Cost function;Network topology;Communications Society;USA Councils},
doi={10.1109/INFOCOM.2008.205},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509801,
author={R. Zhang-Shen and N. McKeown},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Guaranteeing Quality of Service to Peering Traffic},
year={2008},
volume={},
number={},
pages={1472-1480},
abstract={Network operators connect their backbone networks together at peering points. It is well known that the peering points are the most congested parts of the backbone network. Network operators have little incentive to provision them well, and have few tools to decide how best to route traffic over them. In this paper we propose how peering networks can be congestion free, so long as we know the total amount of traffic between them. In particular, we propose the use of valiant load-balancing (VLB), which has been previously studied for individual backbone networks. In our approach, the backbone networks do not need to use VLB internally - they simply load-balance traffic over their peering links. Our analysis shows how the load-balancing should be done, and we conclude that no other method is more efficient than VLB in achieving a congestion-free network.},
keywords={peer-to-peer computing;quality of service;resource allocation;telecommunication traffic;quality of service;peering traffic;network operators;backbone networks;valiant load balancing;congestion free network;Quality of service;Telecommunication traffic;Spine;Traffic control;Peer to peer computing;Delay;Switches;Communications Society;Computer networks;Laboratories},
doi={10.1109/INFOCOM.2008.206},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509802,
author={R. S. Prasad and M. Thottan and T. V. Lakshman},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Stateless and Light-Weight Bandwidth Management Mechanism for Elastic Traffic},
year={2008},
volume={},
number={},
pages={1481-1489},
abstract={Unbounded growth in the number of active flows can lead to severe service quality degradation to existing flows in a network . To guarantee minimum service quality for individual flows, specially for multimedia traffic, it is necessary to estimate and bound the number of active flows in the network. Prior work on estimating the number of active flows has been difficult without maintaining per-flow state. In this paper, we propose a light weight (not requiring per flow state) mechanism to estimate the number of active flows. This estimate is then used to determine the probability of admitting a new flow into the network with the goal of preventing extreme degradation of throughput to existing flows. The key idea here is that the number of active flows can be inferred from the frequency at which a newly arriving packet is part of the same flow as a randomly selected packet in the buffer. Since this scheme relies on information already available in the buffer, no per-flow state is maintained in the network. This mechanism requires very little per-packet processing, even less than a forwarding table lookup. Simulation results show that the proposed scheme can stabilize an overloaded network by bounding the number of active flows without significantly impairing link utilization. This scheme also gives good performance when buffer sizes are small and when the network has a mix of TCP and UDP traffic.},
keywords={computer network management;DiffServ networks;quality of service;telecommunication traffic;light-weight bandwidth management mechanism;elastic traffic;service quality degradation;multimedia traffic;active flows;TCP traffic;UDP traffic;Bandwidth;State estimation;Throughput;Frequency estimation;Table lookup;Degradation;Telecommunication traffic;Communications Society;Communication system traffic control;Communication system control},
doi={10.1109/INFOCOM.2008.207},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509803,
author={G. Rizzo and J. -. Le Boudec},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Stability and Delay Bounds in Heterogeneous Networks of Aggregate Schedulers},
year={2008},
volume={},
number={},
pages={1490-1498},
abstract={Aggregate scheduling is one of the most promising solutions to the issue of scalability in networks, like DiffServ networks and high speed switches, where hard QoS guarantees are required. For networks of FIFO aggregate schedulers, the main existing sufficient conditions for stability (the possibility to derive bounds to delay and backlog at each node) are of little practical utility, as they are either relative to specific topologies, or based on strong ATM-like assumptions on the network (the so-called "RIN" result), or they imply an extremely low node utilization. We use a deterministic approach to this problem. We identify a nonlinear operator on a vector space of finite (but large) dimension, and we derive a first sufficient condition for stability, based on the super-additive closure of this operator. Second, we use different upper bounds of this operator to obtain practical results. We find new sufficient conditions for stability, valid in an heterogeneous environment and without any of the restrictions of existing results. We present a polynomial time algorithm to test our sufficient conditions for stability. We show that with leaky bucket constrained flows the inner bound to the stability region derived with our algorithm is always larger than the one determined by all existing results. We prove that all the main existing results can be derived as special cases of our results. We also present a method to compute delay bounds in practical cases.},
keywords={DiffServ networks;quality of service;scheduling;heterogeneous networks;aggregate schedulers;aggregate scheduling;DiffServ networks;high speed switches;QoS guarantees;super-additive closure;Stability;Aggregates;Sufficient conditions;Scalability;Switches;Network topology;Upper bound;Polynomials;Testing;Delay},
doi={10.1109/INFOCOM.2008.208},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509804,
author={S. Yang and J. Wu and M. Cardei},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Efficient Broadcast in MANETs Using Network Coding and Directional Antennas},
year={2008},
volume={},
number={},
pages={1499-1507},
abstract={In this paper, we consider the issue of efficient broadcasting in mobile ad hoc networks (MANETs) using network coding and directional antennas. Network coding-based broadcasting focuses on reducing the number of transmissions each forwarding node performs in the multiple source/multiple message broadcast application, where each forwarding node combines some of the received messages for transmission. With the help of network coding, the total number of transmissions can be reduced compared to broadcasting using the same forwarding nodes without coding. We exploit the usage of directional antennas to network coding-based broadcasting to further reduce energy consumption. A node equipped with directional antennas can divide the omnidirectional transmission range into several sectors and turns some of them on for transmission. In the proposed scheme using a directional antenna, forwarding nodes selected locally only need to transmit broadcast messages, original or coded, to restricted sectors. We also study two extensions. The first extension applies network coding to both dynamic and static forwarding node selection approaches. In the second extension, we design two approaches for the single source/single message issue in the network coding-based broadcast application. Performance analysis via simulations on the proposed algorithms using a custom simulator is presented.},
keywords={ad hoc networks;broadcasting;directive antennas;mobile ad hoc networks;network coding;directional antennas;multiple source message broadcast;dynamic forwarding node selection;static forwarding node selection;single source message;Broadcasting;Network coding;Directional antennas;Peer to peer computing;Spine;Computer science;Mobile ad hoc networks;Energy consumption;Performance analysis;Analytical models},
doi={10.1109/INFOCOM.2008.209},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509805,
author={Y. Lin and B. Li and B. Liang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Efficient Network Coded Data Transmissions in Disruption Tolerant Networks},
year={2008},
volume={},
number={},
pages={1508-1516},
abstract={Most routing protocols in disruption tolerant networks (DTN) use redundant transmissions to explore the diversities in routing paths in order to reduce data transmission delay. However, mobile nodes in DTN usually have limited energy and may prefer fewer transmissions for longer lifetime. Hence, it is vital to carefully balance the tradeoff between data transmission delay and the amount of transmissions among mobile nodes. In this paper, we consider the problem to route a batch of data packets in DTN. By making an analogy between the routing protocol and low-density erasure codes, we investigate the information-theoretical optimal number of data transmissions in delivering data. With such insights, we propose E-NCP, an efficient protocol in DTNs based on network coding, that reduces data transmissions significantly, while increasing data transmission delay only slightly as compared to the protocol with the best performance. With extensive theoretical analysis and simulations, we show that network coding facilitates a better tradeoff between resource usage and protocol performance, and that our protocol offers unique advantages over replication-based protocols.},
keywords={mobile radio;routing protocols;network coded data transmissions;disruption tolerant networks;routing protocols;mobile nodes;replication-based protocols;Data communication;Disruption tolerant networking;Network coding;Peer to peer computing;Routing protocols;Performance analysis;Delay;Communications Society;Cultural differences;Analytical models},
doi={10.1109/INFOCOM.2008.210},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509806,
author={I. -. Hou and Y. -. Tsai and T. F. Abdelzaher and I. Gupta},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={AdapCode: Adaptive Network Coding for Code Updates in Wireless Sensor Networks},
year={2008},
volume={},
number={},
pages={1517-1525},
abstract={Code updates, such as those for debugging purposes, are frequent and expensive in the early development stages of wireless sensor network applications. We propose AdapCode, a reliable data dissemination protocol that uses adaptive network coding to reduce broadcast traffic in the process of code updates. Packets on every node are coded by linear combination and decoded by Gaussian elimination. The core idea in AdapCode is to adaptively change the coding scheme according to the link quality. Our evaluation shows that AdapCode uses up to 40% less packets than Deluge in large networks. In addition, AdapCode performs much better in terms of load balancing, which prolongs the system lifetime, and has a slightly shorter propagation delay. Finally, we show that network coding is doable on sensor networks in that (i) it imposes only a 3 byte header overhead, (ii) it is easy to find linearly independent packets, and (3) Gaussian elimination needs only 1 KB of memory.},
keywords={adaptive codes;adaptive decoding;Gaussian processes;protocols;telecommunication network reliability;telecommunication traffic;wireless sensor networks;AdapCode reliable data dissemination protocol;adaptive network coding;code updates;wireless sensor networks;broadcast traffic;decoding;Gaussian elimination;Adaptive systems;Network coding;Wireless sensor networks;Debugging;Protocols;Broadcasting;Telecommunication traffic;Decoding;Load management;Propagation delay},
doi={10.1109/INFOCOM.2008.211},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509807,
author={W. Pu and C. Luo and S. Li and C. W. Chen},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Continuous Network Coding in Wireless Relay Networks},
year={2008},
volume={},
number={},
pages={1526-1534},
abstract={Network coding has recently been applied to wireless networks and has achieved some initial success. Researches in wireless network coding have been mostly focusing on utilizing the broadcast nature of the wireless networks. In this paper, we propose a novel network coding framework for wireless relay networks that also takes into consideration the fading and error prone nature of the wireless networks. First, we extend the traditional network coding in lossless networks which operates on 0-1 bits, to a new framework which defines network coding on the posterior probability of each bit. This new framework allows an imperfect decode-recode process at a relay node and avoids possible error propagation when a hard decision is made at the relay node. It implicitly integrates decode-and-forward and estimate-and-forward strategies for wireless network coding to address the technical issues of channel fading and transmission errors. The proposed approach is validated through both theoretical analysis and extensive simulations. Both analysis and simulation confirm that this new framework is able to achieve significant gain over traditional network coding. This new framework also enables the introduction of adaptive scheme into network coding. We demonstrate a basic adaptation scheme and present some preliminary experimental results. The proposed adaptive scheme will lay down an essential foundation in this emerging field of wireless network coding in order to address issues related to link heterogeneity.},
keywords={channel coding;fading channels;relays;wireless sensor networks;continuous network coding;wireless relay networks;decode-and-forward strategies;wireless network coding;channel fading;Network coding;Wireless networks;Diversity reception;Broadcasting;Fading;Decoding;Frame relay;Peer to peer computing;Analytical models;Routing},
doi={10.1109/INFOCOM.2008.212},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509808,
author={N. B. Chang and M. Liu},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Competitive Analysis of Opportunistic Spectrum Access Strategies},
year={2008},
volume={},
number={},
pages={1535-1542},
abstract={We consider opportunistic spectrum access (OSA) strategies for a transmitter in a multichannel wireless system, where a channel may or may not be available and the transmitter must sense/probe the channel to find out before transmission. Applications for this work include joint probing and transmission for a secondary user in a cognitive radio network. Limited by resources, e.g., energy and time, the transmitter must decide on a subset of a potentially very large number of channels to probe and can only use for transmission those that have been found to be available. In contrast to previous works, we do not assume the user has a priori knowledge regarding the statistics of channel states. The main goal of this work is to design strategies that decide, based only on knowledge of the channel bandwidths/data rates, which channels to probe. We derive optimal strategies that maximize the total expected bandwidth/data rate in the worst-case, via a performance measure in the form of a competitive regret (ratio) between the average performance of a strategy and a genie (or omniscient observer). We examine the performance of these optimal strategies under a wide range of system parameters and practical channel models via numerical studies.},
keywords={cognitive radio;radio networks;radio spectrum management;radio transmitters;wireless channels;competitive analysis;opportunistic spectrum access strategy;multichannel wireless system;wireless transmitter;cognitive radio network;optimal channel sensing strategy;bandwidth maximization;data rate maximization;Radio transmitters;Probes;Cognitive radio;Wireless sensor networks;Bandwidth;Frequency;Laboratories;Collaborative work;Physical layer;Communications Society},
doi={10.1109/INFOCOM.2008.213},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509809,
author={F. Wu and S. Zhong and C. Qiao},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Globally Optimal Channel Assignment for Non-Cooperative Wireless Networks},
year={2008},
volume={},
number={},
pages={1543-1551},
abstract={Channel assignment is a very important topic in wireless networks. In this paper, we study FDMA channel assignment in a non-cooperative wireless network, where devices are selfish. Existing work on this problem has considered Nash equilibrium (NE), which is not a very strong solution concept and may not guarantee a good system-wide performance. In contrast, in this work we introduce a payment formula to ensure the existence of a strongly dominant strategy equilibrium (SDSE), a much stronger solution concept. We show that, when the system converges to a SDSE, it also achieves global optimality in terms of effective system-wide throughput. Furthermore, we extend our work to the case in which some radios have limited tunability. We show that, in this case, it is generally impossible to have a similar SDSE solution; but, with additional assumptions on the numbers of radios and the types of channels, etc., we can again achieve a SDSE solution that guarantees globally optimal effective system throughput in the entire system. Besides this extension, we also consider another extension of our strategic game, which is a repeated game that provides fairness. Finally, we evaluate our design in experiments. Our evaluations verify that the system does converge to the globally optimal channel assignment with our designed payment formula, and that the effective system- wide throughput is significantly higher than that of anarchy and Nash equilibrium (NE).},
keywords={channel allocation;frequency division multiple access;game theory;radio networks;globally optimal channel assignment;noncooperative wireless networks;FDMA channel assignment;Nash Equilibrium;strongly dominant strategy equilibrium;strategic game;Wireless networks;Throughput;Frequency division multiaccess;Nash equilibrium;Time division multiple access;Radio transceivers;Degradation;Communications Society;Computer science;Wireless communication},
doi={10.1109/INFOCOM.2008.214},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509810,
author={M. H. Manshaei and J. Freudiger and M. Felegyhazi and P. Marbach and J. -. Hubaux},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={On Wireless Social Community Networks},
year={2008},
volume={},
number={},
pages={1552-1560},
abstract={Wireless social community networks are emerging as a new alternative to providing wireless data access in urban areas. By relying on users in the network deployment, a wireless community can rapidly deploy a high-quality data access infrastructure in an inexpensive way. But, the coverage of such a network is limited by the set of access points deployed by the users. Currently, it is not clear if this paradigm can serve as a replacement of existing centralized networks operating in licensed bands (such as cellular networks) or if it should be considered as a complimentary service only, with limited coverage. This question currently concerns many wireless network operators. In this paper, we study the dynamics of wireless social community networks by using a simple analytical model. In this model, users choose their service provider based on the subscription fee and the offered coverage. We show how the evolution of social community networks depends on their initial coverage, the subscription fee, and the user preferences for coverage. We conclude that by using an efficient static or dynamic pricing strategy, the wireless social community can obtain a high coverage. Using a game-theoretic approach, we then study a case where the mobile users can choose between the services provided by a licensed band operator and those of a social community. We show that for specific distribution of user preferences, there exists a Nash equilibrium for this non-cooperative game.},
keywords={game theory;mobile radio;wireless social community networks;wireless data access;network deployment;high-quality data access infrastructure;dynamic pricing strategy;mobile users;noncooperative game;Nash equilibrium;Wireless networks;Subscriptions;Nash equilibrium;Quality of service;Costs;Investments;Communications Society;Laboratories;Computer applications;Computer networks},
doi={10.1109/INFOCOM.2008.215},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509811,
author={G. Wikstrand and T. Nilsson and M. S. Dougherty},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Prioritized Repeated Eliminations Multiple Access: A Novel Protocol for Wireless Networks},
year={2008},
volume={},
number={},
pages={1561-1569},
abstract={A new and simple MAC protocol is proposed. Each node transmits a burst with length sampled from a geometric distribution with parameter q followed by a carrier sense slot. A node repeats the previous operations until it senses a busy slot. A node will access the channel and transmit its payload when it has sensed <i>h</i> idle slots. The parameter <i>q</i> can be adjusted for each node to achieve desired levels of relative priority with little impact on channel utilization and without a need for explicit knowledge about other traffic. Comparison to existing tree-splitting, carrier sensing and bursting protocols through analysis and simulations shows that our algorithm scales very well to the number of nodes. It has very high success probability, channel utilization and fairness. Extensions are provided for hidden terminal scenarios and are shown in simulations to provide adequate performance.},
keywords={access protocols;channel allocation;probability;radio networks;telecommunication traffic;MAC protocol;geometric distribution;channel utilization;network traffic;probability;wireless networks;Wireless application protocol;Access protocols;Wireless networks;Media Access Protocol;Peer to peer computing;Multiaccess communication;Computer networks;Computational modeling;Communications Society;Computer science},
doi={10.1109/INFOCOM.2008.216},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509812,
author={V. Raghunathan and V. Borkar and M. Cao and P. R. Kumar},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Index Policies for Real-Time Multicast Scheduling for Wireless Broadcast Systems},
year={2008},
volume={},
number={},
pages={1570-1578},
abstract={Motivated by the increasing usage of wireless broadcast networks for multicast real-time applications like video, this paper considers a canonical real-time multicast scheduling problem for a wireless broadcast LAN. A wireless access point (AP) has N latency-sensitive flows, each associated with a deadline and a multicast group of receivers that desire to receive all the packets successfully by their corresponding deadlines. We consider periodic and one-shot models of real-time arrivals. The channel from the AP to each receiver is a wireless erasure channel, independent across users and slots. We wish to find a communication strategy that minimizes the total deadlines missed across all receivers, where a receiver counts a miss if it does not receive a packet by its deadline. We cast this problem as a restless bandit in stochastic control. We use Whittle's relaxation framework for restless bandits to establish Whittle-indexability for multicast realtime scheduling under the assumption of complete feedback from all receivers in every slot. For the Whittle relaxation, we show that for each flow, the AP's decision between transmitting in a slot and idling has a threshold structure. For the homogeneous case where the erasure channel to each receiver is identically distributed with parameter p, the Whittle index of a flow is x<sub>i</sub>(1 - p) , where x<sub>i</sub> is the number of receivers who have yet to receive the current packet of flow i. For the general heterogeneous case in which the erasure channel to receiver j has loss probability p<sub>j</sub>, the Whittle index corresponding to each flow is Sigma<sub>j</sub> (1- p<sub>j</sub>), where the sum is over all multicast receivers who are yet to receive the packet. We bound the performance of the optimal Whittle relaxation with respect to the optimal wireless multicast real-time scheduler. The heuristic index policy that schedules the flow with the maximum Whittle index in each slot is simple. To relax the complete feedback assumption, we design a scalable mechanism based on statistical estimation theory that obtains the required feedback from all the receivers using a single ACK per packet transmission. The resultant policy is amenable to low-complexity implementation.},
keywords={broadcast channels;estimation theory;multicast communication;probability;radio receivers;relaxation theory;scheduling;statistical analysis;wireless channels;wireless LAN;wireless broadcast LAN;real-time multicast scheduling;multicast receivers;wireless access point;wireless erasure channel;Whittle relaxation framework;probability;maximum Whittle index policy;statistical estimation theory;single ACK per packet transmission;Real time systems;Broadcasting;Multimedia communication;Feedback;Wireless LAN;Local area networks;Stochastic processes;Communication system control;Tellurium;Estimation theory},
doi={10.1109/INFOCOM.2008.217},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509813,
author={S. Deb and S. Jaiswal and K. Nagaraj},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Real-Time Video Multicast in WiMAX Networks},
year={2008},
volume={},
number={},
pages={1579-1587},
abstract={IEEE 802.16e WiMAX is a promising new technology for broadband access networks. Amongst the class of applications that can be supported is real time video services (such as IPTV, broadcast of live events etc.). These applications are bandwidth hungry and have stringent delay constraints. Thus, scalable support for such applications is a challenging problem. To address this challenge, we consider a combination of approaches using multicast, layer encoded video and adaptive modulation of transmissions. Using these, we develop algorithms to ensure efficient, fair and timely delivery of video in WiMAX networks. The corresponding resource allocation problem is challenging because scheduling decisions (within a WiMAX base station) are performed in real-time across two dimensions, time and frequency. Moreover, combining layered video with appropriate modulation calls for novel MAC algorithms. We model the multicast resource allocation problem in WiMAX and demonstrate this problem to be NP-hard. We present a fast greedy algorithm that is (i) provably within a constant approximation of the optimal solution (based on a metric that reflects video quality as perceived by the user), and (ii) performs within 87-95% of the optimal as demonstrated by realistic simulations. We also demonstrate that our algorithm offers a 25% improvement over a naive algorithm. Moreover, in terms of the average rate received by each user, our algorithm out-performs the naive algorithm by more than 50%.},
keywords={broadband networks;greedy algorithms;optimisation;video communication;WiMax;real-time video multicast;WiMAX networks;IEEE 802.16e WiMAX;broadband access networks;real time video services;NP-hard problem;greedy algorithm;WiMAX;Multicast algorithms;Resource management;IPTV;Multimedia communication;Broadcasting;Bandwidth;Delay;Base stations;Radio spectrum management},
doi={10.1109/INFOCOM.2008.218},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509814,
author={Z. Li},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Cross-Monotonic Multicast},
year={2008},
volume={},
number={},
pages={1588-1596},
abstract={In the routing and cost sharing of multicast towards a group of potential receivers, cross-monotonicity is a property that states a user's payment can only be smaller when serviced in a larger set. Being cross-monotonic has been shown to be the key in achieving group-strategyproofness. We study multicast schemes that target optimal flow routing, cross-monotonic cost sharing, and budget balance. We show that no multicast scheme can satisfy these three properties simultaneously, and resort to approximate budget balance instead. We derive both positive and negative results that complement each other for directed and undirected networks. We show that in directed networks, no cross-monotonic scheme can recover a constant fraction of optimal multicast cost. We provide a simple scheme that does achieve 1/k-budget-balance, where k is the number of receivers. Using a probabilistic method rooted in random graph theory, we prove an upper-bound of 2/radic(k) for the budget balance ratio. For undirected networks, we derive a constant upper-bound of 1/2 instead. We further apply a smooth dual growing technique to design a cross- monotonic scheme that recovers k+1/2kzeta of optimal multicast cost in undirected networks, where zeta is a network-dependent parameter close to 1. This is almost tight against the upper-bound |. We finally present a two-stage linear optimization model that pursues maximum budget balance in any given specific network, with trade-off in complexity. Optimization results in various network configurations confirm the theoretically established bounds.},
keywords={graph theory;multicast communication;optimisation;telecommunication network routing;cross-monotonic multicast;group strategy proofness;multicast schemes;optimal flow routing;cross-monotonic cost sharing;budget balance;random graph theory;optimal multicast cost;two stage linear optimization model;Routing;Cost function;Multicast algorithms;Algorithm design and analysis;Graph theory;Robustness;Cost accounting;Communications Society;Computer science;Game theory},
doi={10.1109/INFOCOM.2008.219},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509815,
author={K. -. Vik and P. Halvorsen and C. Griwodz},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Multicast Tree Diameter for Dynamic Distributed Interactive Applications},
year={2008},
volume={},
number={},
pages={1597-1605},
abstract={Latency reduction in distributed interactive applications has been studied intensively. Such applications may have stringent latency requirements and dynamic user groups. We focus on using application-layer multicast with a centralized approach to the group management. The groups are organized in overlay networks that are created using graph algorithms. We investigate many spanning tree problems with particular focus on reducing the diameter of a tree, i.e., the maximum pairwise latency between users. In addition, we focus on reducing the time it takes to execute membership changes. In that context, we use core-selection heuristics to find well-placed group nodes, and edge-pruning algorithms to reduce the number of edges in an otherwise fully meshed overlay. Our edge-pruning algorithms strongly connect well-placed group nodes to the remaining group members, to create new and pruned group graphs, such that, when a tree algorithm is applied to a pruned group graph, it is manipulated into creating trees with a smaller diameter. We implemented and analyzed experimentally spanning-tree heuristics, core-selection heuristics and edge-pruning algorithms. We found that faster heuristics that do not explicitly optimize the diameter are able to compete with slower heuristics that do optimize it.},
keywords={computer network management;groupware;multicast protocols;transport protocols;trees (mathematics);IPV4 multicast tree diameter;dynamic distributed interactive application;latency reduction;application-layer multicast;group management;overlay network;graph algorithm;edge-pruning algorithm;spanning-tree heuristics;core-selection heuristics;virtual environment;Delay;Tree graphs;Virtual environment;Laboratories;Algorithm design and analysis;Streaming media;Multicast algorithms;Peer to peer computing;Application software;Communications Society},
doi={10.1109/INFOCOM.2008.220},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509816,
author={L. K. Law and S. V. Krishnamurthy and M. Faloutsos},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Capacity of Hybrid Cellular-Ad Hoc Data Networks},
year={2008},
volume={},
number={},
pages={1606-1614},
abstract={In this paper, towards improving spatial reuse in a cellular network, we consider augmenting it with wireless ad hoc connectivity. The coverage area of each base-station is reduced and the users that are within the area relay traffic to nodes outside the area; these users further relay data to more distant users within the cell. The resulting network is referred to as a hybrid network. While this approach can result in shorter range higher-rate links and improved spatial reuse which, together favor a capacity increase, it relies on multi-hop forwarding which is detrimental to the overall capacity. Our objective in this work is to evaluate the impact of these conflicting factors on the capacity of the hybrid network and determine if this capacity is higher than that of the original cellular network. We formally define the capacity of the network as the maximum possible downlink throughput under the conditions of max-min fairness. We analytically compute the capacity of a two-dimensional hybrid network with <i>regular</i> placements of base-stations (BSs) and users. We validate our analytical results via simulations. Our studies demonstrate that capacity improvements are possible in certain parametric regimes in which the penalty due to multi-hop relaying does not outweigh the gains due to spatial reuse and shorter higher-rate links. Our simulations also demonstrate that if the users are placed randomly, the behavioral results are similar to that with regular placements of users.},
keywords={ad hoc networks;cellular radio;telecommunication traffic;hybrid cellular ad hoc data networks;wireless ad hoc connectivity;base-station;relay traffic;multihop forwarding;max-min fairness;multihop relaying;Land mobile radio cellular systems;Relays;Telecommunication traffic;Downlink;Throughput;Computer networks;Cellular networks;Interference;Communications Society;Computer science},
doi={10.1109/INFOCOM.2008.221},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509817,
author={Y. Yang and J. Wang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Design Guidelines for Routing Metrics in Multihop Wireless Networks},
year={2008},
volume={},
number={},
pages={1615-1623},
abstract={The design of a routing protocol must be based on the characteristics of its target networks. The diversity of wireless networks motivates the design of different routing metrics, capturing different aspects of wireless communications. The design of routing metrics, however, is not arbitrary since it has a great impact on the proper operation of routing protocols. Combining a wrong type of routing metrics with a routing protocol may result in routing loops and suboptimal paths. In this paper, we thoroughly study the relationship between routing metrics and routing protocols. Our work provides important guidelines for designing routing metrics and identifies the specific properties that a routing metric must have in order to be combined with certain type of routing protocols.},
keywords={diversity reception;radio networks;routing protocols;multihop wireless network diversity;routing metrics design guideline;routing protocol;wireless communication;Guidelines;Spread spectrum communication;Wireless networks;Routing protocols;Wireless communication;Mathematical model;Communications Society;Diversity reception;Algorithm design and analysis;Network topology},
doi={10.1109/INFOCOM.2008.222},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509818,
author={Q. Dong and Y. Bejerano},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Building Robust Nomadic Wireless Mesh Networks Using Directional Antennas},
year={2008},
volume={},
number={},
pages={1624-1632},
abstract={Recently, wireless mesh technology has been used for military applications and fast recovery networks, referred to as nomadic wireless mesh networks (NWMNs). In such systems, wireless routers, termed nodes, are mounted on top of vehicles or vessels, which may change their location according to application needs; and the nodes are required to establish a reliable wireless mesh network. For improving network performance, some vendors use directional antennas, and the mesh topology comprises of point-to-point connections between adjacent nodes. The number of point-to-point connections of a node is upper-bounded by the number of directional radios it has, which is typically a small constant. This raises the need to build robust (i.e., two-node/edge- connected) mesh networks with bounded node degree, regardless of node locations. In this paper, we present simple elegant schemes for constructing such efficient and robust wireless mesh networks with provably small constant degree bounds. Our extensive simulations show our schemes build robust and efficient topologies for various settings with node degree bounded by 4 and small hop-count distance between nodes and gateways.},
keywords={directive antennas;radio networks;telecommunication network reliability;robust nomadic wireless mesh networks;directional antennas;wireless mesh technology;reliable wireless mesh network;point-to-point connections;adjacent nodes;directional radios;hop-count distance;Robustness;Wireless mesh networks;Directional antennas;Peer to peer computing;Marine vehicles;Network topology;IP networks;Directive antennas;Interference;Broadband communication},
doi={10.1109/INFOCOM.2008.223},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509819,
author={J. Shi and O. Gurewitz and V. Mancuso and J. Camp and E. W. Knightly},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Measurement and Modeling of the Origins of Starvation in Congestion Controlled Mesh Networks},
year={2008},
volume={},
number={},
pages={1633-1641},
abstract={Significant progress has been made in understanding the behavior of TCP and congestion-controlled traffic over multi- hop wireless networks. Despite these advances, however, no prior work identified severe throughput imbalances in the basic scenario of mesh networks, in which one-hop flows contend with two-hop flows for gateway access. In this paper, we demonstrate via real network measurements, test-bed experiments, and an analytical model that starvation exists in such a scenario, i.e., the one-hop flow receives most of the bandwidth while the two- hop flow starves. Our analytical model yields a solution consisting of a simple contention window policy that can be implemented via mechanisms in IEEE 802.11e. Despite its simplicity, we demonstrate through analysis, experiments, and simulations, that the policy has a powerful effect on network-wide behavior, shifting the network's queuing points, mitigating problematic MAC behavior, and ensuring that TCP flows obtain a fair share of the gateway bandwidth, irrespective of their spatial locations.},
keywords={access protocols;queueing theory;telecommunication congestion control;telecommunication traffic;transport protocols;wireless LAN;congestion controlled mesh network;congestion-controlled traffic;multihop wireless network;TCP;queuing theory;medium access control;gateway access;IEEE 802.11e mechanism;Mesh networks;Analytical models;Bandwidth;Communication system traffic control;Traffic control;Wireless mesh networks;Throughput;Fluid flow measurement;Testing;Queueing analysis},
doi={10.1109/INFOCOM.2008.224},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509820,
author={R. Chertov and S. Fahmy and N. B. Shroff},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Device-Independent Router Model},
year={2008},
volume={},
number={},
pages={1642-1650},
abstract={Several popular simulation and emulation environments fail to account for realistic packet forwarding behaviors of commercial switches and routers. Such simulation or emulation inaccuracies can lead to dramatic and qualitative impacts on the results. In this paper, we present a measurement-based model for routers and other forwarding devices, which we use to simulate two different Cisco routers under varying traffic conditions. The structure of our model is device-independent, but requires device-specific parameters. We construct a profiling tool and use it to derive router parameter tables within a few hours. Our preliminary results indicate that our model can approximate the Cisco routers. The compactness of the parameter tables and simplicity of the model makes it possible to use it for high-fidelity simulations while preserving simulation scalability.},
keywords={telecommunication network routing;telecommunication traffic;device-independent router model;measurement-based model;network traffic;Traffic control;Scalability;Testing;Computational modeling;Emulation;Switches;Delay;IP networks;Computer crime;Predictive models},
doi={10.1109/INFOCOM.2008.225},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509821,
author={Y. Gu and L. Breslau and N. Duffield and S. Sen},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={GRE Encapsulated Multicast Probing: A Scalable Technique for Measuring One-Way Loss},
year={2008},
volume={},
number={},
pages={1651-1659},
abstract={Internet service providers increasingly wish to monitor the performance of customer traffic within their networks. This paper addresses the problem of scalably performing one-way loss measurements across specific network paths. Our solution addresses the issue of scale by exploiting measurement features of the deployed network infrastructure to a large degree. There are three components. Firstly, GRE tunneling is used to control the path followed by measurement traffic in the network. Secondly, innovative probing methods, coupled with standard measurement capabilities, such as NetFlow, are used to isolate the performance of groups of measurement packets. Thirdly, we exploit and extend tomographic inference methods in order to extract the performance of probe traffic on customer paths within the network. This combination yields a powerful yet lightweight method to determine customer performance within the network.},
keywords={Internet;multicast communication;generic routing encapsulation;encapsulated multicast probing;Internet;one way loss measurements;GRE tunneling;innovative probing methods;tomographic inference methods;customer performance;Loss measurement;Remote monitoring;Probes;Telecommunication traffic;Communication system traffic control;Performance loss;Tunneling;Tomography;Unicast;Communications Society},
doi={10.1109/INFOCOM.2008.226},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509822,
author={M. Latapy and C. Magnien},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Complex Network Measurements: Estimating the Relevance of Observed Properties},
year={2008},
volume={},
number={},
pages={1660-1668},
abstract={Complex networks, modeled as large graphs, received much attention during these last years. However, topological information on these networks is only available through intricate measurement procedures. Until recently, most studies assumed that these procedures eventually lead to samples large enough to be representative of the whole, at least concerning some key properties. This has a crucial impact on network modeling and simulation, which rely on these properties. Recent contributions proved that this assumption may be misleading, but no solution has been proposed. We provide here the first practical methodology to distinguish between cases where it is indeed misleading, and cases where the observed properties may be trusted. It consists in studying how the properties of interest evolve when the sample grows, and in particular whether they reach a steady state or not. In order to illustrate this method and to demonstrate its relevance, we apply it to data-sets on complex network measurements that are representative of the ones commonly used. The obtained results show that the method fulfills its goals very well. We moreover identify some properties which seem easier to evaluate in practice, thus opening interesting perspectives.},
keywords={complex networks;graph theory;network theory (graphs);complex network measurement;large graph;topological information;Complex networks;Time measurement;Steady-state;Communications Society;Context modeling;Computer science;Internet;Peer to peer computing;Proteins;Biological system modeling},
doi={10.1109/INFOCOM.2008.227},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509823,
author={J. B. Schmitt and F. A. Zdarsky and M. Fidler},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Delay Bounds under Arbitrary Multiplexing: When Network Calculus Leaves You in the Lurch...},
year={2008},
volume={},
number={},
pages={1669-1677},
abstract={Network calculus has proven as a valuable and versatile methodology for worst-case analysis of communication networks. One issue in which it is still lacking is the treatment of aggregate multiplexing, in particular if the FIFO property cannot be assumed when flows are merged. In this paper, we address the problem of bounding the delay of individual traffic flows in feed-forward networks under arbitrary multiplexing. Somewhat surprisingly, we find that direct application of network calculus results in loose bounds even in seemingly simple scenarios. The reasons for this "failure" of network calculus are discussed in detail and a method to arrive at tight delay bounds for arbitrary (aggregate) multiplexing is presented. This method is based on the solution of an optimization problem. For the special case of sink-tree networks this optimization problem is solved explicitly, thus arriving at a closed-form expression for the delay bound. Numerical experiments illustrate that in sink-tree networks the improvement over bounds based on direct application of network calculus can be considerable.},
keywords={feedforward;multiplexing;optimisation;performance evaluation;queueing theory;telecommunication traffic;arbitrary multiplexing;network calculus;communication networks;worst-case analysis;FIFO property;individual traffic flow delay bounds;optimization problem;queueing system;feed-forward networks;performance evaluation;Calculus;Aggregates;Feedforward systems;Queueing analysis;Network topology;Network servers;Delay;Telecommunication traffic;Traffic control;Quality of service},
doi={10.1109/INFOCOM.2008.228},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509824,
author={M. Vojnovic and V. Gupta and T. Karagiannis and C. Gkantsidis},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Sampling Strategies for Epidemic-Style Information Dissemination},
year={2008},
volume={},
number={},
pages={1678-1686},
abstract={We consider epidemic-style information dissemination strategies that leverage the nonuniformity of host distribution over subnets (e.g., IP subnets) to optimize the information spread. Such epidemic-style strategies are based on random sampling of target hosts according to a sampling rule. In this paper, the objective is to optimize the information spread with respect to minimizing the total number of samplings to reach a target fraction of the host population. This is of general interest for the design of efficient information dissemination systems and more specifically, to identify requirements for the containment of worms that use subnet preference scanning strategies. We first identify the optimum number of samplings to reach a target fraction of hosts, given global information about the host distribution over subnets. We show that the optimum can be achieved by either a dynamic strategy for which the per host sampling rate over subnets is allowed to vary over time or by a static strategy for which the sampling over subnets is fixed. These results appear to be novel and are informative about (a) what best possible performance is achievable and (b) what factors determine the performance gain over oblivious strategies such as uniform random scanning. We then consider several simple, online sampling strategies that require only local knowledge, where each host biases sampling based on its observed sampling outcomes and keeps only O(1) state at any point in time. Using real datasets from several large-scale Internet measurements, we evaluate the significance of the factors revealed by our analytical results on the sampling efficiency.},
keywords={Internet;invasive software;random processes;sampling methods;telecommunication security;random sampling;epidemic-style information dissemination;subnet;worms;Internet;Sampling methods;Internet;Communications Society;Performance gain;Large-scale systems;Software performance;Databases;Broadcasting;Performance analysis;Intrusion detection},
doi={10.1109/INFOCOM.2008.229},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509825,
author={J. Sun and C. Zhang and Y. Fang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Security Architecture Achieving Anonymity and Traceability in Wireless Mesh Networks},
year={2008},
volume={},
number={},
pages={1687-1695},
abstract={Anonymity has received increasing attention in the literature due to the users' awareness of their privacy nowadays. Anonymity provides protection for users to enjoy network services without being traced. While anonymity related issues have been extensively studied in payment-based systems such as e-cash [1] and peer-to-peer (P2P) [2] systems, little effort has been devoted to wireless mesh networks (WMNs). On the other hand, the network authority requires conditional anonymity such that misbehaving entities in the network remain traceable. In this paper, we propose a security architecture to ensure unconditional anonymity for honest users and traceability of misbehaving users for network authorities in WMNs. The proposed architecture strives to resolve the conflicts between the anonymity and traceability objectives, in addition to guaranteeing fundamental security requirements including authentication, confidentiality, data integrity, and non-repudiation [3]. Further security enhancements can be incorporated, rendering the proposed architecture conditionally anonymous in terms of network access activities, location information, and communication paths.},
keywords={radio networks;telecommunication security;security architecture;wireless mesh networks;users protection;network services;payment-based systems;network authority;data integrity;network access activities;location information;communication paths;Wireless mesh networks;Privacy;Communication system security;Data security;Information security;Authentication;Wireless LAN;Routing;Communications Society;Computer security},
doi={10.1109/INFOCOM.2008.230},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509826,
author={P. K. Manna and S. Chen and S. Ranka},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Exact Modeling of Propagation for Permutation-Scanning Worms},
year={2008},
volume={},
number={},
pages={1696-1704},
abstract={Modeling worm propagation has been an important research subject in the Internet-worm research community. An accurate analytical propagation model allows us to study the spreading speed and traffic pattern of a worm under an arbitrary set of worm/network parameters, which is often computationally too intensive for simulations. More importantly, it gives us an insight into the impact of each worm/network parameter on the propagation of the worm and the effectiveness of a potential defense mechanism that is designed to control some of those parameters. Traditionally, most modeling work in the area concentrates on the relatively simple random-scanning worms. However, worm technologies have advanced rapidly in recent years. By enabling close coordination among all infected hosts, the permutation-scanning worms minimize the duplication of effort when scanning the whole Internet address space. They propagate much faster, and more importantly, can be much more stealthy than the random-scanning worms. Modeling these worms, however, remains a challenge to date. This paper proposes a mathematical model that precisely characterizes the propagation patterns of the permutation-scanning worms. The analytical framework captures the interactions among all infected hosts by a series of inter-dependent differential equations, which together present the overall behavior of the worm. We use simulations to verify the numerical results from the model, and demonstrate how the model can be used to study the impact of various worm/network parameters on the propagation.},
keywords={Internet;telecommunication security;telecommunication traffic;permutation-scanning worms;worm propagation;Internet-worm;traffic pattern;random-scanning worms;Internet address space;interdependent differential equations;Internet;Analytical models;Space technology;Pattern analysis;Telecommunication traffic;Traffic control;Computer networks;Computational modeling;Mathematical model;Differential equations},
doi={10.1109/INFOCOM.2008.231},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509827,
author={P. Chhabra and C. Scott and E. D. Kolaczyk and M. Crovella},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Distributed Spatial Anomaly Detection},
year={2008},
volume={},
number={},
pages={1705-1713},
abstract={Detection of traffic anomalies is an important problem that has been the focus of considerable research. Recent work has shown the utility of spatial detection of anomalies via crosslink traffic comparisons. In this paper we identify three advances that are needed to make such methods more useful and practical for network operators. First, anomaly detection methods should avoid global communication and centralized decision making. Second, nonparametric anomaly detection methods are needed to augment current parametric approaches. And finally, such methods should not just identify possible anomalies, but should also annotate each detection with some probabilistic qualifier of its importance. We propose a framework that simultaneously advances the current state of the art on all three fronts. We show that routers can effectively identify volume anomalies through crosslink comparison of traffic observed only on the router's own links. Second, we show that generalized quantile estimators are an effective way to identify high-dimensional sets of local traffic patterns that are potentially anomalous; such methods can be either parametric or nonparametric, and we evaluate both. Third, through the use of false discovery rate as a detection metric, we show that candidate anomalous patterns can be equipped with an estimate of a probability that they truly are anomalous. Overall, our framework provides network operators with an anomaly detection methodology that is distributed, effective, and easily interpretable. Part of the underlying statistical framework, which merges aspects of nonparametric set estimation and multiple hypothesis testing, is novel in itself, although the derivation of that framework is necessarily given elsewhere.},
keywords={Internet;telecommunication traffic;distributed spatial anomaly detection;traffic anomalies;cross-link traffic comparisons;anomaly detection methods;global communication;centralized decision making;local traffic patterns;Telecommunication traffic;Communications Society;Global communication;Decision making;Probability;Testing;Computer crime;Computer worms;Equipment failure;Computer science},
doi={10.1109/INFOCOM.2008.232},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509828,
author={S. Castelli and P. Costa and G. P. Picco},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={HyperCBR: Large-Scale Content-Based Routing in a Multidimensional Space},
year={2008},
volume={},
number={},
pages={1714-1722},
abstract={Content-based routing (CBR) is becoming increasingly popular as a building block for distributed applications. CBR differs from classical routing paradigms as messages are routed based on their content rather than their destination address, which fosters decoupling and flexibility in the application's distributed architecture. However, most available systems realize CBR by relying on a tree-shaped overlay network and adopt a routing strategy based on broadcasting subscription requests, thus hampering applicability in very large-scale networks. In this paper, we observe that a fundamental underpinning of any CBR protocol is for messages and subscriptions to "meet" at some points in the network. In the approach we propose here, called HyperCBR<sup>1</sup>, we enforce this topological property in a multidimensional space, by routing messages and subscriptions on different, albeit intersecting, partitions. We derive an analytical model of HyperCBR, validated through simulation, and use it to evaluate our approach in two relevant CBR contexts - content-based searches in peer-to-peer networks, and content- based publish-subscribe. The results show that our protocol achieves efficient CBR even in very large scale settings (e.g., millions of nodes) while at the same time opening up intriguing opportunities for deployment-time tuning based on the expected traffic profiles. The analytical evaluation is complemented by simulation results relying on a CAN-based implementation, showing that HyperCBR generates a small forwarding and matching load, and that it is able to tolerate high churn with low overhead.},
keywords={message passing;middleware;peer-to-peer computing;routing protocols;telecommunication network routing;HyperCBR;large scale content based routing;multidimensional space;tree shaped overlay network;subscription request broadcasting;routing messages;peer-to-peer networks;content based publish-subscribe;deployment-time tuning;traffic profiles;matching load;CAN-based implementation;Large-scale systems;Routing;Multidimensional systems;Subscriptions;Analytical models;Protocols;Peer to peer computing;Broadcasting;Context modeling;Publish-subscribe},
doi={10.1109/INFOCOM.2008.233},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509829,
author={K. P. N. Puttaswamy and A. Sala and B. Y. Zhao},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Searching for Rare Objects Using Index Replication},
year={2008},
volume={},
number={},
pages={1723-1731},
abstract={Searching for objects is a fundamental problem for popular peer-to-peer file-sharing networks that contribute to much of the traffic on today's Internet. While existing protocols can effectively locate highly popular files, studies show that they fail to locate a significant portion of existing files in the network. High recall for these "rare" objects would drastically improve the user experience, and make these networks the ideal distribution infrastructure for user-generated content such as home videos and photo albums. In this paper, we examine simple techniques that can improve search recall for rare objects while minimizing the overhead incurred by participating peers. We propose several strategies for multi-hop index replication, and demonstrate their effectiveness and efficiency through both analysis and simulation. We further evaluate our simple techniques using detailed traces from a real Gnutella network, and show that they improve the performance of these overlays by orders of magnitude in both lookup success and overhead.},
keywords={indexing;Internet;peer-to-peer computing;rare objects searching;peer-to-peer file-sharing networks;Internet traffic;distribution infrastructure;user-generated content;multihop index replication;Bandwidth;Peer to peer computing;Telecommunication traffic;Floods;Communications Society;Computer science;IP networks;Protocols;User-generated content;Videos},
doi={10.1109/INFOCOM.2008.234},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509830,
author={S. -. Lee and S. Banerjee and P. Sharma and P. Yalagandula and S. Basu},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Bandwidth-Aware Routing in Overlay Networks},
year={2008},
volume={},
number={},
pages={1732-1740},
abstract={In the absence of end-to-end quality of service (QoS), overlay routing has been used as an alternative to the default best effort Internet routing. Using end-to-end network measurement, the problematic parts of the path can be bypassed, resulting in improving the resiliency and robustness to failures. Studies have shown that overlay paths can give better latency, loss rate, and TCP throughput. Overlay routing also offers flexibility as different routes can be used based on application needs. There have been very few proposals of using bandwidth as the main metric of interest, which is of great concern in media applications. We introduce our scheme BARON (Bandwidth-Aware Routing in Overlay Networks) that utilizes capacity between the end hosts to identify viable overlay paths and measures available bandwidth to select the best route. We propose our path selection approaches, and using the measurements between 174 PlanetLab nodes and over 13,189 paths, we evaluate the usefulness of overlay routes in terms of bandwidth gain. Our results show that among 658,526 overlay paths, 25% have larger bandwidth than their native IP routes, and over 86% of (source, destination) pairs have at least one overlay route with larger bandwidth than the default IP routes. We also present the effectiveness of BARON in preserving the bandwidth requirement over time for a few selected Internet paths.},
keywords={bandwidth allocation;Internet;IP networks;quality of service;telecommunication network routing;transport protocols;bandwidth-aware routing;overlay network;end-to-end quality of service;Internet routing;end-to-end network measurement;TCP throughput;IP routing;Routing;Bandwidth;Extraterrestrial measurements;Quality of service;Web and internet services;Robustness;Delay;Throughput;Proposals;Gain measurement},
doi={10.1109/INFOCOM.2008.235},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509831,
author={R. H. Wouhaybi and P. Sharma and S. Banerjee and A. T. Campbell},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Minerva: Learning to Infer Network Path Properties},
year={2008},
volume={},
number={},
pages={1741-1749},
abstract={Knowledge of the network path properties such as latency, hop count, loss and bandwidth is key to the performance of overlay networks, grids and P2P applications. Network operators also use these metrics for managing and diagnosing problems in their networks. However, the size of the Internet makes the task of measuring these metrics immensely difficult. A more scalable approach of inference and estimation of these metrics based on partial measurements has been recently adopted. Current inference approaches do not adapt to different network topologies and the evolution of the network over time. In this paper, we propose a novel learning based approach, called Minerva, for the inferencing of inter-node properties. Minerva uses partial measurements to create signature-like profiles for the participating nodes. These signatures are later used as input to a trained Bayesian network module to estimate the different network properties. We have built a system based on our approach and present performance results from real network measurements obtained from the Planet-Lab testbed. The sensitivity of the system to different parameters including training set, measurement overhead, and size of network have also been studied in this paper.},
keywords={belief networks;estimation theory;inference mechanisms;Internet;learning (artificial intelligence);telecommunication computing;telecommunication network topology;overlay network topology;peer-to-peer application;grid application;Internet;inference mechanism;signature-like profile;Bayesian network module;Extraterrestrial measurements;Size measurement;Delay;Bandwidth;Performance loss;Internet;Network topology;Particle measurements;Bayesian methods;System testing},
doi={10.1109/INFOCOM.2008.236},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509832,
author={Y. Cho and C. -. Hwang and F. A. Tobagi},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Design of Robust Random Access Protocols For Wireless Networks Using Game Theoretic Models},
year={2008},
volume={},
number={},
pages={1750-1758},
abstract={Since many random-access protocols are designed based on the assumption that all terminals follow the same medium-access-control (MAC) policy, they can be vulnerable to the selfish behavior of each terminal. This paper proposes robust random-access protocols for wireless networks in fading environments, where each terminal operates in a Nash equilibrium (NE) of a random-access game. Since any deviation of a terminal from an NE penalizes the terminal, the network with this protocol becomes more robust by preventing a selfish terminal from violating the access protocol. We model slotted ALOHA and carrier-sense multiple access (CSMA) as games and show that the strategies in symmetric equilibria of these games have the properties so that a terminal with a better channel state is more likely to access the channel. Thus, these protocols ensure multiuser diversity as well as robustness, a conclusion confirmed by numerical results.},
keywords={access protocols;fading channels;game theory;radio networks;robust random access protocols;wireless networks;game theoretic models;medium-access-control policy;fading environments;Nash equilibrium;random-access game;carrier-sense multiple access;ALOHA;Robustness;Access protocols;Wireless networks;Game theory;Multiaccess communication;Wireless application protocol;Media Access Protocol;Throughput;Bayesian methods;Fading},
doi={10.1109/INFOCOM.2008.237},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509833,
author={E. Altman and Y. Hayel},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Stochastic Evolutionary Game of Energy Management in a Distributed Aloha Network},
year={2008},
volume={},
number={},
pages={1759-1767},
abstract={A major contribution of biology to competitive decision making is the area of evolutionary games. It describes the evolution of sizes of large populations as a result of many local interactions, each involving a small number of randomly selected individuals. An individual plays only once; it plays in a one shot game against another randomly selected player with the goal of maximizing its utility (fitness) in that game. We introduce here a new more general type of games: a Stochastic Evolutionary Game where each player may be in different states; the player may be involved in several local interactions during his life time and his actions determine not only the utilities but also the transition probabilities and his life duration. This is used to study a large population of mobiles forming a sparse ad-hoc network, where mobiles compete with their neighbors on the access to a radio channel. We study the impact of the level of energy in the battery on the aggressiveness of the access policy of mobiles at equilibrium. We obtain properties of the ESS (Evolutionary Stable Strategy) equilibrium which, Unlike the Nash equilibrium concept, is robust against deviations of a whole positive fraction of the population. We further study dynamical properties of the system when it is not in equilibrium.},
keywords={access protocols;ad hoc networks;decision making;mobile radio;stochastic games;wireless channels;stochastic evolutionary game;energy management;distributed Aloha network;competitive decision making;evolutionary games;sparse ad-hoc network;radio channel;evolutionary stable strategy equilibrium;Stochastic processes;Energy management;Game theory;Evolution (biology);Decision making;Ad hoc networks;Batteries;Electronic switching systems;Nash equilibrium;Robustness},
doi={10.1109/INFOCOM.2008.238},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509834,
author={Y. Sheng and K. Tan and G. Chen and D. Kotz and A. Campbell},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Detecting 802.11 MAC Layer Spoofing Using Received Signal Strength},
year={2008},
volume={},
number={},
pages={1768-1776},
abstract={MAC addresses can be easily spoofed in 802.11 wireless LANs. An adversary can exploit this vulnerability to launch a large number of attacks. For example, an attacker may masquerade as a legitimate access point to disrupt network services or to advertise false services, tricking nearby wireless stations. On the other hand, the received signal strength (RSS) is a measurement that is hard to forge arbitrarily and it is highly correlated to the transmitter's location. Assuming the attacker and the victim are separated by a reasonable distance, RSS can be used to differentiate them to detect MAC spoofing, as recently proposed by several researchers. By analyzing the RSS pattern of typical 802.11 transmitters in a 3-floor building covered by 20 air monitors, we observed that the RSS readings followed a mixture of multiple Gaussian distributions. We discovered that this phenomenon was mainly due to antenna diversity, a widely-adopted technique to improve the stability and robustness of wireless connectivity. This observation renders existing approaches ineffective because they assume a single RSS source. We propose an approach based on Gaussian mixture models, building RSS profiles for spoofing detection. Experiments on the same testbed show that our method is robust against antenna diversity and significantly outperforms existing approaches. At a 3% false positive rate, we detect 73.4%, 89.6% and 97.8% of attacks using the three proposed algorithms, based on local statistics of a single AM, combining local results from AMs, and global multi-AM detection, respectively.},
keywords={access protocols;Gaussian distribution;wireless LAN;802.11 MAC layer spoofing;received signal strength;802.11 wireless LAN;network services;false services;wireless stations;802.11 transmitters;multiple Gaussian distributions;antenna diversity;wireless connectivity;Gaussian mixture models;Tin;Radio frequency;Semiconductor device measurement;Radio transmitters;Cryptography;Microprogramming;Antenna measurements;Diversity reception;Robust stability;Communications Society},
doi={10.1109/INFOCOM.2008.239},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509835,
author={G. Hauksson and M. Alanyali},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Wireless Medium Access via Adaptive Backoff: Delay and Loss Minimization},
year={2008},
volume={},
number={},
pages={1777-1785},
abstract={We consider packet transmission scheduling at the MAC-layer via adaptive backoff algorithms that are favorable in terms of queue occupancies in a wireless network. General network topologies are considered under the operational constraint that transmitters within close proximity of each other cannot be transmitting simultaneously. Transmitters probe the channel at random instants and transmit if the channel is idle. We adopt a measurement based framework in which channel probing rates are adaptively determined based on feedback measured from the network. We consider two separate objectives associated with minimization of packet delay and packet loss rate in the network. We obtain dynamic algorithms that strictly improve channel access rates in a related fluid model. Analytical development of the algorithms is based on a convenient decomposition technique that decouples channel access and queue occupancy statistics, and that leads to a favorable tradeoff between analytical insight and modeling accuracy. Obtained algorithms are oblivious to network load and topology. We also consider versions of these algorithms that are suitable for distributed implementation and study their effectiveness numerically.},
keywords={access protocols;queueing theory;radio networks;telecommunication network topology;wireless channels;wireless medium access;adaptive backoff algorithm;packet transmission scheduling;MAC-layer;queueing theory;wireless network topology;channel probing rate;convenient decomposition technique;loss minimization;delay minimization;Delay;Network topology;Transmitters;Algorithm design and analysis;Scheduling algorithm;Wireless networks;Probes;Feedback;Fluid dynamics;Heuristic algorithms},
doi={10.1109/INFOCOM.2008.240},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509836,
author={W. Jiang and Q. Wang and V. K. Prasanna},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Beyond TCAMs: An SRAM-Based Parallel Multi-Pipeline Architecture for Terabit IP Lookup},
year={2008},
volume={},
number={},
pages={1786-1794},
abstract={Continuous growth in network link rates poses a strong demand on high speed IP lookup engines. While Ternary Content Addressable Memory (TCAM) based solutions serve most of today's high-end routers, they do not scale well for the next-generation. On the other hand, pipelined SRAM- based algorithmic solutions become attractive. Intuitively multiple pipelines can be utilized in parallel to have a multiplicative effect on the throughput. However, several challenges must be addressed for such solutions to realize high throughput. First, the memory distribution across different stages of each pipeline as well as across different pipelines must be balanced. Second, the traffic on various pipelines should be balanced. In this paper, we propose a parallel SRAM-based multi- pipeline architecture for terabit IP lookup. To balance the memory requirement over the stages, a two-level mapping scheme is presented. By trie partitioning and subtrie-to-pipeline mapping, we ensure that each pipeline contains approximately equal number of trie nodes. Then, within each pipeline, a fine-grained node-to-stage mapping is used to achieve evenly distributed memory across the stages. To balance the traffic on different pipelines, both pipelined prefix caching and dynamic subtrie-to-pipeline remapping are employed. Simulation using real-life data shows that the proposed architecture with 8 pipelines can store a core routing table with over 200 K unique routing prefixes using 3.5 MB of memory. It achieves a throughput of up to 3.2 billion packets per second, i.e. 1 Tbps for minimum size (40 bytes) packets.},
keywords={Internet;SRAM chips;telecommunication network routing;ternary content addressable memory;static random access memory;multipipeline architecture;terabit IP lookup;high-end router;memory distribution;trie partitioning;subtrie-to-pipeline mapping;Throughput;Pipeline processing;Random access memory;Clocks;Peer to peer computing;Routing;Energy consumption;Internet;Hardware;Communications Society},
doi={10.1109/INFOCOM.2008.241},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509837,
author={H. Yu and R. Mahapatra},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Memory-Efficient Hashing by Multi-Predicate Bloom Filters for Packet Classification},
year={2008},
volume={},
number={},
pages={1795-1803},
abstract={Hash tables (HTs) are poorly designed for multiple off-chip memory accesses during packet classification and critically affect throughput in high-speed routers. Therefore, an HT with fast on-chip memory and high-capacity off-chip memory for predictable lookup-throughput is desirable. Both a legacy HT (LHT) and a recently proposed fast HT (FHT) have the disadvantage of memory overhead due to pointers and duplicate items in linked lists. Also, memory usage for an FHT did not consider the bits in counters for fair comparison with an LHT. In this paper, we propose a novel hash architecture called a Multi-predicate Bloom-filtered HT (MBHT) using parallel Bloom filters and generating off-chip memory addresses in the base- 2<sup>x</sup> number system, xisin{1,2,hellip}, which removes the overhead of pointers. Using a larger base of number system, an MBHT reduces on-chip memory size by a factor of log<sub>2</sub> b<sub>2</sub>/ log<sub>2</sub> b<sub>1</sub> where b<sub>1</sub> and b<sub>2</sub> are bases of number system (b<sub>2</sub>&gt;b<sub>1</sub>). Compared to an FHT, the MBHT is approximately x(log<sub>2</sub> n + 4)/(2 log<sub>2</sub> n) times more efficient for on-chip memory, where n is the number of keys. This results in a significant reduction in the number of off- chip memory accesses. A simulation with a dataset of packets from NLANR shows the on-chip memory reductions by 1.7 and 2 times over an LHT and an FHT are made. Besides, an MBHT of base-16 needs less off-chip memory accesses by 2117 in total URL queries of NLANR, compared to an FHT.},
keywords={data structures;memory architecture;pattern classification;storage allocation;table lookup;telecommunication network routing;memory-efficient hashing;multipredicate bloom-filtered HT;packet classification;hash tables;off-chip memory accesses;high-speed routers;parallel Bloom filters;number system;lookup-throughput;Filters;Uniform resource locators;Large-scale systems;Costs;Communications Society;Computer science;Throughput;Counting circuits;System-on-a-chip;Access control},
doi={10.1109/INFOCOM.2008.242},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509838,
author={V. -. Nguyen and R. Lo Cigno and Y. Ofek and M. Telek},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Time Blocking Analysis in Time-Driven Switching Networks},
year={2008},
volume={},
number={},
pages={1804-1812},
abstract={This paper presents a general closed-form analysis of the time-blocking probability in time-driven switching networks. Time-blocking occurs when transmission resources are available in both input and output, but there is no schedule, i.e., input and output resources are outside a pre-defined maximum scheduling delay that is allowed between them. This situation may happen in architectures based on pipeline forwarding of packets. The main constraints affecting the schedulability of resources are the load and the maximum scheduling delay. The analysis yields the exact blocking probabilities for all possible scheduling delays and under all load conditions for a node in isolation, as well as initial results for a network of nodes.},
keywords={optical fibre networks;optical switches;probability;scheduling;time blocking probability analysis;time-driven switching network;maximum scheduling delay;pipeline packet forwarding;optical fibre network;sub-lambda switching;fractional lambda switching;Switches;Delay;Global Positioning System;Frequency synchronization;Switching circuits;Packet switching;Telecommunication switching;Pipelines;Peer to peer computing;Synchronous digital hierarchy},
doi={10.1109/INFOCOM.2008.243},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509839,
author={U. Shevade and R. Kokku and H. M. Vin},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Run-Time System for Scalable Network Services},
year={2008},
volume={},
number={},
pages={1813-1821},
abstract={Sophisticated middlebox services-such as network monitoring and intrusion detection, DDoS mitigation, worm scanning, XML parsing and protocol transformation-are becoming increasingly popular in today's Internet. To support high- throughput, these services are often deployed on distributed memory, multi-processor (DM-MP) hardware platforms such as a cluster of network processors. Scaling the throughput of such platforms, however, is challenging because of the difficulties and overheads of accessing persistent, shared state maintained by the services. In this paper, we describe the design and implementation of Oboe, a run-time system for DM-MP platforms that addresses the above challenge through two foundations: (1) category-specific management of shared state, and (2) adaptive flow- level load distribution for addressing persistent processor overload. Our simulations demonstrate that Oboe can achieve performance within 0-5% of an ideal adaptive system. Our prototype implementation of Oboe on a cluster of IXP2400 network processors, demonstrates the scalability achieved with increasing number of processors, number of flows and state size.},
keywords={computer network management;distributed shared memory systems;Oboe run-time system;scalable network services;sophisticated middlebox services;intrusion detection;network monitoring;XML parsing;Internet;distributed memory multiprocessor platforms;DDoS mitigation;worm scanning;protocol transformation;DM-MP hardware platforms;category-specific management;adaptive flow-level load distribution;Throughput;Middleboxes;Monitoring;Intrusion detection;XML;Access protocols;IP networks;Web and internet services;Hardware;Adaptive systems},
doi={10.1109/INFOCOM.2008.244},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509840,
author={Q. Lv and G. N. Rouskas},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={On Optimal Sizing of Tiered Network Services},
year={2008},
volume={},
number={},
pages={1822-1830},
abstract={We develop an economic model for networks offering tiered services and we formulate the problem of selecting the service tiers from three perspectives: one that considers the users' interests only, one that considers only the service provider's interests, and one that considers both simultaneously, i.e., the interests of society as a whole. We also present dynamic programming algorithms that solve these problems optimally. Our work provides a theoretical framework for reasoning about Internet tiered services, as well as a practical toolset for network providers to develop customized menus of service offerings.},
keywords={dynamic programming;Internet;optimal sizing;tiered network services;dynamic programming;Internet tiered services;Bandwidth;Web and internet services;IP networks;Costs;Quality of service;Dynamic programming;Heuristic algorithms;Communications Society;Computer science;USA Councils},
doi={10.1109/INFOCOM.2008.245},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509841,
author={M. Yannuzzi and X. Masip-Bruin and R. Serral-Gracia and E. Marin-Tordera and A. Sprintson and A. Orda},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Maximum Coverage at Minimum Cost for Multi-Domain IP/MPLS Networks},
year={2008},
volume={},
number={},
pages={1831-1839},
abstract={At present, service providers have several incentives to extend the reach of long-lived MPLS paths across domains. Providers, however, will face a number of trade-offs while choosing the optimal set of MPLS paths to be established. In this paper, we focus on the multi-objective decision problem of maximizing the traffic demands to be covered by long-lived MPLS paths from a source domain S to its major destination domains, while minimizing the monetary costs incurred. The problem is formulated subject to a budget constraint, which assures the minimum expected revenue for the provider in S. A major advantage of the analysis and solution proposed in this paper is that it can be easily generalized, and applied in other settings where constrained problems considering maximum coverage vs. cost are critical.},
keywords={IP networks;multiprotocol label switching;virtual private networks;maximum coverage at minimum cost;multidomain IP/MPLS networks;service providers;multiobjective decision problem;traffic demands;MPLS paths;Costs;Multiprotocol label switching;Virtual private networks;Routing;Telecommunication traffic;Computer networks;Communications Society;Computer architecture;USA Councils;Marine technology},
doi={10.1109/INFOCOM.2008.246},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509842,
author={P. Key and L. Massoulie and D. -. Tomozei},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Non-Metric Coordinates for Predicting Network Proximity},
year={2008},
volume={},
number={},
pages={1840-1848},
abstract={We consider the problem of determining the "closest", or best Internet host to connect to, from a list of candidate servers. Most existing approaches rely on the use of metric, or more specifically Euclidean coordinates to infer network proximity. This is problematic, given that network distances such as latency are known to violate the triangle inequality. This leads us to consider non-metric coordinate systems. We perform an empirical comparison between the "min-plus" non-metric coordinates and two metric coordinates, namely L-infinity and Euclidean. We observe that, when sufficiently many dimensions are used, min-plus outperforms metric coordinates for predicting Internet latencies. We also consider the prediction of "widest path capacity" between nodes. In this framework, we propose a generalization of min-plus coordinates. These results apply when node coordinates consist in measured network proximity to a random subset of landmark nodes. We perform empirical validation of these results on widest path bandwidth between PlanetLab nodes. We conclude that appropriate non-metric coordinates such as generalized min-plus systems are better suited than metric systems for representing the underlying structure of Internet distances, measured either via latencies or bandwidth.},
keywords={Internet;nonmetric coordinates;network proximity;Internet host;Euclidean coordinates;widest path capacity;min-plus systems;Peer to peer computing;Delay;Coordinate measuring machines;Internet;Extraterrestrial measurements;Network servers;Bandwidth;Web server;IP networks;Communications Society},
doi={10.1109/INFOCOM.2008.247},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509843,
author={K. V. M. Naidu and D. Panigrahi and R. Rastogi},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Detecting Anomalies Using End-to-End Path Measurements},
year={2008},
volume={},
number={},
pages={1849-1857},
abstract={In this paper, we propose new "low-overhead" network monitoring techniques to detect violations of path-level QoS guarantees like end-to-end delay, loss, etc. Unlike existing path monitoring schemes, our approach does not calculate QoS parameters for all paths. Instead, it monitors QoS values for only a few paths, and exploits the fact that path anomalies are rare and anomalous states are well separated from normal operation, to rule out path QoS violations in most situations. We propose a heuristic to select a small subset of network paths to monitor while ensuring that no QoS violations are missed. Experiments with an ISP topology from the Rocketfuel data set show that our heuristic can deliver almost a 50% decrease in monitoring overhead compared to previous schemes.},
keywords={computer networks;quality of service;telecommunication security;anomaly detection;end-to-end path measurement;network monitoring technique;QoS;ISP topology;Monitoring;Delay;Probes;Loss measurement;Telecommunication traffic;Network-on-a-chip;Costs;Computer networks;Spine;Communications Society},
doi={10.1109/INFOCOM.2008.248},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509844,
author={H. S. Lichte and S. Valentin and H. Karl and I. Aad and L. Loyola and J. Widmer},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Design and Evaluation of a Routing-Informed Cooperative MAC Protocol for Ad Hoc Networks},
year={2008},
volume={},
number={},
pages={1858-1866},
abstract={Cooperative relaying has been shown to provide diversity gains which can significantly improve the packet error rate (PER) in wireless transmissions. In ad hoc wireless routing where packets may travel over a number of hops before reaching the destination, hop-wise cooperative relaying may severely reduce network capacity. This approach was mainly addressed in literature so far. In this paper, we efficiently apply cooperative relaying along a complete path and over multiple hops at the same time. We use information from the routing layer to improve the medium access control (MAC) layer's performance. Simulations and testbed implementation show appealing gains through diversity resulting in up to 66% better PER performance and up to 148% goodput increase compared to conventional approaches.},
keywords={access protocols;ad hoc networks;diversity reception;error statistics;telecommunication network routing;cooperative relaying;diversity gain;packet error rate;wireless transmissions;ad hoc wireless routing network;cooperative MAC protocol;Media Access Protocol;Ad hoc networks;Relays;Spread spectrum communication;Routing protocols;Bandwidth;Wireless application protocol;Error analysis;Performance gain;Mesh networks},
doi={10.1109/INFOCOM.2008.249},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509845,
author={Z. J. Haas and E. Y. Hua},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Residual Link Lifetime Prediction with Limited Information Input in Mobile Ad Hoc Networks},
year={2008},
volume={},
number={},
pages={1867-1875},
abstract={We study the problem of predicting the residual link lifetime (RLL) in MANETs, where the nodes are able to measure the relative distances between them (e.g., by using the UWB technology). We propose a mobile-projected trajectory (MPT) algorithm, whose input is periodically sampled, noisy range measurements between the two nodes of a link. It estimates a projected trajectory, which is then used to compute the predicted RLL. An enhancement technique, which we call incremental sampling, is proposed where the estimated trajectory is further refined to improve the accuracy of the RLL prediction. We have evaluated the performance of the MPT algorithm with two different mobility models and for different parameters, and have shown that MPT yields robust performance; i.e., the main strength of the MPT algorithm lies in its capability to accurately predict the RLL with limited range input data. For example, after a measurement-acquisition time equal to 25% of the link lifetime, the algorithm yields 90% prediction accuracy; 80% accuracy is achieved after 20% of the link lifetime. After only 15% of link lifetime, the algorithm still achieves 60% prediction accuracy.},
keywords={ad hoc networks;mobile radio;telecommunication network reliability;ultra wideband technology;residual link lifetime prediction;mobile ad hoc network;MANET;mobile-projected trajectory algorithm;noisy range measurement;incremental sampling method;ultra wideband technology;Mobile ad hoc networks;Accuracy;Peer to peer computing;Ultra wideband technology;Trajectory;Sampling methods;Robustness;Time measurement;Quality of service;Computational modeling},
doi={10.1109/INFOCOM.2008.250},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509846,
author={R. Chen and J. -. Park and K. Bian},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Robust Distributed Spectrum Sensing in Cognitive Radio Networks},
year={2008},
volume={},
number={},
pages={1876-1884},
abstract={Distributed spectrum sensing (DSS) enables a Cognitive Radio (CR) network to reliably detect licensed users and avoid causing interference to licensed communications. The data fusion technique is a key component of DSS. We discuss the Byzantine failure problem in the context of data fusion, which may be caused by either malfunctioning sensing terminals or Spectrum Sensing Data Falsification (SSDF) attacks. In either case, incorrect spectrum sensing data will be reported to a data collector which can lead to the distortion of data fusion outputs. We investigate various data fusion techniques, focusing on their robustness against Byzantine failures. In contrast to existing data fusion techniques that use a fixed number of samples, we propose a new technique that uses a variable number of samples. The proposed technique, which we call Weighted Sequential Probability Ratio Test (WSPRT), introduces a reputation-based mechanism to the Sequential Probability Ratio Test (SPRT). We evaluate WSPRT by comparing it with a variety of data fusion techniques under various network operating conditions. Our simulation results indicate that WSPRT is the most robust against the Byzantine failure problem among the data fusion techniques that were considered.},
keywords={radio networks;sensor fusion;spectral analysers;robust distributed spectrum sensing;cognitive radio networks;data fusion technique;Byzantine failure;spectrum sensing data falsification attacks;weighted sequential probability ratio test;reputation based mechanism;sequential probability ratio test;Robustness;Cognitive radio;Chromium;Decision support systems;Sequential analysis;Interference;FCC;Radio spectrum management;Wireless sensor networks;TV},
doi={10.1109/INFOCOM.2008.251},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509847,
author={F. Wang and M. Krunz and S. Cui},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Spectrum Sharing in Cognitive Radio Networks},
year={2008},
volume={},
number={},
pages={1885-1893},
abstract={In this paper, we present a novel joint power/channel allocation scheme that uses a distributed pricing strategy to improve the network's performance. According to this scheme, the spectrum allocation problem is modeled as a non-cooperative game. A price-based iterative water-filling (PIWF) algorithm is proposed, which allows users to converge to the Nash Equilibrium (NE). This PIWF algorithm can be implemented distributively, with CRs repeatedly negotiating their best transmission powers and spectrum. We propose a protocol that implements our price- based resource allocation algorithm. The proposed MAC protocol allows multiple CR pairs to first contend through an admission phase, and then to iteratively negotiate their transmission powers and spectrum via control-packet exchanges. Subsequently, CRs proceed concurrently with their data transmissions. Simulations are used to study the performance of our protocol and demonstrate its effectiveness in improving the overall network throughput and reducing the average transmission power.},
keywords={access protocols;cognitive radio;spectrum sharing;cognitive radio networks;power allocation;channel allocation;price-based iterative water-filling algorithm;average transmission power;Cognitive radio;Iterative algorithms;Channel allocation;Pricing;Nash equilibrium;Resource management;Media Access Protocol;Chromium;Data communication;Throughput},
doi={10.1109/INFOCOM.2008.252},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509848,
author={Y. -. Choi and K. G. Shin},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Power-Adjusted Random Access to a Wireless Channel},
year={2008},
volume={},
number={},
pages={1894-1902},
abstract={The operation of widely-deployed random access to wireless networks is based on limited information on the result of each access attempt. When making a random access attempt, users usually do not know the exact amount of transmit power to make it successful. Also, upon failure of a transmission attempt, a user cannot tell whether the failure was caused by collision with other simultaneously-transmitting users or by his use of insufficient transmit power. To handle lack of information on the cause of failure, we propose an innovative Cause-of-Failure (CoF) resolution which increases the transmit power after a given number of consecutive unsuccessful access attempts when the probability that a given failure is caused by collision becomes sufficiently low. To exploit the thus-achieved transmit power for the next random access attempt, we also determine the Cause-of-Success (CoS) based on the number of consecutive successful attempts, i.e., whether to decrease or maintain the present transmit power probabilistically. This way, users can adjust their transmit power for random access, which we call Auto Power Fallback (APF). We evaluate APF by modeling analysis and numerical computation based on the slotted Aloha, showing that APF makes significant energy-savings for uplink random accesses while achieving good performance.},
keywords={cellular radio;wireless channels;power-adjusted random access;wireless channel;wireless networks;cause of failure resolution;cause-of-success;auto power fallback;Power control;Energy resolution;Multiaccess communication;Power system modeling;Communications Society;Wireless networks;Numerical models;Performance analysis;Land mobile radio cellular systems;Energy management},
doi={10.1109/INFOCOM.2008.253},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509849,
author={P. Nuggehalli and M. Sarkar and K. Kulkarni and R. R. Rao},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Game-Theoretic Analysis of QoS in Wireless MAC},
year={2008},
volume={},
number={},
pages={1903-1911},
abstract={Many wireless network standards include quality-of-service (QoS) features at the MAC layer. These features provide nodes transmitting high priority delay sensitive traffic such as voice and video preferential access to the channel over nodes carrying low priority delay tolerant traffic such as file transfer and email. However, such schemes are unfair to low priority users, depriving them of equitable transmission opportunities, and causing throughput starvation for their applications. Such unfairness can provoke rational nodes carrying low priority traffic to falsely declare their traffic as high priority in order to maximize their throughput, thereby defeating the very purpose of QoS differentiation. In this paper, we provide game-theoretic analysis of a slotted Aloha like MAC that resembles the IEEE 802.11e MAC in many essential respects. Our MAC model allows traffic to be classified as either high-priority (HP) or low-priority (LP), and allows for both random access (contention) and polled (contention-free) channel access. We advocate an incentive mechanism to stimulate LP users to be truthful. This incentive mechanism makes use of the contention-free channel access feature of our MAC as an efficient and protocol-compliant mechanism to encourage low priority users to be truthful. We discuss appropriate utility functions for HP and LP traffic and use a fixed point analysis to derive the performance of the system in terms of the fraction of time the system is operated in contention-free mode. We find the condition for which our incentive mechanism results in a truthful Nash equilibrium, i.e., no user has an incentive to unilaterally lie about her traffic type. We then use the Nash bargaining solution (NBS) concept to suggest how an AP can pick an operating point using our incentive mechanism to ensure fairness and Pareto- optimality.},
keywords={access protocols;decision theory;DiffServ networks;game theory;Pareto optimisation;quality of service;telecommunication traffic;wireless channels;wireless LAN;game theoretic analysis;wireless MAC protocol;quality-of-service;network traffic;QoS differentiation;IEEE 802.11e;contention-free channel access;incentive mechanism;Nash equilibrium;Nash bargaining solution;Pareto-optimality;Traffic control;Delay;Telecommunication traffic;Throughput;Games;Wireless sensor networks;Wireless networks;Quality of service;Media Access Protocol;Performance analysis},
doi={10.1109/INFOCOM.2008.254},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509850,
author={L. X. Cai and L. Cai and X. Shen and J. W. Mark},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Optimizing Distributed MAC Protocol for Multi-Hop Ultra-Wideband Wireless Networks},
year={2008},
volume={},
number={},
pages={1912-1920},
abstract={By considering the characteristics of Ultra- wideband (UWB) communications networks, ie., short transmission range, accurate ranging, and low transmission/interference power, we propose a Distributed, Exclusive region (DEX) based MAC protocol for multi-hop UWB based wireless networks. DEX can effectively explore the spatial multiplexing gain of UWB networks and allow users to efficiently and fairly share network resources in a distributed manner by reserving exclusive regions (ER) around the sender and receiver for data and acknowledgment (ACK) transmissions. We further quantify the network performance bounds and derive the optimal ER size to maximize the expected network transport throughput for a dense multi-hop UWB network. Extensive simulation results demonstrate the efficiency and effectiveness of the DEX protocol.},
keywords={access protocols;ultra wideband communication;wireless LAN;distributed MAC protocol;multiple hop ultra-wideband wireless networks;short transmission range;accurate ranging;low transmission/interference power;distributed exclusive region;spatial multiplexing gain;exclusive regions;data transmissions;acknowledgment transmissions;Media Access Protocol;Wireless application protocol;Spread spectrum communication;Ultra wideband technology;Wireless networks;Erbium;Ultra wideband communication;Communication networks;Interference;Throughput},
doi={10.1109/INFOCOM.2008.255},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509851,
author={A. W. Min and K. G. Shin},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Exploiting Multi-Channel Diversity in Spectrum-Agile Networks},
year={2008},
volume={},
number={},
pages={1921-1929},
abstract={Recently, spectrum-agile (SA) networks have been recognized as a viable solution to the spectrum-scarcity problem in spectrum-scarcity problem. In SA networks, secondary (unlicensed) users are allowed to opportunistically utilize idle licensed spectrum bands, thus improving spectrum utilization efficiency and accommodating more users and applications. We take a two- step approach to the problem of maximizing the throughput of an SA network. The first step is to determine a subset of "candidate" channels that a secondary device will consider for its channel-switching. The candidate channels are selected based on their estimated utilization. We then propose channel-aware switching to determine when and where to switch to, among the candidate channels. Wireless channels are assumed to experience independent Rayleigh fading, and modeled with a finite-state Markov channel (FSMC) model. Our evaluation results show that the proposed channel-aware switching scheme significantly outperforms the traditional forced-switching scheme in terms of average throughput.},
keywords={channel estimation;diversity reception;Markov processes;Rayleigh channels;multi channel diversity;spectrum-agile network;spectrum-scarcity problem;idle licensed spectrum band utilization;channel-aware switching;candidate channel estimation;wireless channel;Rayleigh fading channel;finite-state Markov channel model;Wireless sensor networks;Throughput;Switches;Communication switching;Communications Society;Cultural differences;Computer networks;Laboratories;Fading;Shadow mapping},
doi={10.1109/INFOCOM.2008.256},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509852,
author={X. Wang and W. Yu and X. Fu and D. Xuan and W. Zhao},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={iLOC: An invisible LOCalization Attack to Internet Threat Monitoring Systems},
year={2008},
volume={},
number={},
pages={1930-1938},
abstract={In this paper, we study a new class of attacks, the <i>i</i>nvisible <i>LOC</i>alization (<i>iLOC</i>) attack, which can accurately and invisibly localize monitors of Internet threat monitoring (ITM) systems, a class of widely deployed facilities to characterize Internet threats, such as worm propagation, denial-of-service (DoS) attacks. In the <i>iLOC</i> attack, the attacker launches low-rate port-scan traffic, encoded with a selected <i>pseudo-noise</i> <i>code</i> (PN- code), to targeted networks. While the secret PN-code is invisible to others, the attacker can accurately determine the existence of monitors in the targeted networks based on whether the PN-code is embedded in the report data queried from the data center of the ITM system. We conduct extensive simulations on the <i>iLOC</i> attack using real-world traces. Our data demonstrate that the <i>iLOC</i> attack can accurately identify monitors while remaining invisible to the ITM. Finally, we present a set of guidelines to counteract the <i>iLOC</i> attack.},
keywords={Internet;invasive software;pseudonoise codes;telecommunication security;invisible localization attack;Internet threat monitoring systems;worm propagation;denial of service attacks;low rate port scan traffic;pseudonoise code;Internet;Monitoring;Telecommunication traffic;Computer crime;Computer displays;Guidelines;Computer worms;Computer science;Algorithm design and analysis;Communications Society},
doi={10.1109/INFOCOM.2008.257},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509853,
author={A. Banerjee and D. Barman and M. Faloutsos and L. N. Bhuyan},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Cyber-Fraud is One Typo Away},
year={2008},
volume={},
number={},
pages={1939-1947},
abstract={Spelling errors when typing a URL can be exploited by website-squatters: users are led to phony sites in a phenomenon we call parasitic URL naming. These phony sites imitate popular websites and try to extract personal information from unsuspecting users, or simply advertise and sell products to users. In this paper, we conduct a massive study in order to quantify the extent of this parasitic URL naming We start with a corpus of 900 popular websites, which we refer to as original URLs, and generate roughly 3 million URLs by varying the original names systematically and exhaustively. Over a period of 60 days, we analyze how many sites have URLs very similar to our original URLs. We find that parasitic URL naming is a wide-spread problem and quantify the extent of this issue. We believe that this work will provide the first step towards research and tools to combat web-fraud.},
keywords={computer crime;fraud;Web sites;cyber-fraud;URL;Website-squatters;phony sites;Uniform resource locators;Internet;Communications Society;Computer science;Computer errors;Data mining;Credit cards;Security;Portals;Costs},
doi={10.1109/INFOCOM.2008.258},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509854,
author={J. C. Bolot and M. Lelarge},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A New Perspective on Internet Security using Insurance},
year={2008},
volume={},
number={},
pages={1948-1956},
abstract={Managing security risks in the Internet has so far mostly involved methods to reduce the risks and the severity of the damages. Those methods (such as firewalls, intrusion detection and prevention, etc) reduce but do not eliminate risk, and the question remains on how to handle the residual risk. In this paper, we take a new approach to the problem of Internet security and advocate managing this residual risk by buying insurance against it. Using insurance in the Internet raises several questions because entities in the Internet face correlated risks, which means that insurance claims will likely be correlated, making those entities less attractive to insurance companies. Furthermore, risks are interdependent, meaning that the decision by an entity to invest in security and self-protect affects the risk faced by others. We analyze the impact of these externalities on the security investments of users using a simple 2-agent model. Our key results are that there are sound economic reasons for agents to not invest much in self-protection, and that insurance is a desirable incentive mechanism which pushes agents over a threshold into a desirable state where they all invest in self-protection. In other words, insurance increases the level of self-protection, and therefore the level of security, in the Internet. Therefore, we believe that insurance should become an important component of risk management in the Internet.},
keywords={insurance;risk management;security;Web services;Internet security;residual risk;insurance companies;security investments;self-protection level;risk management;Security;Insurance;Protection;Investments;Web and internet services;Costs;Risk management;Communications Society;USA Councils;Intrusion detection},
doi={10.1109/INFOCOM.2008.259},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509855,
author={M. S. Ahmed and E. Al-Shaer and L. Khan},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Novel Quantitative Approach For Measuring Network Security},
year={2008},
volume={},
number={},
pages={1957-1965},
abstract={Evaluation of network security is an essential step in securing any network. This evaluation can help security professionals in making optimal decisions about how to design security countermeasures, to choose between alternative security architectures, and to systematically modify security configurations in order to improve security. However, the security of a network depends on a number of dynamically changing factors such as emergence of new vulnerabilities and threats, policy structure and network traffic. Identifying, quantifying and validating these factors using security metrics is a major challenge in this area. In this paper, we propose a novel security metric framework that identifies and quantifies objectively the most significant security risk factors, which include existing vulnerabilities, historical trend of vulnerability of the remotely accessible services, prediction of potential vulnerabilities for any general network service and their estimated severity and finally policy resistance to attack propagation within the network. We then describe our rigorous validation experiments using real- life vulnerability data of the past 6 years from National Vulnerability Database (NVD) [10] to show the high accuracy and confidence of the proposed metrics. Some previous works have considered vulnerabilities using code analysis. However, as far as we know, this is the first work to study and analyze these metrics for network security evaluation using publicly available vulnerability information and security policy configuration.},
keywords={risk analysis;security;network security;policy structure;network traffic;security metric framework;remotely accessible services;real life vulnerability data;national vulnerability database;Data security;Information security;National security;Communication system security;Telecommunication traffic;Databases;Information analysis;Computer science;Communications Society;Risk analysis},
doi={10.1109/INFOCOM.2008.260},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509856,
author={X. Liu and W. Wei and C. Qiao and T. Wang and W. Hu and W. Guo and M. -. Wu},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Task Scheduling and Lightpath Establishment in Optical Grids},
year={2008},
volume={},
number={},
pages={1966-1974},
abstract={Data-intensive Grid applications require huge data transferring between multiple geographically separated computing nodes where computing tasks are executed. For a future WDM network to efficiently support this type of emerging applications, traditional approaches to establishing lightpaths between given source destination pairs are not sufficient because a computing task may be executed on any one of several computing nodes having the necessary resources. Therefore, lightpath establishment has to be considered jointly with task scheduling to achieve best performance. In this paper, we study the optimization problems of jointly scheduling both computing resources and network resources. We first present the formulation of two optimization problems with the objectives being the minimization of the completion time of a job and minimization of the resource usage/cost to satisfy a job with a deadline respectively. When the objective is to minimize the completion time, we devise an optimal algorithm for a special type of applications. Furthermore, we propose efficient heuristics to deal with general applications with either optimization objective and demonstrate their good performances via simulation.},
keywords={grid computing;optical neural nets;scheduling;wavelength division multiplexing;task scheduling;lightpath establishment;optical grids;data intensive grid applications;WDM network;computing resources;network resources;Computer networks;Processor scheduling;WDM networks;Grid computing;Distributed computing;Optical fiber networks;USA Councils;Application software;Optical computing;Peer to peer computing},
doi={10.1109/INFOCOM.2008.261},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509857,
author={G. Li and A. Chiu and R. Doverspike and M. Birk and D. Husa and N. Zanki},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={On Transient-Constrained Wavelength Assignment},
year={2008},
volume={},
number={},
pages={1975-1983},
abstract={In a network with Reconfigurable Optical Add/drop Multiplexers (ROADMs), transient effects from network failures trigger sudden power changes to channels, even on spans (ROADM-to-ROADM links) upstream/downstream from the failure. Some ROADMs rely on restrictive wavelength assignment rules, called <i>field-of-use</i> (FOU), to insure surviving channels meet performance objectives. <i>Drop</i> <i>depth</i> (DD) (ratio of surviving channels) and <i>span</i> <i>overlap</i> <i>count</i> (SOC) (length of spans of overlapping lightpaths) are two parameters defined for FOU guidelines. For a given DD, FOU guidelines specify a corresponding maximum SOC for each wavelength. Generally, a wavelength can support a longer span overlap count under a smaller drop depth restriction. However, the measured drop-depth changes as wavelengths are assigned. We propose methods to optimize transient-constrained wavelength assignment in commercial ROADM networks. Our solutions have been shown to provide 40% more channels than with typical FOU guidelines.},
keywords={multiplexing equipment;optical fibre networks;wavelength assignment;wavelength division multiplexing;transient-constrained wavelength assignment rule;reconfigurable optical add/drop multiplexer network link;drop depth;span overlap count;WDM;Wavelength assignment;Optical fiber networks;Guidelines;USA Councils;Testing;Optical fiber devices;Optical fiber amplifiers;Power amplifiers;Stimulated emission;Optical feedback},
doi={10.1109/INFOCOM.2008.262},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509858,
author={Q. Zhu and L. Pavel},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Theory of Linear Games with Constraints and Its Application to Power Control of Optical Networks},
year={2008},
volume={},
number={},
pages={1984-1992},
abstract={In this paper, we introduce a class of linear non- cooperative games with linearly coupled constraints. It bears striking connections with classical linear systems theory and finds itself pervasively used in network engineering applications. In the second part of the paper, we will illustrate this type of games by an application from OSNR-based power control in optical networks, where we can view the slack variables as fictitious players. This powerful interpretation allows us to bridge over the theory and the issue of implementation in engineering.},
keywords={game theory;optical fibre networks;power control;telecommunication control;optical network;power control;linear noncooperative game theory;linearly coupled constraint;Game theory;Constraint theory;Power control;Optical fiber networks;Signal to noise ratio;Optical noise;Application software;Power engineering and energy;Iterative algorithms;Communications Society},
doi={10.1109/INFOCOM.2008.263},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509859,
author={X. Wang and Y. Cai and S. Xiao and W. Gong},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Three-Stage Load-Balancing Switch},
year={2008},
volume={},
number={},
pages={1993-2001},
abstract={Recently there has been a great deal of interest in load-balancing switches due to their simple architecture and high bandwidth. In this paper we propose a three-stage load- balancing switch along with output load-balancing to address the mis-sequencing problem. We show that our proposed scheme provides a delay guarantee bounded by the delay of an OQ switch with the same input traffic plus a constant while achieving 100% throughput for admissible traffic with (sigma, rho) -upper constraint.},
keywords={delays;telecommunication switching;telecommunication traffic;three-stage load-balancing switch;missequencing problem;delay guarantee;admissible traffic;Fabrics;Optical switches;Packet switching;Delay;Bandwidth;Throughput;Traffic control;Field-flow fractionation;Scheduling algorithm;Communications Society},
doi={10.1109/INFOCOM.2008.264},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509860,
author={T. T. Lee},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={The Kraft's Inequality of Scheduling for Packet-Switched Clos Networks},
year={2008},
volume={},
number={},
pages={2002-2010},
abstract={The traffic matrix decomposition provides an effective scheduling approach to guarantee capacity of services supported by input queued packet switches. The Birkhoff-von Neumann (BvN) decomposition is widely used to express the doubly stochastic rate matrix as a weighted sum of permutation matrices. The switch then schedules these permutation matrices, using weights corresponding to the coefficients in the decomposition. In a Clos network, each permutation matrix corresponds to a connection pattern in the middle stage. If we regard this set of predetermined connection patterns as a code book, scheduling incoming packets in the input buffer according to predetermined connection patterns is a process similar to the encoding of source signals. In light of the concerns on delay jitter, it is expected that the scheduling should be as smooth as possible. A measurement of smoothness is defined in terms of interstate time of the scheduled sequence. In this paper, we show that the smoothness of scheduling is bounded by the entropy of BvN decomposition, and satisfies the Kraft's inequality. The optimal scheduling can be achieved if and only if the Kraft's equality holds.},
keywords={delays;jitter;matrix algebra;packet switching;scheduling;telecommunication traffic;scheduling;packet-switched Clos networks;traffic matrix decomposition;Birkhoff-von Neumann decomposition;permutation matrices;delay jitter;Optimal scheduling;Matrix decomposition;Switches;Linear matrix inequalities;Telecommunication traffic;Traffic control;Packet switching;Stochastic processes;Books;Encoding},
doi={10.1109/INFOCOM.2008.265},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509861,
author={G. Paillard and V. Ravelomanana},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Limit Theorems for Degree of Coverage and Lifetime in Large Sensor Networks},
year={2008},
volume={},
number={},
pages={2011-2019},
abstract={In this paper, we investigate the fundamental limits of sensor network lifetime that any algorithm can achieve. In our settings, n nodes are deployed as a Poisson point process with density lambda in a region of size S and each sensor node can cover a unit-area disk. For any k and lambda, let V(k, lambda) be the random variable (r.v.) of the size of the region that is covered by at most k - 1 nodes. Under these assumptions, we first show that for any function omega satisfying 1 Gtomega (k) Lt k<sup>1/2</sup> the r.v. V(k, k- omega(k)k<sup>1/2</sup>) converges almost surely to S. In contrast, if the intensity is set to lambda = k + omega(k)k<sup>1/2</sup> we obtain that V(k, k +omega(k)k <sup>1/2</sup>) converges almost surely to 0. These limit theorems extend the results of Zhang and Hou in [21], [22] where the authors worked with fixed degree of coverage (k = O(1)) and lambda = log S+O(k) log log S. Assume that each sensor has the same lifetime T. As consequences of our analytical results, we derive randomized algorithms (working with high probability) that can maintain constantly high degrees of coverage while prolonging the lifetime of the network.},
keywords={computational complexity;probability;random processes;randomised algorithms;stochastic processes;telecommunication network reliability;wireless sensor networks;limit theorems;large sensor network lifetime;Poisson point process;random variable;randomized algorithms;probability;Monitoring;Peer to peer computing;Batteries;Sensor phenomena and characterization;Protocols;Temperature sensors;Communications Society;Computer science;Random variables;Algorithm design and analysis},
doi={10.1109/INFOCOM.2008.266},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509862,
author={G. Yang and V. Shukla and D. Qiao},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Novel On-Demand Framework for Collaborative Object Detection in Sensor Networks},
year={2008},
volume={},
number={},
pages={2020-2028},
abstract={Quality of object detection and network lifetime hold critical importance to many sensor network applications such as military surveillance. Unfortunately, improving one of these aspects comes at the expense of the other. In this paper, based on the probabilistic sensing model, we propose a novel framework for object detection in sensor networks, called DeCODe (on-demand framework for collaborative object detection), which provides a desired object detection performance (characterized in terms of detection probability and false detection probability), while attempting to prolong the network lifetime. The design of DeCODe is motivated by a counterintuitive observation that simple collaboration among active sensors indeed degrades the object detection performance. By contrast, each active sensor in DeCODe can trigger its neighboring inactive sensors to participate in the detection process in an on-demand fashion, so as to achieve the same low false detection probability while increasing the probability of detection. The effectiveness of the proposed DeCODe framework is supported by theoretical analysis and simulation-based validation.},
keywords={object detection;wireless sensor networks;on-demand framework for collaborative object detection;sensor networks;probabilistic sensing model;low false detection probability;Collaboration;Object detection;Sensor phenomena and characterization;Decoding;Wireless sensor networks;Degradation;Analytical models;Peer to peer computing;Collaborative work;Surveillance},
doi={10.1109/INFOCOM.2008.267},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509863,
author={H. Lee and A. Keshavarzian},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Towards Energy-Optimal and Reliable Data Collection via Collision-Free Scheduling in Wireless Sensor Networks},
year={2008},
volume={},
number={},
pages={2029-2037},
abstract={We consider the problem of supervision or periodic data collection for stationary wireless sensor networks and present a practical, energy-efficient, and reliable solution. Energy-efficiency is achieved by combining three design methods: (a) adopting network flow optimization techniques, the optimal scheme for balancing the communication load among all the nodes in the network is calculated. This gives the lower bound for the energy required for data collection process, (b) instead of using a fixed network topology (communication tree), a set of optimized trees is constructed and the communication tree varies over different data collection cycles. We show that this method achieves an average energy consumption rate very close to the optimal value, (c) the packet exchange procedure is designed based on collision-free schedules, to minimize the number of packets and the transmission and reception times for each node. Reliability of the process is guaranteed by including many retransmission opportunities in the schedules. The performance is evaluated through simulations.},
keywords={optimisation;resource allocation;scheduling;telecommunication network reliability;telecommunication network topology;trees (mathematics);wireless sensor networks;collision-free scheduling;wireless sensor networks;energy efficiency;network flow optimization;communication load balancing;reliable data collection;network topology;packet exchange procedure;communication tree;Wireless sensor networks;Energy efficiency;Telecommunication network reliability;Peer to peer computing;Energy consumption;Base stations;Data security;Routing;Communications Society;Design methodology},
doi={10.1109/INFOCOM.2008.268},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509864,
author={T. Melodia and I. F. Akyildiz},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Cross-Layer Quality of Service Support for UWB Wireless Multimedia Sensor Networks},
year={2008},
volume={},
number={},
pages={2038-2046},
abstract={Wireless multimedia sensor networks (WMSNs) are networks of wirelessly interconnected devices that allow retrieving video and audio streams, still images, and scalar sensor data. WMSN require the sensor network paradigm to be rethought in view of the need for mechanisms to deliver multimedia content with a pre-defined level of quality of service (QoS). In this paper, a new cross-layer communication architecture based on the time-hopping impulse radio ultra wide band technology is described, designed to reliably and flexibly deliver QoS to heterogeneous applications in WMSNs, by leveraging and controlling interactions among different layers of the protocol stack according to applications requirements. Simulations show that the proposed system achieves the performance objectives of WMSNs without sacrificing on design modularity.},
keywords={multimedia communication;protocols;quality of service;ultra wideband communication;wireless sensor networks;quality of service;UWB wireless multimedia sensor network;cross-layer communication architecture;time-hopping impulse radio;protocol stack;Wireless sensor networks;Quality of service;Streaming media;Image sensors;Information retrieval;Image retrieval;Ultra wideband communication;Telecommunication network reliability;Radio control;Communication system control},
doi={10.1109/INFOCOM.2008.269},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509865,
author={M. Demirbas and O. Soysal and M. Hussain},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Singlehop Collaborative Feedback Primitive for Wireless Sensor Networks},
year={2008},
volume={},
number={},
pages={2047-2055},
abstract={To achieve scalability, energy-efficiency, and timeliness, wireless sensor network deployments increasingly employ in-network processing. In this paper, we identify singlehop feedback collection as a key building block for in-network processing applications, and introduce a basic singlehop primitive, pollcast. The key idea behind this primitive is to exploit the receiver-side collision detection information at the MAC-layer to speed-up collaborative feedback collection. Using pollcast, a node can get an affirmation about the existence of a node-level predicate P in its neighborhood in constant time by asking all nodes where P hold to reply simultaneously. We have implemented pollcast on Tmotes using Chipcon 2420 radio. Our results show that this primitive is indeed lightweight, resilient, and effective. Our paper is also the first time receiver-side collision detection is achieved in a practical manner for Chipcon 2420 radio.},
keywords={access protocols;feedback;wireless sensor networks;singlehop collaborative feedback primitive;wireless sensor network;receiver-side collision detection information;medium access control layer;pollcast operation;Collaboration;Feedback;Wireless sensor networks;Broadcasting;Peer to peer computing;Delay;Protocols;Voting;Communications Society;Computer science},
doi={10.1109/INFOCOM.2008.270},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509866,
author={H. Al-Mefleh and J. M. Chang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A New ACK Policy To Mitigate the Effects of Coexisting IEEE 802.11/802.11e Devices},
year={2008},
volume={},
number={},
pages={2056-2064},
abstract={In IEEE 802.11 wireless networks, EDCA users' performance may be degraded because of the existence of legacy users and therefore would get a lower priority service. Such effects are mainly due to the fact that EDCA users are controlled by different contention parameters that are distributed in the beacon frames, but there is no control over legacy users as their contention parameters are PHY dependent, i.e. they have constant values. In this paper, we discuss different aspects of the legacy DCF and EDCA users coexistence. Also, we propose a simple distributed management scheme (called NZ-ACK) to mitigate the influence of legacy DCF on EDCA performance in networks consisting of both types of users without any modifications to legacy users. Finally, we use Opnet simulation to evaluate the performance of NZ-ACK. Results show that NZ-ACK outperforms 802.11 in terms of maintaining the priority of service and delay bounds of EDCA users while providing acceptable throughput for legacy users.},
keywords={wireless LAN;IEEE 802.11e wireless network device;ACK policy;enhanced distributed access control;EDCA user coexistence;NZ-ACK distributed management scheme;distributed coordination function;legacy DCF;Physical layer;Quality of service;Wireless networks;Degradation;Communications Society;Computer networks;USA Councils;Delay;Throughput;Broadcasting},
doi={10.1109/INFOCOM.2008.271},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509867,
author={C. Phillips and S. Singh},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={An Empirical Activity Model for WLAN Users},
year={2008},
volume={},
number={},
pages={2065-2073},
abstract={Understanding user behavior in wireless environments is useful for a variety of reasons ranging from the design of better sleep algorithms for components of mobile devices to appropriately provisioning the wireless network itself to better serve the user. Our work goes in a different direction from prior work on WLAN modeling and attempts to undersand the protocol independent behavior of users by developing packet-level models for user activity using diverse training data. Additionally we validate the derived model using a stochastic similarity metric adapted from human control strategy modeling and present a novel way to compare traces using this metric.},
keywords={human factors;stochastic processes;wireless LAN;WLAN;user behavior;mobile device;user activity model;stochastic similarity metric;human control strategy modeling;protocol;Wireless LAN;Wireless networks;Algorithm design and analysis;Telecommunication traffic;Traffic control;Communications Society;Protocols;Training data;Stochastic processes;Humans},
doi={10.1109/INFOCOM.2008.272},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509868,
author={S. -. Hahm and J. -. Lee and C. -. Kim},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Opportunistic Waiver of Data Reception for Exploiting Multiuser Diversity in the Uplink of IEEE 802.11 WLAN},
year={2008},
volume={},
number={},
pages={2074-2082},
abstract={In this paper, we consider how to exploit multiuser diversity in the uplink of IEEE 802.11 WLAN when its uplink and downlink are asymmetric. In the uplink, there is no central node that arranges transmission schedules of all stations. To overcome this limitation, we devise a novel MAC protocol, called WLAN Opportunistic Waiver (WOW), where an access point (AP) indirectly controls transmission instants of stations by not sending a CTS frame if the received signal strength (RSS) of an RTS frame is below a certain threshold that is station- dependent. We develop an analytic model for WOW with a three- dimensional Markov chain. We can find the optimal threshold to maximize the system throughput. To avoid optimization burdens, we propose a simple algorithm which can determine a near- optimal threshold. Both analysis and ns-2 simulation results show that the throughput of WOW increases with the number of stations and the improvement compared with Receiver Based Auto Rate (RBAR) is up to 43 % while maintaining access fairness as achieved by IEEE 802.11.},
keywords={access protocols;diversity reception;Markov processes;optimisation;radio links;scheduling;wireless LAN;WLAN Opportunistic Waiver;WOW MAC protocol;data reception;multiuser diversity;IEEE 802.11 WLAN uplink;transmission schedules;access point;three-dimensional Markov chain;optimization tool;Wireless LAN;Throughput;Media Access Protocol;Downlink;Access protocols;Diversity reception;Computer science;Analytical models;Fading;Communications Society},
doi={10.1109/INFOCOM.2008.273},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509869,
author={J. Zhang and K. Tan and J. Zhao and H. Wu and Y. Zhang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Practical SNR-Guided Rate Adaptation},
year={2008},
volume={},
number={},
pages={2083-2091},
abstract={Rate adaptation is critical to the system performance of wireless networks. Typically, rate adaptation is considered as a MAC layer mechanism in IEEE 802.11. Most previous work relies only on frame losses to infer channel quality, but performs poorly if frame losses are mainly caused by interference. Recently SNR- based rate adaptation schemes have been proposed, but most of them have not been studied in a real environment. In this paper, we first conduct a systematic measurement-based study to confirm that in general SNR is a good prediction tool for channel quality, and identify two key challenges for this to be used in practice: (1) The SNR measures in hardware are often uncalibrated, and thus the SNR thresholds are hardware dependent. (2) The direct prediction from SNR to frame delivery ratio (FDR) is often over optimistic under interference conditions. Based on these observations, we present a novel practical SNR- Guided Rate Adaptation (SGRA) scheme. We implement and evaluate SGRA in a real test-bed and compare it with other three algorithms: ARF, RRAA and HRC. Our results show that SGRA outperforms the other three algorithms in all cases we have tested.},
keywords={wireless channels;wireless LAN;SNR-guided rate adaptation;IEEE 802.11 wireless network;MAC layer mechanism;channel quality;frame delivery ratio;Interference;Hardware;Testing;Physical layer;Robustness;Propagation losses;Guidelines;Calibration;Distortion measurement;Communications Society},
doi={10.1109/INFOCOM.2008.274},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509870,
author={Y. Bejerano and D. Lee and P. Sinha and L. Zhang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Approximation Algorithms for Scheduling Real-Time Multicast Flows in Wireless LANs},
year={2008},
volume={},
number={},
pages={2092-2100},
abstract={In recent years, numerous large-scale Wireless LANs (WLAN) have been deployed all over the world. However, the shortage of non-interfering channels makes it a challenge for WLANs to efficiently support real-time multicast services. In this paper, we study the problem of efficient scheduling of <i>real-time</i> <i>multicast</i> <i>flows</i>. For mitigating interferences, we allow access-points (APs) to transmit simultaneously only if they are mutually non-interfering and our objective is minimizing the fraction of time used by the APs for servicing the multicast flows. We introduce two multicast strategies, the <i>association</i> <i>strategy</i> for which each user is restricted to receive flows only from its associated AP and the <i>non-association</i> strategy for which a user may also decode transmissions from other APs in its vicinity. Under both strategies, the scheduling problem of minimizing the multicast service time is NP-hard and we propose simple approximation algorithms with provable performance bounds. Our simulations clearly demonstrate that the proposed algorithms yield efficient multicast scheduling.},
keywords={approximation theory;multicast communication;scheduling;wireless LAN;approximation algorithms;scheduling;real-time multicast flows;wireless LAN;wireless local area networks;real-time multicast services;Approximation algorithms;Multicast algorithms;Scheduling algorithm;Wireless LAN;Broadcasting;Unicast;Interference elimination;Frequency conversion;Large-scale systems;Multimedia communication},
doi={10.1109/INFOCOM.2008.275},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509871,
author={X. Yu and S. Chandra},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Delay Tolerant Collaborations among Campus-Wide Wireless Users},
year={2008},
volume={},
number={},
pages={2101-2109},
abstract={The ubiquitous deployment of wireless LAN networks are allowing students to embrace laptops as their preferred computing platform. We investigated the viability of building collaborative applications to share contents amongst student groups. In our application scenario, the university will provide wireless infrastructure throughout the campus but not the storage infrastructure required to store the shared contents. Laptops will likely exhibit weak availability. Hence, these collaborative applications need to tolerate long delays in propagating updates amongst the participants. In this paper, we presented a preliminary analysis of message forwarding behavior under realistically resource constrained node scenarios. Our experiments were based on the observed wireless user behavior at the University of Notre Dame. Our experiments showed the inherent limits of epidemic propagation in real campus wireless network scenarios.},
keywords={delays;groupware;laptop computers;message passing;ubiquitous computing;wireless LAN;delay tolerant collaboration;campus-wide wireless user;ubiquitous wireless LAN network deployment;laptop;message forwarding behavior;resource constrained node;Collaboration;Portable computers;Propagation delay;Peer to peer computing;Availability;Wireless LAN;Disruption tolerant networking;Aggregates;Communications Society;Computer science},
doi={10.1109/INFOCOM.2008.276},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509872,
author={Q. Yuan and J. Wu},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={DRIP: A Dynamic VoRonoi RegIons-Based Publish/Subscribe Protocol in Mobile Networks},
year={2008},
volume={},
number={},
pages={2110-2118},
abstract={The publish/subscribe (pub/sub for short) paradigm is used to deliver events from a source to interested clients in an asynchronous way. Recently, extending a pub/sub system in wireless networks has become a promising topic. However, most existing works focus on pub/sub systems in infrastructured wireless networks. To adapt pub/sub systems to mobile ad hoc networks, we propose DRIP, a dynamic Voronoi region-based pub/sub protocol. In our design, the network is dynamically divided into several Voronoi regions after choosing proper nodes as broker nodes. Each broker node is used to collect subscriptions and detected events, as well as efficiently notify subscribers with matched events in its Voronoi region. Other nodes join their nearest broker nodes to submit subscriptions, publish events, and wait for notifications of their requested events. Broker nodes cooperate with each other for sharing subscriptions and useful events. Our proposal includes two major components: a Voronoi regions construction protocol, and a delivery mechanism that implements the pub/sub paradigm. The effectiveness of DRIP is demonstrated through comprehensive simulation studies.},
keywords={ad hoc networks;computational geometry;message passing;middleware;mobile computing;mobile radio;protocols;dynamic Voronoi regions-based publish/subscribe protocol;DRIP;mobile ad hoc network;wireless network;Peer to peer computing;Subscriptions;Event detection;Wireless networks;Spine;Mobile ad hoc networks;Multicast protocols;Mobile radio mobility management;Ad hoc networks;Network servers},
doi={10.1109/INFOCOM.2008.277},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509873,
author={S. Sarkar and E. Altman and R. El-Azouzi and Y. Hayel},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Information Concealing Games},
year={2008},
volume={},
number={},
pages={2119-2127},
abstract={A decision maker (Actor) has to decide which of several available resources to use in the presence of an adversary (Controller) that can prevent the Actor of receiving information on the state of some of the resources. The Controller has a limitation on the amount of information it can conceal. We formulate this problem as a game and compute the most harmful behavior of the Controller and the best choice of a resource for the Actor. We identify cases in which the exact solution is computationally intractable, and provide approximate solutions with polynomial complexity.},
keywords={computational complexity;decision making;game theory;information concealing games;decision making;polynomial complexity;leader-follower game;Jamming;Control systems;Transmitters;Probes;Probability distribution;Communications Society;USA Councils;Polynomials;Signal generators;Authentication},
doi={10.1109/INFOCOM.2008.278},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509874,
author={H. Song and S. Zhu and G. Cao},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={SVATS: A Sensor-Network-Based Vehicle Anti-Theft System},
year={2008},
volume={},
number={},
pages={2128-2136},
abstract={Today vehicle theft rate is very high, thus tracking/alarming systems are being deployed with an increasingly popularity. These systems however bear some limitations such as high cost, high false-alarm rate, and easy to be disabled. This paper describes the design, implementation and evaluation of a Sensor-network-based Vehicle Anti-Theft System (SVATS) to address these limitations. In this system, the sensors in the vehicles that are parked within the same parking area first form a sensor network, then monitor and identify possible vehicle thefts by detecting unauthorized vehicle movement. When an unauthorized movement is detected, an alert will be reported to a base station in the parking area, which sends warning messages to the security office. This paper focuses on the technical issues specific to the system such as topology management, theft detection, and intra-vehicle networking.},
keywords={alarm systems;road vehicles;traffic engineering computing;wireless sensor networks;sensor-network-based vehicle anti-theft system;vehicle theft rate;alarming system;tracking system;false-alarm rate;unauthorized vehicle movement detection;base station;topology management;intra-vehicle networking;Vehicle detection;Global Positioning System;Costs;Base stations;Computer science;Sensor systems;Monitoring;Alarm systems;Broadcasting;Communications Society},
doi={10.1109/INFOCOM.2008.279},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509875,
author={L. Sang and A. Arora},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Spatial Signatures for Lightweight Security in Wireless Sensor Networks},
year={2008},
volume={},
number={},
pages={2137-2145},
abstract={This paper experimentally investigates the feasibility of crypto-free communications in resource-constrained wireless sensor networks. We exploit the spatial signature induced by the radio communications of a node on its neighboring nodes. We design a primitive that robustly and efficiently realizes this concept, even at the level of individual packets and when the network is relatively sparse. Using this primitive, we design a protocol that robustly and efficiently validates the authenticity of the source of messages: authentic messages incur no communication overhead whereas masqueraded communications are detected cooperatively by the neighboring nodes. The protocol enables lightweight collusion-resistant methods for broadcast authentication, unicast authentication, non-repudiation and integrity of communication. We have implemented our primitive and protocol, and quantified the high-level of accuracy of the protocol via testbed experiments with <i>CC1000</i> radio-enabled motes.},
keywords={digital signatures;protocols;telecommunication security;wireless sensor networks;spatial signature;resource-constrained wireless sensor network;radio communication;crypto-free communication;protocol design;message authentication;lightweight collusion-resistant security method;broadcast authentication;unicast authentication;communication nonrepudiation;communication integrity;Wireless sensor networks;Peer to peer computing;Authentication;Cryptography;Robustness;Base stations;Broadcasting;Cryptographic protocols;Communications Society;Computer security},
doi={10.1109/INFOCOM.2008.280},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509876,
author={K. Jacobsson and L. L. H. Andrew and A. Tang and K. H. Johansson and H. Hjalmarsson and S. H. Low},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={ACK-Clocking Dynamics: Modelling the Interaction between Windows and the Network},
year={2008},
volume={},
number={},
pages={2146-2152},
abstract={A novel continuous time fluid flow model of the dynamics of the interaction between ACK-clocking and the link buffer is presented. A fundamental integral equation relating the instantaneous flow rate and the window dynamics is derived. Properties of the model, such as well-posedness and stability, are investigated. Packet level experiments verify that this new model is more accurate than existing models, correctly predicting qualitatively different behaviors, for example when round trip delays are heterogeneous.},
keywords={delays;integral equations;telecommunication congestion control;telecommunication traffic;transport protocols;ACK-clocking dynamics;continuous time fluid flow model;link buffer;integral equation;round trip delays;transmission control protocol;Delay estimation;Traffic control;Fluid flow;Stability;Telecommunication traffic;Jacobian matrices;USA Councils;Fluid dynamics;Predictive models;Transport protocols},
doi={10.1109/INFOCOM.2008.281},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509877,
author={K. Misra and S. Karande and H. Radha},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Maximal Recovery Network Coding under Topology Constraint},
year={2008},
volume={},
number={},
pages={2153-2161},
abstract={Recent advances have shown that channel codes can be mapped onto networks to realize efficient Network Coding (NC); this has led to the emergence of Code-on-Network-Graphs (CNG). Traditional CNG approaches (e.g Decentralized Erasure Codes) focus on a generating a sequence of encoded symbols from a given input source (of size K), such that the original symbols can be recovered from any subset of the encoded symbols of size equal to or slightly larger than K. However in all cases the number of source symbols recovered falls rapidly if the number of encoded symbols received falls below K. In this paper we determine the CNG code-ensembles (under statistical toplogy constraint) which result in maximal recovery of WSN source data (for different erasure-rates), thereby minimizing the deterioration in data recovery. We also perform fixed point stability analysis on the underlying LDPC code ensemble. We then propose a distributed algorithm for generating a sequence of encoded symbols adhering to the designed code ensemble. Optimal solutions for a sensor network with 1000 nodes is determined using the Differential Evolution algorithm, and the solution sensitivity to variance in number of sensor nodes and node-interconnectivity is evaluated.},
keywords={channel coding;distributed algorithms;graph theory;network theory (graphs);parity check codes;source coding;wireless channels;wireless sensor networks;code-on-network-graph approach;source encoding;topology constraint;wireless sensor network coding;fixed point stability analysis;LDPC code;distributed algorithm;differential evolution algorithm;channel codes;Network coding;Network topology;Wireless sensor networks;Parity check codes;Decoding;Algorithm design and analysis;Stability analysis;Communications Society;USA Councils;Belief propagation},
doi={10.1109/INFOCOM.2008.282},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509878,
author={J. Jin and B. Li and T. Kong},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Is Random Network Coding Helpful in WiMAX?},
year={2008},
volume={},
number={},
pages={2162-2170},
abstract={The IEEE 802.16 standard, or WiMAX, has emerged to facilitate high-bandwidth wireless access in real- world metropolitan areas, commonly referred to as 4G. In WiMAX, hybrid automatic repeat request (HARQ) is adopted to transmit data packets reliably. However, it sacrifices resilience in time varying channels, and it may under-utilize the wireless medium in the cases of multi-path and multi-hop transmissions. On the other hand, random network coding has been shown to be effective towards improving throughput in multi-hop wireless networks, when deployed above the physical and MAC layers. It would be encouraging to observe that network coding is also helpful at the MAC layer in practice, especially within the emerging WiMAX standard. Is random network coding beneficial in WiMAX at the MAC layer? In this paper, we seek to answer this question by evaluating network coding in three cases: single-hop transmissions, handovers, and multi-hop transmissions. We show that random network coding has indeed offered important advantages as compared to traditional HARQ. Our observations may lead to the use of random network coding at the MAC layer in practical WiMAX systems.},
keywords={4G mobile communication;automatic repeat request;channel coding;multipath channels;random codes;time-varying channels;WiMax;random network coding;WiMAX;IEEE 802.16 standard;high-bandwidth wireless access;real-world metropolitan area;4G network;hybrid automatic repeat request;time varying channel;multi path transmission;multi hop wireless network;MAC layer;Network coding;WiMAX;Spread spectrum communication;Wireless networks;Automatic repeat request;Throughput;Communication standards;Physical layer;Resilience;Protocols},
doi={10.1109/INFOCOM.2008.283},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509879,
author={M. Ghaderi and D. Towsley and J. Kurose},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Reliability Gain of Network Coding in Lossy Wireless Networks},
year={2008},
volume={},
number={},
pages={2171-2179},
abstract={The capacity gain of network coding has been extensively studied in wired and wireless networks. Recently, it has been shown that network coding improves network reliability by reducing the number of packet retransmissions in lossy networks. However, the extent of the reliability benefit of network coding is not known. This paper quantifies the reliability gain of network coding for reliable multicasting in wireless networks, where network coding is most promising. We define the expected number of transmissions per packet as the performance metric for reliability and derive analytical expressions characterizing the performance of network coding. We also analyze the performance of reliability mechanisms based on rateless codes and automatic repeat request (ARQ), and compare them with network coding. We first study network coding performance in an access point model, where an access point broadcasts packets to a group of K receivers over lossy wireless channels. We show that the expected number of transmissions using ARQ, compared to network coding, scales as ominus (log K) as the number of receivers becomes large. We then use the access point model as a building block to study reliable multicast in a tree topology. In addition to scaling results, we derive expressions for the expected number of transmissions for finite multicast groups as well. Our results show that network coding significantly reduces the number of retransmissions in lossy networks compared to an ARQ scheme. However, rateless coding achieves asymptotic performance results similar to that of network coding.},
keywords={channel coding;multicast communication;radio networks;telecommunication network reliability;telecommunication network topology;trees (mathematics);network coding reliability gain;lossy wireless networks;reliable multicasting;packet retransmissions;tree topology;access point model;Network coding;Wireless networks;Automatic repeat request;Computer network reliability;Performance analysis;Network topology;Telecommunication network reliability;Computer science;Broadcasting;Error correction},
doi={10.1109/INFOCOM.2008.284},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509880,
author={C. -. Tai and J. Zhu and N. Dukkipati},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Making Large Scale Deployment of RCP Practical for Real Networks},
year={2008},
volume={},
number={},
pages={2180-2188},
abstract={We recently proposed the rate control protocol (RCP) as a way to minimize download times (or flow-completion times). Simulations suggest that if RCP were widely deployed, downloads would frequently finish an order of magnitude faster than with TCP. This is because RCP involves explicit feedback from the routers along the path, allowing a sender to pick a fast starting rate, and adapt quickly to network conditions. RCP is particularly appealing because it can be shown to be stable under broad operating conditions, and its performance is independent of the flow-size distribution and the RTT. Although it requires changes to the routers, the changes are small: The routers keep no per-flow state or per-flow queues, and the per-packet processing is minimal. However, the bar is high for a new congestion control mechanism - introducing a new scheme requires enormous change, and the argument needs to be compelling. And so, to enable incremental deployment of RCP, we have built and tested an open and public implementation of RCP, and proposed solutions for deployments that require no fork-lift network upgrades. In this paper we describe our end-host and router implementation of RCP in Linux, and solutions to how RCP can coexist in a network carrying predominantly non-RCP traffic, and coordinate with routers that don't implement RCP. We hope that these solutions will take us closer to having an impact in real networks, not just for RCP but also for many other explicit congestion control protocols proposed in literature.},
keywords={computer networks;Linux;telecommunication congestion control;telecommunication network routing;telecommunication traffic;transport protocols;RCP;rate control protocol;network routing;real network;telecommunication congestion control;Linux;telecommunication traffic;Large-scale systems;Protocols;Traffic control;Stability;Size measurement;Communications Society;Computer networks;Laboratories;Communication system control;Control systems},
doi={10.1109/INFOCOM.2008.285},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509881,
author={I. C. Paschalidis and W. Lai and X. Song},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Decomposition Method for Transmission Scheduling in Multi-Channel Wireless Sensor Networks},
year={2008},
volume={},
number={},
pages={2189-2197},
abstract={We consider wireless sensor networks with multiple frequency channels, multiple gateways and multiple classes of traffic carrying data generated by different sensory inputs. The objective is to devise joint routing and transmission scheduling policies in order to gather data in the most efficient manner while respecting the needs of different sensing tasks (fairness). We formulate the problem as maximizing the utility of transmissions subject to explicit fairness constraints and propose a decomposition algorithm drawing upon large-scale decomposition ideas in mathematical programming. We show that our algorithm terminates in a finite number of iterations. Every iteration requires the solution of a subproblem which is NP-hard. To solve the subproblem we (i) devise a particular relaxation that is solvable in polynomial time and (ii) leverage polynomial time approximation schemes. A combination of both approaches enables an improved decomposition algorithm which is much more efficient for solving large problem instances.},
keywords={approximation theory;computational complexity;internetworking;mathematical programming;scheduling;wireless channels;wireless sensor networks;decomposition method;transmission scheduling;multi-channel wireless sensor networks;multiple frequency channels;multiple gateways;joint routing;mathematical programming;polynomial time approximation;Wireless sensor networks;Frequency;Polynomials;Scheduling;Routing;Telecommunication traffic;Receiving antennas;Transmitting antennas;Physical layer;Communications Society},
doi={10.1109/INFOCOM.2008.286},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509882,
author={Z. Zhou and J. -. Cui and A. Bagtzoglou},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Scalable Localization with Mobility Prediction for Underwater Sensor Networks},
year={2008},
volume={},
number={},
pages={2198-2206},
abstract={Due to adverse aqueous environments, non-negligible node mobility and large network scale, localization for large-scale mobile underwater sensor networks is very challenging. In this paper, by utilizing the predictable mobility patterns of underwater objects, we propose a scheme, called Scalable Localization scheme with mobility prediction (SLMP), for underwater sensor networks. In SLMP, localization is performed in a hierarchical way, and the whole localization process is divided into two parts: anchor node localization and ordinary node localization. During the localization process, every node predicts its future mobility pattern according to its past known location information, and it can estimate its future location based on its predicted mobility pattern. Anchor nodes with known locations in the network will control the whole localization process in order to balance the tradeoff between localization accuracy, localization coverage and communication cost. We conduct extensive simulations, and our results show that SLMP can greatly reduce localization communication cost while maintaining relatively high localization coverage and localization accuracy.},
keywords={distributed sensors;mobility management (mobile radio);underwater acoustic communication;scalable localization;mobility prediction;large-scale mobile underwater sensor networks;localization communication cost;Acoustic sensors;Peer to peer computing;Large-scale systems;Costs;Underwater acoustics;Mobile communication;Communication system control;Communications Society;Computer science;Mobile computing},
doi={10.1109/INFOCOM.2008.287},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509883,
author={J. Wang and Y. Liu and S. K. Das},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Asynchronous Sampling Benefits Wireless Sensor Networks},
year={2008},
volume={},
number={},
pages={2207-2215},
abstract={Intensive research has focused on redundance reduction in wireless sensor networks among sensory data due to the spatial and temporal correlation embedded therein. In this paper, we propose a novel approach termed asynchronous sampling that complements existing study. The key idea of asynchronous sampling is to spread the sampling times of the sensor nodes over the time line instead of performing them in a synchronous manner. Compared with existing strategies, asynchronous sampling introduces another dimension for optimization, without additional computation or communication overhead on sensor nodes. Theoretically, we show that asynchronous sampling benefits sensor networks through increased entropy of the sensory data or reduced reconstruction distortion. Furthermore, we formulate the optimal asynchronous sampling problem for determining the time shifts among the nodes. A heuristic solution, termed O-ASYN, is presented that uses local optimum search to approximate the global optimal solution. Simulation results based on simulated data and real experimental data both demonstrate the entropy increases.},
keywords={distortion;entropy;optimisation;sampling methods;search problems;wireless sensor networks;wireless sensor network;optimal asynchronous sampling problem;sensory data entropy;reduced reconstruction distortion;O-ASYN heuristic solution;local optimum search;global optimal solution;Sampling methods;Wireless sensor networks;Peer to peer computing;Entropy;Capacitive sensors;Costs;Sensor phenomena and characterization;Communications Society;Computer science;Data engineering},
doi={10.1109/INFOCOM.2008.288},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509884,
author={G. Yang and B. Tong and D. Qiao and W. Zhang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Sensor-Aided Overlay Deployment and Relocation for Vast-Scale Sensor Networks},
year={2008},
volume={},
number={},
pages={2216-2224},
abstract={The overlay-based network architecture has been recognized as an effective way to deal with the funneling effect in sensor networks, where sensors closer to the sink are usually responsible for relaying more network traffic. Such funneling effect is particularly harmful when the number of sensors in the network is vast. In an overlay-based sensor network, a special type of resource-rich multi-radio mobile wireless devices (we call them syphons) are deployed along with sensors. Syphons form an overlay network and help nearby sensors relay their data to the sink via the overlay network, thus mitigating the funneling effect. In this paper, we study one of the fundamental challenges in overlay-based sensor networks: syphon deployment problem, i.e., how to deploy a limited number of syphons to cover a vast sensing field while maintaining the connectivity and balanced loads among them. We propose a novel sensor-aided overlay deployment and relocation (SODaR) protocolas a possible solution. The key idea is to take advantage of sensors' assistance and to relocate syphons by circling them around the sink in an orderly manner until all syphons are connected. Simulation results show that, with SODaR, syphons are able to self-form and self-maintain a connected tree structure which provides excellent load balancing among syphons with modest message and movement overhead.},
keywords={mobile radio;protocols;telecommunication traffic;wireless sensor networks;sensor-aided overlay network architecture;vast-scale sensor network relocation;network traffic;funneling effect;resource-rich multiradio mobile wireless device;syphon deployment problem;sensor-aided overlay deployment/relocation protocol;Wireless sensor networks;Telecommunication traffic;Base stations;Relays;Acoustic imaging;Monitoring;Communications Society;Protocols;Tree data structures;Load management},
doi={10.1109/INFOCOM.2008.289},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509885,
author={Y. Yu and L. J. Rittle},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Utility-Driven Spatiotemporal Sampling Using Mobile Sensors},
year={2008},
volume={},
number={},
pages={2225-2233},
abstract={Many real-world applications for sensor networks require event sampling with sufficient resolution over both spatial and temporal dimensions. When the deployed nodes are insufficient to fully cover the sensor field to satisfy this spatiotemporal sampling requirements of all events, nodes with intelligent mobility are crucial to improve the sampling quality. We measure the sampling quality using a utility function that incorporates both the importance and spatiotemporal properties of events. We then describe a Utility Driven Mobility (UDM) scheme, which enables autonomous mobility scheduling of nodes in a distributed fashion. We evaluate the performance of UDM via extensive simulation studies. Our work provides a practical framework for spatiotemporal sampling using mobile sensor networks.},
keywords={mobile radio;sampling methods;spatiotemporal phenomena;wireless sensor networks;utility-driven spatiotemporal sampling;mobile sensors;sensor networks;event sampling;intelligent mobility;utility driven mobility scheme;UDM scheme;Spatiotemporal phenomena;Sampling methods;Peer to peer computing;Intelligent sensors;Phasor measurement units;Event detection;Communications Society;Spatial resolution;Information entropy;Protocols},
doi={10.1109/INFOCOM.2008.290},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509886,
author={A. Mei and J. Stefa},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Routing in Outer Space},
year={2008},
volume={},
number={},
pages={2234-2242},
abstract={In this paper we consider security-related and energy-efficiency issues in multi-hop wireless networks. We start our work from the observation, known in the literature, that shortest path routing creates congested areas in multi- hop wireless networks. These areas are critical-they generate both security and energy efficiency issues. We attack these problems and set out routing in outer space, a new routing mechanism that transforms any shortest path routing protocol (or an approximated version of it) into a new protocol that, in case of uniform traffic, guarantees that every node of the network is responsible for relaying the same number of messages, on expectation. We can show that a network that uses routing in outer space does not have congested areas, does not have the associated security-related issues and does not encourage selfish positioning.},
keywords={radio networks;routing protocols;telecommunication security;multihop wireless network;shortest path routing protocol;associated security-related issue;Spread spectrum communication;Energy efficiency;Wireless sensor networks;Relays;Wireless networks;Routing protocols;Telecommunication traffic;Jamming;Communication system control;Gears},
doi={10.1109/INFOCOM.2008.291},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509887,
author={T. Liu and W. Liao},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Location-Dependent Network Performance and Design Strategies for Wireless Mesh Networks},
year={2008},
volume={},
number={},
pages={2243-2251},
abstract={In this paper, we model the location-dependent throughput and delay in wireless mesh networks. We analyze packet arrival rates and packet departure rates for the forwarding queues at relaying nodes and then derive the throughput and packet delay experienced by nodes at different hop count distances to the gateway. Based on this model, we further analyze how network design strategies affect the throughput and delay of each node. We then conduct simulations to validate our analytical model and evaluate the performance of different network design strategies. This paper not only provides a framework for studying the location-dependent throughput and delay in wireless mesh networks but also gives insights into the network design strategy for wireless mesh networks.},
keywords={queueing theory;radio networks;telecommunication network topology;location-dependent network performance;wireless mesh network design strategy;location-dependent throughput model;location-dependent delay model;packet arrival rate;packet departure rate;queue forwarding;Wireless mesh networks;Throughput;Analytical models;Delay;Relays;Peer to peer computing;Wireless networks;Spread spectrum communication;Ad hoc networks;Telecommunication traffic},
doi={10.1109/INFOCOM.2008.292},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509888,
author={B. Radunovic and C. Gkantsidis and P. Key and P. Rodriguez},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={An Optimization Framework for Opportunistic Multipath Routing in Wireless Mesh Networks},
year={2008},
volume={},
number={},
pages={2252-2260},
abstract={We consider wireless mesh networks, and exploit the inherent broadcast nature of wireless by making use of multipath routing. We present an optimization framework that enables us to derive optimal flow control, routing, scheduling, and rate adaptation schemes, where we use network coding to ease the routing problem. We prove optimality and derive a primal-dual algorithm that lays the basis for a practical protocol. We use simulation to show on realistic topologies that we can achieve 20-200% throughput improvement compared to single path routing, and several times compared to a recent related opportunistic protocol (MORE).},
keywords={network routing;optimisation;wireless channels;optimization framework;multipath routing;wireless mesh networks;network coding;optimal flow control;rate adaptation schemes;Wireless mesh networks;Broadcasting;Network coding;Network topology;Throughput;Communications Society;Optimal control;Routing protocols;Mesh networks;Joining processes},
doi={10.1109/INFOCOM.2008.293},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509889,
author={P. Dutta and S. Jaiswal and D. Panigrahi and R. Rastogi},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A New Channel Assignment Mechanism for Rural Wireless Mesh Networks},
year={2008},
volume={},
number={},
pages={2261-2269},
abstract={In this paper we present a new channel allocation scheme for IEEE 802.11 based mesh networks with point-to- point links, designed for rural areas. Our channel allocation scheme allows continuous full-duplex data transfer on every link in the network. Moreover, we do not require any synchronization across the links as the channel assignment prevents cross link interference. Our approach is simple. We consider any link in the network as made up of two directed edges. To each directed edge at a node, we assign a non-interfering IEEE 802.11 channel so that the set of channels assigned to the outgoing edges is disjoint from channels assigned to the incoming edges. Evaluation of this scheme in a testbed demonstrate throughput gains of between 50 - 100%, and significantly less end-to-end delays, over existing link scheduling/channel allocation protocols (such as 2P [11]) designed for point-to-point mesh networks. Formally speaking, this channel allocation scheme is equivalent to an edge-coloring problem, that we call the directed edge coloring (DEC) problem. We establish a relationship between this coloring problem and the classical vertex coloring problem, and thus, show that this problem is NP-hard. More precisely, we give an algorithm that, given k vertex coloring of a graph can directed edge color it using xi(k) colors, where xi(k) is the smallest integer n such that (lfloorn/2rfloor/n ) ges k.},
keywords={channel allocation;graph colouring;radiofrequency interference;wireless LAN;rural wireless mesh network;channel assignment mechanism;channel allocation scheme;IEEE 802.11 channel;point-to- point link;continuous full-duplex data transfer;directed edge-coloring problem;vertex coloring problem;NP-hard problem;cross link interference;Wireless mesh networks;Interference;Mesh networks;Channel allocation;Peer to peer computing;Throughput;Internet;Directional antennas;Communications Society;Testing},
doi={10.1109/INFOCOM.2008.294},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509890,
author={M. Sartipi and B. N. Vellambi and N. Rahnavard and F. Fekri},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={DSCM: An Energy-Efficient and Rate-Optimal Multicast Protocol for Multihop Wireless Networks Using Distributed Source Coding},
year={2008},
volume={},
number={},
pages={2270-2278},
abstract={In this paper, we propose a new multicast subgraph construction algorithm and a distributed source coding-based multicast (DSCM) protocol for multihop wireless networks. The DSCM emphasize reliability, rate optimality, and energy efficiency. Both algorithms are based on local knowledge of the network. DSCM uses rateless error correcting codes to provide reliability and rate optimality, and distributed source coding to ensure the energy efficiency. We compared our scheme to energy-efficient methods such as network coding (NC) and multicast incremental power (MIP). Simulation results show DSCM performs close to these algorithms. However unlike the proposed algorithm, NC and MIP assume full knowledge of the network topology and have much higher decoding complexity than DSCM.},
keywords={computational complexity;decoding;error correction codes;graph theory;multicast protocols;radio networks;source coding;telecommunication network topology;multicast protocol;multihop wireless networks;distributed source coding;multicast subgraph construction algorithm;rate optimality;energy efficiency;rateless error correcting codes;network coding;multicast incremental power;network topology;decoding complexity;Energy efficiency;Multicast protocols;Spread spectrum communication;Wireless networks;Source coding;Multicast algorithms;Wireless application protocol;Error correction codes;Network coding;Network topology},
doi={10.1109/INFOCOM.2008.295},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509891,
author={A. Hess and S. Sengupta and V. P. Kumar},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Joint Traffic Routing and Distribution of Security Services in High Speed Networks},
year={2008},
volume={},
number={},
pages={2279-2287},
abstract={The continued explosion of new virus/worm and other security attacks in the Internet and the tremendous propagation speed of self-propagating attacks has led to network security being considered as a design criterion rather than an afterthought. Attack prevention, detection, and mitigation mechanisms can be broadly classified as network based or host based. Network based security mechanisms have been shown to be much more effective than host based mechanisms, primarily because of the former's ability in identifying attack traffic that is further upstream from the victim and closer to the attack source. In the context of network based mechanisms, we consider a flexible overlay network of security systems running on top of programmable (active) routers. In such an architecture, security services can be dynamically distributed across the network, which provides flexibility for load-balancing of services across nodes and addition of new services over time. Such network based mechanisms inevitably decrease network performance as all packets are analyzed for malicious content before being forwarded. In this paper, we consider traffic routing, placement of active router nodes, and distribution of security services across such nodes so as to optimize certain objectives, including (i) minimize the total number of active router deployed nodes, and (ii) minimize the maximum utilization of any router node in the network. Based on an emulation in the Deter testbed we show the benefit of the presented approach.},
keywords={Internet;resource allocation;telecommunication network routing;telecommunication security;telecommunication traffic;traffic routing;security services;high speed networks;security attacks;Internet;self-propagating attacks;load-balancing;Telecommunication traffic;Routing;High-speed networks;Peer to peer computing;USA Councils;Web and internet services;Protection;Traffic control;Communications Society;Explosions},
doi={10.1109/INFOCOM.2008.296},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509892,
author={Y. -. Choi and M. -. Jung and S. -. Seo},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={L+1-MWM: A Fast Pattern Matching Algorithm for High-Speed Packet Filtering},
year={2008},
volume={},
number={},
pages={2288-2296},
abstract={A signature-based network intrusion detection system (NIDS) identifies intrusions by comparing the data traffic with known signature patterns. In this process, matching of packet strings against signature patterns dominates the overall system performance. The MWM algorithm has been known as the fastest pattern matching algorithm when the patterns in a rule set rarely appear in packets. However, the matching time does not decrease if the length of the shortest pattern in a signature group is too short. In this paper, by extending the length of the shortest pattern, we minimize the pattern matching time of the algorithm which uses multi-byte unit. For example, when the length of the shortest pattern is less than 5, the proposed algorithm shows 38.87% enhancement in average.},
keywords={computer networks;digital signatures;security of data;string matching;telecommunication security;MWM algorithm;pattern string matching algorithm;high-speed packet filtering;signature-based NIDS;network intrusion detection system;SHIFT table;Pattern matching;Filtering algorithms;Matched filters;Intrusion detection;Computer networks;Telecommunication traffic;Communications Society;System performance;Communication networks;Computer security},
doi={10.1109/INFOCOM.2008.297},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509893,
author={U. Ben-Porat and A. Bremler-Barr and H. Levy},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Evaluating the Vulnerability of Network Mechanisms to Sophisticated DDoS Attacks},
year={2008},
volume={},
number={},
pages={2297-2305},
abstract={The design of computer and communication systems has been based, for decades, on the fundamental assumption that the objective of all users is to improve their own performance. In recent years we have experienced a wave of DDoS attacks threatening the welfare of the Internet. These are launched by malicious users whose pure incentive is to degrade the performance of other, innocent, users. The traditional systems turn out to be quite vulnerable to these attacks. The objective of this work is to take a first step to close this fundamental gap, aiming at laying a foundation that can be used in future computer/network designs taking into account the malicious users. Our approach is based on proposing a metric that evaluates the vulnerability of a system. We then evaluate the commonly used data structure in network mechanisms, the hash data structure, using our vulnerability metric. We show that a Closed Hash is much more vulnerable than an Open Hash to DDoS attacks, even though the two systems are considered to be equivalent via traditional performance evaluation. We also apply the metric to queueing mechanisms common to computer and communications systems. Lastly we apply it to the practical case of a hash table whose requests are controlled by a queue, showing that even after the attack has ended, the regular users still suffer from performance degradation or even a total denial of service.},
keywords={data structures;Internet;queueing theory;telecommunication security;DDoS attack;Internet;hash data structure;queueing mechanism;distributed denial of service;Computer crime;Degradation;Computer networks;Data structures;Web server;Traffic control;Computer science;Network servers;Telecommunication traffic;Communications Society},
doi={10.1109/INFOCOM.2008.298},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509894,
author={Z. Chen and C. Ji and P. Barford},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Spatial-Temporal Characteristics of Internet Malicious Sources},
year={2008},
volume={},
number={},
pages={2306-2314},
abstract={This paper presents a large scale longitudinal study of the spatial and temporal features of malicious source addresses. The basis of our study is a 402-day trace of over 7 billion Internet intrusion attempts provided by DShield.org, which includes 160 million unique source addresses. Specifically, we focus on spatial distributions and temporal characteristics of malicious sources. First, we find that one out of 27 hosts is potentially a scanning source among 2<sup>32</sup> IPv4 addresses. We then show that malicious sources have a persistent, non-uniform spatial distribution. That is, more than 80% of the sources send packets from the same 20% of the IPv4 address space over time. We also find that 7.3% of malicious source addresses are unroutable, and that some source addresses are correlated. Next, we show that most sources have a short lifetime. 57.9 % of the source addresses appear only once in the trace, and 90% of source addresses appear less than 5 times. These results have implications for both attacks and defenses.},
keywords={Internet;IP networks;security of data;telecommunication security;Internet malicious source;spatial-temporal characteristics;IPV4 address;Internet;Telecommunication traffic;Intrusion detection;Communications Society;Paper technology;Large-scale systems;Information filtering;Information filters;Gain measurement;Network address translation},
doi={10.1109/INFOCOM.2008.299},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509895,
author={P. K. Manna and S. Ranka and S. Chen},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={DAWN: A Novel Strategy for Detecting ASCII Worms in Networks},
year={2008},
volume={},
number={},
pages={2315-2323},
abstract={While a considerable amount of research has been done for detecting the binary worms exploiting the vulnerability of buffer overflow, very little effort has been spent in detecting worms that consist of only text, Le., printable ASCII characters. We show that the existing worm detectors often either do not examine the ASCII stream or are not well suited to efficiently detect worms in the ASCII stream due to the structural properties of the ASCII payload. In this paper, we analyze the potentials and constraints of the ASCII worms vis-a-vis their binary counterpart, and devise a detection technique that would exploit those limitations. We introduce DAWN, a novel ASCII worm detection strategy that is fast, easily deployable, and has very little overhead. Unlike many signature-based detection methods, DAWN is completely signature-free and therefore capable of detecting zero-day outbreak of ASCII worms.},
keywords={computer networks;digital signatures;security of data;telecommunication security;ASCII worm detection;binary worm;DAWN;signature-based detection method;zero-day outbreak detection;computer networks;Computer worms;Detectors;Payloads;Web server;Frequency;Communications Society;Computer networks;Information science;Buffer overflow;Computer security},
doi={10.1109/INFOCOM.2008.300},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509896,
author={R. Cohen and G. Nakibly},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Maximizing Restorable Throughput in MPLS Networks},
year={2008},
volume={},
number={},
pages={2324-2332},
abstract={MPLS recovery mechanisms are increasing in popularity because they can guarantee fast restoration and high QoS assurance. Their main advantage is that their backup paths are established in advance, before a failure event takes place. Most research on the establishment of primary and backup paths has focused on minimizing the added capacity required by the backup paths in the network. However, this so-called spare capacity allocation (SCA) metric is less practical for network operators who have a fixed capacitated network and want to maximize their revenues. In this paper we present a comprehensive study on restorable throughput maximization in MPLS networks. We present the first polynomial-time algorithms for the splittable version of the problem. For the unsplittable version, we provide a lower bound for the approximation ratio. We present efficient heuristics which are shown to have excellent performance. One of our most important conclusions is that when one seeks to maximize revenue, local recovery should be the recovery scheme of choice.},
keywords={communication complexity;multiprotocol label switching;optimisation;quality of service;telecommunication network reliability;telecommunication security;restorable throughput maximization;MPLS network recovery mechanism;QoS assurance;failure event;backup path;spare capacity allocation metric;polynomial-time algorithm;MPLS-based protection mechanism;Throughput;Multiprotocol label switching;Protection;Bandwidth;Communications Society;Computer science;Polynomials;IP networks;Availability;Telecommunication network reliability},
doi={10.1109/INFOCOM.2008.301},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509897,
author={F. Wang and L. Gao},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A backup route aware routing protocol - fast recovery from transient routing failures},
year={2008},
volume={},
number={},
pages={2333-2341},
abstract={As the Internet becomes the critical information infrastructure for both personal and business applications, survivable routing protocols need to be designed that maintain the performance of those services in the presence of failures. This paper examines the survivability of interdoamin routing protocols in the presence of routing failure events, and provides a backup route aware routing protocol that performs non-stop routing in the presence of failures. We demonstrate through simulation its effectiveness in preventing packet losses during transient routing failures.},
keywords={Internet;routing protocols;backup route aware routing protocol;transient routing failure;Internet;business application;personal application;Routing protocols;Web and internet services;Protection;Communications Society;Maintenance engineering;Design engineering;Business;Computational modeling;Traffic control;Terminology},
doi={10.1109/INFOCOM.2008.302},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509898,
author={Y. He and M. Faloutsos and S. V. Krishnamurthy and M. Chrobak},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Policy-Aware Topologies for Efficient Inter-Domain Routing Evaluations},
year={2008},
volume={},
number={},
pages={2342-2350},
abstract={The Internet community has not reached a consensus on an appropriate topological model for evaluating the performance of inter-domain routing protocols. Using the current Internet topology is not realistic, since its size is prohibitively large for, say, a packet-level BGP simulation. Furthermore, routing policies, which play a critical role in inter-domain routing, are often ignored in many simulation studies. In this paper, we address this issue by designing an algorithm to generate small-scale, realistic, and policy-aware topologies. We propose HBR, a network sampling method, which produces topologies that preserve the fundamental properties of the Internet graph, including, in particular, its hierarchical structure. Our approach provides a long-term solution to the difficult problem of AS-level routing evaluations: it can be used to generate small realistic topologies in the future, starting from any newer or more complete Internet instance.},
keywords={graph theory;Internet;routing protocols;sampling methods;telecommunication network topology;Internet;inter-domain routing protocols;BGP;policy-aware network topology;network sampling method;graph theory;Internet;Network topology;Routing protocols;Sampling methods;IP networks;Testing;Communications Society;Helium;Algorithm design and analysis;Analytical models},
doi={10.1109/INFOCOM.2008.303},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509899,
author={R. Khalili and J. Kurose},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Distributed Minimum-Distortion Routing Algorithm with In-Network Data Processing},
year={2008},
volume={},
number={},
pages={2351-2359},
abstract={In many wired and wireless networks, nodes process input traffic to satisfy a network constraint (e.g., a capacity constraint) and to increase the utility of data in the output flows given these constraints. In this paper we focus on the special case in which data processing is applied to satisfy capacity constraints. This occurs when the sum of the rate of the input traffic at a node exceeds the sum of the capacity of its output links, or in a more general case, when the sum of the input rates is larger than any cut capacity in the network. In this case, nodes process data to decrease the output flow rate. This decrease from input rate to output rate distorts the transmitted data, which we characterize by a distortion metric. We show that the distortion cost of distributively processing input traffic in a network can be written as the sum of the distortion at individual nodes.. We present a distributed algorithm for a data-gathering network with many sources and a data sink that routes traffic and performs in-network data processing to minimize the distortion cost. In this algorithm, each node determines its routing table based on gradient information from neighboring nodes.},
keywords={distributed algorithms;telecommunication network routing;telecommunication traffic;distributed minimum-distortion routing algorithm;in-network data processing;network constraint;distortion cost;gradient information;Routing;Data processing;Telecommunication traffic;Peer to peer computing;Costs;Monitoring;Atmospheric measurements;Temperature sensors;Computer science;Rate distortion theory},
doi={10.1109/INFOCOM.2008.304},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509900,
author={R. Zhang-Shen and N. McKeown},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Designing a Fault-Tolerant Network Using Valiant Load-Balancing},
year={2008},
volume={},
number={},
pages={2360-2368},
abstract={Commercial backbone networks must continue to operate even when links and routers fail. Routing schemes such as OSPF, IS-IS, and MPLS reroute traffic, but they cannot guarantee that the resulting network will be congestion-free. As a result, backbone networks are grossly over-provisioned - sometimes running at a utilization below 10% so they can remain uncongested under failure. Yet even with such large over-provisioning, they still cannot guarantee to be uncongested, sometimes even with just a single failure. With our proposed approach, a network can be designed to tolerate an almost arbitrary number of failures, and guarantee no congestion, usually with an extremely small amount of over- provisioning. In a typical case, a 50 node network can continue to run congestion-free when any 5 links or routers fail, with only 10% over-provisioning. The key to the approach is Valiant Load-Balancing (VLB). VLB's path diversity allows it to tolerate k arbitrary failures in an N node network, with over-provisioning ratio of approximately k/N.},
keywords={computer network reliability;fault tolerant computing;resource allocation;telecommunication network routing;fault-tolerant network;valiant load-balancing;backbone networks;routing schemes;Fault tolerance;Telecommunication traffic;Spine;Protection;Routing;Multiprotocol label switching;Peer to peer computing;Communications Society;Fault tolerant systems;Computer networks},
doi={10.1109/INFOCOM.2008.305},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509901,
author={J. Zhang and Q. Zhang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Cooperative Routing in Multi-Source Multi-Destination Multi-Hop Wireless Networks},
year={2008},
volume={},
number={},
pages={2369-2377},
abstract={In a network supporting cooperative communication, the sender of a transmission is no longer a single node, which causes the concept of a traditional link to be reinvestigated. Thus, the routing scheme basing on the link concept should also be reconsidered to ";truly"; exploit the potential performance gain introduced by cooperative communication. In this paper, we investigate the joint problem of routing selection in network layer and contention avoidance among multiple links in MAC layer for multi-hop wireless networks in a cooperative communication aware network. To the best of our knowledge, it is the first work to investigate the problem of cooperative communication aware routing in multi-source multi-destination multi-hop wireless networks. Several important concepts, including virtual node, virtual link and virtual link based contention graph are introduced. Basing on those concepts, an optimal cooperative routing is achieved and a distributed routing scheme is proposed after some practical approximations. The simulation results show that our scheme reduces the total transmission power comparing with non-cooperative routing and greatly increases the network throughput comparing with single flow cooperative routings.},
keywords={radio networks;telecommunication congestion control;telecommunication network routing;multi source multi destination multihop wireless network;cooperative network routing selection;network contention avoidance;MAC layer;cooperative communication aware network;distributed routing scheme;optimal cooperative routing scheme;Routing;Spread spectrum communication;Wireless networks;Physical layer;Broadcasting;Peer to peer computing;Performance gain;Throughput;Communications Society;Computer science},
doi={10.1109/INFOCOM.2008.306},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509902,
author={N. Singh and R. S. Sreenivas},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={On a Non-Linear Optimization Approach for Proportional Fairness in Ad-Hoc Wireless Networks},
year={2008},
volume={},
number={},
pages={2378-2386},
abstract={In this paper we present a partially asynchronous, fully distributed flow-based access scheme for slotted-time protocols, that guarantees proportional-fairness in ad hoc wireless networks. This problem of providing fairness in wireless networks is considered in the framework of non-linear optimization. We say a medium access control algorithm is proportionally fair with respect to individual end-to-end flows in a network, if the product of the end-to-end flow-success probabilities is maximized.},
keywords={access protocols;ad hoc networks;nonlinear programming;probability;radio networks;telecommunication traffic;nonlinear optimization;ad-hoc wireless networks;slotted-time protocol;medium access control algorithm;probability;Wireless networks;Access protocols;Wireless application protocol;Media Access Protocol;Costs;Delay estimation;Iterative algorithms;Communications Society;Computer industry;Systems engineering and theory},
doi={10.1109/INFOCOM.2008.307},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509903,
author={S. Luo and J. H. Li and K. Park and R. Levy},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Exploiting Heavy-Tailed Statistics for Predictable QoS Routing in Ad Hoc Wireless Networks},
year={2008},
volume={},
number={},
pages={2387-2395},
abstract={Extensive measurement and analysis have shown that traffic in data networks is better described by heavy-tailed distributions. In this paper, we present our current work on ad hoc routing by exploiting the heavy-tailed nature of the flow lifetime. In particular, the long- and short-lived flows are differentiated based on the unique predictability nascent in heavy- tailed distributions, with the long- and short-lived flows being handled separately. A modified AODV has been implemented to enable heavy-tailedness awareness. Simulation results reveal that the new protocol, AODV-HT, significantly outperforms the state-of-the-art under heavy-tailed workload.},
keywords={ad hoc networks;quality of service;routing protocols;statistical distributions;ad hoc wireless network routing;QoS;heavy-tailed statistical distribution;Statistics;Routing;Wireless networks;Telecommunication traffic;Traffic control;Delay;Quality of service;Probability distribution;Intelligent networks;Wireless LAN},
doi={10.1109/INFOCOM.2008.308},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509904,
author={S. Singh and U. Madhow and E. M. Belding},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Beyond Proportional Fairness: A Resource Biasing Framework for Shaping Throughput Profiles in Multihop Wireless Networks},
year={2008},
volume={},
number={},
pages={2396-2404},
abstract={Throughput performance of multihop wireless networks is governed by how the network's transport capacity (in bit-meters per second) is partitioned among different network flows. Max-min fair allocation leads to poor throughput performance for all flows because connections traversing a large number of hops consume a disproportionate share of resources. While proportional fair allocation provides a significant improvement, we point out here that there is a much richer space of resource allocation strategies for introducing a controlled bias against resource-intensive long connections in order to significantly improve the performance of shorter connections. We present an analytical model that gives insight into the impact of a particular resource allocation strategy on network performance, in a manner that captures the effect of finite network size and spatial traffic patterns. Our simulation results demonstrate that it is possible to provide significantly better performance to shorter connections than max-min fair or proportional fair resource allocations, with minimal impact on the performance of long connections, using mixed bias strategies blending "fair" allocations with a strong bias against long connections.},
keywords={radio networks;resource allocation;telecommunication traffic;resource biasing framework;throughput profile shaping;multihop wireless networks;network transport capacity;proportional fair allocation;resource allocation strategies;network performance;traffic patterns;Throughput;Spread spectrum communication;Wireless networks;Resource management;Telecommunication traffic;Analytical models;Traffic control;Degradation;Communications Society;Proportional control},
doi={10.1109/INFOCOM.2008.309},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509905,
author={Q. Du and X. Zhang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Resource Allocation for Downlink Statistical Multiuser QoS Provisionings in Cellular Wireless Networks},
year={2008},
volume={},
number={},
pages={2405-2413},
abstract={We propose the adaptive resource allocation schemes for multiuser downlink quality-of-service (QoS) provisionings over broadcast fading channels in time-division (TD) based cellular wireless networks. Specifically, we apply the effective capacity theory to control the service rates with the diverse delay QoS requirements for different mobile users. Subject to the proportional- effective-capacity constraint and the diverse statistical delay-QoS requirements over different downlink users, we formulate the sum effective capacity maximization problem via channel-aware power and time-slot allocation. We decompose the above optimization problem into two sub-problems and then derive the optimal power and time-slot adaptation policy. We also develop a suboptimal scheme called the equal-length TD policy. Simulation results are presented to show the impact of QoS provisionings on the resource allocation across different users and on the network performance.},
keywords={broadcast channels;cellular radio;fading channels;multiuser channels;optimisation;quality of service;radio links;resource allocation;statistical analysis;cellular wireless network;quality of service;adaptive resource allocation;downlink statistical multiuser QoS provisioning;broadcast fading channel;mobile user;capacity maximization problem;optimization problem;Resource management;Downlink;Cellular networks;Wireless networks;Delay effects;Broadcasting;Fading;Information theory;Quality of service;Land mobile radio cellular systems},
doi={10.1109/INFOCOM.2008.310},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509906,
author={J. -. Tsai},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={State-Dependent Proportional Fair Scheduling Algorithms for Wireless Forward Link Data Services},
year={2008},
volume={},
number={},
pages={2414-2422},
abstract={For wireless forward link data services, the proportional fair algorithm (PFA) incorporating opportunistic scheduling was originally designed to maintain service fairness among users as well as to exploit channel fluctuations in fading environments. The operation of PFA necessitates throughput monitoring for each user, in general through exponentially smoothing received services. In the paper, we adapt the PFA for use in more generic traffic conditions where the number of active users may change with time and data for a user may arrive intermittently. Our approach is not only to simply stop throughput monitoring for idle users but also to further accommodate in throughput monitoring changes in the number of active users. Moreover, we use in throughput monitoring different smoothing factors depending on the state of backlogged data loads, in order to exploit the variation of multiuser diversity gain. Consequently, our PFA, taking into account both channel state and system queue fluctuations, presents several favorable performance features.},
keywords={fading;radio links;scheduling;telecommunication links;telecommunication traffic;state-dependent proportional fair scheduling algorithms;wireless forward link data services;service fairness;fading environments;traffic conditions;multiuser diversity gain;Scheduling algorithm;Throughput;Monitoring;Diversity methods;Smoothing methods;Fluctuations;Delay;Niobium;Fading;Traffic control},
doi={10.1109/INFOCOM.2008.311},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509907,
author={B. Ishibashi and N. Bouabdallah and R. Boutaba},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={QoS Performance Analysis of Cognitive Radio-Based Virtual Wireless Networks},
year={2008},
volume={},
number={},
pages={2423-2431},
abstract={Cognitive radio presents a new approach to wireless spectrum utilization and management. In this work, the potential performance improvement gained by applying cognitive radio to multiple-provider wireless systems is investigated. It is shown that virtual wireless networks can be created, utilizing only the residual wasted bandwidth of the primary service providers. These virtual networks are able to support large volumes of users, while still ensuring that QoS reliability requirements, such as blocking and dropping guarantees, are achieved. A Markov chain-based analysis of classic and cognitive systems is complemented by simulations in order to present a quantified perspective of the potential benefits of cognitive radio techniques.},
keywords={cognitive radio;Markov processes;quality of service;radio networks;telecommunication network management;telecommunication network reliability;telecommunication traffic;virtual private networks;cognitive radio;virtual wireless networks;wireless spectrum utilization;wireless spectrum management;multiple-provider wireless systems;QoS reliability requirements;Markov chain-based analysis;Performance analysis;Wireless networks;Cognitive radio;Chromium;Radio spectrum management;Bandwidth;Analytical models;Frequency;Resource management;Computer science},
doi={10.1109/INFOCOM.2008.312},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509908,
author={D. Giustiniano and G. Bianchi and L. Scalia and I. Tinnirello},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={An Explanation for Unexpected 802.11 Outdoor Link-level Measurement Results},
year={2008},
volume={},
number={},
pages={2432-2440},
abstract={This paper provides experimental evidence that "weird'/poor outdoor link-level performance measurements may be caused by driver/card-specific antenna diversity algorithms unexpectedly supported/activated at the WLAN transmitter side. We focus our analysis on the Atheros/MADWiFi card/driver case, and we observe that the transmit antenna diversity mechanisms remain by default enabled when the available antennas are not homogeneous in terms of gain or, even worse, when only a single antenna is connected. This may cause considerable performance impairments (large frame loss ratio), in conditions frequently encountered in outdoor link deployments. The negative impact of transmit antenna diversity is not limited to the transmission of broadcast frames (where a cyclic shift between the "two" assumed antennas is performed), but under certain circumstances it can severely affect the delivery of unicast frames as well, and despite the fact that in this case the ACK receptions may provide a feedback about the best receiving antenna. While, as obvious, driver developers are expectedly fully aware of the existence of such mechanisms, we believe that the scientific research community has very limited awareness of the implications these mechanisms have on the measured link-level performance. Indeed, to the best of our knowledge, ours is the first research paper which explicitly raises this issue.},
keywords={diversity reception;receiving antennas;transmitting antennas;wireless LAN;unexpected 802.11 outdoor link-level measurement result;Atheros/MADWiFi driver/card;WLAN transmitter;transmit antenna diversity mechanism;receiving antenna;Transmitting antennas;Receiving antennas;Antenna feeds;Antenna measurements;Wireless LAN;Transmitters;Performance loss;Broadcasting;Unicast;Negative feedback},
doi={10.1109/INFOCOM.2008.313},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509909,
author={R. Krishnan and A. Raniwala and T. -. Chiueh},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Design of a Channel Characteristics-Aware Routing Protocol},
year={2008},
volume={},
number={},
pages={2441-2449},
abstract={Radio channel quality of real-world wireless networks tends to exhibit both short-term and long-term temporal variations that are in general difficult to model. To maximize the utilization efficiency of radio resources, it is critical that these temporal fluctuations in radio signal quality be incorporated into wireless routing decisions. In this paper, we explore the design considerations in leveraging accurate real-time radio channel quality information when making routing decisions. Specifically, we propose a channel characteristics-aware routing protocol (CARP) that (1) uses per-packet transmission time to estimate the effective residual capacity of a wireless link, (2) employs a bandwidth probability distribution model to better approximate a wireless path's capacity profile, and (3) applies multi-path routing to exploit diversity among alternative paths and deliver more robust throughputs despite temporal fluctuations in wireless link quality. We evaluated the performance gains of incorporating each of these mechanisms on a miniaturized multi-hop wireless network testbed- MiNT-m.},
keywords={approximation theory;decision making;diversity reception;multipath channels;probability;radio links;resource allocation;routing protocols;wireless channels;multipath routing protocol;radio channel quality;real-world wireless network;long-term temporal variation;short-term temporal variation;radio resource utilization;decision making;wireless link;bandwidth probability distribution model;approximation theory;Routing protocols;Wireless networks;Fluctuations;Capacity planning;Bandwidth;Probability distribution;Robustness;Throughput;Performance gain;Spread spectrum communication},
doi={10.1109/INFOCOM.2008.314},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509910,
author={D. Guo and Y. Liu and X. Li},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={BAKE: A Balanced Kautz Tree Structure for Peer-to-Peer Networks},
year={2008},
volume={},
number={},
pages={2450-2457},
abstract={In order to improve scalability and reduce maintenance overhead for structured peer-to-peer systems, researchers design optimal architectures with constant degree and logarithmical diameter. The expected topologies, however, require the number of peers to be some given values determined by the average degree and the diameter. Hence, existing designs fail to address the issue due to the fact that (1) we cannot guarantee how many peers to join a P2P system at a given time, and (2) a P2P system is typically dynamic with peers frequently coming and leaving. In this work, we propose BAKE scheme based on balanced Kautz tree structure with <i>logdn</i> in diameter and constant degree even the number of peers is an arbitrary value. Resources that are similar in single or multi-dimensional attributes space are stored on a same peer or neighboring peers. Through formal analysis and comprehensive simulations, we show that BAKE achieves optimal diameter and good connectivity as the Kautz digraph does. Indeed, the concepts of balanced Kautz tree introduced in this work can also be extended and applied to other interconnection networks after minimal modifications, for example, de Bruijn digraph.},
keywords={computer network management;peer-to-peer computing;telecommunication network routing;telecommunication network topology;trees (mathematics);peer-to-peer networks;balanced Kautz tree structure;network topology management;formal analysis;interconnection network;network routing;Tree data structures;Peer to peer computing;Topology;Robustness;Routing;Algorithm design and analysis;Computer science;Scalability;Analytical models;Multiprocessor interconnection networks},
doi={10.1109/INFOCOM.2008.315},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509911,
author={B. Chang and Y. Cui and Y. Xue},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Maximizing Resilient Throughput in Peer-to-Peer Network: A Generalized Flow Approach},
year={2008},
volume={},
number={},
pages={2458-2466},
abstract={A unique challenge in P2P network is that the peer dynamics (departure or failure) cause unavoidable disruption to the downstream peers. While many works have been dedicated to consider fault resilience in peer selection, little understanding is achieved regarding the solvability and solution complexity of this problem from the optimization perspective. To this end, we propose an optimization framework based on the generalized flow theory. Key concepts introduced by this framework include resilience factor, resilience index, and generalized throughput, which collectively model the peer resilience in a probabilistic measure. Under this framework, we divide the domain of optimal peer selection along several dimensions including network topology, overlay organization, and the definition of resilience factor and generalized flow. Within each subproblem, we focus on studying the problem complexity and finding optimal solutions. Simulation study is also performed to evaluate the effectiveness of our model and performance of the proposed algorithms.},
keywords={optimisation;peer-to-peer computing;probability;telecommunication network reliability;telecommunication network topology;peer-to-peer network;resilient network throughput maximizing;network fault resilience;optimization framework;generalized flow theory;resilience index;probabilistic measure;network topology;overlay organization;Throughput;Peer to peer computing;Resilience;Measurement;Network servers;Performance analysis;Communications Society;Constraint optimization;Operations research;Economic indicators},
doi={10.1109/INFOCOM.2008.316},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509912,
author={W. Acosta and S. Chandra},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Exploiting the Properties of Query Workload and File Name Distributions to Improve P2P Synopsis-Based Searches},
year={2008},
volume={},
number={},
pages={2467-2475},
abstract={Modern P2P systems use hybrid searches to improve search efficiency. They use a synopsis of neighborhood content to determine whether to use a structured or unstructured overlay to satisfy a particular query. Because of their size restrictions, a synopsis cannot hold all the terms from every file in the neighborhood. The challenge is to choose the terms that should be represented in the synopsis. In this work, we investigated the distribution of query terms and file terms in Gnutella networks. We observed that there was a mismatch between terms that were popular among file names and the terms that were popular among the queries generated by the user. Because the query behavior changed with time, a synopsis based on only static set of popular file terms was ill-suited to support efficient searches. We used these observations to design a synopsis creation algorithm that dynamically adapted to the query workload and selected terms for the synopsis to reflect popular terms in both the query workload and file distribution. Our preliminary experimental analysis showed that our Query-Adaptive synopsis improved the search performance over the traditional file-based synopsis model.},
keywords={peer-to-peer computing;query processing;query workload;file name distribution;P2P synopsis-based search;Gnutella network;synopsis creation algorithm;query-adaptive synopsis;file-based synopsis model;Algorithm design and analysis;Performance analysis;Monitoring;Routing;Communications Society;Computer science;Heuristic algorithms;Filters;Floods;Indexing},
doi={10.1109/INFOCOM.2008.317},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509913,
author={Y. Tang and E. Al-Shaer},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={Towards Collaborative User-Level Overlay Fault Diagnosis},
year={2008},
volume={},
number={},
pages={2476-2484},
abstract={Overlay networks have emerged as a powerful and flexible platform for developing new disruptive network applications. The attractive characteristics of overlay networks such as planetary-scale distributions, user-level flexibility (e.g. overlay routing) and manageability bring to overlay fault diagnosis new challenges, which include inaccessible underlying network information, incomplete and inaccurate network status observations; dynamic symptom-fault causality relationships, and multi-layer complexity. To address these challenges, we propose a collaborative overlay <i>User</i> <i>Observation</i> based fault diagnosis technique called OUD. OUD can passively use observed overlay symptoms as reported by overlay monitoring agents to correlate multiple users' observations to diagnose faults. OUD can diagnose faults without relying on underlying network fault probabilistic quantifications (e.g. prior fault probability). Simulations and experimental studies show that OUD can efficiently (e.g. low latency) and accurately localize root causes of overlay faults/problems, even when the observed symptoms are incomplete.},
keywords={computer network management;computer network reliability;fault diagnosis;collaborative user-level overlay fault diagnosis;overlay networks;disruptive network applications;planetary-scale distributions;user-level flexibility;network management;network information;dynamic symptom-fault causality relationships;network status observations;multilayer complexity;OUD technique;Collaboration;Fault diagnosis;Monitoring;USA Councils;Delay;Fault detection;Communications Society;Information technology;Computer science;Management information systems},
doi={10.1109/INFOCOM.2008.318},
ISSN={0743-166X},
month={April},}
@INPROCEEDINGS{4509914,
author={J. Bi and J. Wu and W. Zhang},
booktitle={IEEE INFOCOM 2008 - The 27th Conference on Computer Communications},
title={A Trust and Reputation based Anti-SPIM Method},
year={2008},
volume={},
number={},
pages={2485-2493},
abstract={Instant Messaging (IM) service is a killer application in the Internet. Due to the problem of IM spam (SPIM), building an effective anti-spim method is an important research topic. At present, most of anti-spim solutions are based on email-spam prevention techniques, which are not directly applicable to anti-spim. In this paper, we present a new anti-spim method SpimRank, which integrates trust and reputation mechanisms with black-list technique. SpimRank also tracks user's historical action to deal with spim attacks in nearly real time, which is applicable to IM environment.},
keywords={authorisation;information retrieval;Internet;unsolicited e-mail;instant messaging service;Internet;IM spam;reputation based anti-SPIM method;email-spam prevention techniques;SpimRank;trust mechanisms;black-list technique;Web and internet services;Filters;Communications Society;Bismuth;IP networks;Privacy;Web pages;Damping;Convergence;Algorithm design and analysis},
doi={10.1109/INFOCOM.2008.319},
ISSN={0743-166X},
month={April},}

